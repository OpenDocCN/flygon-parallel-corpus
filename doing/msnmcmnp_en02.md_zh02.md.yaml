- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二、NumPy 线性代数
- en: Linear Algebra with NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ''
- en: One of the major divisions of mathematics is **algebra**, and linear algebra
    in particular, focuses on linear equations and mapping linear spaces, namely vector
    spaces. When we create a linear map between vector spaces, we are actually creating
    a data structure called a matrix. The main usage of linear algebra is to solve
    simultaneous linear equations, but it can also be used for approximations for
    non-linear systems. Imagine a complex model or system that you are trying to understand,
    think of it as a non-linear model. In such cases, you can reduce the complex,
    non-linear characteristics of the problem into simultaneous linear equations,
    and you can solve them with the help of linear algebra.
  prefs: []
  type: TYPE_NORMAL
  zh: 数学的主要分类之一是**代数**，尤其是线性代数专注于线性方程和映射线性空间（即向量空间）。 当我们在向量空间之间创建线性映射时，实际上是在创建称为矩阵的数据结构。
    线性代数的主要用途是求解联立线性方程，但也可用于非线性系统的近似。 想象一下您要理解的复杂模型或系统，将其视为非线性模型。 在这种情况下，您可以将问题的复杂的非线性特征简化为联立的线性方程，并且可以在线性代数的帮助下求解它们。
- en: In computer science, linear algebra is heavily used in **machine learning**
    (**ML**) applications. In ML applications, you deal with high-dimensional arrays,
    which can easily be turned into linear equations where you can analyze the interaction
    of features in a given space. Imagine a case where you are working on an image
    recognition project and your task is to detect a tumor in the brain from MRI images. Technically, your
    algorithm should act like a doctor, where it scans the given input and detects
    the tumor in the brain. A doctor has the advantage of being able to spot anomalies;
    the human brain has been evolving through thousands of years to interpret visual
    input. Without much effort, a human can capture anomalies intuitively. However,
    for an algorithm to perform a similar task, you should think about this process
    in as much detail as possible to understand how you can formally express it so
    that the machines can understand.
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中， 线性代数在**机器学习**（**ML**）应用中大量使用。 在 ML 应用中，您处理的是高维数组，可以轻松地将其转换为线性方程式，您可以在其中分析给定空间中要素的相互作用。
    想像一下您正在从事图像识别项目并且您的任务是从 MRI 图像中检测出大脑中的肿瘤的情况。 从技术上讲，您的算法应该像医生一样工作，在其中扫描给定的输入并检测大脑中的肿瘤。
    医生的优势在于能够发现异常情况。 人类的大脑已经进化了数千年，以解释视觉输入。 无需付出太多努力，人类就可以直观地捕获异常。 但是，对于执行相似任务的算法，您应该尽可能详细地考虑此过程，以了解如何正式表达它，以便机器可以理解。
- en: 'First, you should think about how MRI data is stored in a computer, which processes
    only 0s and 1s. The computer actually stores pixel intensities in structures,
    called matrices. In other words, you will convert an MRI as a vector of dimensions, **N2**,
    where each element consists of pixel values. If this MRI has a 512 x 512 dimension,
    each pixel will be one point in 262,144 in pixels. Therefore, any computational
    manipulation that you will do in this Matrix would most likely use Linear Algebra
    principles. If this example is not enough to demonstrate the importance of Linear
    Algebra in ML, then let''s look at a popular example in deep learning. In a nutshell,
    deep learning is an algorithm that uses neural network structure to learn the
    desired output (label) by continuously updating the weights of neuron connections
    between layers. A graphical representation of a simple deep learning algorithm
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您应该考虑 MRI 数据如何存储在仅处理 0 和 1 的计算机中。 计算机实际上将像素强度存储在称为矩阵的结构中。 换句话说，您将 MRI 转换为大小为`N2`的向量，其中每个元素均由像素值组成。
    如果此 MRI 大小为`512 x 512`，则每个像素将是 262,144 像素中的一个点。 因此，您将在此矩阵中进行的任何计算操作都极有可能会使用线性代数原理。
    如果该示例不足以证明 ML 中的线性代数的重要性，那么让我们看一下深度学习中的一个流行示例。 简而言之，深度学习是一种算法，该算法使用神经网络结构通过不断更新各层之间神经元连接的权重来学习所需的输出（标签）。
    一个简单的深度学习算法的图形表示如下：
- en: '![](img/f2ae22eb-b145-4cb4-90c0-f87ecaf03671.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2ae22eb-b145-4cb4-90c0-f87ecaf03671.png)'
- en: Neural networks store weights between layers and bias values in matrices. As
    these are the parameters that you try to tune in to your deep learning model in
    order to minimize your loss function, you continuously make computations and update
    them. In general, ML models require heavy calculations and need to be trained
    for big datasets to provide efficient results. This is why linear algebra is a
    fundamental part of ML.
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络存储各层之间的权重和矩阵中的偏差值。 由于这些参数是您尝试调整到深度学习模型中以最小化损失函数的参数，因此您不断进行计算并更新它们。 通常，机器学习模型需要大量计算，并且需要针对大型数据集进行训练以提供有效的结果。
    这就是为什么线性代数是 ML 的基本部分的原因。
- en: In this chapter, we will use numpy library but note that most linear algebra
    functions are also imported by scipy and they are more properly belong to it.
    Ideally, in most cases, you import both of these libraries and perform computations.
    One important feature of scipy is to have fully-featured versions of the linear
    algebra modules. We highly encourage you to review scipy documentation and practice
    using same operations with scipy throughout this chapter. Here's the link for
    linear algebra module of scipy: [https://docs.scipy.org/doc/scipy/reference/linalg.html](https://docs.scipy.org/doc/scipy/reference/linalg.html).
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 numpy 库，但请注意，大多数线性代数函数也是通过 scipy 导入的，它们更恰当地属于它。 理想情况下，在大多数情况下，您都可以导入这两个库并执行计算。
    scipy 的一个重要特征是拥有功能齐全的线性代数模块。 在本章中，我们强烈建议您阅读 scipy 文档并使用与 scipy 相同的操作进行练习。 这是 [scipy
    的线性代数模块的链接](https://docs.scipy.org/doc/scipy/reference/linalg.html)。
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主题：
- en: Vector and matrix mathematics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量和矩阵数学
- en: What's an eigenvalue and how do we compute it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是特征值？我们如何计算它？
- en: Computing the norm and determinant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算范数和行列式
- en: Solving linear equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 求解线性方程
- en: Computing gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算梯度
- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ''
- en: Vector and matrix mathematics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量和矩阵运算
- en: 'In the previous chapter, you practiced introductory operations with vectors
    and matrices. In this section, you will practice more advanced vector and matrix
    operations that are heavily used in linear algebra. Let''s remember the dot product
    perspective on matrix manipulation and how it can be done with different methods
    when you have 2-D arrays. The following code block shows alternative ways of performing
    dot product calculation:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您练习了向量和矩阵的入门操作。 在本节中，您将练习在线性代数中大量使用的更高级的向量和矩阵运算。 让我们记住关于矩阵操作的点积透视图，以及当您具有
    2D 数组时如何使用不同的方法来完成。 以下代码块显示了执行点积计算的替代方法：
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Inner products and dot products are very important in ML algorithms such as
    supervised learning. Let''s get back to our example about tumor detection. Imagine
    we have three images (MRIs): the first with a tumor (A), the second without a
    tumor (B) and the third one an unknown MRI that you want to label as *with tumor* or
    *without tumor.* The following graph shows a geometric representation of a dot
    productfor vector a and b:'
  prefs: []
  type: TYPE_NORMAL
  zh: 内积和点积在 ML 算法（如监督学习）中非常重要。 让我们回到有关肿瘤检测的示例。 想象我们有三张图像（MRI）：第一张图像有肿瘤（A），第二张图像无肿瘤（B），第三张图像是您要标记为*肿瘤*或*无肿瘤*的未知
    MRI。下图显示了向量`a`和`b`的点积的几何表示：
- en: '![](img/8094526b-174a-424e-a3c3-c34e58ed84ce.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8094526b-174a-424e-a3c3-c34e58ed84ce.png)'
- en: 'As a very simple example, the dot product will show you the similarity between
    these two vectors, so if your unknown MRI''s direction is close to vector A, then
    your algorithm will classify as MRI with tumor. Otherwise, it will classify as
    without tumor. If you want to multiply two or more arrays in a single function,
    `linalg.multi_dot()` is very convenient. Another way to do this operation is by
    multiplying the arrays nested by `np.dot()` but you need to know which computation
    order will be fastest because in `linalg.multi_dot()` this optimization occurs automatically. The
    following code block does the same dot product operations, with different methods,
    as we mentioned previously:'
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个非常简单的例子，点积将向您显示这两个向量之间的相似性，因此，如果您未知的 MRI 方向接近向量 A，则您的算法将被归类为具有肿瘤的 MRI。 否则，它将分类为无肿瘤。
    如果要在一个函数中乘以两个或多个数组，`linalg.multi_dot()`非常方便。 进行此操作的另一种方法是将`np.dot()`嵌套的数组相乘，但您需要知道哪个计算顺序最快，因为在`linalg.multi_dot()`中此优化是自动进行的。
    如前所述，以下代码块使用不同的方法执行相同的点积运算：
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you see in the following code, the `multi_dot()` method decreases running
    time by 40% even with three small matrices. This time gap can increase tremendously
    if the matrix amount and size increase. Consequently, you should use the `multi_dot()`
    method in order to be sure of the fastest evaluation order. The following code
    will compare the execution time of these two methods:'
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码所示，即使使用三个小型矩阵，`multi_dot()`方法也将运行时间减少了 40%。 如果矩阵数量和大小增加，则该时间间隔会大大增加。 因此，您应该使用`multi_dot()`方法以确保最快的求值顺序。
    以下代码将比较这两种方法的执行时间：
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are two more important methods in NumPy''s linear algebra library, namely `outer()`
    and `inner()`***.*** The `outer` method computes the outer product of two vectors.
    The `inner` method behaves differently depending on the arguments it takes. If
    you have two vectors as arguments, it produces an ordinary dot product, but when
    you have a higher dimensional array, it returns the sum product over the last
    axes as similarly in `tensordot()`***.*** You will see `tensordot()` later in
    this chapter. Now, let''s focus on the `outer()` and `inner()` methods first.
    The following example will help you to understand what these functions do:'
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 的线性代数库中还有另外两种重要的方法，即`outer()`和`inner()`。`outer`方法计算两个向量的外积。`inner`方法的行为取决于所采用的参数。
    如果您有两个向量作为参数，它将产生一个普通的点积，但是当您具有一个更高维的数组时，它会像`tensordot()`一样返回最后一个轴的积的和。您将在本章后面看到`tensordot()`。
    现在，让我们首先关注`outer()`和`inner()`方法。 以下示例将帮助您了解这些函数的作用：
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding example for the `inner()` method, the i^([th]) row of the
    array produces a scalar product with the vector and the sum becomes the i^([th])
    element of the output array, so the output array is constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述`inner()`方法的示例中，数组的第`i`行与向量产生标量积，总和成为输出数组的第`i`个元素，因此输出数组的构造如下：
- en: ''
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '* [0x0+1x1+2x2, 0x3+1x4 +2x5, 0x6 +1x7+2x8] = [5, 14, 23]*'
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们对同一数组但具有一维执行`outer()`方法。 如您所见，结果与我们对二维数组所做的完全相同：
- en: ''
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the following code block, we perform the `outer()` method for the same array
    but with one-dimension. As you noticed the result is exactly the same as we do
    with 2-D array:'
  prefs: []
  type: TYPE_NORMAL
  zh: '`outer()`方法计算向量的外积，在我们的示例中为 2D 数组。 这不会改变方法的功能，而是将 2D 数组展平为向量并进行计算。'
- en: ''
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行分解之前，本小节介绍的最后一件事是`tensordot()`方法。 在数据科学项目中，我们通常处理需要进行发现和应用 ML 算法的 n 维数据。
    以前，您了解了向量和矩阵。 张量是向量和矩阵的通用数学对象，可以将向量的关系保持在高维空间中。`tensordot()`方法用于两个张量的收缩； 换句话说，它通过将指定轴上两个张量的乘积相加来减小维数（张量级）。
    以下代码块显示了两个数组的`tensordot()`操作示例：
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是特征值，我们如何计算？
- en: The `outer()` method computes the outer product of vectors, in our example,
    a 2-D array. That does not change the functionality of the method but flattens
    the 2-D array to a vector and does the computation.
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值是特征向量的系数。 根据定义，**特征向量**是一个非零向量，在应用线性变换时仅会按标量因子进行更改。 通常，将线性变换应用于向量时，其跨度（穿过其原点的线）会移动，但是某些特殊向量不受这些线性变换的影响，而是保留在自己的跨度上。
    这些就是我们所说的特征向量。 线性变换仅在将向量与标量相乘时才通过拉伸或挤压它们来影响它们。 该标量的值称为特征值。 假设我们有一个 *A* 矩阵，该矩阵将用于线性变换。
    我们可以用以下数学表达式表示特征值和特征向量：
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36986fa4-6232-4efb-9de8-912754654a98.png)'
- en: 'Before moving to decomposition, the last thing this subsection covers is the`tensordot()` method.
    In a data science project, we mostly work with n-dimensional data which you need
    to do discovery and apply ML algorithms. Previously, you learned about vectors
    and matrices. A tensor is a generic mathematical object of vectors and matrices
    that can keep the relationships of vectors in high-dimensional spaces. The `tensordot()`
    method is used for the contraction of two tensors; in other words, it reduces
    the dimensionality (tensor order) by summing the products of two tensors over
    the specified axes. The following code block shows an example of `tensordot()`
    operation for two arrays:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/5bb72e21-92d3-44e2-8b9b-b096407781c2.png) 是特征向量，![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png)
    是特征值。 在等式的左侧，向量通过矩阵进行变换，结果只是同一向量的标量形式。 另外，请注意，左侧实际上是矩阵向量乘法，而右侧是标量向量乘法。 为了使边的乘法类型相同，我们可以将
    ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png) 与恒等矩阵相乘，这不会改变右侧。 但是将 ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png)
    从标量更改为矩阵，并且两边都是矩阵向量乘法：
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
  zh: ''
- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ''
- en: What's an eigenvalue and how do we compute it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ''
- en: 'An eigenvalue is a coefficient of an eigenvector. By definition, an **eigenvector**
    is a non zero vector that only changes by a scalar factor when linear transformation
    is applied. In general, when linear transformation is applied to a vector, its
    span (the line passing through its origin) is shifted, but some special vectors
    are not affected by these linear transformations and remain on their own span.
    These are what we call eigenvectors. The linear transformation affects them only
    by stretching or squishing them as you are multiplying this vector with a scalar.
    The value of this scalar is called the eigenvalue. Let''s say we have a matrix
    *A*, which will be used in linear transformation. We can represent the eigenvalue
    and eigenvector in a mathematical statements as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: '![](img/36986fa4-6232-4efb-9de8-912754654a98.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb5b1c8e-2ec9-4a97-a10b-a8e38b56c82c.png)'
- en: 'Here, ![](img/5bb72e21-92d3-44e2-8b9b-b096407781c2.png) is the eigenvector
    and ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png) denotes the eigenvalue.
    In the left part of the equation, the vector is transformed by a matrix and the
    result is just a scalar version of the same vector. In addition, notice that the
    left-hand side is actually a matrix-vector multiplication but the right-hand side
    is a scalar vector multiplication. In order to make the multiplication type for
    sides the same, we can multiply the ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png) with
    the identity matrix, which will not change the right-hand side. But change ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png) from
    scalar to matrix, and both sides will be a matrix- vector multiplication:'
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们减去右侧并分解为 ![](img/89730e46-aa7f-4a55-ab70-9863fafac4bc.png)，您将具有以下等式：
- en: '![](img/fb5b1c8e-2ec9-4a97-a10b-a8e38b56c82c.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/deaf62c0-d304-4622-849c-2665458c9797.png)'
- en: 'If we subtract the right-hand side and factor out ![](img/89730e46-aa7f-4a55-ab70-9863fafac4bc.png)
    you will have the following equation:'
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的新矩阵将如下所示：
- en: '![](img/deaf62c0-d304-4622-849c-2665458c9797.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa3fb9f1-45ba-4de6-8f0b-4e37b0f5d86a.png)'
- en: 'Therefore, our new matrix will be as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，您正在尝试计算`0`以外的特征向量的特征值； （如果 ![](img/7360bfbd-2cce-4c74-810b-1c60981ad34b.png)
    `= 0`，则没有意义），因此特征向量（![](img/55451371-dc34-4251-b721-cb4d1f1552c3.png)）不能为`0`。
    因此，您正在尝试求解以下等式：
- en: '![](img/fa3fb9f1-45ba-4de6-8f0b-4e37b0f5d86a.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a6d056a-3c20-4093-80af-774dba304c4c.png)'
- en: 'As you will already know, you are trying to calculate the eigenvalue for the
    eigenvector other than *0*; (if ![](img/7360bfbd-2cce-4c74-810b-1c60981ad34b.png)=0,
    that doesn''t make sense) so the eigenvector (![](img/55451371-dc34-4251-b721-cb4d1f1552c3.png))
    cannot be *0*. Therefore, you are trying to solve the equation for the following:'
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公式正在计算矩阵的行列式，因为只有一个条件，即非零矩阵等于`0`，即矩阵的行列式等于零。 给定矩阵的行列式为零意味着使用该矩阵进行的转换会将所有内容压缩为较小的维度。
    在下一部分中，您将详细了解行列式。 此处的真正目的是找到使行列式为零并将空间压缩为较小维度的 ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png)。
    找到 ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png) 后，我们可以根据以下公式计算特征向量（![](img/7818d6b6-5b66-4d9d-a919-d1a190f820f3.png)）：
- en: '![](img/5a6d056a-3c20-4093-80af-774dba304c4c.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3dbef5e5-8808-4ed0-aed8-77994d75731d.png)'
- en: 'The preceding formula is calculating the determinant of a matrix, as there
    is only one condition where a non-zero matrix is equal to *0*, which is when its
    determinant equals zero. Having a zero determinant for a given matrix means that
    the transformation with that matrix squishes everything into a smaller dimension.
    You will see determinants in more detail in the next section. The real aim here
    is to find the ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png)  that makes
    the determinant zero and squishes the space into a lower dimension. After finding ![](img/0d8c932d-e857-4256-a918-c9845c8996d9.png),
    we can calculate the eigenvector (![](img/7818d6b6-5b66-4d9d-a919-d1a190f820f3.png))
    from the following equation:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ML 算法中，您需要处理大量维度。 主要问题不是规模很大，而是算法与它们的兼容性和性能。 例如，在 **PCA**（**主成分分析**）中，您尝试发现大小最有意义的线性组合。
    PCA 的主要思想是减少数据集的规模，同时最大程度地减少信息丢失。 此处的信息丢失实际上是您特征的差异。 假设您具有五个特征，并且具有五个示例的类标签，如下所示：
- en: '![](img/3dbef5e5-8808-4ed0-aed8-77994d75731d.png)'
  prefs: []
  type: TYPE_IMG
  zh: ''
- en: 'In ML algorithms, you work with large numbers of dimensions. The main problem
    is not having a huge dimension, but your algorithm''s compatibility and performance
    with them. For example, in **PCA** (**principal component analysis**), you try
    to discover the most meaningful linear combination of your dimension. The main
    idea in PCA is reducing the dimension of your dataset while minimizing the loss
    of information. The loss of information here is actually variance of your features.
    Let''s say you have five features and a class label for five samples as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: '| **Feature 1** | **Feature 2** | **Feature 3** | **Feature 4** | **Feature
    5** | **Class** |'
  prefs: []
  type: TYPE_TB
  zh: '| **特征 1** | **特征 2** | **特征 3** | **特征 4** | **特征 5** | **类别** |'
- en: '| 1.45 | 42 | 54 | 1.001 | 1.05 | Dog |'
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 2 | 12 | 34 | 1.004 | 24.1 | Cat |'
  prefs: []
  type: TYPE_TB
  zh: '| 1.45 | 42 | 54 | 1.001 | 1.05 | 狗 |'
- en: '| 4 | 54 | 10 | 1.004 | 13.4 | Dog |'
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 12 | 34 | 1.004 | 24.1 | 猫 |'
- en: '| 1.2 | 31 | 1 | 1.003 | 42.1 | Cat |'
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54 | 10 | 1.004 | 13.4 | 狗 |'
- en: '| 5 | 4 | 41 | 1.003 | 41.4 | Dog |'
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 31 | 1 | 1.003 | 42.1 | 猫 |'
- en: ''
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 41 | 1.003 | 41.4 | 狗 |'
- en: In the preceding table, in **Feature 4,** it actually makes no significant difference
    whether the class label is **Dog** or **Cat**. This feature will be redundant
    in my analysis. The main goal here is to keep the features that strongly differ
    across the classes, so the feature value plays an important role in my decision.
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，在**特征 4** 中，类别标签是**狗**还是**猫**实际上并没有明显的区别。 在我的分析中，此特征将是多余的。 这里的主要目标是使各个类之间的特征保持很大的差异，因此特征值在我的决定中起着重要的作用。
- en: In the following example, we will use an additional library which is called
    `scikit-learn` in order to load the dataset and perform PCA by using this library
    as well. Scikit-learn is a free machine learning library for python which is built
    on top of SciPy. It has many built-in functions and modules for many machine learning
    algorithms such as classification, regression, clustering. SVM, DBSCAN, k-means
    and many others.
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用一个名为`scikit-learn`的附加库，以便加载数据集并通过使用该库来执行 PCA。 Scikit-learn 是一个免费的
    python 机器学习库，它基于 SciPy 构建。 它具有用于许多机器学习算法（例如分类，回归，聚类）的许多内置函数和模块。 SVM，DBSCAN，K 均值等。
- en: Scikit-learn has a great compatibility with numpy, scipy and pandas. You can
    use numpy array in `sklearn` as a data structure. Moreover, you can also use `sklearn_pandas`
    to transform scikit-learn pipeline outputs to pandas dataframe. Scikit-learn also
    support automating machine learning algorithms with the help of `auto_ml` and
    `auto-sklearn` libraries. `sklearn` is a way of typing scikit-learn name in python.
    That's why you use `sklearn` when you import skicit-learn and using functions.
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 与 numpy，scipy 和 pandas 具有很好的兼容性。 您可以将`sklearn`中的 numpy 数组用作数据结构。
    此外，您还可以使用`sklearn_pandas`将 scikit-learn 管道输出转换为 pandas 数据帧。 Scikit-learn 还通过`auto_ml`和`auto-sklearn`库支持自动化机器学习算法。`sklearn`是在
    python 中键入 scikit-learn 名称的一种方法。 这就是为什么在导入 skicit-learn 和使用函数时使用`sklearn`的原因。
- en: 'Now it''s time to do a practical exercise in order to see the usage and importance
    of eigenvalues and eigenvectors in PCA. In the following code, you import and
    apply PCA with NumPy, then you will compare your results with `sklearn` is built-in
    methods for validation. You will use a breast-cancer dataset from the sklearn
    datasets library. Let''s import the required libraries and the dataset first,
    then you need to standardize your data. Standardization is very important, and
    sometimes it''s even a requirement for estimators in some ML libraries such as scikit-learn.
    In our example, the `StandardScaler()` method is used in order to standardize
    the features; the main purpose here is to have features as in standard normally
    distributed data (mean = 0 and unit variance). The `fit_transform()` method is
    used to transform original data in a form that the distribution will have a mean
    value 0 and standard deviation 1\. It will calculate the required parameters and
    apply the transformation. As we use `StandardScaler()` this parameters will be ![](img/6b4f4ba1-e6b5-4d3d-a706-535db7b4a91f.png) and ![](img/49736463-aa50-4ceb-acf3-5dce84f2d4a6.png).
    Please keep in mind that standardization does not produce a normally distributed
    data from our original dataset. It''s just rescaled the data where we have a mean
    of zero and standard deviation one:'
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行实际练习，以了解特征值和特征向量在 PCA 中的用法和重要性。 在下面的代码中，您将 PCA 与 NumPy 一起导入并应用，然后将您的结果与`sklearn`是用于验证的内置方法进行比较。
    您将使用 sklearn 数据集库中的乳腺癌数据集。 让我们先导入所需的库和数据集，然后再对数据进行标准化。 标准化非常重要，有时甚至对于 scikit-learn
    之类的某些 ML 库中的估计器来说也是必需的。 在我们的示例中，使用`StandardScaler()`方法来标准化特征； 主要目的是具有标准正态分布数据中的特征（均值`=
    0`和单位方差）。`fit_transform()`方法用于转换原始数据，其形式为分布的平均值为 0，标准差为 1。它将计算所需的参数并应用转换。 当我们使用`StandardScaler()`时，此参数将为
    ![](img/6b4f4ba1-e6b5-4d3d-a706-535db7b4a91f.png) 和 ![](img/49736463-aa50-4ceb-acf3-5dce84f2d4a6.png)。
    请记住，标准化不会从我们的原始数据集中产生正态分布的数据。 只是重新缩放了数据，我们的平均值为零，标准差为 1：
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you checked the shape of your data in the preceding code, it shows the data
    consists of `569` rows and `30` columns. You can also compare the data in the
    beginning and after standardization just to understand more clearly how the original
    data was transformed. Following code block shows you an example of a transformation
    of a column in cancer data:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中检查数据形状时，它显示数据由`569`行和`30`列组成。 您也可以在标准化开始和标准化之后比较数据，以便更清楚地了解原始数据是如何转换的。
    以下代码块为您展示了癌症数据中列转换的示例：
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you see from the results, the original data has been transformed into a
    form of standardized version. This is calculated by the following formula:'
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看到，原始数据已转换为标准化版本的形式。 通过以下公式计算得出：
- en: '![](img/e7a0be79-089f-4531-8529-edda61c35fd0.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7a0be79-089f-4531-8529-edda61c35fd0.png)'
- en: '![](img/0769fb03-5594-4756-9829-04748f2139c9.png)= The value that is being
    standardized (values in original data)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: ''
- en: '![](img/c0dff95e-8033-4a9b-b8bc-9cf7833c23dc.png)= The mean of the distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/0769fb03-5594-4756-9829-04748f2139c9.png)：正在标准化的值（原始数据中的值）'
- en: '![](img/d9ffbbc2-5700-4fdc-9f09-5677e549439b.png)= Standard deviation of
    the distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/c0dff95e-8033-4a9b-b8bc-9cf7833c23dc.png)：分布的平均值'
- en: '![](img/fa8a85d1-da83-4b63-b229-100538c5147c.png)= Standardized values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/d9ffbbc2-5700-4fdc-9f09-5677e549439b.png)：分布的标准差'
- en: ''
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/fa8a85d1-da83-4b63-b229-100538c5147c.png)：标准化值'
- en: 'After transforming the data, you will calculate the covariance matrix as follows
    in order to calculate eigenvalue and eigenvector with the `np.linalg.eig()` method,
    then use them in decomposition:'
  prefs: []
  type: TYPE_NORMAL
  zh: 转换数据后，您将如下计算协方差矩阵，以便使用`np.linalg.eig()`方法计算特征值和特征向量，然后将其用于分解：
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you will have noticed in the preceding code, we calculate the covariance
    matrix constructed for all features. As we have 30 features in the dataset, the
    covariance matrix has a shape of *(30,30) *2-D array. The following code block
    sorts the eigenvalues in descending order:'
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的代码中已经注意到的那样，我们计算为所有要素构造的协方差矩阵。 由于我们在数据集中具有 30 个特征，因此协方差矩阵是形状为`(30, 30)`的
    2D 数组。 以下代码块按降序对特征值进行排序：
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You need to sort the eigenvalues in decreasing order to decide which eigenvector(s)
    you want to drop in order to lower the dimensional workspace. As you''ve seen
    in the preceding sorted list, the first two eigenvectors with high eigenvalues
    bear the most information about the distribution of the data; therefore the rest
    will be dropped for lower-dimensional subspace:'
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要以降序对特征值进行排序，以确定要降低维度工作空间要删除的特征向量。 正如您在前面的排序列表中所看到的那样，具有较高特征值的前两个特征向量具有有关数据分布的最多信息。
    因此，其余的将针对低维子空间删除：
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding code block, the first two eigenvectors stacked horizontally
    as they will be used in matrix multiplication to transform our data for the new
    dimensions onto the new subspace. The final data transformed from `(569,30)` to
    `(569,2)` matrix, which means that `28` features dropped off during the PCA process:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，前两个特征向量将水平堆叠，因为它们将用于矩阵乘法，以将新维度的数据转换到新的子空间上。 最终数据从`(569,30)`转换为`(569,2)`矩阵，这意味着`28`特征在
    PCA 过程中下降了：
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: On the other hand, there are built-in functions in other libraries that do the
    same operations that you do. In scikit-learn, there are many built-in methods
    that you can use for ML algorithms. As you see in the preceding code block, the
    same PCA was performed by three lines of code with two methods. Nevertheless,
    the intention of this example is to show you the importance of eigenvalues and
    eigenvectors therefore the book shows the plain vanilla way of PCA with NumPy.
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，其他库中有内置函数可以执行与您相同的操作。 在 scikit-learn 中，有许多内置方法可用于 ML 算法。 如您在前面的代码块中看到的，同一
    PCA 是通过三行代码和两种方法执行的。 不过，本示例的目的是向您展示特征值和特征向量的重要性，因此，本书展示了使用 NumPy 进行 PCA 的普通方法。
- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ''
- en: Computing the norm and determinant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算范数和行列式
- en: 'This subsection will introduce two important values in linear algebra, namely
    the norm and determinant. Briefly, the norm gives length of a vector. The most
    commonly used norm is the *L²*-norm, which is also known as the Euclidean norm.
    Formally, the *L^p*-norm of *x* is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节将介绍线性代数中的两个重要值，即范数和行列式。 简而言之，范数给出向量的长度。 最常用的范数是 L2 范数，也称为欧几里得范数。 正式地，Lp 的范数计算如下：
- en: '![](img/0a073828-6fbf-4465-bebb-0d4fd0d491d6.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a073828-6fbf-4465-bebb-0d4fd0d491d6.png)'
- en: 'The *L⁰*-norm is actually the cardinality of a vector. You can calculate it
    by just counting the total number of non-zero elements. For example, the vector
    *A =[2,5,9,0]* contains three non-zero elements, therefore *||A||[0] = 3*. The
    following code block shows the same norm calculation with numpy:'
  prefs: []
  type: TYPE_NORMAL
  zh: L0 范数实际上是向量的基数。 您可以通过仅计算非零元素的总数来计算它。 例如，向量`A = [2,5,9,0]`包含三个非零元素，因此`||A||[0]
    = 3`。 以下代码块显示了与 numpy 相同的范数计算：
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In NumPy, you can calculate the norm of the vector with the use of the `linalg.norm()`
    method. The first parameter is the input array and the `ord` parameter is for
    order of the norm. The L¹-norm is also known as *Taxicab norm* or *Manhattan norm*.
    It calculates the length of the vectors by calculating the rectilinear distances,
    therefore *||A||[1]=(2+5+9)* *so ||A||[1] = 16. *The following code block shows
    the same norm calculation with numpy:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，您可以使用`linalg.norm()`方法来计算向量的范数。 第一个参数是输入数组，而`ord`参数用于范数的顺序。 L1 范数也称为*出租车范数*或*曼哈顿范数*。
    它通过计算直线距离来计算向量的长度，因此`||A||[1] = (2 + 5 + 9)`，如此`||A||[1] = 16`。以下代码块显示了相同的 numpy
    范数计算：
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One of the uses of the L¹- norm is for the calculation of **mean-absolute error**
    (**MAE**) as in the following formula:'
  prefs: []
  type: TYPE_NORMAL
  zh: L1 范数的用途之一是用于计算**平均绝对误差**（**MAE**），如下式所示：
- en: '![](img/f56b83d5-b276-4927-aaef-a8bf6b44edb5.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f56b83d5-b276-4927-aaef-a8bf6b44edb5.png)'
- en: 'The L²-norm is the most popular norm. It calculates the length of the vector
    by applying the Pythagorean theorem, therefore ||A||[2] = ![](img/1027f89e-45fc-4a53-a4d3-1ec12876346b.png),
    so *||A||[2] = 10.48*:'
  prefs: []
  type: TYPE_NORMAL
  zh: L2 范数是最受欢迎的范数。 它通过应用勾股定理来计算向量的长度，因此`||A||[2] =` ![](img/1027f89e-45fc-4a53-a4d3-1ec12876346b.png)，所以`||A||[2]
    = 10.48`：
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'One of the well-known applications of the L²-norm is the **mean-squared error**
    (**MSE**) calculations. As a matrix consists of vectors, similarly, the norm gives
    the length or size, but the interpretation and computation is slightly different.
    In previous chapters, you learned about matrix multiplication with vectors. When
    you multiply the matrix with a vector in the result, you stretch the vector. The
    norm of the matrix reveals how much that matrix could possibly stretch a vector.
    Let''s see how the L¹and L^(![](img/96b6c4fb-6beb-4e5e-954a-f81ad25248c6.png))-norm
    is calculated for matrix *A* as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: L2 范数的众所周知的应用之一是**均方误差**（**MSE**）计算。 类似地，由于矩阵由向量组成，因此范数给出了长度或大小，但解释和计算略有不同。
    在前面的章节中，您学习了向量矩阵乘法。 在结果中将矩阵与向量相乘时，将拉伸向量。 矩阵的范数揭示了该矩阵可以拉伸向量的程度。 让我们看看如何为矩阵`A`计算
    L1 和 L∞ 范数的模数：
- en: '![](img/37de3d77-c062-472a-863a-a7e4ca391358.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37de3d77-c062-472a-863a-a7e4ca391358.png)'
- en: 'Let''s assume that you have an *m x n* matrix. The calculation for *||A||[1]*
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个`m x n`矩阵。`||A||[1]`的计算如下：
- en: '||A||[1] = ![](img/a8d5484c-c637-4366-84fc-72a86f46d8e8.png)'
  prefs: []
  type: TYPE_NORMAL
  zh: '`||A||[1] =` ![](img/a8d5484c-c637-4366-84fc-72a86f46d8e8.png)'
- en: 'So the result will be as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 因此结果将如下所示：
- en: ''
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*||A||[1] = max(3+|-2|+1; 7+|-5|+3; 6+4+|-14|) = max(6,15,24) =24*'
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`linalg.norm()`方法，用 NumPy 计算同一数组的范数：
- en: ''
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s calculate the norm of the same array with NumPy using the `linalg.norm()` method
    as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的计算中`||A||[1]`首先按列计算最大元素，并在所有列中给出最大结果，这将是一阶矩阵的范数。 对于 L∞ 元素计算按行执行，并在所有获得无穷范数范数矩阵的行中给出最大值。
    计算结果如下：
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding calculation *||A||[1]* has calculated the max element as column-wise
    first and gives the maximum result in all columns, which will be the norm of the
    matrix for the first order. For L^(![](img/96b6c4fb-6beb-4e5e-954a-f81ad25248c6.png)), the
    max element calculations perform row-wise and the give the maximum result in all
    rows that get the norm matrix for infinity order. The calculation looks as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证和使用 NumPy 函数，与我们在向量范数计算中所做的一样，使用`linalg.norm()`方法进行了相同的计算。 一阶和无限阶的计算比 Lp
    范数（其中`p > 2`）的常规计算相对更直接，例如欧几里德/弗罗宾纽斯范数（其中`p = 2`）。 下面的公式显示了`p`范数的形式公式，只需将`p`替换为
    2，就可以为给定数组制定欧几里得/弗罗比纽斯范数：
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d178ed07-0811-44e6-964d-72c08ad4cf2d.png)'
- en: '* ||A||[![](img/b7ce885d-2e25-480d-a9b6-cdf963751a27.png) ]= max(3+7+6;
    |-2|+|-5|+4; 1+3+|-14|) = max(16,11,18) =18*'
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`p = 2`的特殊情况，它变为：
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90394fc4-74e9-4000-a475-00c2b31822b6.png)'
- en: 'In order to validate and use NumPy functionality, the same calculations are
    done with the `linalg.norm()` method as we do in vector norm calculations. Calculations
    for the first and infinite order are relatively more straightforward than the
    general calculation for the *p*-norm (where *p*>2), such as the Euclidean/Frobenius
    norm (where *p=2*). The following formula shows the formal formula for the p-norm,
    and just by replacing the *p* with 2 you can formulate the Euclidean/Frobenius
    norm for a given array:'
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了数组`a`的 L2 范数计算：
- en: ''
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ''
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，可以通过在`linalg.norm()`方法中将`order`参数设置为 2 来计算欧几里德/弗罗比纽斯范数，如您在前面的代码中所见。
    在 ML 算法中，特征在特征空间的距离计算中大量使用。 例如，在模式识别中， 范数计算用于 K 最近邻算法（KNN），以便为连续变量或离散变量创建距离度量。
    同样，范数对于 K 均值聚类中的距离度量也非常重要。 最受欢迎的顺序是曼哈顿范数和欧几里得范数。
- en: ''
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数中的另一个重要概念是计算矩阵的行列式。 根据定义，行列式是线性变换中给定矩阵的比例因子。 在上一节中，当您计算特征值和特征向量时，将给定矩阵乘以特征向量，并假定它们的行列式将为零。
    换句话说，您假设将特征向量乘以矩阵后，会将矩阵展平为较低的维度。`2 x 2`和`3 x 3`矩阵的行列式如下：
- en: '![](img/d178ed07-0811-44e6-964d-72c08ad4cf2d.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/814c4887-069a-4a6e-a499-d84b8fbc0428.png)'
- en: 'For the special case of *p=2,* this becomes:'
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: '![](img/90394fc4-74e9-4000-a475-00c2b31822b6.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/091dc3a0-ffdd-43ff-b823-1ea81ae7d4bb.png)'
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74a38501-a442-4b7f-985c-8afae464cc55.png)'
- en: 'The following code block shows L-2 norm calculation for array `a`:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，您可以使用`linalg.det()`方法来计算矩阵的行列式。 让我们用上式计算一个`2 x 2`和`3 x 3`矩阵行列式的示例，然后用
    NumPy 交叉验证我们的结果：
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
  zh: ''
- en: In NumPy, the Euclidean/ Frobenius norm can be calculated by setting the order
    parameter to 2 in `linalg.norm()` method as you see in the preceding code. In
    ML algorithms, the norm is heavily used in distance calculation for the feature
    space. For example, in pattern recognition, norm calculations used in k-nearest
    neighbors algorithm (k-NN) in order to create distance metric for continuous variables
    or discrete variables. Similarly, norms are very important for distance metrics
    in k-means clustering as well. The most popular orders are the Manhattan norm
    and the Euclidean norm.
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: 'Another important concept in linear algebra is calculating the determinant
    of the matrices. By definition, the determinant is the scaling factor of a given
    matrix in linear transformation. In the previous section, while you are calculating
    eigenvalues and eigenvectors, you multiply a given matrix by an eigenvector and
    assume that their determinant will be zero. In another words, you assume that
    after you multiply the eigenvector by your matrix, you will flatten your matrix
    into a lower dimension. The determinant formula for a *2 x 2* and *3 x 3* matrix
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: '![](img/814c4887-069a-4a6e-a499-d84b8fbc0428.png)![](img/091dc3a0-ffdd-43ff-b823-1ea81ae7d4bb.png)![](img/74a38501-a442-4b7f-985c-8afae464cc55.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85bac9d0-836e-49ee-b14c-607478125f05.png)'
- en: 'In NumPy, you can use the `linalg.det()` method to calculate the determinant
    of your matrix. Let''s calculate an example of the *2 x 2* and *3 x 3* matrix
    determinant with the preceding formula and cross-validate our result with NumPy:'
  prefs: []
  type: TYPE_NORMAL
  zh: 因此计算如下：
- en: '![](img/85bac9d0-836e-49ee-b14c-607478125f05.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/539639de-d892-4e71-8052-d96aa5b7625a.png)'
- en: 'So the calculation will be as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们有`3 x 3`矩阵的情况，它变为：
- en: '![](img/539639de-d892-4e71-8052-d96aa5b7625a.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd9a93f0-28d3-4ce5-bfd0-4fdb5b3dd272.png)'
- en: 'For the case where we have *3 x 3* matrix, this becomes:'
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: '![](img/bd9a93f0-28d3-4ce5-bfd0-4fdb5b3dd272.png)![](img/805e892b-5c76-4a93-a12c-122abc1cf61e.png)![](img/c29fbcb4-2917-4996-83a6-bbfb03516214.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/805e892b-5c76-4a93-a12c-122abc1cf61e.png)'
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c29fbcb4-2917-4996-83a6-bbfb03516214.png)'
- en: 'Let''s calculate the determinant of the same arrays with NumPy using the `linalg.det()` method
    as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`linalg.det()`方法，用 NumPy 计算相同数组的行列式：
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The determinant of a transformation actually shows the factor of how much the
    volume will extend or compress. If the determinant of a matrix *A* equals *2*,
    that means the transformation of this matrix will extend the volume by *2*. If
    you are doing a chain multiplication, you can also calculate how much the volume
    changes after the transformation. Let's say you multiply two matrices, that means
    that there will be two transformations. If *det(A) = 2* and *det(B)=3*, the total
    transformation factor will be multiplied by 6 as *det(AB) = det(A)det(B)*.
  prefs: []
  type: TYPE_NORMAL
  zh: 转换的决定因素实际上显示了将扩展或压缩多少体积的因素。 如果矩阵的行列式等于`2`，则意味着此矩阵的变换将使体积增加`2`。 如果要进行链乘法，则还可以计算变换后音量的变化量。
    假设您将两个矩阵相乘，这意味着将有两个转换。 如果`det(A)= 2`并且`det(B)= 3`，则总转换因子将乘以 6 作为`det(AB) = det(A)det(B)`。
- en: 'Lastly, this chapter will cover a very useful value for your ML models, the
    value called **trace**. By definition, the trace is the sum of diagonal elements
    of a matrix. In ML models, in most cases, you work with several regression models
    to explain your data. It''s very likely that some of these models explain your
    data quality more or less the same, therefore in such cases, you always tend to
    move forward with the simpler model. Whenever you have to do this trade-off, the
    trace value becomes very valuable for quantifying the complexity. The following
    code block shows trace calculation with numpy for 2-D and 3-D matrices:'
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章将为您的 ML 模型介绍一个非常有用的值，该值称为迹。 根据定义，迹线是矩阵对角线元素的总和。 在 ML 模型中，大多数情况下，您将使用几种回归模型来解释数据。
    这些模型中的某些很有可能在某种程度上解释了您的数据质量，因此在这种情况下，您总是倾向于使用更简单的模型。 每当您必须进行权衡时，跟踪值对于量化复杂性就变得非常有价值。
    以下代码块显示了使用 numpy 进行 2D 和 3D 矩阵的跟踪计算：
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In NumPy, you can use the `trace()` method to calculate the trace value of
    your matrix. If your matrix is 2-D then the trace is the sum along the diagonal.
    If your matrix has a dimension more than *2*, then the trace is an array of sums
    along the diagonals. In the preceding example, matrix *B* has three dimensions
    so the trace array is constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，可以使用`trace()`方法来计算矩阵的跟踪值。 如果您的矩阵是二维的，那么轨迹就是对角线的总和。 如果矩阵的维数大于`2`，则迹线是沿对角线的和数组。
    在前面的示例中，矩阵`B`具有三个维度，因此跟踪数组的构造如下：
- en: '![](img/9201d620-0590-4863-89ad-8492a89a3aea.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9201d620-0590-4863-89ad-8492a89a3aea.png)'
- en: In this subsection, you learned how to calculate the norm, determinant, trace,
    and usage in ML algorithms. The main aim was to learn these concepts and become
    familiar with the NumPy linear algebra library.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，您学习了如何在 ML 算法中计算范数，行列式，跟踪和用法。 主要目的是学习这些概念并熟悉 NumPy 线性代数库。
- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ''
- en: Solving linear equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 求解线性方程
- en: 'In this section, you will learn how to solve linear equations by using the `linalg.solve()`
    method. When you have a linear equation to solve, as in the form ![](img/0e3b6a5c-7f67-4ab8-aea6-cde4b3521b35.png),
    in simple cases you can just calculate the inverse of *A* and then multiply it
    by *B* to get the solution, but when *A* has a high dimensionality, that makes
    it very hard computationally to calculate the inverse of *A*. Let''s start with
    an example of three linear equations with three unknowns, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何使用`linalg.solve()`方法求解线性方程。 当您有线性方程要求解时，例如形式 ![](img/0e3b6a5c-7f67-4ab8-aea6-cde4b3521b35.png)
    ，在简单情况下，您只需计算`A`的反函数即可。然后将其乘以`B`即可得到解，但是当`A`具有高维数时，这使得计算起来很难进行计算`A`的倒数。 让我们从具有三个未知数的三个线性方程式的示例开始，如下所示：
- en: '![](img/df1cc562-7e5d-4f08-a064-2422ce4396cf.png)![](img/d3c7d7f4-45b7-4f99-9140-96d85bd7eb26.png)![](img/97486ea5-22df-4a72-8d10-20a0415182e1.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df1cc562-7e5d-4f08-a064-2422ce4396cf.png)'
- en: 'So, these equations can be formalized as follows with matrices:'
  prefs: []
  type: TYPE_NORMAL
  zh: ''
- en: '![](img/46e30ae2-34f0-42d4-af84-332288db9ce6.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3c7d7f4-45b7-4f99-9140-96d85bd7eb26.png)'
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97486ea5-22df-4a72-8d10-20a0415182e1.png)'
- en: 'Then, our problem is to solve ![](img/6be36489-de17-4e8a-b83a-79a6708912de.png).
    We can calculate the solution with a plain vanilla NumPy without using `linalg.solve()`***. ***After
    inverting the *A* matrix, you will multiply with *B* in order to get results for
    *x.* In the following code block, we calculate the dot product for the inverse
    matrix of A and B in order to calculate ![](img/8831a01b-8325-482a-8e28-31ebc3e92107.png):'
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些方程式可以用矩阵形式化如下：
- en: '![](img/ef1be631-814e-4c87-97a2-37178f104461.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46e30ae2-34f0-42d4-af84-332288db9ce6.png)'
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
  zh: ''
- en: 'Finally, you get the result for *a =-0.2*, *b= -0.4* and *c=4.4*. Now, let''s
    perform the same calculation with `linalg.solve()` as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的问题是解决 ![](img/6be36489-de17-4e8a-b83a-79a6708912de.png)。 我们可以不使用`linalg.solve()`，而使用普通
    NumPy 计算解决方案。颠倒`A`矩阵后，您将与`B`相乘以获得`x`的结果。在下面的代码块中，我们计算`A`和`B`的逆矩阵的点积，以便计算 ![](img/8831a01b-8325-482a-8e28-31ebc3e92107.png)：
- en: ''
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef1be631-814e-4c87-97a2-37178f104461.png)'
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In order to check our results, we can use the `allclose()` function, which
    is used to compare two arrays element-wise:'
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，得到`a = -0.2`，`b = -0.4`和`c = 4.4`的结果。 现在，让我们使用`linalg.solve()`执行相同的计算，如下所示：
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Another important function for solving linear equations, which returns the
    least-square solution, is the `linalg.lstsq()` method. This function will return
    the parameters for the regression line. The main idea of the regression line is
    to minimize the sum of the squares of distance from each data point to the regression
    line. The sum of squares of distances actually quantify the total error of the
    regression line. Higher distance means higher error. As a result, we are looking
    for parameters that will minimize this error. In order to visualize our linear
    regression model, we will use a very popular 2-D plotting library for python which
    called `matplotlib`. The following code block runs the least-squares solution
    in our matrix and returns the weight and bias:'
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的结果，我们可以使用`allclose()`函数，该函数用于按元素比较两个数组：
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: '`linalg.lstsq()`方法是求解线性方程组的另一个重要函数，它返回最小二乘解。 此函数将返回回归线的参数。 回归线的主要思想是最小化每个数据点到回归线的距离平方和。
    距离的平方和实际上量化了回归线的总误差。 距离越大，误差越大。 因此，我们正在寻找可以最大程度减少此误差的参数。 为了可视化我们的线性回归模型，我们将使用一个非常流行的
    Python 二维绘图库`matplotlib`。 以下代码块在矩阵中运行最小二乘解，并返回权重和偏差：'
- en: ''
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ''
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '![](img/815cb0d4-de6a-49f4-b83b-3d6ff47e06b9.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/815cb0d4-de6a-49f4-b83b-3d6ff47e06b9.png)'
- en: The preceding plot gives a result of regression line fitting which fits on our
    data. The model generates a linear line that can be used for prediction or forecasting.
    Although linear regressions have many assumptions (constant variance, independence
    of errors, linearity, and so on), it is still the most commonly used approach
    for modeling linear relationships between data points.
  prefs: []
  type: TYPE_NORMAL
  zh: 上图给出了适合我们数据的回归线拟合结果。 该模型生成可用于预测或预测的线性线。 尽管线性回归有很多假设（恒定方差，误差的独立性，线性等），但它仍然是建模数据点之间线性关系的最常用方法。
- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ''
- en: Computing gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度
- en: 'When you have a linear line, you take the derivative so the derivative shows
    the slope of this line. Gradient is a generalization of the derivative when you
    have a multiple variable in your function, therefore the result of gradient is
    actually a vector function rather than a scalar value in derivative. The main
    goal of ML is actually finding the best model that fits your data. You can evaluate
    the meaning of the best as minimizing your loss function or objective function.
    Gradient is used for finding the value of the coefficients or a function that
    will minimize your loss or cost function. A well-known way of finding optimum
    points is taking the derivative of the objective function then setting it to zero
    to find your model coefficients. If you have more than one coefficient then it
    becomes a gradient rather than a derivative, and it becomes a vector equation
    rather than a scalar value. You can interpret a gradient as a vector at every
    point, which is directed to a next local minimum for your function. There is a
    very popular optimization technique for finding the minimum of a function by computing
    the gradient for each point and moving the coefficients along this direction,
    aiming to find the minimum. This method called **gradient descent**. In NumPy,
    the `gradient()` function is used to calculate the gradient of an array:'
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有一条直线时，您可以使用导数，以便导数显示该线的斜率。 当函数中有多个变量时，梯度是导数的泛化，因此，梯度的结果实际上是向量函数，而不是导数中的标量值。
    ML 的主要目标实际上是找到适合您数据的最佳模型。 您可以通过将损失函数或目标函数最小化来求值最佳均值。 梯度用于查找系数或函数，以最大程度地减少损失或成本函数。
    查找最佳点的一种众所周知的方法是采用目标函数的导数，然后将其设置为零以找到模型系数。 如果系数不止一个，则它将成为梯度而不是导数，并且将成为向量方程式而不是标量值。
    您可以在每个点上将梯度解释为向量，该梯度将指向函数的下一个局部最小值。 有一种非常流行的优化技术，可以通过计算每个点的梯度并沿该方向移动系数来寻找函数的最小值，以寻求最小值。
    该方法称为**梯度下降**。 在 NumPy 中，`gradient()`函数用于计算数组的梯度：
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s see how this gradient is calculated:'
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何计算此梯度：
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding code blocks, we calculated the gradient for a one-dimensional
    array. Let''s add another dimension and see how the calculation changes as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们计算了一维数组的梯度。 让我们添加另一个维度，并查看计算如何更改，如下所示：
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the case of a 2-D array, the gradient is calculated column-wise and row-wise
    as in the preceding code. Therefore, there will be a two-array return as a result;
    the first array stands for row direction and the second array is for column direction.
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二维数组，与前面的代码一样，按列和按行计算梯度。 因此，结果将是两个数组的返回。 第一个数组代表行方向，第二个数组代表列方向。
- en: Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ''
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered vector and matrix operations for linear algebra.
    We looked at advanced matrix operations, especially featuring dot operations.
    You also learned about eigenvalues and eigenvectors and then practiced their use
    in **principal component analysis **(**PCA**). Moreover, we covered the norm and
    determinant calculation and mentioned their importance and usage in ML. In the
    last two subsections, you learned how to convert linear equations into matrices
    and solve them, and looked at the computation and importance of gradients.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了线性代数的向量和矩阵运算。 我们研究了高级矩阵运算，尤其是特征点运算。 您还了解了的特征值和特征向量，然后在**主成分分析**（**PCA**）中实践了它们的用法。
    此外，我们介绍了范数和行列式计算，并提到了它们在 ML 中的重要性和用法。 在最后两个小节中，您学习了如何将线性方程式转换为矩阵并求解它们，并研究了梯度的计算和重要性。
- en: In the next chapter, we will use NumPy statistics to do explanatory data analysis
    to explore the 2015 United States Housing data.
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用 NumPy 统计数据进行解释性数据分析，以探索 2015 年美国住房数据。

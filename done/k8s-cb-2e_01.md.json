["```\n//see the Component Statuses\n# kubectl get cs\nNAME                 STATUS    MESSAGE              ERROR\ncontroller-manager   Healthy   ok                   nil\nscheduler            Healthy   ok                   nil\netcd-0               Healthy   {\"health\": \"true\"}   nil\n\n//see the nodes\n# kubectl get nodes\nNAME          LABELS                           STATUS    AGE\nkub-node1   kubernetes.io/hostname=kub-node1   Ready     26d\nkub-node2   kubernetes.io/hostname=kub-node2   Ready     26d\n```", "```\n//the result will be vary and dynamically changed by kube-proxy\n# sudo iptables -t nat -S\n-P PREROUTING ACCEPT\n-P INPUT ACCEPT\n-P OUTPUT ACCEPT\n-P POSTROUTING ACCEPT\n-N DOCKER\n-N FLANNEL\n-N KUBE-NODEPORT-CONTAINER\n-N KUBE-NODEPORT-HOST\n-N KUBE-PORTALS-CONTAINER\n-N KUBE-PORTALS-HOST\n-A PREROUTING -m comment --comment \"handle ClusterIPs; NOTE: this must be before the NodePort rules\" -j KUBE-PORTALS-CONTAINER\n-A PREROUTING -m addrtype --dst-type LOCAL -m comment --comment \"handle service NodePorts; NOTE: this must be the last rule in the chain\" -j KUBE-NODEPORT-CONTAINER\n-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER\n-A OUTPUT -m comment --comment \"handle ClusterIPs; NOTE: this must be before the NodePort rules\" -j KUBE-PORTALS-HOST\n-A OUTPUT -m addrtype --dst-type LOCAL -m comment --comment \"handle service NodePorts; NOTE: this must be the last rule in the chain\" -j KUBE-NODEPORT-HOST\n-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER\n-A POSTROUTING -s 192.168.90.0/24 ! -o docker0 -j MASQUERADE\n-A POSTROUTING -s 192.168.0.0/16 -j FLANNEL\n-A FLANNEL -d 192.168.0.0/16 -j ACCEPT\n-A FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE\n```", "```\n//example: etcd server is localhost and default port is 4001\n# curl -L http://127.0.0.1:4001/v2/keys/registry\n{\"action\":\"get\",\"node\":{\"key\":\"/registry\",\"dir\":true,\"nodes\":[{\"key\":\"/registry/namespaces\",\"dir\":true,\"modifiedIndex\":6,\"createdIndex\":6},{\"key\":\"/registry/pods\",\"dir\":true,\"modifiedIndex\":187,\"createdIndex\":187},{\"key\":\"/registry/clusterroles\",\"dir\":true,\"modifiedIndex\":196,\"createdIndex\":196},{\"key\":\"/registry/replicasets\",\"dir\":true,\"modifiedIndex\":178,\"createdIndex\":178},{\"key\":\"/registry/limitranges\",\"dir\":true,\"modifiedIndex\":202,\"createdIndex\":202},{\"key\":\"/registry/storageclasses\",\"dir\":true,\"modifiedIndex\":215,\"createdIndex\":215},{\"key\":\"/registry/apiregistration.k8s.io\",\"dir\":true,\"modifiedIndex\":7,\"createdIndex\":7},{\"key\":\"/registry/serviceaccounts\",\"dir\":true,\"modifiedIndex\":70,\"createdIndex\":70},{\"key\":\"/registry/secrets\",\"dir\":true,\"modifiedIndex\":71,\"createdIndex\":71},{\"key\":\"/registry/deployments\",\"dir\":true,\"modifiedIndex\":177,\"createdIndex\":177},{\"key\":\"/registry/services\",\"dir\":true,\"modifiedIndex\":13,\"createdIndex\":13},{\"key\":\"/registry/configmaps\",\"dir\":true,\"modifiedIndex\":52,\"createdIndex\":52},{\"key\":\"/registry/ranges\",\"dir\":true,\"modifiedIndex\":4,\"createdIndex\":4},{\"key\":\"/registry/minions\",\"dir\":true,\"modifiedIndex\":58,\"createdIndex\":58},{\"key\":\"/registry/clusterrolebindings\",\"dir\":true,\"modifiedIndex\":171,\"createdIndex\":171}],\"modifiedIndex\":4,\"createdIndex\":4}}\n```", "```\n$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n```", "```\n$ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \\\n&& chmod +x docker-machine-driver-hyperkit \\\n&& sudo mv docker-machine-driver-hyperkit /usr/local/bin/ \\\n&& sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \\\n&& sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit\n```", "```\n//install kubectl command by \"kubernetes-cli\" package\n$ brew install kubernetes-cli\n```", "```\n//add \"cask\" option\n$ brew cask install minikube\n```", "```\n//only if you don't have a Docker for Mac\n$ brew cask install docker\n\n//start Docker\n$ open -a Docker.app\n```", "```\n//check installed package by homebrew\n$ brew list\nkubernetes-cli\n\n//check installed package by homebrew-cask\n$ brew cask list\nminikube\n```", "```\n//use --vm-driver=hyperkit to specify to use hyperkit\n$ /usr/local/bin/minikube start --vm-driver=hyperkit\nStarting local Kubernetes v1.10.0 cluster...\nStarting VM...\nDownloading Minikube ISO\n 150.53 MB / 150.53 MB [============================================] 100.00% 0s\nGetting VM IP address...\nMoving files into cluster...\nDownloading kubeadm v1.10.0\nDownloading kubelet v1.10.0\nFinished Downloading kubelet v1.10.0\nFinished Downloading kubeadm v1.10.0\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nLoading cached images from config file.\n\n//check whether .kube/config is configured or not\n$ cat ~/.kube/config \napiVersion: v1\nclusters:\n- cluster:\n certificate-authority: /Users/saito/.minikube/ca.crt\n server: https://192.168.64.26:8443\n name: minikube\ncontexts:\n- context:\n cluster: minikube\n user: minikube\n name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: minikube\n user:\n as-user-extra: {}\n client-certificate: /Users/saito/.minikube/client.crt\n client-key: /Users/saito/.minikube/client.key \n```", "```\n//it shows kubectl (Client) is 1.10.1, and Kubernetes master (Server) is 1.10.0\n$ /usr/local/bin/kubectl version --short\nClient Version: v1.10.1\nServer Version: v1.10.0\n\n//get cs will shows Component Status\n$ kubectl get cs\nNAME                 STATUS    MESSAGE              ERROR\ncontroller-manager   Healthy   ok \nscheduler            Healthy   ok \netcd-0               Healthy   {\"health\": \"true\"} \n\n//Kubernetes node (minikube) is ready\n$ /usr/local/bin/kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nminikube  Ready  master  2m  v1.10.0\n```", "```\n// check MAC address of your NIC $ ifconfig -a\n// check the product UUID on your host\n$ sudo cat /sys/class/dmi/id/product_uuid\n```", "```\n// list every listening port\n$ sudo netstat -tulpn | grep LISTEN\n```", "```\n$ sudo apt-get update && sudo apt-get install -y apt-transport-https\n```", "```\n$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\nOK\n```", "```\n$ sudo bash -c 'echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list'\n```", "```\n// on Kubernetes master\n$ sudo apt-get update && sudo apt-get install -y kubelet kubeadm kubectl\n// on Kubernetes node\n$ sudo apt-get update && sudo apt-get install -y kubelet\n```", "```\n$ sudo vim /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\n https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n```", "```\n// on Kubernetes master\n$ sudo yum install -y kubelet kubeadm kubectl\n// on Kubernetes node\n$ sudo yum install -y kubelet\n```", "```\n// take it easy! server connection failed since there is not server running\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.2\", GitCommit:\"81753b10df112992bf51bbc2c2f85208aad78335\", GitTreeState:\"clean\", BuildDate:\"2018-04-27T09:22:21Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nThe connection to the server 192.168.122.101:6443 was refused - did you specify the right host or port?\n```", "```\n// check the state of SELinux, if it has already been disabled, bypass below commands\n$ sestatus\n```", "```\n// disable SELinux through command\n$  sudo setenforce 0\n// or modify the configuration file **$ sudo sed \u2013I 's/ SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux** \n```", "```\n// reboot is required\n$ sudo reboot\n```", "```\n// enable the parameters by setting them to 1\n$ sudo bash -c 'echo \"net.bridge.bridge-nf-call-ip6tables = 1\" > /etc/sysctl.d/k8s.conf'\n$ sudo bash -c 'echo \"net.bridge.bridge-nf-call-iptables = 1\" >> /etc/sysctl.d/k8s.conf'\n// reload the configuration\n$ sudo sysctl --system\n```", "```\n$ sudo systemctl enable kubelet && sudo systemctl start kubelet\n```", "```\n$ sudo kubeadm init\n```", "```\n$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```", "```\n// Your kubectl command works great now\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.2\", GitCommit:\"81753b10df112992bf51bbc2c2f85208aad78335\", GitTreeState:\"clean\", BuildDate:\"2018-04-27T09:22:21Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.2\", GitCommit:\"81753b10df112992bf51bbc2c2f85208aad78335\", GitTreeState:\"clean\", BuildDate:\"2018-04-27T09:10:24Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n```", "```\n// check the status of kubelet\n$ sudo systemctl status kubelet\n...\nActive: active (running) Mon 2018-04-30 18:46:58 EDT; 2min 43s ago\n...\n```", "```\n$ kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml\n```", "```\n$ sudo systemctl enable kubelet && sudo systemctl start kubelet\n```", "```\n// on master node, list the token you have in the cluster\n$ sudo kubeadm token list\nTOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS\nda3a90.9a119695a933a867   6h       2018-05-01T18:47:10-04:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token\n```", "```\n// The master IP is 192.168.122.101, token is da3a90.9a119695a933a867, 6443 is the port of api server.\n$ sudo kubeadm join --token da3a90.9a119695a933a867 192.168.122.101:6443 --discovery-token-unsafe-skip-ca-verification\n```", "```\n// fire kubectl subcommand on master\n$ kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nubuntu01   Ready     master    11h       v1.10.2\nubuntu02   Ready     <none>    26s       v1.10.2\n```", "```\n// rejoining the same cluster\n$ HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //')\n$ sudo kubeadm join --token da3a90.9a119695a933a867 192.168.122.101:6443 --discovery-token-ca-cert-hash sha256:$HASH\n```", "```\n//Use capital V\n$ python -V\nPython 2.7.5\n```", "```\n//use capital V\n$ pip -V\npip 9.0.1 from /Library/Python/2.7/site-packages (python 2.7)\n```", "```\n//this result shows you don't have pip yet\n$ pip -V\n-bash: pip: command not found\n```", "```\n//download pip install script\n$ curl -LO https://bootstrap.pypa.io/get-pip.py\n\n//run get-pip.py by privileged user (sudo)\n$ sudo python get-pip.py \nCollecting pip\n Downloading pip-9.0.1-py2.py3-none-any.whl (1.3MB)\n 100% |################################| 1.3MB 779kB/s \nCollecting wheel\n Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\n 100% |################################| 51kB 1.5MB/s \nInstalling collected packages: pip, wheel\nSuccessfully installed pip-9.0.1 wheel-0.30.0\n\n//now you have pip command\n$ pip -V\npip 9.0.1 from /usr/lib/python2.7/site-packages (python 2.7)\n```", "```\n//ran by privileged user (sudo)\n$ sudo pip install ansible\n```", "```\n$ which ansible\n/usr/bin/ansible\n\n$ ansible --version\nansible 2.4.1.0\n```", "```\n$ sudo pip install netaddr\n```", "```\n//with \u2013q means, quiet output\n$ ssh-keygen -q\n```", "```\n//use ssh-agent to remember your private key and passphrase (if you set)\nansible_machine$ ssh-agent bash\nansible_machine$ ssh-add\nEnter passphrase for /home/saito/.ssh/id_rsa: Identity added: /home/saito/.ssh/id_rsa (/home/saito/.ssh/id_rsa)\n\n//logon from ansible machine to k8s machine which you copied public key\nansible_machine$ ssh 10.128.0.2\nLast login: Sun Nov  5 17:05:32 2017 from 133.172.188.35.bc.googleusercontent.com\nk8s-master-1$\n```", "```\n//download tar.gz format\nansible_machine$ curl -LO https://github.com/kubernetes-incubator/kubespray/archive/v2.5.0.tar.gz\n\n//untar\nansible_machine$ tar zxvf v2.5.0.tar.gz \n\n//it unarchives under kubespray-2.5.0 directory\nansible_machine$ ls -F\nget-pip.py  kubespray-2.5.0/  v2.5.0.tar.gz\n\n//change to kubespray-2.5.0 directory\nansible_machine$ cd kubespray-2.5.0/\n```", "```\n//copy sample to mycluster\nansible_machine$ cp -rfp inventory/sample inventory/mycluster \n//edit hosts.ini\nansible_machine$ vi inventory/mycluster/hosts.ini \n```", "```\nmy-master-1 ansible_ssh_host=10.128.0.2 ansible_user=kirito\nmy-node-1 ansible_ssh_host=10.128.0.4 ansible_user=asuna\n```", "```\nmy-master-1 ansible_ssh_host=10.128.0.2\nmy-node-1 ansible_ssh_host=10.128.0.4 ansible_port=10022\n```", "```\n[ssh_connection]\ncontrol_path = %(directory)s/%%h-%%r\n```", "```\n[ssh_connection]\nforks = 50\ntimeout = 30\n```", "```\n//use \u2013b (become), -i (inventory) and specify cluster.yml as playbook\n$ ansible-playbook -b -i inventory/mycluster/hosts.ini cluster.yml \n```", "```\n//use \u2013b (become), -i (inventory) and \u2013v (verbose)\n$ ansible-playbook -v -b -i inventory/mycluster/hosts.ini cluster.yml\n```", "```\n// get the components status\n$ kubectl get cs\nNAME                 STATUS    MESSAGE              ERROR\ncontroller-manager   Healthy   ok\nscheduler            Healthy   ok\netcd-0               Healthy   {\"health\": \"true\"}\n```", "```\n// check if the master is running\n$ kubectl cluster-info\nKubernetes master is running at https://192.168.122.101:6443\nKubeDNS is running at https://192.168.122.101:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. \n```", "```\n$ kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nubuntu01   Ready     master    20m       v1.10.2\nubuntu02   Ready     <none>    2m        v1.10.2\n```", "```\n$ kubectl run <replication controller name> --image=<image name> --replicas=<number of replicas> [--port=<exposing port>]\n```", "```\n// run a deployment with 2 replicas for the image nginx and expose the container port 80\n$ kubectl run my-first-nginx --image=nginx --replicas=2 --port=80\ndeployment \"my-first-nginx\" created\n```", "```\nError from server (AlreadyExists): deployments.extensions \"my-first-nginx\" already exists\n```", "```\n// get all pods\n$ kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nmy-first-nginx-7dcd87d4bf-jp572   1/1       Running   0          7m\nmy-first-nginx-7dcd87d4bf-ns7h4   1/1       Running   0          7m\n```", "```\n// check the status of your deployment\n$ kubectl get deployment\nNAME             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nmy-first-nginx   2         2         2            2           2m\n```", "```\n// expose port 80 for replication controller named my-first-nginx\n$ kubectl expose deployment my-first-nginx --port=80 --type=LoadBalancer\nservice \"my-first-nginx\" exposed\n```", "```\n// get all services\n$ kubectl get service\nNAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes       ClusterIP      10.96.0.1       <none>        443/TCP        2h\nmy-first-nginx   LoadBalancer   10.102.141.22   <pending>     80:31620/TCP   3m\n```", "```\n// stop deployment named my-first-nginx\n$ kubectl delete deployment my-first-nginx\ndeployment.extensions \"my-first-nginx\" deleted\n\n// stop service named my-first-nginx\n$ kubectl delete service my-first-nginx\nservice \"my-first-nginx\" deleted\n```", "```\n$ kubectl describe service my-first-nginx\nName:                     my-first-nginx\nNamespace:                default\nLabels:                   run=my-first-nginx\nAnnotations:              <none>\nSelector:                 run=my-first-nginx\nType:                     LoadBalancer\nIP:                       10.103.85.175\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  31723/TCP\nEndpoints:                192.168.79.10:80,192.168.79.9:80\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n```", "```\n// curl from service IP\n$ curl 10.103.85.175:80\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n body {\n width: 35em;\n margin: 0 auto;\n font-family: Tahoma, Verdana, Arial, sans-serif;\n }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```", "```\n// curl from endpoint, the content is the same as previous nginx html\n$ curl 192.168.79.10:80\n<!DOCTYPE html>\n<html>\n...\n```", "```\n$ curl http://<clusterIP>\n```", "```\n$ curl http://<nodeIP>:<nodePort>\n```"]
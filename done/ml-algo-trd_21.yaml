- en: '21'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '21'
- en: Generative Adversarial Networks for Synthetic Time-Series Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于合成时间序列数据的生成对抗网络
- en: 'Following the coverage of autoencoders in the previous chapter, this chapter
    introduces a second unsupervised deep learning technique: **generative adversarial
    networks** (**GANs**). As with autoencoders, GANs complement the methods for dimensionality
    reduction and clustering introduced in *Chapter 13*, *Data-Driven Risk Factors
    and Asset Allocation with Unsupervised Learning*.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中介绍了自动编码器后，本章介绍了第二种无监督深度学习技术：**生成对抗网络**（**GAN**）。与自动编码器一样，GAN补充了*第13章*中介绍的降维和聚类方法，即*基于数据的风险因子和无监督学习的资产配置*。
- en: '**GANs** were invented by Goodfellow et al. in 2014\. Yann LeCun has called
    GANs the "most exciting idea in AI in the last ten years." A **GAN** trains two
    neural networks, called the **generator** and **discriminator**, in a competitive
    setting. The generator aims to produce samples that the discriminator is unable
    to distinguish from a given class of training data. The result is a generative
    model capable of producing synthetic samples representative of a certain target distribution
    but artificially and, thus, inexpensively created.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAN**是由Goodfellow等人于2014年发明的。Yann LeCun称GAN为“过去十年人工智能中最激动人心的想法”。**GAN**在竞争环境中训练两个神经网络，称为**生成器**和**鉴别器**。生成器旨在生成鉴别器无法区分的样本，以模拟给定类别的训练数据。结果是一个能够生成代表某个目标分布的合成样本的生成模型，但是这些样本是人工制造的，因此成本低廉。'
- en: GANs have produced an avalanche of research and successful applications in many
    domains. While originally applied to images, Esteban, Hyland, and Rätsch (2017)
    applied GANs to the medical domain to generate **synthetic time-series data**.
    Experiments with financial data ensued (Koshiyama, Firoozye, and Treleaven 2019;
    Wiese et al. 2019; Zhou et al. 2018; Fu et al. 2019) to explore whether GANs can
    generate data that simulates alternative asset price trajectories to train supervised
    or reinforcement algorithms, or to backtest trading strategies. We will replicate
    the Time-Series GAN presented at the 2019 NeurIPS by Yoon, Jarrett, and van der
    Schaar (2019) to illustrate the approach and demonstrate the results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: GAN在许多领域产生了大量的研究和成功的应用。虽然最初应用于图像，但Esteban，Hyland和Rätsch（2017）将GAN应用于医学领域，以生成**合成时间序列数据**。随后进行了与金融数据的实验（Koshiyama，Firoozye和Treleaven
    2019; Wiese等2019; Zhou等2018; Fu等2019），以探索GAN是否能够生成模拟替代资产价格轨迹的数据，以训练监督或强化算法，或者进行交易策略的回测。我们将复制Yoon，Jarrett和van
    der Schaar（2019）在2019年NeurIPS上提出的时间序列GAN，以说明该方法并展示结果。
- en: 'More specifically, in this chapter you will learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在本章中，您将了解以下内容：
- en: How GANs work, why they are useful, and how they can be applied to trading
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的工作原理，它们为何有用，以及如何应用于交易
- en: Designing and training GANs using TensorFlow 2
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2设计和训练GAN
- en: Generating synthetic financial data to expand the inputs available for training
    ML models and backtesting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成金融数据以扩展用于训练ML模型和回测的输入
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: Creating synthetic data with GANs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAN创建合成数据
- en: This book mostly focuses on supervised learning algorithms that receive input
    data and predict an outcome, which we can compare to the ground truth to evaluate
    their performance. Such algorithms are also called **discriminative models** because
    they learn to differentiate between different output values.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要关注接收输入数据并预测结果的监督学习算法，我们可以将其与基本事实进行比较，以评估其性能。这样的算法也被称为**鉴别模型**，因为它们学会区分不同的输出值。
- en: GANs are an instance of **generative models** like the variational autoencoder
    we encountered in the previous chapter. As described there, a generative model
    takes a training set with samples drawn from some distribution *p*[data] and learns
    to represent an estimate *p*[model] of that data-generating distribution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是**生成模型**的一个实例，就像我们在上一章中遇到的变分自动编码器一样。如前所述，生成模型使用从某个分布*p*[data]中抽取的样本的训练集，并学习表示该数据生成分布的估计*p*[model]。
- en: As mentioned in the introduction, GANs are considered one of the most exciting
    recent machine learning innovations because they appear capable of generating
    high-quality samples that faithfully mimic a range of input data. This is very
    attractive given the absence or high cost of labeled data required for supervised
    learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在介绍中提到的，GAN被认为是最令人兴奋的最近的机器学习创新之一，因为它们似乎能够生成高质量的样本，忠实地模仿一系列输入数据。鉴于监督学习所需的标记数据的缺失或高成本，这一点非常吸引人。
- en: GANs have triggered a wave of research that initially focused on the generation
    of surprisingly realistic images. More recently, GAN instances have emerged that
    produce synthetic time series with significant potential for trading since the
    limited availability of historical market data is a key driver of the risk of
    backtest overfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GAN引发了一波研究热潮，最初集中在生成令人惊讶的逼真图像。最近，出现了产生合成时间序列的GAN实例，对于交易具有重要潜力，因为历史市场数据的有限可用性是回测过度拟合风险的关键驱动因素。
- en: In this section, we explain in more detail how generative models and adversarial
    training work and review various GAN architectures. In the next section, we will
    demonstrate how to design and train a GAN using TensorFlow 2\. In the last section,
    we will describe how to adapt a GAN so that it creates synthetic time-series data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地解释生成模型和对抗训练的工作原理，并回顾各种GAN架构。在下一节中，我们将演示如何使用TensorFlow 2设计和训练GAN。在最后一节中，我们将描述如何调整GAN，以便它创建合成时间序列数据。
- en: Comparing generative and discriminative models
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较生成模型和鉴别模型
- en: 'Discriminative models learn how to differentiate among outcomes *y*, given
    input data *X*. In other words, they learn the probability of the outcome given
    the data: *p*(*y* | *X*). Generative models, on the other hand, learn the joint
    distribution of inputs and outcome *p*(*y*, *X*). While generative models can
    be used as discriminative models using Bayes'' rule to compute which class is
    most likely (see *Chapter 10*, *Bayesian ML – Dynamic Sharpe Ratios and Pairs
    Trading*), it often seems preferable to solve the prediction problem directly
    rather than by solving the more general generative challenge first (Ng and Jordan
    2002).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 辨别模型学习如何区分结果*y*，给定输入数据*X*。换句话说，他们学习了在给定数据的情况下结果的概率：*p*(*y* | *X*)。另一方面，生成模型学习输入和结果的联合分布*p*(*y*,
    *X*)。虽然生成模型可以使用贝叶斯定理作为辨别模型，计算哪个类别最有可能（见*第10章*，*贝叶斯机器学习-动态夏普比率和配对交易*），但通常似乎更倾向于直接解决预测问题，而不是首先解决更一般的生成挑战（Ng和Jordan
    2002）。
- en: 'GANs have a generative objective: they produce complex outputs, such as realistic
    images, given simple inputs that can even be random numbers. They achieve this
    by modeling a probability distribution over the possible outputs. This probability
    distribution can have many dimensions, for example, one for each pixel in an image,
    each character or token in a document, or each value in a time series. As a result,
    the model can generate outputs that are very likely representative of the class
    of outputs.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GAN具有生成目标：它们产生复杂的输出，例如逼真的图像，给定甚至可以是随机数字的简单输入。它们通过对可能输出建模概率分布来实现这一点。这个概率分布可以有许多维度，例如图像中的每个像素，文档中的每个字符或标记，或时间序列中的每个值。因此，该模型可以生成非常可能代表输出类别的输出。
- en: Richard Feynman's quote "**What I cannot create, I do not understand**" emphasizes
    that modeling generative distributions is an important step towards more general
    AI and resembles human learning, which succeeds using much fewer samples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理查德·费曼的引言“**我不能创造的东西，我就不了解**”强调了建模生成分布是通往更一般人工智能的重要一步，类似于人类学习，使用更少的样本就能成功。
- en: Generative models have several **use cases** beyond their ability to generate
    additional samples from a given distribution. For example, they can be incorporated
    into model-based **reinforcement learning** (**RL**) algorithms (see the next
    chapter). Generative models can also be applied to time-series data to simulate
    alternative past or possible future trajectories that can be used for planning
    in RL or supervised learning more generally, including for the design of trading
    algorithms. Other use cases include semi-supervised learning where GANs facilitate
    feature matching to assign missing labels with much fewer training samples than
    current approaches.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型除了能够从给定分布生成额外样本之外，还有几个**用例**。例如，它们可以被纳入基于模型的**强化学习**（**RL**）算法（见下一章）。生成模型还可以应用于时间序列数据，以模拟替代的过去或可能的未来轨迹，这可以用于强化学习或更一般的监督学习中，包括设计交易算法。其他用例包括半监督学习，其中GAN促进特征匹配，以比当前方法更少的训练样本分配缺失标签。
- en: Adversarial training – a zero-sum game of trickery
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗训练-一个欺诈的零和游戏
- en: The key innovation of GANs is a new way of learning the data-generating probability
    distribution. The algorithm sets up a competitive, or adversarial game between
    two neural networks called the **generator** and the **discriminator**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的关键创新是学习数据生成概率分布的新方法。该算法在两个称为**生成器**和**鉴别器**的神经网络之间建立了一个竞争性或对抗性游戏。
- en: The generator's goal is to convert random noise input into fake instances of
    a specific class of objects, such as images of faces or stock price time series.
    The discriminator, in turn, aims to differentiate the generator's deceptive output
    from a set of training data containing true samples of the target objects. The
    overall GAN objective is for both networks to get better at their respective tasks
    so that the generator produces outputs that a machine can no longer distinguish
    from the originals (at which point we don't need the discriminator, which is no
    longer necessary, and can discard it).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的目标是将随机噪声输入转换为特定类别对象的假实例，例如脸部图像或股价时间序列。鉴别器则旨在区分生成器的欺骗性输出和包含目标对象真实样本的训练数据集。整体GAN目标是使两个网络在各自的任务上变得更好，以便生成器产生的输出机器无法再与原始输出区分开（在这一点上我们不再需要鉴别器，它不再必要，可以丢弃它）。
- en: '*Figure 21.1* illustrates adversarial training using a generic GAN architecture
    designed to generate images. We assume the generator uses a deep CNN architecture
    (such as the VGG16 example from *Chapter 18*, *CNNs for Financial Time Series
    and Satellite Images*) that is reversed just like the decoder part of the convolutional
    autoencoder we discussed in the previous chapter. The generator receives an input
    image with random pixel values and produces a *fake* output image that is passed
    on to the discriminator network, which uses a mirrored CNN architecture. The discriminator
    network also receives *real* samples that represent the target distribution and
    predicts the probability that the input is *real*, as opposed to *fake*. Learning
    takes place by backpropagating the gradients of the discriminator and generator
    losses to the respective network''s parameters:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图21.1*说明了使用通用GAN架构进行对抗训练，旨在生成图像。我们假设生成器使用深度CNN架构（例如*第18章*中的VGG16示例，用于金融时间序列和卫星图像的CNNs）,它被反转，就像我们在上一章中讨论的卷积自编码器的解码器部分一样。生成器接收具有随机像素值的输入图像，并产生传递给鉴别器网络的*假*输出图像，鉴别器网络使用镜像CNN架构。鉴别器网络还接收代表目标分布的*真实*样本，并预测输入是*真实*还是*假*的概率。学习是通过将鉴别器和生成器损失的梯度反向传播到各自网络的参数来进行的：'
- en: '![](img/B15439_21_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_01.png)'
- en: 'Figure 21.1: GAN architecture'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.1：GAN架构
- en: The recent GAN Lab is a great interactive tool inspired by TensorFlow Playground,
    which allows the user to design GANs and visualize various aspects of the learning
    process and performance over time (see resource links on GitHub).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的GAN Lab是一个受TensorFlow Playground启发的出色的交互式工具，允许用户设计GAN并可视化学习过程和性能随时间的各个方面（请参阅GitHub上的资源链接）。
- en: The rapid evolution of the GAN architecture zoo
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN架构动物园的快速演变
- en: Since the publication of the paper by Goodfellow et al. in 2014, GANs have attracted
    an enormous amount of interest and triggered a corresponding flurry of research.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年Goodfellow等人发表了该论文以来，GAN已经吸引了大量的兴趣，并引发了相应的研究热潮。
- en: The bulk of this work has refined the original architecture to adapt it to different
    domains and tasks, as well as expanding it to include additional information and
    create conditional GANs. Additional research has focused on improving methods
    for the challenging training process, which requires achieving a stable game-theoretic
    equilibrium between two networks, each of which can be tricky to train on its
    own.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的大部分工作是对原始架构进行改进，以使其适应不同的领域和任务，并将其扩展以包括额外的信息并创建条件GANs。额外的研究集中在改进具有挑战性的训练过程的方法，这需要在两个网络之间实现稳定的博弈均衡，每个网络本身都可能很难训练。
- en: The GAN landscape has become more diverse than we can cover here; see Creswell
    et al. (2018) and Pan et al. (2019) for recent surveys, and Odena (2019) for a
    list of open questions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的发展已经变得比我们在这里可以涵盖的更加多样化；请参阅Creswell等人（2018）和Pan等人（2019）进行最新调查，以及Odena（2019）列出的一系列开放问题。
- en: Deep convolutional GANs for representation learning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于表示学习的深度卷积GAN
- en: '**Deep convolutional GANs** (**DCGANs**) were motivated by the successful application
    of CNNs to supervised learning for grid-like data (Radford, Metz, and Chintala
    2016). The architecture pioneered the use of GANs for unsupervised learning by
    developing a feature extractor based on adversarial training. It is also easier
    to train and generates higher-quality images. It is now considered a baseline
    implementation, with numerous open source examples available (see references on
    GitHub).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度卷积GANs**（**DCGANs**）受到CNN成功应用于网格数据的监督学习的启发（Radford，Metz和Chintala 2016）。该架构通过开发基于对抗训练的特征提取器，开创了使用GAN进行无监督学习的先河。它也更容易训练并生成更高质量的图像。现在它被认为是一个基准实现，有许多开源示例可用（请参阅GitHub上的参考资料）。'
- en: A DCGAN network takes uniformly distributed random numbers as input and outputs
    a color image with a resolution of 64×64 pixels. As the input changes incrementally,
    so do the generated images. The network consists of standard CNN components, including
    deconvolutional layers that reverse convolutional layers as in the convolutional
    autoencoder example in the previous chapter, or fully connected layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个DCGAN网络以均匀分布的随机数作为输入，并输出分辨率为64×64像素的彩色图像。随着输入的逐渐变化，生成的图像也会发生变化。网络由标准的CNN组件组成，包括反卷积层，这些层与上一章中的卷积自编码器示例中的卷积层相反，或者全连接层。
- en: The authors experimented exhaustively and made several recommendations, such
    as the use of batch normalization and ReLU activations in both networks. We will
    explore a TensorFlow implementation later in this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者进行了详尽的实验，并提出了一些建议，例如在两个网络中都使用批量归一化和ReLU激活。我们将在本章后面探讨一个TensorFlow实现。
- en: Conditional GANs for image-to-image translation
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于图像到图像转换的条件GANs
- en: '**Conditional GANs** (**cGANs**) introduce additional label information into
    the training process, resulting in better quality and some control over the output.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件GANs**（**cGANs**）将额外的标签信息引入训练过程中，从而提高了输出的质量并对输出结果进行了一定的控制。'
- en: cGANs alter the baseline architecture displayed previously in *Figure 21.1*
    by adding a third input to the discriminator that contains class labels. These
    labels, for example, could convey gender or hair color information when generating
    images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: cGANs通过向鉴别器添加第三个输入来改变先前显示的基线架构*图21.1*，该输入包含类标签。例如，当生成图像时，这些标签可以传达性别或头发颜色信息。
- en: Extensions include the **generative adversarial what-where network** (**GAWWN**;
    Reed et al. 2016), which uses bounding box information not only to generate synthetic
    images but also to place objects at a given location.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展包括**生成对抗性what-where网络**（**GAWWN**；Reed等人2016），它不仅使用边界框信息生成合成图像，还将对象放置在给定位置。
- en: GAN applications to images and time-series data
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN应用于图像和时间序列数据
- en: Alongside a large variety of extensions and modifications of the original architecture,
    numerous applications to images, as well as sequential data like speech and music,
    have emerged. Image applications are particularly diverse, ranging from image
    blending and super-resolution to video generation and human pose identification.
    Furthermore, GANs have been used to improve supervised learning performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始架构的大量扩展和修改之外，还出现了许多应用于图像以及语音和音乐等序列数据的应用。图像应用特别多样，从图像混合和超分辨率到视频生成和人体姿势识别。此外，GAN已被用于提高监督学习的性能。
- en: We will look at a few salient examples and then take a closer look at applications
    to time-series data that may become particularly relevant to algorithmic trading
    and investment. See Alqahtani, Kavakli-Thorne, and Kumar (2019) for a recent survey
    and GitHub references for additional resources.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一些显著的例子，然后更详细地研究应用于时间序列数据的应用，这可能对算法交易和投资特别相关。有关最新调查和GitHub参考资料，请参阅Alqahtani，Kavakli-Thorne和Kumar（2019）。
- en: CycleGAN – unpaired image-to-image translation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CycleGAN - 无配对图像到图像的转换
- en: Supervised image-to-image translation aims to learn a mapping between aligned
    input and output images. CycleGAN solves this task when paired images are not
    available and transforms images from one domain to match another.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 监督图像到图像的转换旨在学习对齐输入和输出图像之间的映射关系。当配对图像不可用并且需要将图像从一个域转换为另一个域时，CycleGAN解决了这个任务。
- en: Popular examples include the synthetic "painting" of horses as zebras and vice
    versa. It also includes the transfer of styles, by generating a realistic sample
    of an impressionistic print from an arbitrary landscape photo (Zhu et al. 2018).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的例子包括将马的合成“绘画”成斑马，反之亦然。它还包括通过从任意风景照片生成印象派印刷的逼真样本来转移风格（Zhu等，2018年）。
- en: StackGAN – text-to-photo image synthesis
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: StackGAN-文本到照片图像合成
- en: One of the earlier applications of GANs to domain-transfer is the generation
    of images based on text. **Stacked GAN**, often shortened to **StackGAN**, uses
    a sentence as input and generates multiple images that match the description.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: GANs较早的一个应用是基于文本生成图像。**Stacked GAN**，通常缩写为**StackGAN**，使用一个句子作为输入，并生成与描述匹配的多个图像。
- en: The architecture operates in two stages, where the first stage yields a low-resolution
    sketch of shape and colors, and the second stage enhances the result to a high-resolution
    image with photorealistic details (Zhang et al. 2017).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构分为两个阶段，第一阶段产生形状和颜色的低分辨率草图，第二阶段将结果增强到具有逼真细节的高分辨率图像（Zhang等，2017年）。
- en: SRGAN – photorealistic single image super-resolution
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SRGAN-逼真的单图像超分辨率
- en: Super-resolution aims at producing higher-resolution photorealistic images from
    low-resolution input. GANs applied to this task have deep CNN architectures that
    use batch normalization, ReLU, and skip connection as encountered in ResNet (see
    *Chapter 18*, *CNNs for Financial Time Series and Satellite Images*) to produce
    impressive results that are already finding commercial applications (Ledig et
    al. 2017).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 超分辨率旨在从低分辨率输入产生更高分辨率的逼真图像。应用于此任务的GAN具有使用批量归一化、ReLU和跳跃连接的深度CNN架构，这些都在ResNet中遇到（参见*第18章*，*金融时间序列和卫星图像的CNNs*），以产生令人印象深刻的结果，已经找到商业应用（Ledig等，2017年）。
- en: Synthetic time series with recurrent conditional GANs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具有循环条件GAN的合成时间序列
- en: '**Recurrent GANs** (**RGANs**) and **recurrent conditional GANs** (**RCGANs**)
    are two model architectures that aim to synthesize realistic real-valued multivariate
    time series (Esteban, Hyland, and Rätsch 2017). The authors target applications
    in the medical domain, but the approach could be highly valuable to overcome the
    limitations of historical market data.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Recurrent GANs**（**RGANs**）和**recurrent conditional GANs**（**RCGANs**）是两种旨在合成逼真的实值多变量时间序列的模型架构（Esteban，Hyland和Rätsch，2017年）。作者们针对医学领域的应用，但这种方法可能对克服历史市场数据的局限性非常有价值。'
- en: RGANs rely on **recurrent neural networks** (**RNNs**) for the generator and
    the discriminator. RCGANs add auxiliary information in the spirit of cGANs (see
    the previous *Conditional GANs for image-to-image translation* section).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: RGANs依赖于**循环神经网络**（**RNNs**）用于生成器和鉴别器。RCGANs添加辅助信息，符合cGANs的精神（参见前面的*图像到图像翻译的条件GANs*部分）。
- en: The authors succeed in generating visually and quantitatively compelling realistic
    samples. Furthermore, they evaluate the quality of the synthetic data, including
    synthetic labels, by using it to train a model with only minor degradation of
    the predictive performance on a real test set. The authors also demonstrate the
    successful application of RCGANs to an early warning system using a medical dataset
    of 17,000 patients from an intensive care unit. Hence, the authors illustrate
    that RCGANs are capable of generating time-series data useful for supervised training.
    We will apply this approach to financial market data this chapter in the *TimeGAN
    – adversarial training for synthetic financial data* section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者成功地生成了视觉上和数量上令人信服的逼真样本。此外，他们通过使用合成数据来训练模型，包括合成标签，评估了合成数据的质量，只有轻微的降低了对真实测试集的预测性能。作者还展示了RCGANs成功应用于使用来自重症监护病房的1.7万名患者的医学数据的早期预警系统。因此，作者说明了RCGANs能够生成对监督训练有用的时间序列数据。我们将在本章的*TimeGAN-用于合成金融数据的对抗训练*部分中应用这种方法到金融市场数据。
- en: How to build a GAN using TensorFlow 2
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用TensorFlow 2构建GAN
- en: To illustrate the implementation of a GAN using Python, we will use the DCGAN
    example discussed earlier in this section to synthesize images from the Fashion-MNIST
    dataset that we first encountered in *Chapter 13*, *Data-Driven Risk Factors and
    Asset Allocation with Unsupervised Learning*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明使用Python实现GAN，我们将使用本节早期讨论的DCGAN示例，从我们在*第13章*中首次遇到的Fashion-MNIST数据集中合成图像。
- en: See the notebook `deep_convolutional_generative_adversarial_network` for implementation
    details and references.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有关实施细节和参考资料，请参阅笔记本`deep_convolutional_generative_adversarial_network`。
- en: Building the generator network
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建生成器网络
- en: 'Both generator and discriminator use a deep CNN architecture along the lines
    illustrated in *Figure 20.1*, but with fewer layers. The generator uses a fully
    connected input layer, followed by three convolutional layers, as defined in the
    following `build_generator()` function, which returns a Keras model instance:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和鉴别器都使用深度CNN架构，沿着*图20.1*中所示的线路，但层数较少。生成器使用全连接输入层，然后是三个卷积层，如下所定义的`build_generator()`函数，它返回一个Keras模型实例：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The generator accepts 100 one-dimensional random values as input, and it produces
    images that are 28 pixels wide and high and, thus, contain 784 data points.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受100个一维随机值作为输入，并产生宽高为28像素的图像，因此包含784个数据点。
- en: A call to the `.summary()` method of the model returned by this function shows
    that this network has over 2.3 million parameters (see the notebook for details,
    including a visualization of the generator output prior to training).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用此函数返回的模型的`.summary()`方法，可以看到该网络有超过230万个参数（有关详细信息，请参阅笔记本，包括在训练之前可视化生成器输出）。
- en: Creating the discriminator network
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建鉴别器网络
- en: 'The discriminator network uses two convolutional layers that translate the
    input received from the generator into a single output value. The model has around
    212,000 parameters:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络使用两个卷积层，将从生成器接收的输入转换为单个输出值。该模型有大约212,000个参数：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure 21.2* depicts how the random input flows from the generator to the
    discriminator, as well as the input and output shapes of the various network components:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图21.2*描述了随机输入如何从生成器流向鉴别器，以及各种网络组件的输入和输出形状：'
- en: '![](img/B15439_21_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_02.png)'
- en: 'Figure 21.2: DCGAN TensorFlow 2 model architecture'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.2：DCGAN TensorFlow 2模型架构
- en: Setting up the adversarial training process
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置对抗训练过程
- en: 'Now that we have built the generator and the discriminator models, we will
    design and execute the adversarial training process. To this end, we will define
    the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了生成器和鉴别器模型，我们将设计并执行对抗训练过程。为此，我们将定义以下内容：
- en: The loss functions for both models that reflect their competitive interaction
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反映它们竞争性互动的两个模型的损失函数
- en: A single training step that runs the backpropagation algorithm
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行反向传播算法的单个训练步骤
- en: The training loop that repeats the training step until the model performance
    meets our expectations
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复训练步骤的训练循环，直到模型性能符合我们的期望
- en: Defining the generator and discriminator loss functions
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义生成器和鉴别器损失函数
- en: The generator loss reflects the discriminator's decision regarding the fake
    input. It will be low if the discriminator mistakes an image produced by the generator
    for a real image, and high otherwise; we will define the interaction between both
    models when we create the training step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失反映了鉴别器对假输入的决定。如果鉴别器误将生成器生成的图像误认为是真实图像，则损失将很低，否则将很高；我们将在创建训练步骤时定义两个模型之间的互动。
- en: 'The generator loss is measured by the binary cross-entropy loss function as
    follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失由二元交叉熵损失函数测量如下：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The discriminator receives both real and fake images as input. It computes
    a loss for each and attempts to minimize the sum with the goal of accurately recognizing
    both types of inputs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器接收真实和假图像作为输入。它为每个计算损失，并试图最小化总和，以准确识别两种类型的输入：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To train both models, we assign each an Adam optimizer with a learning rate
    lower than the default:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练两个模型，我们为每个模型分配一个Adam优化器，学习率低于默认值：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The core – designing the training step
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心-设计训练步骤
- en: 'Each training step implements one round of stochastic gradient descent using
    the Adam optimizer. It consists of five steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练步骤使用Adam优化器实现一轮随机梯度下降。它包括五个步骤：
- en: Providing the minibatch inputs to each model
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个模型提供小批量输入
- en: Getting the models' outputs for the current weights
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取当前权重的模型输出
- en: Computing the loss given the models' objective and output
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据模型的目标和输出计算损失
- en: Obtaining the gradients for the loss with respect to each model's weights
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取损失相对于每个模型权重的梯度
- en: Applying the gradients according to the optimizer's algorithm
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据优化器的算法应用梯度
- en: 'The function `train_step()` carries out these five steps. We use the `@tf.function`
    decorator to speed up execution by compiling it to a TensorFlow operation rather
    than relying on eager execution (see the TensorFlow documentation for details):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`train_step()`执行这五个步骤。我们使用`@tf.function`装饰器通过将其编译为TensorFlow操作来加速执行，而不是依赖急切执行（有关详细信息，请参阅TensorFlow文档）：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Putting it together – the training loop
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将其整合在一起-训练循环
- en: The training loop is very straightforward to implement once we have the training
    step properly defined. It consists of a simple `for` loop, and during each iteration,
    we pass a new batch of real images to the training step. We also will sample some
    synthetic images and occasionally save the model weights.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们正确定义了训练步骤，训练循环就非常容易实现。它由一个简单的`for`循环组成，在每次迭代期间，我们将一批新的真实图像传递给训练步骤。我们还将抽样一些合成图像，并偶尔保存模型权重。
- en: 'Note that we track progress using the `tqdm` package, which shows the percentage
    complete during training:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`tqdm`包跟踪进度，该包在训练期间显示完成的百分比。
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Evaluating the results
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'After 100 epochs that only take a few minutes, the synthetic images created
    from random noise clearly begin to resemble the originals, as you can see in *Figure
    21.3* (see the notebook for the best visual quality):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 经过100个仅需几分钟的时代，从随机噪声中创建的合成图像明显开始类似于原始图像，如*图21.3*中所示（请参阅笔记本以获得最佳视觉质量）：
- en: '![](img/B15439_21_03.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_03.png)'
- en: 'Figure 21.3: A sample of synthetic Fashion-MNIST images'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.3：合成时尚MNIST图像的样本
- en: The notebook also creates a dynamic GIF image that visualizes how the quality
    of the synthetic images improves during training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本还创建了一个动态的GIF图像，可视化了合成图像在训练过程中的质量改善情况。
- en: Now that we understand how to build and train a GAN using TensorFlow 2, we will
    move on to a more complex example that produces synthetic time series from stock
    price data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何使用TensorFlow 2构建和训练GAN，我们将转向一个更复杂的例子，从股价数据生成合成时间序列。
- en: TimeGAN for synthetic financial data
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于合成金融数据的TimeGAN
- en: Generating synthetic time-series data poses specific challenges above and beyond
    those encountered when designing GANs for images. In addition to the distribution
    over variables at any given point, such as pixel values or the prices of numerous
    stocks, a generative model for time-series data should also learn the temporal
    dynamics that shape how one sequence of observations follows another. (Refer also
    to the discussion in *Chapter 9*, *Time-Series Models for Volatility Forecasts
    and Statistical Arbitrage*).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 生成合成时间序列数据在设计用于图像的GAN时面临特定挑战之外还有其他挑战。除了在任何给定点的变量分布，如像素值或大量股票的价格之外，时间序列数据的生成模型还应该学习塑造一个观察序列如何跟随另一个的时间动态。（还请参阅*第9章*，*用于波动率预测和统计套利的时间序列模型*中的讨论）。
- en: Very recent and promising research by Yoon, Jarrett, and van der Schaar, presented
    at NeurIPS in December 2019, introduces a novel **time-series generative adversarial
    network** (**TimeGAN**) framework that aims to account for temporal correlations
    by combining supervised and unsupervised training. The model learns a time-series
    embedding space while optimizing both supervised and adversarial objectives, which
    encourage it to adhere to the dynamics observed while sampling from historical
    data during training. The authors test the model on various time series, including
    historical stock prices, and find that the quality of the synthetic data significantly
    outperforms that of available alternatives.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Yoon，Jarrett和van der Schaar在2019年12月的NeurIPS上介绍了最新和有前途的研究，他们引入了一个新颖的时间序列生成对抗网络（TimeGAN）框架，旨在通过结合监督和无监督训练来考虑时间相关性。该模型在优化监督和对抗目标的同时学习时间序列嵌入空间，这些目标鼓励模型在训练期间从历史数据中采样时遵循观察到的动态。作者们在各种时间序列上测试了模型，包括历史股票价格，并发现合成数据的质量明显优于现有的替代品。
- en: In this section, we will outline how this sophisticated model works, highlight
    key implementation steps that build on the previous DCGAN example, and show how
    to evaluate the quality of the resulting time series. Please see the paper for
    additional information.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述这个复杂模型的工作原理，突出强调建立在先前DCGAN示例上的关键实施步骤，并展示如何评估生成的时间序列的质量。请参阅论文获取更多信息。
- en: Learning to generate data across features and time
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习跨特征和时间生成数据
- en: A successful generative model for time-series data needs to capture both the
    cross-sectional distribution of features at each point in time and the longitudinal
    relationships among these features over time. Expressed in the image context we
    just discussed, the model needs to learn not only what a realistic image looks
    like, but also how one image evolves from the previous as in a video.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的时间序列数据生成模型需要捕捉每个时间点上特征的横截面分布以及这些特征随时间的纵向关系。用我们刚讨论过的图像上下文来表达，该模型不仅需要学习真实图像的外观，还需要学习一个图像如何从前一个图像演变而来，就像视频中一样。
- en: Combining adversarial and supervised training
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合对抗和监督训练
- en: As mentioned in the first section, prior attempts at generating time-series
    data, like RGANs and RCGANs, relied on RNNs (see *Chapter 19*, *RNNs for Multivariate
    Time Series and Sentiment Analysis*) in the roles of generator and discriminator.
    TimeGAN explicitly incorporates the autoregressive nature of time series by combining
    the **unsupervised adversarial loss** on both real and synthetic sequences familiar
    from the DCGAN example with a **stepwise supervised loss** with respect to the
    original data. The goal is to reward the model for learning the **distribution
    over transitions** from one point in time to the next that are present in the
    historical data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第一节中提到的，以前生成时间序列数据的尝试，如RGAN和RCGAN，依赖于RNNs（参见*第19章*，*用于多变量时间序列和情感分析的RNNs*）作为生成器和鉴别器的角色。TimeGAN通过将DCGAN示例中的**无监督对抗损失**与**关于原始数据的逐步监督损失**相结合，明确地结合了时间序列的自回归性质。其目标是奖励模型学习历史数据中存在的从一个时间点到下一个时间点的**转换分布**。
- en: Furthermore, TimeGAN includes an embedding network that maps the time-series
    features to a lower-dimensional latent space to reduce the complexity of the adversarial
    space. The motivation is to capture the drivers of temporal dynamics that often
    have lower dimensionality. (Refer also to the discussions of manifold learning
    in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with Unsupervised
    Learning* and nonlinear dimensionality reduction in *Chapter 20*, *Autoencoders
    for Conditional Risk Factors and Asset Pricing*).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TimeGAN还包括一个嵌入网络，将时间序列特征映射到较低维度的潜在空间，以减少对抗空间的复杂性。动机是捕捉通常具有较低维度的时间动态的驱动因素。（还可以参考*第13章*中的流形学习讨论，*使用无监督学习进行数据驱动风险因素和资产配置*以及*第20章*中的非线性降维讨论，*用于条件风险因素和资产定价的自编码器*）。
- en: A key element of the TimeGAN architecture is that both the generator and the
    embedding (or autoencoder) network are responsible for minimizing the supervised
    loss that measures how well the model learns the dynamic relationship. As a result,
    the model learns a latent space conditioned on facilitating the generator's task
    to faithfully reproduce the temporal relationships observed in the historical
    data. In addition to time-series data, the model can also process static data
    that does not change or changes less frequently over time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN架构的一个关键元素是生成器和嵌入（或自编码器）网络都负责最小化衡量模型学习动态关系的监督损失。因此，模型学习了一个潜在空间，条件是促进生成器忠实地再现历史数据中观察到的时间关系。除了时间序列数据，该模型还可以处理静态数据，即随时间不变或变化较少的数据。
- en: The four components of the TimeGAN architecture
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TimeGAN架构的四个组件
- en: 'The TimeGAN architecture combines an adversarial network with an autoencoder
    and thus has four network components, as depicted in *Figure 21.4*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN架构将对抗网络与自编码器结合在一起，因此具有四个网络组件，如*图21.4*所示：
- en: '**Autoencoder**: embedding and recovery networks'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自编码器**：嵌入和恢复网络'
- en: '**Adversarial network**: sequence generator and sequence discriminator components'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对抗网络**：序列生成器和序列鉴别器组件'
- en: The authors emphasize the **joint training** of the autoencoder and the adversarial
    networks by means of **three different loss functions**. The **reconstruction
    loss** optimizes the autoencoder, the **unsupervised loss** trains the adversarial
    net, and the **supervised loss** enforces the temporal dynamics. As a result of
    this key insight, the TimeGAN simultaneously learns to encode features, generate
    representations, and iterate across time. More specifically, the embedding network
    creates the latent space, the adversarial network operates within this space,
    and supervised loss synchronizes the latent dynamics of both real and synthetic
    data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作者强调了通过三种不同的损失函数对自动编码器和对抗网络进行联合训练的重要性。重构损失优化自动编码器，无监督损失训练对抗网络，监督损失强制时间动态。由于这一关键观点，TimeGAN同时学习编码特征、生成表示，并在时间上迭代。更具体地说，嵌入网络创建潜在空间，对抗网络在该空间内运行，监督损失同步真实和合成数据的潜在动态。
- en: '![](img/B15439_21_04.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_04.png)'
- en: 'Figure 21.4: The components of the TimeGAN architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.4：TimeGAN架构的组件
- en: The **embedding and recovery** components of the autoencoder map the feature
    space into the latent space and vice versa. This facilitates the learning of the
    temporal dynamics by the adversarial network, which learns in a lower-dimensional
    space. The authors implement the embedding and recovery network using a stacked
    RNN and a feedforward network. However, these choices can be flexibly adapted
    to the task at hand as long as they are autoregressive and respect the temporal
    order of the data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的嵌入和恢复组件将特征空间映射到潜在空间，反之亦然。这有助于通过对抗网络学习时间动态，后者在较低维空间中学习。作者使用堆叠的RNN和前馈网络实现了嵌入和恢复网络。然而，只要它们是自回归的并且尊重数据的时间顺序，这些选择可以灵活地适应手头的任务。
- en: The **generator and the discriminator** elements of the adversarial network
    differ from the DCGAN not only because they operate on sequential data but also
    because the synthetic features are generated in the latent space that the model
    learns simultaneously. The authors chose an RNN as the generator and a bidirectional
    RNN with a feedforward output layer for the discriminator.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络的生成器和鉴别器元素不仅因为它们在顺序数据上操作而与DCGAN不同，而且因为合成特征是在模型同时学习的潜在空间中生成的。作者选择了RNN作为生成器，选择了双向RNN和前馈输出层作为鉴别器。
- en: Joint training of an autoencoder and adversarial network
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动编码器和对抗网络的联合训练
- en: 'The three loss functions displayed in *Figure 21.4* drive the joint optimization
    of the network elements just described while training on real and randomly generated
    time series. In more detail, they aim to accomplish the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.4中显示的三个损失函数驱动了对实际和随机生成的时间序列进行联合优化的网络元素的训练。更详细地说，它们旨在实现以下目标：
- en: The **reconstruction loss** is familiar from our discussion of autoencoders
    in *Chapter 20*, *Autoencoders for Conditional Risk Factors and Asset Pricing*;
    it compares how well the reconstruction of the encoded data resembles the original.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构损失在我们在第20章“条件风险因素和资产定价”中讨论的自动编码器中是熟悉的；它比较了编码数据的重构与原始数据的相似程度。
- en: The **unsupervised loss** reflects the competitive interaction between the generator
    and the discriminator described in the DCGAN example; while the generator aims
    to minimize the probability that the discriminator classifies its output as fake,
    the discriminator aims to optimize the correct classification or real and fake
    inputs.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督损失反映了在DCGAN示例中描述的生成器和鉴别器之间的竞争性互动；生成器旨在最小化鉴别器将其输出分类为伪造的概率，而鉴别器旨在优化对真实和伪造输入的正确分类。
- en: The **supervised loss** captures how well the generator approximates the actual
    next time step in latent space when receiving encoded real data for the prior
    sequence.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督损失捕捉了生成器在接收编码的真实数据进行先前序列的实际下一个时间步的潜在空间中的近似程度。
- en: 'Training takes place in **three phases**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分为三个阶段：
- en: Training the autoencoder on real time series to optimize reconstruction
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在真实时间序列上训练自动编码器以优化重构
- en: Optimizing the supervised loss using real time series to capture the temporal
    dynamics of the historical data
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化监督损失，使用真实时间序列捕捉历史数据的时间动态
- en: Jointly training the four components while minimizing all three loss functions
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时训练四个组件，同时最小化所有三个损失函数
- en: TimeGAN includes several **hyperparameters** used to weigh the components of
    composite loss functions; however, the authors find the network to be less sensitive
    to these settings than one might expect given the notorious difficulties of GAN
    training. In fact, they **do not discover significant challenges during training**
    and suggest that the embedding task serves to regularize adversarial learning
    because it reduces its dimensionality while the supervised loss constrains the
    stepwise dynamics of the generator.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN包括几个用于权衡复合损失函数组件的超参数；然而，作者发现网络对这些设置的敏感性要小于人们可能期望的，鉴于GAN训练的困难。事实上，他们在训练过程中并没有发现重大挑战，并建议嵌入任务有助于规范对抗学习，因为它降低了其维度，而监督损失约束了生成器的逐步动态。
- en: We now turn to the TimeGAN implementation using TensorFlow 2; see the paper
    for an in-depth explanation of the math and methodology of the approach.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用TensorFlow 2实现TimeGAN；请参阅论文以深入了解该方法的数学和方法论。
- en: Implementing TimeGAN using TensorFlow 2
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2实现TimeGAN
- en: 'In this section, we will implement the TimeGAN architecture just described.
    The authors provide sample code using TensorFlow 1 that we will port to TensorFlow
    2\. Building and training TimeGAN requires several steps:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现刚刚描述的TimeGAN架构。作者提供了使用TensorFlow 1的示例代码，我们将把它移植到TensorFlow 2。构建和训练TimeGAN需要几个步骤：
- en: Selecting and preparing real and random time series inputs
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择和准备真实和随机时间序列输入
- en: Creating the key TimeGAN model components
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建关键的TimeGAN模型组件
- en: Defining the various loss functions and training steps used during the three
    training phases
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义在三个训练阶段中使用的各种损失函数和训练步骤
- en: Running the training loops and logging the results
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行训练循环并记录结果
- en: Generating synthetic time series and evaluating the results
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成合成时间序列并评估结果
- en: We'll walk through the key items for each of these steps; please refer to the
    notebook `TimeGAN_TF2` for the code examples in this section (unless otherwise
    noted), as well as additional implementation details.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步介绍每个步骤的关键项目；请参考笔记本`TimeGAN_TF2`中的代码示例（除非另有说明），以及其他实现细节。
- en: Preparing the real and random input series
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备真实和随机输入系列
- en: The authors demonstrate the applicability of TimeGAN to financial data using
    15 years of daily Google stock prices downloaded from Yahoo Finance with six features,
    namely open, high, low, close and adjusted close price, and volume. We'll instead
    use close to 20 years of adjusted close prices for six different tickers because
    it introduces somewhat higher variability. We will follow the original paper in
    targeting synthetic series with 24 time steps.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用从Yahoo Finance下载的15年每日谷歌股票价格数据来演示TimeGAN对金融数据的适用性，其中包括开盘价、最高价、最低价、收盘价、调整后的收盘价和成交量等六个特征。我们将使用六种不同股票的近20年调整后的收盘价，因为这样会引入更高的变化性。我们将遵循原始论文，以目标合成系列具有24个时间步长。
- en: Among the stocks with the longest history in the Quandl Wiki dataset are those
    displayed in normalized format, that is, starting at 1.0, in *Figure 21.5*. We
    retrieve the adjusted close from 2000-2017 and obtain over 4,000 observations.
    The correlation coefficient among the series ranges from 0.01 for GE and CAT to
    0.94 for DIS and KO.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Quandl Wiki数据集中历史最悠久的股票中，有一些以标准化格式显示，即从1.0开始，在*图21.5*中显示。我们从2000年至2017年获取调整后的收盘价，并获得超过4,000个观察结果。这些系列之间的相关系数从GE和CAT的0.01到DIS和KO的0.94不等。
- en: '![](img/B15439_21_05.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_05.png)'
- en: 'Figure 21.5: The TimeGAN input—six real stock prices series'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.5：TimeGAN输入 - 六个真实股票价格系列
- en: 'We scale each series to the range [0, 1] using scikit-learn''s `MinMaxScaler`
    class, which we will later use to rescale the synthetic data:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用scikit-learn的`MinMaxScaler`类将每个系列缩放到范围[0,1]，稍后我们将用它来重新缩放合成数据：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the next step, we create rolling windows containing overlapping sequences
    of 24 consecutive data points for the six series:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建包含重叠的24个连续数据点的滚动窗口，用于这六个系列：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then create a `tf.data.Dataset` instance from the list of `NumPy` arrays,
    ensure the data gets shuffled while training, and set a batch size of 128:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从`NumPy`数组列表中创建一个`tf.data.Dataset`实例，确保在训练时对数据进行洗牌，并设置批量大小为128：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We also need a random time-series generator that produces simulated data with
    24 observations on the six series for as long as the training continues.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个随机时间序列生成器，它会在六个系列上产生包含24个观察值的模拟数据，直到训练结束。
- en: 'To this end, we will create a generator that draws the requisite data uniform
    at random and feeds the result into a second `tf.data.Datase`t instance. We set
    this dataset to produce batches of the desired size and to repeat the process
    for as long as necessary:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将创建一个生成器，以随机均匀地绘制所需数据，并将结果馈送到第二个`tf.data.Dataset`实例中。我们设置这个数据集以产生所需大小的批量，并重复这个过程直到必要时为止：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We'll now proceed to define and instantiate the TimeGAN model components.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续定义和实例化TimeGAN模型组件。
- en: Creating the TimeGAN model components
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建TimeGAN模型组件
- en: We'll now create the two autoencoder components and the two adversarial network
    elements, as well as the supervisor that encourages the generator to learn the
    temporal dynamic of the historical price series.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建两个自动编码器组件和两个对抗网络元素，以及鼓励生成器学习历史价格序列的监督器。
- en: 'We will follow the authors'' sample code in creating RNNs with three hidden
    layers, each with 24 GRU units, except for the supervisor, which uses only two
    hidden layers. The following `make_rnn` function automates the network creation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循作者的示例代码，创建具有三个隐藏层的RNN，每个隐藏层有24个GRU单元，除了监督器只使用两个隐藏层。以下的`make_rnn`函数自动创建网络：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `autoencoder` consists of the `embedder` and the recovery networks that
    we instantiate here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`自动编码器`由我们在这里实例化的`嵌入器`和`恢复网络`组成：'
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then create the generator, the discriminator, and the supervisor like so:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建生成器、鉴别器和监督器如下：
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We also define two generic loss functions, namely `MeanSquaredError` and `BinaryCrossEntropy`,
    which we will use later to create the various specific loss functions during the
    three phases:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了两个通用损失函数，即`均方误差`和`二元交叉熵`，稍后我们将使用它们来创建三个阶段的各种特定损失函数：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now it's time to start the training process.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始训练过程了。
- en: Training phase 1 – autoencoder with real data
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段1 - 使用真实数据的自动编码器
- en: 'The autoencoder integrates the embedder and the recovery functions, as we saw
    in the previous chapter:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器集成了嵌入器和恢复函数，就像我们在上一章中看到的那样：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It has 21,054 parameters. We will now instantiate the optimizer for this training
    phase and define the training step. It follows the pattern introduced with the
    DCGAN example, using `tf.GradientTape` to record the operations that generate
    the reconstruction loss. This allows us to rely on the automatic differentiation
    engine to obtain the gradients with respect to the trainable embedder and recovery
    network weights that drive `backpropagation`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 它有21,054个参数。我们现在将为这个训练阶段实例化优化器并定义训练步骤。它遵循了DCGAN示例引入的模式，使用`tf.GradientTape`记录生成重构损失的操作。这使我们能够依赖自动微分引擎来获得相对于可训练的嵌入器和恢复网络权重的梯度，从而推动`反向传播`：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The reconstruction loss simply compares the autoencoder outputs with its inputs.
    We train for 10,000 steps in a little over one minute using this training loop
    that records the step loss for monitoring with TensorBoard:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 重构损失简单地比较自动编码器的输出与其输入。我们使用这个训练循环进行10,000步训练，仅用了一分钟多一点的时间，记录了步骤损失以便在TensorBoard上监控：
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Training phase 2 – supervised learning with real data
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二阶段训练 - 使用真实数据进行监督学习
- en: 'We already created the supervisor model so we just need to instantiate the
    optimizer and define the train step as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了监督员模型，所以我们只需要实例化优化器，并定义训练步骤如下：
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this case, the loss compares the output of the supervisor with the next timestep
    for the embedded sequence so that it learns the temporal dynamics of the historical
    price sequences; the training loop works similarly to the autoencoder example
    in the previous chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，损失比较监督员的输出和嵌入序列的下一个时间步，以便学习历史价格序列的时间动态；训练循环与上一章中自编码器示例类似地工作。
- en: Training phase 3 – joint training with real and random data
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三阶段训练 - 与真实数据和随机数据的联合训练
- en: The joint training involves all four network components, as well as the supervisor.
    It uses multiple loss functions and combinations of the base components to achieve
    the simultaneous learning of latent space embeddings, transition dynamics, and
    synthetic data generation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 联合训练涉及所有四个网络组件，以及监督员。它使用多个损失函数和基本组件的组合，以实现潜在空间嵌入、转换动态和合成数据生成的同时学习。
- en: We will highlight a few salient examples; please see the notebook for the full
    implementation that includes some repetitive steps that we will omit here.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点介绍一些显著的例子；请参阅笔记本，其中包括一些我们将在此省略的重复步骤的完整实现。
- en: 'To ensure that the generator faithfully reproduces the time series, TimeGAN
    includes a moment loss that penalizes when the mean and variance of the synthetic
    data deviate from the real version:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保生成器忠实地复制时间序列，TimeGAN包括一个矩阵损失，当合成数据的均值和方差偏离真实版本时进行惩罚：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The end-to-end model that produces synthetic data involves the generator, supervisor,
    and recovery components. It is defined as follows and has close to 30,000 trainable
    parameters:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 生成合成数据的端到端模型涉及生成器、监督员和恢复组件。它的定义如下，并且有接近30,000个可训练参数：
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The joint training involves three optimizers for the autoencoder, the generator,
    and the discriminator:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 联合训练涉及三个优化器，分别用于自编码器、生成器和鉴别器：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The train step for the generator illustrates the use of four loss functions
    and corresponding combinations of network components to achieve the desired learning
    outlined at the beginning of this section:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的训练步骤说明了使用四个损失函数和相应的网络组件组合来实现本节开头概述的所需学习：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, the joint training loop pulls the various training steps together
    and builds on the learning from phase 1 and 2 to train the TimeGAN components
    on both real and random data. We run the loop for 10,000 iterations in under 40
    minutes:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，联合训练循环将各种训练步骤汇总，并建立在阶段1和2的学习基础上，对TimeGAN组件在真实数据和随机数据上进行训练。我们在不到40分钟内运行了10,000次循环：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we can finally generate synthetic time series!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以生成合成时间序列了！
- en: Generating synthetic time series
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成合成时间序列
- en: 'To evaluate the `TimeGAN` results, we will generate synthetic time series by
    drawing random inputs and feeding them to the `synthetic_data` network just described
    in the preceding section. More specifically, we''ll create roughly as many artificial
    series with 24 observations on the six tickers as there are overlapping windows
    in the real dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估`TimeGAN`的结果，我们将通过随机输入生成合成时间序列，并将它们馈送到前面描述的`synthetic_data`网络。更具体地说，我们将创建大约与真实数据集中重叠窗口数量相同的24个观测值的人工系列：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result is 35 batches containing 128 samples, each with the dimensions 24×6,
    that we stack like so:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是包含128个样本的35个批次，每个样本的维度为24×6，我们将它们堆叠如下：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can use the trained `MinMaxScaler` to revert the synthetic output to the
    scale of the input series:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用训练好的`MinMaxScaler`将合成输出恢复到输入序列的尺度：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Figure 21.6* displays samples of the six synthetic series and the corresponding
    real series. The synthetic data generally reflects a variation of behavior not
    unlike its real counterparts and, after rescaling, roughly (due to the random
    input) matches its range:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*图21.6*显示了六个合成系列的样本和相应的真实系列。合成数据通常反映了与其真实对应物类似的行为变化，并且在重新缩放后，大致（由于随机输入）与其范围相匹配：'
- en: '![](img/B15439_21_06.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_06.png)'
- en: 'Figure 21.6: TimeGAN output—six synthetic prices series and their real counterparts'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.6：TimeGAN输出——六个合成价格系列及其真实对应物
- en: Now it's time to take a closer look at how to more thoroughly evaluate the quality
    of the synthetic data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候更仔细地评估合成数据的质量了。
- en: Evaluating the quality of synthetic time-series data
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估合成时间序列数据的质量
- en: 'The TimeGAN authors assess the quality of the generated data with respect to
    three practical criteria:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN的作者根据三个实际标准评估生成数据的质量：
- en: '**Diversity**: The distribution of the synthetic samples should roughly match
    that of the real data.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：合成样本的分布应该大致与真实数据相匹配。'
- en: '**Fidelity**: The sample series should be indistinguishable from the real data.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度**：样本系列应该和真实数据无法区分。'
- en: '**Usefulness**: The synthetic data should be as useful as its real counterparts
    for solving a predictive task.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有用性**：合成数据应该和真实数据一样有用，用于解决预测任务。'
- en: 'They apply three methods to evaluate whether the synthetic data actually exhibits
    these characteristics:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 他们应用了三种方法来评估合成数据是否实际表现出这些特征：
- en: '**Visualization**: For a qualitative diversity assessment of diversity, we
    use dimensionality reduction—**principal component analysis** (**PCA**) and **t-SNE**
    (see *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with Unsupervised
    Learning*)—to visually inspect how closely the distribution of the synthetic samples
    resembles that of the original data.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化**：为了定性地评估多样性，我们使用降维技术——**主成分分析**（**PCA**）和**t-SNE**（见*第13章*，*使用无监督学习进行数据驱动的风险因素和资产配置*）——来直观地检查合成样本的分布与原始数据的相似程度。'
- en: '**Discriminative score**: For a quantitative assessment of fidelity, the test
    error of a time-series classifier, such as a two-layer LSTM (see *Chapter 18*,
    *CNNs for Financial Time Series and Satellite Images*), lets us evaluate whether
    real and synthetic time series can be differentiated or are, in fact, indistinguishable.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别分数**：为了定量评估忠实度，例如两层LSTM（见*第18章*，*用于金融时间序列和卫星图像的CNN*）的时间序列分类器的测试误差，让我们评估真实和合成时间序列是否可以区分，或者实际上是不可区分的。'
- en: '**Predictive score**: For a quantitative measure of usefulness, we can compare
    the test errors of a sequence prediction model trained on, alternatively, real
    or synthetic data to predict the next time step for the real data.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测分数**：为了定量衡量有用性，我们可以比较在真实或合成数据上训练的序列预测模型的测试误差，以预测真实数据的下一个时间步。'
- en: We'll apply and discuss the results of each method in the following sections.
    See the notebook `evaluating_synthetic_data` for the code samples and additional
    details.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的部分中应用和讨论每种方法的结果。有关代码示例和更多细节，请参阅笔记本`evaluating_synthetic_data`。
- en: Assessing diversity – visualization using PCA and t-SNE
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估多样性-使用PCA和t-SNE进行可视化
- en: 'To visualize the real and synthetic series with 24 time steps and six features,
    we will reduce their dimensionality so that we can plot them in two dimensions.
    To this end, we will sample 250 normalized sequences with six features each and
    reshape them to obtain data with the dimensionality 1,500×24 (showing only the
    steps for real data; see the notebook for the synthetic data):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化具有24个时间步长和六个特征的真实和合成系列，我们将减少它们的维度，以便在两个维度中绘制它们。为此，我们将采样250个具有六个特征的归一化序列，并将其重塑以获得维度为1,500×24的数据（仅显示真实数据的步骤；有关合成数据，请参见笔记本）：
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'PCA is a linear method that identifies a new basis with mutually orthogonal
    vectors that, successively, capture the directions of maximum variance in the
    data. We will compute the first two components using the real data and then project
    both real and synthetic samples onto the new coordinate system:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种线性方法，它确定具有相互正交向量的新基础，依次捕获数据中方差的最大方向。我们将使用真实数据计算前两个组件，然后将真实和合成样本投影到新的坐标系上：
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 't-SNE is a nonlinear manifold learning method for the visualization of high-dimensional
    data. It converts similarities between data points to joint probabilities and
    aims to minimize the Kullback-Leibler divergence between the joint probabilities
    of the low-dimensional embedding and the high-dimensional data (see *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*). We
    compute t-SNE for the combined real and synthetic data as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种用于可视化高维数据的非线性流形学习方法。它将数据点之间的相似性转换为联合概率，并旨在最小化低维嵌入的联合概率与高维数据之间的Kullback-Leibler散度（见*第13章*，*使用无监督学习进行数据驱动风险因素和资产配置*）。我们计算组合真实和合成数据的t-SNE如下：
- en: '[PRE29]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*Figure 21.7* displays the PCA and t-SNE results for a qualitative assessment
    of the similarity of the real and synthetic data distributions. Both methods reveal
    strikingly similar patterns and significant overlap, suggesting that the synthetic
    data captures important aspects of the real data characteristics.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*图21.7*显示了PCA和t-SNE的结果，用于定性评估真实和合成数据分布的相似性。两种方法都显示出非常相似的模式和显著的重叠，表明合成数据捕捉到了真实数据特征的重要方面。'
- en: '![](img/B15439_21_07.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_07.png)'
- en: 'Figure 21.7: 250 samples of real and synthetic data in two dimensions'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.7：两个维度中的250个真实和合成数据样本
- en: Assessing fidelity – time-series classification performance
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估忠实度-时间序列分类性能
- en: The visualization only provides a qualitative impression. For a quantitative
    assessment of the fidelity of the synthetic data, we will train a time-series
    classifier to distinguish between real and fake data and evaluate its performance
    on a held-out test set.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化仅提供定性印象。为了定量评估合成数据的忠实度，我们将训练一个时间序列分类器来区分真实数据和伪造数据，并在保留的测试集上评估其性能。
- en: 'More specifically, we will select the first 80 percent of the rolling sequences
    for training and the last 20 percent as a test set, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将选择滚动序列的前80%用于训练，最后20%用作测试集，如下所示：
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then we will create a simple RNN with six units that receives mini batches
    of real and synthetic series with the shape 24×6 and uses a sigmoid activation.
    We will optimize it using binary cross-entropy loss and the Adam optimizer, while
    tracking the AUC and accuracy metrics:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创建一个简单的RNN，它具有六个单元，接收形状为24×6的真实和合成系列的小批量，并使用sigmoid激活。我们将使用二元交叉熵损失和Adam优化器进行优化，同时跟踪AUC和准确度指标：
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The model has 259 trainable parameters. We will train it for 250 epochs on
    batches of 128 randomly selected samples and track the validation performance:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型具有259个可训练参数。我们将在128个随机选择的样本批次上进行250个时期的训练，并跟踪验证性能：
- en: '[PRE32]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once the training completes, evaluation of the test set yields a classification
    error of almost 56 percent on the balanced test set and a very low AUC of 0.15:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，对测试集的评估显示，在平衡测试集上的分类错误接近56%，AUC非常低，为0.15：
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Figure 21.8* plots the accuracy and AUC performance metrics for both train
    and test data over the 250 training epochs:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*图21.8*绘制了250个训练时期内的准确度和AUC性能指标，分别针对训练和测试数据：'
- en: '![](img/B15439_21_08.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_08.png)'
- en: 'Figure 21.8: Train and test performance of the time-series classifier over
    250 epochs'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.8：250个时期内时间序列分类器的训练和测试性能
- en: The plot shows that that model is not able to learn the difference between the
    real and synthetic data in a way that generalizes to the test set. This result
    suggests that the quality of the synthetic data meets the fidelity standard.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，该模型无法学习区分真实和合成数据的差异，并且无法推广到测试集。这一结果表明合成数据的质量符合忠实度标准。
- en: Assessing usefulness – train on synthetic, test on real
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估有用性-在合成数据上训练，对真实数据进行测试
- en: Finally, we want to know how useful synthetic data is when it comes to solving
    a prediction problem. To this end, we will train a time-series prediction model
    alternatively on the synthetic and the real data to predict the next time step
    and compare the performance on a test set created from the real data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想知道在解决预测问题时，合成数据有多大用处。为此，我们将交替在合成数据和真实数据上训练时间序列预测模型，以预测下一个时间步，并在从真实数据创建的测试集上比较性能。
- en: 'More specifically, we will select the first 23 time steps of each sequence
    as input, and the final time step as output. At the same time, we will split the
    real data into train and test sets using the same temporal split as in the previous
    classification example:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将选择每个序列的前23个时间步作为输入，最后一个时间步作为输出。同时，我们将使用与先前分类示例中相同的时间分割将真实数据分为训练集和测试集：
- en: '[PRE34]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will select the complete synthetic data for training since abundance is
    one of the reasons we generated it in the first place:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择完整的合成数据进行训练，因为丰富性是我们生成它的原因之一：
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We will create a one-layer RNN with 12 GRU units that predicts the last time
    steps for the six stock price series and, thus, has six linear output units. The
    model uses the Adam optimizer to minimize the mean absolute error (MAE):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个具有12个GRU单元的单层RNN，用于预测六个股价系列的最后时间步，并且具有六个线性输出单元。该模型使用Adam优化器来最小化平均绝对误差（MAE）：
- en: '[PRE36]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will train the model twice using the synthetic and real data for training,
    respectively, and the real test set to evaluate the out-of-sample performance.
    Training on synthetic data works as follows; training on real data works analogously
    (see the notebook):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用合成和真实数据分别对模型进行两次训练，并使用真实测试集来评估外样本性能。在合成数据上进行训练的工作方式如下；在真实数据上进行训练的方式类似（请参阅笔记本）：
- en: '[PRE37]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*Figure 21.9* plots the MAE on the train and test sets (on a log scale so we
    can spot the differences) for both models. It turns out that the MAE is slightly
    lower after training on the synthetic dataset:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*图21.9*绘制了两个模型在训练集和测试集上的MAE（以对数刻度绘制，以便我们可以发现差异）。结果表明，在合成数据集上训练后，MAE略低：'
- en: '![](img/B15439_21_09.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_09.png)'
- en: 'Figure 21.9: Train and test performance of the time-series prediction model
    over 100 epochs'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.9：100个时期内时间序列预测模型的训练和测试性能
- en: The result shows that synthetic training data may indeed be useful. On the specific
    predictive task of predicting the next daily stock price for six tickers, a simple
    model trained on synthetic TimeGAN data delivers equal or better performance than
    training on real data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，合成训练数据可能确实有用。在预测下一个六个股票的每日股价的具体预测任务中，一个简单的模型在合成TimeGAN数据上训练的性能与在真实数据上训练相等或更好。
- en: Lessons learned and next steps
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所学到的教训和下一步
- en: 'The perennial problem of overfitting that we encountered throughout this book
    implies that the ability to generate useful synthetic data would be quite valuable.
    The TimeGAN example justifies cautious optimism in this regard. At the same time,
    there are some **caveats**: we generated price data for a small number of assets
    at a daily frequency. In reality, we are probably interested in returns for a
    much larger number of assets, possibly at a higher frequency. The **cross-sectional
    and temporal dynamics** will certainly become more complex and may require adjustments
    to the TimeGAN architecture and training process.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整本书中遇到的过拟合问题意味着，生成有用的合成数据的能力将非常有价值。TimeGAN的例子在这方面证明了谨慎乐观。与此同时，还有一些**警告**：我们为少数资产以每日频率生成了价格数据。实际上，我们可能对更多资产的回报感兴趣，可能是更高的频率。**横截面和时间动态**肯定会变得更加复杂，并且可能需要对TimeGAN架构和训练过程进行调整。
- en: 'These limitations of the experiment, however promising, imply natural next
    steps: we need to expand the scope to higher-dimensional time series containing
    information other than prices and also need to test their usefulness in the context
    of more complex models, including for feature engineering. These are very early
    days for synthetic training data, but this example should equip you to pursue
    your own research agenda towards more realistic solutions.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些实验的局限性很有前景，意味着自然的下一步是：我们需要扩大范围，包括除价格以外的信息的高维时间序列，并且需要在更复杂的模型环境中测试它们的有用性，包括特征工程。对于合成训练数据来说，现在还处于非常早期阶段，但这个例子应该让您能够朝着更现实的解决方案开展自己的研究议程。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced GANs that learn a probability distribution over
    the input data and are thus capable of generating synthetic samples that are representative
    of the target data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了GAN，它们学习输入数据上的概率分布，因此能够生成代表目标数据的合成样本。
- en: While there are many practical applications for this very recent innovation,
    they could be particularly valuable for algorithmic trading if the success in
    generating time-series training data in the medical domain can be transferred
    to financial market data. We learned how to set up adversarial training using
    TensorFlow. We also explored TimeGAN, a recent example of such a model, tailored
    to generating synthetic time-series data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种非常新的创新有许多实际应用，但如果在医学领域生成时间序列训练数据的成功可以转移到金融市场数据，那么它可能对算法交易特别有价值。我们学会了如何使用TensorFlow设置对抗训练。我们还探讨了TimeGAN，这是一个最近的例子，专门用于生成合成时间序列数据。
- en: In the next chapter, we focus on reinforcement learning where we will build
    agents that interactively learn from their (market) environment.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于强化学习，我们将构建能够与（市场）环境进行互动学习的代理。

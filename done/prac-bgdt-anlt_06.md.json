["```scala\n# COMMAND ----------\n\n# The SparkContext/SparkSession is the entry point for all Spark operations\n# sc = the SparkContext = the execution environment of Spark, only 1 per JVM\n# Note that SparkSession is now the entry point (from Spark v2.0)\n# This tutorial uses SparkContext (was used prior to Spark 2.0)\n\nfrom pyspark import SparkContext\n# sc = SparkContext(appName = \"some_application_name\") # You'd normally run this, but in this case, it has already been created in the Databricks' environment\n\n# COMMAND ----------\n\nquote = \"To be, or not to be, that is the question: Whether 'tis nobler in the mind to suffer The slings and arrows of outrageous fortune, Or to take Arms against a Sea of troubles, And by opposing end them: to die, to sleep No more; and by a sleep, to say we end the heart-ache, and the thousand natural shocks that Flesh is heir to? 'Tis a consummation devoutly to be wished. To die, to sleep, To sleep, perchance to Dream; aye, there's the rub, for in that sleep of death, what dreams may come, when we have shuffled off this mortal coil, must give us pause.\"\n\n# COMMAND ----------\nsparkdata = sc.parallelize(quote.split(' '))\n\n# COMMAND ----------\nprint \"sparkdata = \", sparkdata\nprint \"sparkdata.collect = \", sparkdata.collect\nprint \"sparkdata.collect() = \", sparkdata.collect()[1:10]\n\n# COMMAND ----------\n# A simple transformation - map\ndef mapword(word):\n return (word,1)\n\nprint sparkdata.map(mapword) # Nothing has happened here\nprint sparkdata.map(mapword).collect()[1:10] # collect causes the DAG to execute\n\n# COMMAND ----------\n# Another Transformation\n\ndef charsmorethan2(tuple1):\n if len(tuple1[0])>2:\n return tuple1\n pass\n\nrdd3 = sparkdata.map(mapword).filter(lambda x: charsmorethan2(x))\n# Multiple Transformations in 1 statement, nothing is happening yet\nrdd3.collect()[1:10] \n# The DAG gets executed. Note that since we didn't remove punctuation marks ... 'be,', etc are also included\n\n# COMMAND ----------\n# With Tables, a general example\ncms = sc.parallelize([[1,\"Dr. A\",12.50,\"Yale\"],[2,\"Dr. B\",5.10,\"Duke\"],[3,\"Dr. C\",200.34,\"Mt. Sinai\"],[4,\"Dr. D\",5.67,\"Duke\"],[1,\"Dr. E\",52.50,\"Yale\"]])\n\n# COMMAND ----------\ndef findPayment(data):\n return data[2]\n\nprint \"Payments = \", cms.map(findPayment).collect()\nprint \"Mean = \", cms.map(findPayment).mean() # Mean is an action\n\n# COMMAND ----------\n# Creating a DataFrame (familiar to Python programmers)\n\ncms_df = sqlContext.createDataFrame(cms, [\"ID\",\"Name\",\"Payment\",\"Hosp\"])\nprint cms_df.show()\nprint cms_df.groupby('Hosp').agg(func.avg('Payment'), func.max('Payment'),func.min('Payment'))\nprint cms_df.groupby('Hosp').agg(func.avg('Payment'), func.max('Payment'),func.min('Payment')).collect()\nprint\nprint \"Converting to a Pandas DataFrame\"\nprint \"--------------------------------\"\npd_df = cms_df.groupby('Hosp').agg(func.avg('Payment'), func.max('Payment'),func.min('Payment')).toPandas()\nprint type(pd_df)\nprint\nprint pd_df\n\n# COMMAND ----------\nwordsList = ['to','be','or','not','to','be']\nwordsRDD = sc.parallelize(wordsList, 3) # Splits into 2 groups\n# Print out the type of wordsRDD\nprint type(wordsRDD)\n\n# COMMAND ----------\n# Glom coallesces all elements within each partition into a list\nprint wordsRDD.glom().take(2) # Take is an action, here we are 'take'-ing the first 2 elements of the wordsRDD\nprint wordsRDD.glom().collect() # Collect\n\n# COMMAND ----------\n# An example with changing the case of words\n# One way of completing the function\ndef makeUpperCase(word):\n return word.upper()\n\nprint makeUpperCase('cat')\n\n# COMMAND ----------\nupperRDD = wordsRDD.map(makeUpperCase)\nprint upperRDD.collect()\n\n# COMMAND ----------\nupperLambdaRDD = wordsRDD.map(lambda word: word.upper())\nprint upperLambdaRDD.collect()\n\n# COMMAND ----------\n\n# Pair RDDs\nwordPairs = wordsRDD.map(lambda word: (word, 1))\nprint wordPairs.collect()\n\n# COMMAND ----------\n\n# #### Part 2: Counting with pair RDDs \n# There are multiple ways of performing group-by operations in Spark\n# One such method is groupByKey()\n# \n# ** Using groupByKey() **\n# \n# This method creates a key-value pair whereby each key (in this case word) is assigned a value of 1 for our wordcount operation. It then combines all keys into a single list. This can be quite memory intensive, especially if the dataset is large.\n\n# COMMAND ----------\n# Using groupByKey\nwordsGrouped = wordPairs.groupByKey()\nfor key, value in wordsGrouped.collect():\n print '{0}: {1}'.format(key, list(value))\n\n# COMMAND ----------\n# Summation of the key values (to get the word count)\nwordCountsGrouped = wordsGrouped.map(lambda (k,v): (k, sum(v)))\nprint wordCountsGrouped.collect()\n\n# COMMAND ----------\n\n# ** (2c) Counting using reduceByKey **\n# \n# reduceByKey creates a new pair RDD. It then iteratively applies a function first to each key (i.e., within the key values) and then across all the keys, i.e., in other words it applies the given function iteratively.\n\n# COMMAND ----------\n\nwordCounts = wordPairs.reduceByKey(lambda a,b: a+b)\nprint wordCounts.collect()\n\n# COMMAND ----------\n# %md\n# ** Combining all of the above into a single statement **\n\n# COMMAND ----------\n\nwordCountsCollected = (wordsRDD\n .map(lambda word: (word, 1))\n .reduceByKey(lambda a,b: a+b)\n .collect())\nprint wordCountsCollected\n\n# COMMAND ----------\n\n# %md\n# \n# This tutorial has provided a basic overview of Spark and introduced the Databricks community edition where users can upload and execute their own Spark notebooks. There are various in-depth tutorials on the web and also at Databricks on Spark and users are encouraged to peruse them if interested in learning further about Spark.\n```"]
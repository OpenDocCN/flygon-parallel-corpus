["```scala\nval spark = SparkSession.builder\n                     .master(\"local[*]\") //change acordingly\n                     .config(\"spark.sql.warehouse.dir\", \"/home/exp/\")\n                     .appName(\"OneVsRestExample\") \n                     .getOrCreate()\n\n```", "```scala\nval inputData = spark.read.format(\"libsvm\")\n                     .load(\"data/Letterdata_libsvm.data\")\ninputData.show()\n\n```", "```scala\nval Array(train, test) = inputData.randomSplit(Array(0.7, 0.3))\n\n```", "```scala\n val classifier = new LogisticRegression()\n                        .setMaxIter(500)          \n                        .setTol(1E-4)                                                                                                  \n                        .setFitIntercept(true)\n                        .setStandardization(true) \n                        .setAggregationDepth(50) \n                        .setRegParam(0.0001) \n                        .setElasticNetParam(0.01)\n\n```", "```scala\nval ovr = new OneVsRest().setClassifier(classifier)\n\n```", "```scala\nval ovrModel = ovr.fit(train)\n\n```", "```scala\nval predictions = ovrModel.transform(test)\n\n```", "```scala\nval evaluator = new MulticlassClassificationEvaluator()\n                           .setLabelCol(\"label\")\n                           .setPredictionCol(\"prediction\")    \nval evaluator1 = evaluator.setMetricName(\"accuracy\")\nval evaluator2 = evaluator.setMetricName(\"weightedPrecision\")\nval evaluator3 = evaluator.setMetricName(\"weightedRecall\")\nval evaluator4 = evaluator.setMetricName(\"f1\")\n\n```", "```scala\nval accuracy = evaluator1.evaluate(predictions)\nval precision = evaluator2.evaluate(predictions)\nval recall = evaluator3.evaluate(predictions)\nval f1 = evaluator4.evaluate(predictions)\n\n```", "```scala\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Precision = \" + precision)\nprintln(\"Recall = \" + recall)\nprintln(\"F1 = \" + f1)\nprintln(s\"Test Error = ${1 - accuracy}\")\n\n```", "```scala\nAccuracy = 0.5217246545696688\nPrecision = 0.488360500637862\nRecall = 0.5217246545696688\nF1 = 0.4695649096879411\nTest Error = 0.47827534543033123\n\n```", "```scala\nspark.stop() // Stop Spark session\n\n```", "```scala\nimport org.apache.spark.ml.classification.NaiveBayes\nimport org.apache.spark.ml.evaluation\n                                 .MulticlassClassificationEvaluator\nimport org.apache.spark.sql.SparkSession\n\n```", "```scala\nval spark = SparkSession\n              .builder\n              .master(\"local[*]\")\n              .config(\"spark.sql.warehouse.dir\", \"/home/exp/\")\n              .appName(s\"NaiveBayes\")\n              .getOrCreate()\n\n```", "```scala\nval data = spark.read.format(\"libsvm\")\n                     .load(\"data/pendigits.data\")\n\n```", "```scala\nval Array(trainingData, testData) = data\n                  .randomSplit(Array(0.75, 0.25), seed = 12345L)\n\n```", "```scala\nval nb = new NaiveBayes()\nval model = nb.fit(trainingData)\n\n```", "```scala\nval predictions = model.transform(testData)\npredictions.show()\n\n```", "```scala\nval evaluator = new MulticlassClassificationEvaluator()\n                           .setLabelCol(\"label\")\n                           .setPredictionCol(\"prediction\")    \nval evaluator1 = evaluator.setMetricName(\"accuracy\")\nval evaluator2 = evaluator.setMetricName(\"weightedPrecision\")\nval evaluator3 = evaluator.setMetricName(\"weightedRecall\")\nval evaluator4 = evaluator.setMetricName(\"f1\")\n\n```", "```scala\nval accuracy = evaluator1.evaluate(predictions)\nval precision = evaluator2.evaluate(predictions)\nval recall = evaluator3.evaluate(predictions)\nval f1 = evaluator4.evaluate(predictions)\n\n```", "```scala\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Precision = \" + precision)\nprintln(\"Recall = \" + recall)\nprintln(\"F1 = \" + f1)\nprintln(s\"Test Error = ${1 - accuracy}\")\n\n```", "```scala\nAccuracy = 0.8284365162644282\nPrecision = 0.8361211320692463\nRecall = 0.828436516264428\nF1 = 0.8271828540349192\nTest Error = 0.17156348373557184\n\n```", "```scala\nimport org.apache.spark.ml.classification.NaiveBayes\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.Pipeline;\nimport org.apache.spark.ml.PipelineStage;\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n\n```", "```scala\nval spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"/home/exp/\")\n      .appName(\"Tuned NaiveBayes\")\n      .getOrCreate()\n\n```", "```scala\n// Load the data stored in LIBSVM format as a DataFrame.\n val data = spark.read.format(\"libsvm\").load(\"hdfs://data/ webspam_wc_normalized_trigram.svm\")\n // Split the data into training and test sets (30% held out for testing)\n val Array(trainingData, testData) = data.randomSplit(Array(0.75, 0.25), seed = 12345L)\n // Train a NaiveBayes model with using the training set\n val nb = new NaiveBayes().setSmoothing(0.00001)\n val model = nb.fit(trainingData)\n\n```", "```scala\nval predictions = model.transform(testData)\npredictions.show()\n\n```", "```scala\nval evaluator = new MulticlassClassificationEvaluator()\n                    .setLabelCol(\"label\")\n                    .setPredictionCol(\"prediction\")    \nval evaluator1 = evaluator.setMetricName(\"accuracy\")\nval evaluator2 = evaluator.setMetricName(\"weightedPrecision\")\nval evaluator3 = evaluator.setMetricName(\"weightedRecall\")\nval evaluator4 = evaluator.setMetricName(\"f1\")\n\n```", "```scala\nval accuracy = evaluator1.evaluate(predictions)\nval precision = evaluator2.evaluate(predictions)\nval recall = evaluator3.evaluate(predictions)\nval f1 = evaluator4.evaluate(predictions)   \n// Print the performance metrics\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Precision = \" + precision)\nprintln(\"Recall = \" + recall)\nprintln(\"F1 = \" + f1)\nprintln(s\"Test Error = ${1 - accuracy}\")\n\n```", "```scala\nAccuracy = 0.8839357429715676\nPrecision = 0.86393574297188752\nRecall = 0.8739357429718876\nF1 = 0.8739357429718876\nTest Error = 0.11606425702843237\n\n```", "```scala\nval nb = new NaiveBayes().setSmoothing(00001)\nval pipeline = new Pipeline().setStages(Array(nb))\n\n```", "```scala\nval paramGrid = new ParamGridBuilder()\n              .addGrid(nb.smoothing, Array(0.001, 0.0001))\n              .build()\n\n```", "```scala\nval cv = new CrossValidator()\n            .setEstimator(pipeline)\n            .setEvaluator(new BinaryClassificationEvaluator)\n            .setEstimatorParamMaps(paramGrid)\n            .setNumFolds(10)  // Use 3+ in practice\n\n```", "```scala\nval model = cv.fit(trainingData)\n\n```", "```scala\nval predictions = model.transform(validationData)\npredictions.show()\n\n```", "```scala\nval evaluator = new MulticlassClassificationEvaluator()\n                            .setLabelCol(\"label\")\n                            .setPredictionCol(\"prediction\")    \nval evaluator1 = evaluator.setMetricName(\"accuracy\")\nval evaluator2 = evaluator.setMetricName(\"weightedPrecision\")\nval evaluator3 = evaluator.setMetricName(\"weightedRecall\")\nval evaluator4 = evaluator.setMetricName(\"f1\")\n\n```", "```scala\nval accuracy = evaluator1.evaluate(predictions)\nval precision = evaluator2.evaluate(predictions)\nval recall = evaluator3.evaluate(predictions)\nval f1 = evaluator4.evaluate(predictions)\n\n```", "```scala\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Precision = \" + precision)\nprintln(\"Recall = \" + recall)\nprintln(\"F1 = \" + f1)\nprintln(s\"Test Error = ${1 - accuracy}\")\n\n```", "```scala\nAccuracy = 0.9678714859437751\nPrecision = 0.9686742518830365\nRecall = 0.9678714859437751\nF1 = 0.9676697179934564\nTest Error = 0.032128514056224855\n\n```", "```scala\nAccuracy = 0.5217246545696688\nPrecision = 0.488360500637862\nRecall = 0.5217246545696688\nF1 = 0.4695649096879411\nTest Error = 0.47827534543033123\n\n```", "```scala\nimport org.apache.spark.ml.Pipeline // for Pipeline creation\nimport org.apache.spark.ml.classification\n                         .DecisionTreeClassificationModel \nimport org.apache.spark.ml.classification.DecisionTreeClassifier \nimport org.apache.spark.ml.evaluation\n                         .MulticlassClassificationEvaluator \nimport org.apache.spark.ml.feature\n                         .{IndexToString, StringIndexer, VectorIndexer} \nimport org.apache.spark.sql.SparkSession //For a Spark session\n\n```", "```scala\nval spark = SparkSession\n              .builder\n              .master(\"local[*]\")\n              .config(\"spark.sql.warehouse.dir\", \"/home/exp/\")\n              .appName(\"DecisionTreeClassifier\")\n              .getOrCreate()\n\n```", "```scala\nval data = spark.read.format(\"libsvm\").load(\"datab\n                             /Letterdata_libsvm.data\")\n\n```", "```scala\nval labelIndexer = new StringIndexer()\n               .setInputCol(\"label\")\n               .setOutputCol(\"indexedLabel\")\n               .fit(data)\n\n```", "```scala\nval featureIndexer = new VectorIndexer()\n              .setInputCol(\"features\")\n              .setOutputCol(\"indexedFeatures\")\n              .setMaxCategories(4)\n              .fit(data)\n\n```", "```scala\nval Array(trainingData, testData) = data.randomSplit\n                                      (Array(0.75, 0.25), 12345L)\n\n```", "```scala\nval dt = new DecisionTreeClassifier()\n                     .setLabelCol(\"indexedLabel\")\n                     .setFeaturesCol(\"indexedFeatures\")\n\n```", "```scala\nval labelConverter = new IndexToString()\n                .setInputCol(\"prediction\")\n                .setOutputCol(\"predictedLabel\")\n                .setLabels(labelIndexer.labels)\n\n```", "```scala\nval pipeline = new Pipeline().setStages(Array(labelIndexer,\n                              featureIndexer, dt, labelconverter))\n\n```", "```scala\nval model = pipeline.fit(trainingData)\n\n```", "```scala\nval predictions = model.transform(testData)\npredictions.show()\n\n```", "```scala\nval evaluator = new MulticlassClassificationEvaluator()\n                             .setLabelCol(\"label\")\n                             .setPredictionCol(\"prediction\")    \nval evaluator1 = evaluator.setMetricName(\"accuracy\")\nval evaluator2 = evaluator.setMetricName(\"weightedPrecision\")\nval evaluator3 = evaluator.setMetricName(\"weightedRecall\")\nval evaluator4 = evaluator.setMetricName(\"f1\")\n\n```", "```scala\nval accuracy = evaluator1.evaluate(predictions)\nval precision = evaluator2.evaluate(predictions)\nval recall = evaluator3.evaluate(predictions)\nval f1 = evaluator4.evaluate(predictions)\n\n```", "```scala\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Precision = \" + precision)\nprintln(\"Recall = \" + recall)\nprintln(\"F1 = \" + f1)\nprintln(s\"Test Error = ${1 - accuracy}\")\n\n```", "```scala\nAccuracy = 0.994277821625888\nPrecision = 0.9904583933020722\nRecall = 0.994277821625888\nF1 = 0.9919966504321712\nTest Error = 0.005722178374112041\n\n```", "```scala\nval treeModel = model.stages(2).asInstanceOf\n                                [DecisionTreeClassificationModel]\nprintln(\"Learned classification tree model:\\n\" + treeModel\n                 .toDebugString)\n\n```"]
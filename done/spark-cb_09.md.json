["```scala\n    $ hdfs dfs -put saratoga.csv saratoga.csv\n\n    ```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.clustering.KMeans\n\n    ```", "```scala\n    scala> val data = sc.textFile(\"saratoga.csv\")\n\n    ```", "```scala\n    scala> val parsedData = data.map( line => Vectors.dense(line.split(',').map(_.toDouble)))\n\n    ```", "```scala\n    scala> val kmmodel= KMeans.train(parsedData,4,5)\n\n    ```", "```scala\n    scala> val houses = parsedData.collect\n\n    ```", "```scala\n    scala> val prediction = kmmodel.predict(houses(0))\n\n    ```", "```scala\n    scala> val prediction = kmmodel.predict(houses(18))\n    resxx: Int = 3\n\n    ```", "```scala\n    scala> val prediction = kmmodel.predict(houses(35))\n    resxx: Int = 1\n\n    ```", "```scala\n    scala> val prediction = kmmodel.predict(houses(6))\n    resxx: Int = 0\n\n    ```", "```scala\n    scala>  val prediction = kmmodel.predict(houses(15))\n    resxx: Int = 2\n\n    ```", "```scala\n    $ hdfs dfs -put scaledhousedata.csv scaledhousedata.csv\n\n    ```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix\n\n    ```", "```scala\n    scala> val data = sc.textFile(\"scaledhousedata.csv\")\n\n    ```", "```scala\n    scala> val parsedData = data.map( line => Vectors.dense(line.split(',').map(_.toDouble)))\n\n    ```", "```scala\n    scala> val mat = new RowMatrix(parsedData)\n\n    ```", "```scala\n    scala> val pc= mat.computePrincipalComponents(1)\n\n    ```", "```scala\n    scala> val projected = mat.multiply(pc)\n\n    ```", "```scala\n    scala> val projectedRDD = projected.rows\n\n    ```", "```scala\n    scala> projectedRDD.saveAsTextFile(\"phdata\")\n\n    ```", "```scala\n    scala> hdfs dfs -get phdata phdata\n\n    ```", "```scala\n    $ hdfs dfs -put pres.csv scaledhousedata.csv\n\n    ```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix\n\n    ```", "```scala\n    scala> val data = sc.textFile(\"pres.csv\")\n\n    ```", "```scala\n    scala> val parsedData = data.map( line => Vectors.dense(line.split(',').map(_.toDouble)))\n\n    ```", "```scala\n    scala> val mat = new RowMatrix(parsedData)\n\n    ```", "```scala\n    scala> val svd = mat.computeSVD(2,true)\n\n    ```", "```scala\n    scala> val U = svd.U\n\n    ```", "```scala\n    scala> val s = svd.s\n\n    ```", "```scala\n    scala> val s = svd.s\n\n    ```"]
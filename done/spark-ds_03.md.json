["```scala\n   //Create a list of colours \n>>> colors = ['white','green','yellow','red','brown','pink'] \n//Distribute a local collection to form an RDD \n//Apply map function on that RDD to get another RDD containing colour, length tuples \n>>> color_df = sc.parallelize(colors) \n        .map(lambda x:(x,len(x))).toDF([\"color\",\"length\"]) \n\n>>> color_df \nDataFrame[color: string, length: bigint] \n\n>>> color_df.dtypes        //Note the implicit type inference \n[('color', 'string'), ('length', 'bigint')] \n\n>>> color_df.show()  //Final output as expected. Order need not be the same as shown \n+------+------+ \n| color|length| \n+------+------+ \n| white|     5| \n| green|     5| \n|yellow|     6| \n|   red|     3| \n| brown|     5| \n|  pink|     4| \n+------+------+ \n\n```", "```scala\n//Create a list of colours \nScala> val colors = List(\"white\",\"green\",\"yellow\",\"red\",\"brown\",\"pink\") \n//Distribute a local collection to form an RDD \n//Apply map function on that RDD to get another RDD containing colour, length tuples \nScala> val color_df = sc.parallelize(colors) \n         .map(x => (x,x.length)).toDF(\"color\",\"length\") \n\nScala> color_df \nres0: org.apache.spark.sql.DataFrame = [color: string, length: int] \n\nScala> color_df.dtypes  //Note the implicit type inference   \nres1: Array[(String, String)] = Array((color,StringType), (length,IntegerType)) \n\nScala> color_df.show()//Final output as expected. Order need not be the same as shown \n+------+------+ \n| color|length| \n+------+------+ \n| white|     5| \n| green|     5| \n|yellow|     6| \n|   red|     3| \n| brown|     5| \n|  pink|     4| \n+------+------+ \n\n```", "```scala\n//Pass the source json data file path \n>>> df = sqlContext.read.json(\"./authors.json\") \n>>> df.show() //json parsed; Column names and data    types inferred implicitly \n+----------+---------+ \n|first_name|last_name| \n+----------+---------+ \n|      Mark|    Twain| \n|   Charles|  Dickens| \n|    Thomas|    Hardy| \n+----------+---------+ \n\n```", "```scala\n//Pass the source json data file path \nScala> val df = sqlContext.read.json(\"./authors.json\") \nScala> df.show()  //json parsed; Column names and    data types inferred implicitly \n+----------+---------+ \n|first_name|last_name| \n+----------+---------+ \n|      Mark|    Twain| \n|   Charles|  Dickens| \n|    Thomas|    Hardy| \n+----------+---------+ \n\n```", "```scala\n//Launch shell with driver-class-path as a command line argument \npyspark --driver-class-path /usr/share/   java/mysql-connector-java.jar \n   //Pass the connection parameters \n>>> peopleDF = sqlContext.read.format('jdbc').options( \n                        url = 'jdbc:mysql://localhost', \n                        dbtable = 'test.people', \n                        user = 'root', \n                        password = 'mysql').load() \n   //Retrieve table data as a DataFrame \n>>> peopleDF.show() \n+----------+---------+------+----------+----------+---------+ \n|first_name|last_name|gender|       dob|occupation|person_id| \n+----------+---------+------+----------+----------+---------+ \n|    Thomas|    Hardy|     M|1840-06-02|    Writer|      101| \n|     Emily|   Bronte|     F|1818-07-30|    Writer|      102| \n| Charlotte|   Bronte|     F|1816-04-21|    Writer|      103| \n|   Charles|  Dickens|     M|1812-02-07|    Writer|      104| \n+----------+---------+------+----------+----------+---------+ \n\n```", "```scala\n//Launch shell with driver-class-path as a command line argument \nspark-shell --driver-class-path /usr/share/   java/mysql-connector-java.jar \n   //Pass the connection parameters \nscala> val peopleDF = sqlContext.read.format(\"jdbc\").options( \n           Map(\"url\" -> \"jdbc:mysql://localhost\", \n               \"dbtable\" -> \"test.people\", \n               \"user\" -> \"root\", \n               \"password\" -> \"mysql\")).load() \npeopleDF: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string, gender: string, dob: date, occupation: string, person_id: int] \n//Retrieve table data as a DataFrame \nscala> peopleDF.show() \n+----------+---------+------+----------+----------+---------+ \n|first_name|last_name|gender|       dob|occupation|person_id| \n+----------+---------+------+----------+----------+---------+ \n|    Thomas|    Hardy|     M|1840-06-02|    Writer|      101| \n|     Emily|   Bronte|     F|1818-07-30|    Writer|      102| \n| Charlotte|   Bronte|     F|1816-04-21|    Writer|      103| \n|   Charles|  Dickens|     M|1812-02-07|    Writer|      104| \n+----------+---------+------+----------+----------+---------+ \n\n```", "```scala\n//Write DataFrame contents into Parquet format \n>>> peopleDF.write.parquet('writers.parquet') \n//Read Parquet data into another DataFrame \n>>> writersDF = sqlContext.read.parquet('writers.parquet')  \nwritersDF: org.apache.spark.sql.DataFrame = [first_name:    string, last_name: string, gender: string, dob:    date, occupation: string, person_id: int]\n```", "```scala\n//Write DataFrame contents into Parquet format \nscala> peopleDF.write.parquet(\"writers.parquet\") \n//Read Parquet data into another DataFrame \nscala> val writersDF = sqlContext.read.parquet(\"writers.parquet\")  \nwritersDF: org.apache.spark.sql.DataFrame = [first_name:    string, last_name: string, gender: string, dob:    date, occupation: string, person_id: int]\n```", "```scala\n//Create a local collection of colors first \n>>> colors = ['white','green','yellow','red','brown','pink'] \n//Distribute the local collection to form an RDD \n//Apply map function on that RDD to get another RDD containing colour, length tuples and convert that RDD to a DataFrame \n>>> color_df = sc.parallelize(colors) \n        .map(lambda x:(x,len(x))).toDF(['color','length']) \n//Check the object type \n>>> color_df \nDataFrame[color: string, length: bigint] \n//Check the schema \n>>> color_df.dtypes \n[('color', 'string'), ('length', 'bigint')] \n\n//Check row count \n>>> color_df.count() \n6 \n//Look at the table contents. You can limit displayed rows by passing parameter to show \ncolor_df.show() \n+------+------+ \n| color|length| \n+------+------+ \n| white|     5| \n| green|     5| \n|yellow|     6| \n|   red|     3| \n| brown|     5| \n|  pink|     4| \n+------+------+ \n\n//List out column names \n>>> color_df.columns \n[u'color', u'length'] \n\n//Drop a column. The source DataFrame color_df remains the same. //Spark returns a new DataFrame which is being passed to show \n>>> color_df.drop('length').show() \n+------+ \n| color| \n+------+ \n| white| \n| green| \n|yellow| \n|   red| \n| brown| \n|  pink| \n+------+ \n//Convert to JSON format \n>>> color_df.toJSON().first() \nu'{\"color\":\"white\",\"length\":5}' \n//filter operation is similar to WHERE clause in SQL \n//You specify conditions to select only desired columns and rows \n//Output of filter operation is another DataFrame object that is usually passed on to some more operations \n//The following example selects the colors having a length of four or five only and label the column as \"mid_length\" \nfilter \n------ \n>>> color_df.filter(color_df.length.between(4,5)) \n      .select(color_df.color.alias(\"mid_length\")).show() \n+----------+ \n|mid_length| \n+----------+ \n|     white| \n|     green| \n|     brown| \n|      pink| \n+----------+ \n\n//This example uses multiple filter criteria \n>>> color_df.filter(color_df.length > 4) \n     .filter(color_df[0]!=\"white\").show() \n+------+------+ \n| color|length| \n+------+------+ \n| green|     5| \n|yellow|     6| \n| brown|     5| \n+------+------+ \n\n//Sort the data on one or more columns \nsort \n---- \n//A simple single column sorting in default (ascending) order \n>>> color_df.sort(\"color\").show() \n+------+------+ \n| color|length| \n+------+------+ \n| brown|     5| \n| green|     5| \n|  pink|     4| \n|   red|     3| \n| white|     5| \n|yellow|     6| \n+------+------+ \n//First filter colors of length more than 4 and then sort on multiple columns \n//The Filtered rows are sorted first on the column length in default ascending order. Rows with same length are sorted on color in descending order   \n>>> color_df.filter(color_df['length']>=4).sort(\"length\", 'color',ascending=False).show()\n+------+------+ \n| color|length| \n+------+------+ \n|yellow|     6| \n| white|     5| \n| green|     5| \n| brown|     5| \n|  pink|     4| \n+------+------+ \n\n//You can use orderBy instead, which is an alias to sort \n>>> color_df.orderBy('length','color').take(4)\n[Row(color=u'red', length=3), Row(color=u'pink', length=4), Row(color=u'brown', length=5), Row(color=u'green', length=5)]\n\n//Alternative syntax, for single or multiple columns.  \n>>> color_df.sort(color_df.length.desc(),   color_df.color.asc()).show() \n+------+------+ \n| color|length| \n+------+------+ \n|yellow|     6| \n| brown|     5| \n| green|     5| \n| white|     5| \n|  pink|     4| \n|   red|     3| \n+------+------+ \n//All the examples until now have been acting on one row at a time, filtering or transforming or reordering.  \n//The following example deals with regrouping the data \n//These operations require \"wide dependency\" and often involve shuffling.  \ngroupBy \n------- \n>>> color_df.groupBy('length').count().show() \n+------+-----+ \n|length|count| \n+------+-----+ \n|     3|    1| \n|     4|    1| \n|     5|    3| \n|     6|    1| \n+------+-----+ \n//Data often contains missing information or null values. We may want to drop such rows or replace with some filler information. dropna is provided for dropping such rows \n//The following json file has names of famous authors. Firstname data is missing in one row. \ndropna \n------ \n>>> df1 = sqlContext.read.json('./authors_missing.json')\n>>> df1.show() \n+----------+---------+ \n|first_name|last_name| \n+----------+---------+ \n|      Mark|    Twain| \n|   Charles|  Dickens| \n|      null|    Hardy| \n+----------+---------+ \n\n//Let us drop the row with incomplete information \n>>> df2 = df1.dropna() \n>>> df2.show()  //Unwanted row is dropped \n+----------+---------+ \n|first_name|last_name| \n+----------+---------+ \n|      Mark|    Twain| \n|   Charles|  Dickens| \n+----------+---------+ \n\n```", "```scala\n//Create a local collection of colors first \nScala> val colors = List(\"white\",\"green\",\"yellow\",\"red\",\"brown\",\"pink\") \n//Distribute a local collection to form an RDD \n//Apply map function on that RDD to get another RDD containing color, length tuples and convert that RDD to a DataFrame \nScala> val color_df = sc.parallelize(colors) \n        .map(x => (x,x.length)).toDF(\"color\",\"length\") \n//Check the object type \nScala> color_df \nres0: org.apache.spark.sql.DataFrame = [color: string, length: int] \n//Check the schema \nScala> color_df.dtypes \nres1: Array[(String, String)] = Array((color,StringType), (length,IntegerType)) \n//Check row count \nScala> color_df.count() \nres4: Long = 6 \n//Look at the table contents. You can limit displayed rows by passing parameter to show \ncolor_df.show() \n+------+------+ \n| color|length| \n+------+------+ \n| white|     5| \n| green|     5| \n|yellow|     6| \n|   red|     3| \n| brown|     5| \n|  pink|     4| \n+------+------+ \n//List out column names \nScala> color_df.columns \nres5: Array[String] = Array(color, length) \n//Drop a column. The source DataFrame color_df remains the same. \n//Spark returns a new DataFrame which is being passed to show \nScala> color_df.drop(\"length\").show() \n+------+ \n| color| \n+------+ \n| white| \n| green| \n|yellow| \n|   red| \n| brown| \n|  pink| \n+------+ \n//Convert to JSON format \ncolor_df.toJSON.first() \nres9: String = {\"color\":\"white\",\"length\":5} \n\n//filter operation is similar to WHERE clause in SQL \n//You specify conditions to select only desired columns and rows \n//Output of filter operation is another DataFrame object that is usually passed on to some more operations \n//The following example selects the colors having a length of four or five only and label the column as \"mid_length\" \nfilter \n------ \nScala> color_df.filter(color_df(\"length\").between(4,5)) \n       .select(color_df(\"color\").alias(\"mid_length\")).show() \n+----------+ \n|mid_length| \n+----------+ \n|     white| \n|     green| \n|     brown| \n|      pink| \n+----------+ \n\n//This example uses multiple filter criteria. Notice the not equal to operator having double equal to symbols  \nScala> color_df.filter(color_df(\"length\") > 4).filter(color_df( \"color\")!==\"white\").show() \n+------+------+ \n| color|length| \n+------+------+ \n| green|     5| \n|yellow|     6| \n| brown|     5| \n+------+------+ \n//Sort the data on one or more columns \nsort \n---- \n//A simple single column sorting in default (ascending) order \nScala> color_df..sort(\"color\").show() \n+------+------+                                                                  \n| color|length| \n+------+------+ \n| brown|     5| \n| green|     5| \n|  pink|     4| \n|   red|     3| \n| white|     5| \n|yellow|     6| \n+------+------+ \n//First filter colors of length more than 4 and then sort on multiple columns \n//The filtered rows are sorted first on the column length in default ascending order. Rows with same length are sorted on color in descending order  \nScala> color_df.filter(color_df(\"length\")>=4).sort($\"length\", $\"color\".desc).show() \n+------+------+ \n| color|length| \n+------+------+ \n|  pink|     4| \n| white|     5| \n| green|     5| \n| brown|     5| \n|yellow|     6| \n+------+------+ \n//You can use orderBy instead, which is an alias to sort. \nscala> color_df.orderBy(\"length\",\"color\").take(4) \nres19: Array[org.apache.spark.sql.Row] = Array([red,3], [pink,4], [brown,5], [green,5]) \n//Alternative syntax, for single or multiple columns \nscala> color_df.sort(color_df(\"length\").desc, color_df(\"color\").asc).show() \n+------+------+ \n| color|length| \n+------+------+ \n|yellow|     6| \n| brown|     5| \n| green|     5| \n| white|     5| \n|  pink|     4| \n|   red|     3| \n+------+------+ \n//All the examples until now have been acting on one row at a time, filtering or transforming or reordering. \n//The following example deals with regrouping the data.  \n//These operations require \"wide dependency\" and often involve shuffling. \ngroupBy \n------- \nScala> color_df.groupBy(\"length\").count().show() \n+------+-----+ \n|length|count| \n+------+-----+ \n|     3|    1| \n|     4|    1| \n|     5|    3| \n|     6|    1| \n+------+-----+ \n//Data often contains missing information or null values.  \n//The following json file has names of famous authors. Firstname data is missing in one row. \ndropna \n------ \nScala> val df1 = sqlContext.read.json(\"./authors_missing.json\") \nScala> df1.show() \n+----------+---------+ \n|first_name|last_name| \n+----------+---------+ \n|      Mark|    Twain| \n|   Charles|  Dickens| \n|      null|    Hardy| \n+----------+---------+ \n//Let us drop the row with incomplete information \nScala> val df2 = df1.na.drop() \nScala> df2.show()  //Unwanted row is dropped \n+----------+---------+ \n|first_name|last_name| \n+----------+---------+ \n|      Mark|    Twain| \n|   Charles|  Dickens| \n+----------+---------+ \n\n```"]
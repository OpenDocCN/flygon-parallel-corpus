["```py\nclass IO_csv(object):\n\n    def __init__(self, filepath, filename, filesuffix='csv'):\n        self.filepath = filepath       # /path/to/file without the /' at the end\n        self.filename = filename       # FILE_NAME\n        self.filesuffix = filesuffix\n```", "```py\n    def save(self, data, NTname, fields):\n        # NTname = Name of the NamedTuple\n        # fields = header of CSV - list of the fields name\n        NTuple = namedtuple(NTname, fields)\n\n        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n            # Append existing file\n            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'ab') as f:\n                writer = csv.writer(f)\n                # writer.writerow(fields) # fields = header of CSV\n                writer.writerows([row for row in map(NTuple._make, data)])\n                # list comprehension using map on the NamedTuple._make() iterable and the data file to be saved\n                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file\n        else:\n            # Create new file\n            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'wb') as f:\n                writer = csv.writer(f)\n                writer.writerow(fields) # fields = header of CSV - list of the fields name\n                writer.writerows([row for row in map(NTuple._make, data)])\n                #  list comprehension using map on the NamedTuple._make() iterable and the data file to be saved\n                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file\n```", "```py\n    def load(self, NTname, fields):\n        # NTname = Name of the NamedTuple\n        # fields = header of CSV - list of the fields name\n        NTuple = namedtuple(NTname, fields)\n        with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix),'rU') as f:\n            reader = csv.reader(f)\n            for row in map(NTuple._make, reader):\n                # Using map on the NamedTuple._make() iterable and the reader file to be loaded\n                yield row \n```", "```py\nfields01 = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text', 'url']\nTweet01 = namedtuple('Tweet01',fields01)\n\ndef parse_tweet(data):\n    \"\"\"\n    Parse a ``tweet`` from the given response data.\n    \"\"\"\n    return Tweet01(\n        id=data.get('id', None),\n        created_at=data.get('created_at', None),\n        user_id=data.get('user_id', None),\n        user_name=data.get('user_name', None),\n        tweet_text=data.get('tweet_text', None),\n        url=data.get('url')\n    )\n```", "```py\nclass IO_json(object):\n    def __init__(self, filepath, filename, filesuffix='json'):\n        self.filepath = filepath        # /path/to/file without the /' at the end\n        self.filename = filename        # FILE_NAME\n        self.filesuffix = filesuffix\n        # self.file_io = os.path.join(dir_name, .'.join((base_filename, filename_suffix)))\n```", "```py\n    def save(self, data):\n        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n            # Append existing file\n            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'a', encoding='utf-8') as f:\n                f.write(unicode(json.dumps(data, ensure_ascii= False))) # In python 3, there is no \"unicode\" function \n                # f.write(json.dumps(data, ensure_ascii= False)) # create a \\\" escape char for \" in the saved file        \n        else:\n            # Create new file\n            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:\n                f.write(unicode(json.dumps(data, ensure_ascii= False)))\n                # f.write(json.dumps(data, ensure_ascii= False))\n```", "```py\n    def load(self):\n        with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), encoding='utf-8') as f:\n            return f.read()\n```", "```py\n    sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\n\n    ```", "```py\n    echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(\"lsb_release -sc)\"/ mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list\n\n    ```", "```py\n    sudo apt-get update\n\n    ```", "```py\n    sudo apt-get install -y mongodb-org\n\n    ```", "```py\n    sudo service mongodb start\n\n    ```", "```py\n    an@an-VB:/usr/bin$ ps -ef | grep mongo\n    mongodb    967     1  4 07:03 ?        00:02:02 /usr/bin/mongod --config /etc/mongod.conf\n    an        3143  3085  0 07:45 pts/3    00:00:00 grep --color=auto mongo\n\n    ```", "```py\n    an@an-VB:/var/lib/mongodb$ ls -lru\n    total 81936\n    drwxr-xr-x 2 mongodb nogroup     4096 Apr 25 11:19 _tmp\n    -rw-r--r-- 1 mongodb nogroup       69 Apr 25 11:19 storage.bson\n    -rwxr-xr-x 1 mongodb nogroup        5 Apr 25 11:19 mongod.lock\n    -rw------- 1 mongodb nogroup 16777216 Apr 25 11:19 local.ns\n    -rw------- 1 mongodb nogroup 67108864 Apr 25 11:19 local.0\n    drwxr-xr-x 2 mongodb nogroup     4096 Apr 25 11:19 journal\n\n    ```", "```py\n    sudo service mongodb stop\n\n    ```", "```py\nan@an-VB:/usr/bin$ mongo\nMongoDB shell version: 3.0.2\nconnecting to: test\nServer has startup warnings: \n2015-05-30T07:03:49.387+0200 I CONTROL  [initandlisten] \n2015-05-30T07:03:49.388+0200 I CONTROL  [initandlisten] \n\n```", "```py\n> show dbs\nlocal  0.078GB\ntest   0.078GB\n```", "```py\n> use test\nswitched to db test\n```", "```py\n> show collections\nrestaurants\nsystem.indexes\n```", "```py\n> db.restaurants.find()\n{ \"_id\" : ObjectId(\"553b70055e82e7b824ae0e6f\"), \"address : { \"building : \"1007\", \"coord\" : [ -73.856077, 40.848447 ], \"street : \"Morris Park Ave\", \"zipcode : \"10462 }, \"borough : \"Bronx\", \"cuisine : \"Bakery\", \"grades : [ { \"grade : \"A\", \"score\" : 2, \"date\" : ISODate(\"2014-03-03T00:00:00Z\") }, { \"date\" : ISODate(\"2013-09-11T00:00:00Z\"), \"grade : \"A\", \"score\" : 6 }, { \"score\" : 10, \"date\" : ISODate(\"2013-01-24T00:00:00Z\"), \"grade : \"A }, { \"date\" : ISODate(\"2011-11-23T00:00:00Z\"), \"grade : \"A\", \"score\" : 9 }, { \"date\" : ISODate(\"2011-03-10T00:00:00Z\"), \"grade : \"B\", \"score\" : 14 } ], \"name : \"Morris Park Bake Shop\", \"restaurant_id : \"30075445\" }\n```", "```py\nconda install pymongo\n```", "```py\nfrom pymongo import MongoClient as MCli\n\nclass IO_mongo(object):\n    conn={'host':'localhost', 'ip':'27017'}\n```", "```py\n    def __init__(self, db='twtr_db', coll='twtr_coll', **conn ):\n        # Connects to the MongoDB server \n        self.client = MCli(**conn)\n        self.db = self.client[db]\n        self.coll = self.db[coll]\n```", "```py\n    def save(self, data):\n        # Insert to collection in db  \n        return self.coll.insert(data)\n```", "```py\n    def load(self, return_cursor=False, criteria=None, projection=None):\n\n            if criteria is None:\n                criteria = {}\n\n            if projection is None:\n                cursor = self.coll.find(criteria)\n            else:\n                cursor = self.coll.find(criteria, projection)\n\n            # Return a cursor for large amounts of data\n            if return_cursor:\n                return cursor\n            else:\n                return [ item for item in cursor ]\n```", "```py\n    class TwitterAPI(object):\n        \"\"\"\n        TwitterAPI class allows the Connection to Twitter via OAuth\n        once you have registered with Twitter and receive the \n        necessary credentials \n        \"\"\"\n\n        def __init__(self): \n            consumer_key = 'get_your_credentials'\n            consumer_secret = get your_credentials'\n            access_token = 'get_your_credentials'\n            access_secret = 'get your_credentials'\n            self.consumer_key = consumer_key\n            self.consumer_secret = consumer_secret\n            self.access_token = access_token\n            self.access_secret = access_secret\n            self.retries = 3\n            self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)\n            self.api = twitter.Twitter(auth=self.auth)\n    ```", "```py\n            # logger initialisation\n            appName = 'twt150530'\n            self.logger = logging.getLogger(appName)\n            #self.logger.setLevel(logging.DEBUG)\n            # create console handler and set level to debug\n            logPath = '/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data'\n            fileName = appName\n            fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logPath, fileName))\n            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n            fileHandler.setFormatter(formatter)\n            self.logger.addHandler(fileHandler) \n            self.logger.setLevel(logging.DEBUG)\n    ```", "```py\n            # Save to JSON file initialisation\n            jsonFpath = '/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data'\n            jsonFname = 'twtr15053001'\n            self.jsonSaver = IO_json(jsonFpath, jsonFname)\n    ```", "```py\n            # Save to MongoDB Intitialisation\n            self.mongoSaver = IO_mongo(db='twtr01_db', coll='twtr01_coll')\n    ```", "```py\n        def searchTwitter(self, q, max_res=10,**kwargs):\n            search_results = self.api.search.tweets(q=q, count=10, **kwargs)\n            statuses = search_results['statuses']\n            max_results = min(1000, max_res)\n\n            for _ in range(10):\n                try:\n                    next_results = search_results['search_metadata']['next_results']\n                    # self.logger.info('info' in searchTwitter - next_results:%s'% next_results[1:])\n                except KeyError as e:\n                    self.logger.error('error' in searchTwitter: %s', %(e))\n                    break\n\n                # next_results = urlparse.parse_qsl(next_results[1:]) # python 2.7\n                next_results = urllib.parse.parse_qsl(next_results[1:])\n                # self.logger.info('info' in searchTwitter - next_results[max_id]:', next_results[0:])\n                kwargs = dict(next_results)\n                # self.logger.info('info' in searchTwitter - next_results[max_id]:%s'% kwargs['max_id'])\n                search_results = self.api.search.tweets(**kwargs)\n                statuses += search_results['statuses']\n                self.saveTweets(search_results['statuses'])\n\n                if len(statuses) > max_results:\n                    self.logger.info('info' in searchTwitter - got %i tweets - max: %i' %(len(statuses), max_results))\n                    break\n            return statuses\n    ```", "```py\n        def saveTweets(self, statuses):\n            # Saving to JSON File\n            self.jsonSaver.save(statuses)\n\n            # Saving to MongoDB\n            for s in statuses:\n                self.mongoSaver.save(s)\n    ```", "```py\n        def parseTweets(self, statuses):\n            return [ (status['id'], \n                      status['created_at'], \n                      status['user']['id'],\n                      status['user']['name'] \n                      status['text''text'], \n                      url['expanded_url']) \n                            for status in statuses \n                                for url in status['entities']['urls'] ]\n    ```", "```py\n        def getTweets(self, q,  max_res=10):\n            \"\"\"\n            Make a Twitter API call whilst managing rate limit and errors.\n            \"\"\"\n            def handleError(e, wait_period=2, sleep_when_rate_limited=True):\n                if wait_period > 3600: # Seconds\n                    self.logger.error('Too many retries in getTweets: %s', %(e))\n                    raise e\n                if e.e.code == 401:\n                    self.logger.error('error 401 * Not Authorised * in getTweets: %s', %(e))\n                    return None\n                elif e.e.code == 404:\n                    self.logger.error('error 404 * Not Found * in getTweets: %s', %(e))\n                    return None\n                elif e.e.code == 429: \n                    self.logger.error('error 429 * API Rate Limit Exceeded * in getTweets: %s', %(e))\n                    if sleep_when_rate_limited:\n                        self.logger.error('error 429 * Retrying in 15 minutes * in getTweets: %s', %(e))\n                        sys.stderr.flush()\n                        time.sleep(60*15 + 5)\n                        self.logger.info('error 429 * Retrying now * in getTweets: %s', %(e))\n                        return 2\n                    else:\n                        raise e # Caller must handle the rate limiting issue\n                elif e.e.code in (500, 502, 503, 504):\n                    self.logger.info('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period))\n                    time.sleep(wait_period)\n                    wait_period *= 1.5\n                    return wait_period\n                else:\n                    self.logger.error('Exit - aborting - %s', %(e))\n                    raise e\n    ```", "```py\n            while True:\n                try:\n                    self.searchTwitter( q, max_res=10)\n                except twitter.api.TwitterHTTPError as e:\n                    error_count = 0 \n                    wait_period = handleError(e, wait_period)\n                    if wait_period is None:\n                        return\n    ```", "```py\nimport numpy as np\nimport pandas as pd\nfrom blaze import Data, by, join, merge\nfrom odo import odo\nBokehJS successfully loaded.\n```", "```py\ntwts_pd_df = pd.DataFrame(twts_csv_read, columns=Tweet01._fields)\ntwts_pd_df.head()\n\nOut[65]:\nid    created_at    user_id    user_name    tweet_text    url\n1   598831111406510082   2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...\n2   598831111406510082   2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...\n3   98808944719593472   2015-05-14 11:15:52   14755521 raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...    http://www.webex.com/ciscospark/\n4   598808944719593472   2015-05-14 11:15:52   14755521 raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://sparkjava.com/\n```", "```py\ntwts_pd_df.describe()\nOut[66]:\nid    created_at    user_id    user_name    tweet_text    url\ncount  19  19  19  19  19  19\nunique    7  7   6   6     6   7\ntop    598808944719593472    2015-05-14 11:15:52    14755521 raulsaeztapia    RT @alvaroagea: Simply @ApacheSpark http://t.c...    http://bit.ly/1Hfd0Xm\nfreq    6    6    9    9    6    6\n```", "```py\n#\n# Blaze dataframe\n#\ntwts_bz_df = Data(twts_pd_df)\n```", "```py\ntwts_bz_df.schema\nOut[73]:\ndshape(\"\"\"{\n  id: ?string,\n  created_at: ?string,\n  user_id: ?string,\n  user_name: ?string,\n  tweet_text: ?string,\n  url: ?string\n  }\"\"\")\n```", "```py\ntwts_bz_df.dshape\nOut[74]: \ndshape(\"\"\"19 * {\n  id: ?string,\n  created_at: ?string,\n  user_id: ?string,\n  user_name: ?string,\n  tweet_text: ?string,\n  url: ?string\n  }\"\"\")\n```", "```py\ntwts_bz_df.data\nOut[75]:\nid    created_at    user_id    user_name    tweet_text    url\n1    598831111406510082    2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...    http://www.mango-solutions.com/wp/2015/05/the-...\n2    598831111406510082    2015-05-14 12:43:57    14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...    http://www.mango-solutions.com/wp/2015/05/the-...\n... \n18   598782970082807808    2015-05-14 09:32:39    1377652806 embeddedcomputer.nl    RT @BigDataTechCon: Moving Rating Prediction w...    http://buff.ly/1QBpk8J\n19   598777933730160640     2015-05-14 09:12:38   294862170    Ellen Friedman   I'm still on Euro time. If you are too check o...http://bit.ly/1Hfd0Xm\n```", "```py\ntwts_bz_df.tweet_text.distinct()\nOut[76]:\n    tweet_text\n0   RT @pacoid: Great recap of @StrataConf EU in L...\n1   RT @alvaroagea: Simply @ApacheSpark http://t.c...\n2   RT @PrabhaGana: What exactly is @ApacheSpark a...\n3   RT @Ellen_Friedman: I'm still on Euro time. If...\n4   RT @BigDataTechCon: Moving Rating Prediction w...\n5   I'm still on Euro time. If you are too check o...\n```", "```py\ntwts_bz_df[['id', 'user_name','tweet_text']].distinct()\nOut[78]:\n  id   user_name   tweet_text\n0   598831111406510082   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...\n1   598808944719593472   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...\n2   598796205091500032   John Humphreys   RT @PrabhaGana: What exactly is @ApacheSpark a...\n3   598788561127735296   Leonardo D'Ambrosi   RT @Ellen_Friedman: I'm still on Euro time. If...\n4   598785545557438464   Alexey Kosenkov   RT @Ellen_Friedman: I'm still on Euro time. If...\n5   598782970082807808   embeddedcomputer.nl   RT @BigDataTechCon: Moving Rating Prediction w...\n6   598777933730160640   Ellen Friedman   I'm still on Euro time. If you are too check o...\n```", "```py\nOdo(source, target)\n```", "```py\nmongodb://username:password@hostname:port/database_name::collection_name\n```", "```py\nfilepath   = csvFpath\nfilename   = csvFname\nfilesuffix = csvSuffix\ntwts_odo_df = Data('{0}/{1}.{2}'.format(filepath, filename, filesuffix))\n```", "```py\ntwts_odo_df.count()\nOut[81]:\n19\n```", "```py\ntwts_odo_df.head(5)\nOut[82]:\n  id   created_at   user_id   user_name   tweet_text   url\n0   598831111406510082   2015-05-14 12:43:57   14755521   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...\n1   598831111406510082   2015-05-14 12:43:57   14755521   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...\n2   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://www.webex.com/ciscospark/\n3   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://sparkjava.com/\n4   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   https://www.sparkfun.com/\n```", "```py\ntwts_odo_df.dshape\nOut[83]:\ndshape(\"var * {\n  id: int64,\n  created_at: ?datetime,\n  user_id: int64,\n  user_name: ?string,\n  tweet_text: ?string,\n  url: ?string\n  }\"\"\")\n```", "```py\nodo(twts_odo_distinct_df, '{0}/{1}.{2}'.format(jsonFpath, jsonFname, jsonSuffix))\nOut[92]:\n<odo.backends.json.JSONLines at 0x7f77f0abfc50>\n```", "```py\nodo('{0}/{1}.{2}'.format(jsonFpath, jsonFname, jsonSuffix), '{0}/{1}.{2}'.format(csvFpath, csvFname, csvSuffix))\nOut[94]:\n<odo.backends.csv.CSV at 0x7f77f0abfe10>\n```", "```py\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext, Row\nIn [95]:\nsc\nOut[95]:\n<pyspark.context.SparkContext at 0x7f7829581890>\nIn [96]:\nsc.master\nOut[96]:\nu'local[*]'\n''In [98]:\n# Instantiate Spark  SQL context\nsqlc =  SQLContext(sc)\n```", "```py\ntwts_sql_df_01 = sqlc.jsonFile (\"/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401_distinct.json\")\nIn [101]:\ntwts_sql_df_01.show()\ncreated_at           id                 tweet_text           user_id    user_name          \n2015-05-14T12:43:57Z 598831111406510082 RT @pacoid: Great... 14755521   raulsaeztapia      \n2015-05-14T11:15:52Z 598808944719593472 RT @alvaroagea: S... 14755521   raulsaeztapia      \n2015-05-14T10:25:15Z 598796205091500032 RT @PrabhaGana: W... 48695135   John Humphreys     \n2015-05-14T09:54:52Z 598788561127735296 RT @Ellen_Friedma... 2385931712 Leonardo D'Ambrosi\n2015-05-14T09:42:53Z 598785545557438464 RT @Ellen_Friedma... 461020977  Alexey Kosenkov    \n2015-05-14T09:32:39Z 598782970082807808 RT @BigDataTechCo... 1377652806 embeddedcomputer.nl\n2015-05-14T09:12:38Z 598777933730160640 I'm still on Euro... 294862170  Ellen Friedman     \n```", "```py\ntwts_sql_df_01.printSchema()\nroot\n |-- created_at: string (nullable = true)\n |-- id: long (nullable = true)\n |-- tweet_text: string (nullable = true)\n |-- user_id: long (nullable = true)\n |-- user_name: string (nullable = true)\n```", "```py\ntwts_sql_df_01.select('user_name').show()\nuser_name          \nraulsaeztapia      \nraulsaeztapia      \nJohn Humphreys     \nLeonardo D'Ambrosi\nAlexey Kosenkov    \nembeddedcomputer.nl\nEllen Friedman     \n```", "```py\ntwts_sql_df_01.registerAsTable('tweets_01')\n```", "```py\ntwts_sql_df_01_selection = sqlc.sql(\"SELECT * FROM tweets_01 WHERE user_name = 'raulsaeztapia'\")\nIn [109]:\ntwts_sql_df_01_selection.show()\ncreated_at           id                 tweet_text           user_id  user_name    \n2015-05-14T12:43:57Z 598831111406510082 RT @pacoid: Great... 14755521 raulsaeztapia\n2015-05-14T11:15:52Z 598808944719593472 RT @alvaroagea: S... 14755521 raulsaeztapia\n```", "```py\ntweets_sqlc_inf = sqlc.jsonFile(infile)\n```", "```py\ntweets_sqlc_inf.printSchema()\nroot\n |-- contributors: string (nullable = true)\n |-- coordinates: string (nullable = true)\n |-- created_at: string (nullable = true)\n |-- entities: struct (nullable = true)\n |    |-- hashtags: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- indices: array (nullable = true)\n |    |    |    |    |-- element: long (containsNull = true)\n |    |    |    |-- text: string (nullable = true)\n |    |-- media: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- display_url: string (nullable = true)\n |    |    |    |-- expanded_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- id_str: string (nullable = true)\n |    |    |    |-- indices: array (nullable = true)\n... (snip) ...\n|    |-- statuses_count: long (nullable = true)\n |    |-- time_zone: string (nullable = true)\n |    |-- url: string (nullable = true)\n |    |-- utc_offset: long (nullable = true)\n |    |-- verified: boolean (nullable = true)\n```", "```py\ntweets_extract_sqlc = tweets_sqlc_inf[['created_at', 'id', 'text', 'user.id', 'user.name', 'entities.urls.expanded_url']].distinct()\nIn [145]:\ntweets_extract_sqlc.show()\ncreated_at           id                 text                 id         name                expanded_url        \nThu May 14 09:32:... 598782970082807808 RT @BigDataTechCo... 1377652806 embeddedcomputer.nl ArrayBuffer(http:...\nThu May 14 12:43:... 598831111406510082 RT @pacoid: Great... 14755521   raulsaeztapia       ArrayBuffer(http:...\nThu May 14 12:18:... 598824733086523393 @rabbitonweb spea... \n\n...   \nThu May 14 12:28:... 598827171168264192 RT @baandrzejczak... 20909005   Pawe\u0142 Szulc         ArrayBuffer()       \n```", "```py\ntweets_extract_sqlc_sel = sqlc.sql(\"SELECT * from Tweets_xtr_001 WHERE name='raulsaeztapia'\")\n```", "```py\ntweets_extract_sqlc_sel.explain(extended = True)\n== Parsed Logical Plan ==\n'Project [*]\n 'Filter ('name = raulsaeztapia)'name'  'UnresolvedRelation' [Tweets_xtr_001], None\n== Analyzed Logical Plan ==\nProject [created_at#7,id#12L,text#27,id#80L,name#81,expanded_url#82]\n Filter (name#81 = raulsaeztapia)\n  Distinct \n   Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]\n    Relation[contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29] JSONRelation(/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401.json,1.0,None)\n== Optimized Logical Plan ==\nFilter (name#81 = raulsaeztapia)\n Distinct \n  Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]\n   Relation[contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29] JSONRelation(/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401.json,1.0,None)\n== Physical Plan ==\nFilter (name#81 = raulsaeztapia)\n Distinct false\n  Exchange (HashPartitioning [created_at#7,id#12L,text#27,id#80L,name#81,expanded_url#82], 200)\n   Distinct true\n    Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]\n     PhysicalRDD [contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29], MapPartitionsRDD[165] at map at JsonRDD.scala:41\nCode Generation: false\n== RDD ==\n```", "```py\ntweets_extract_sqlc_sel.show()\ncreated_at           id                 text                 id       name          expanded_url        \nThu May 14 12:43:... 598831111406510082 RT @pacoid: Great... 14755521 raulsaeztapia ArrayBuffer(http:...\nThu May 14 11:15:... 598808944719593472 RT @alvaroagea: S... 14755521 raulsaeztapia ArrayBuffer(http:...\nIn [148]:\n```", "```py\n$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0\n\n```", "```py\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0\n\n```", "```py\n... (snip) ...\nIvy Default Cache set to: /home/an/.ivy2/cache\nThe jars for the packages stored in: /home/an/.ivy2/jars\n:: loading settings :: url = jar:file:/home/an/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n  confs: [default]\n  found com.databricks#spark-csv_2.11;1.2.0 in central\n  found org.apache.commons#commons-csv;1.1 in central\n  found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 835ms :: artifacts dl 48ms\n  :: modules in use:\n  com.databricks#spark-csv_2.11;1.2.0 from central in [default]\n  com.univocity#univocity-parsers;1.5.1 from central in [default]\n  org.apache.commons#commons-csv;1.1 from central in [default]\n  ----------------------------------------------------------------\n  |               |          modules            ||   artifacts   |\n  |    conf     | number| search|dwnlded|evicted|| number|dwnlded|\n  ----------------------------------------------------------------\n  |    default     |   3   |   0   |   0   |   0   ||   3   |   0   \n  ----------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n  confs: [default]\n  0 artifacts copied, 3 already retrieved (0kB/45ms)\n```", "```py\n#\n# Read csv in a Spark DF\n#\nsqlContext = SQLContext(sc)\nspdf_in = sqlContext.read.format('com.databricks.spark.csv')\\\n                                    .options(delimiter=\";\").options(header=\"true\")\\\n                                    .options(header='true').load(csv_in)\n```", "```py\nIn [10]:\nspdf_in.printSchema()\nroot\n |-- : string (nullable = true)\n |-- id: string (nullable = true)\n |-- created_at: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- user_name: string (nullable = true)\n |-- tweet_text: string (nullable = true)\n```", "```py\nIn [12]:\nspdf_in.columns\nOut[12]:\n['', 'id', 'created_at', 'user_id', 'user_name', 'tweet_text']\n```", "```py\nIn [13]:\nspdf_in.show()\n+---+------------------+--------------------+----------+------------------+--------------------+\n|   |                id|          created_at|   user_id|         user_name|          tweet_text|\n+---+------------------+--------------------+----------+------------------+--------------------+\n|  0|638830426971181057|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|\n|  1|638830426727911424|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|\n|  2|638830425402556417|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|\n... (snip) ...\n| 41|638830280988426250|Tue Sep 01 21:46:...| 951081582|      Jack Baldwin|RT @cloudaus: We ...|\n| 42|638830276626399232|Tue Sep 01 21:46:...|   6525302|Masayoshi Nakamura|PynamoDB\u4f7f\u3044\u3084\u3059\u3044\u3067\u3059  |\n+---+------------------+--------------------+----------+------------------+--------------------+\nonly showing top 20 rows\n```", "```py\n$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1\n\n```", "```py\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1\n... (snip) ... \nIvy Default Cache set to: /home/an/.ivy2/cache\nThe jars for the packages stored in: /home/an/.ivy2/jars\n:: loading settings :: url = jar:file:/home/an/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.stratio.datasource#spark-mongodb_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n  confs: [default]\n  found com.stratio.datasource#spark-mongodb_2.10;0.10.1 in central\n[W 22:10:50.910 NotebookApp] Timeout waiting for kernel_info reply from 764081d3-baf9-4978-ad89-7735e6323cb6\n  found org.mongodb#casbah-commons_2.10;2.8.0 in central\n  found com.github.nscala-time#nscala-time_2.10;1.0.0 in central\n  found joda-time#joda-time;2.3 in central\n  found org.joda#joda-convert;1.2 in central\n  found org.slf4j#slf4j-api;1.6.0 in central\n  found org.mongodb#mongo-java-driver;2.13.0 in central\n  found org.mongodb#casbah-query_2.10;2.8.0 in central\n  found org.mongodb#casbah-core_2.10;2.8.0 in central\ndownloading https://repo1.maven.org/maven2/com/stratio/datasource/spark-mongodb_2.10/0.10.1/spark-mongodb_2.10-0.10.1.jar ...\n  [SUCCESSFUL ] com.stratio.datasource#spark-mongodb_2.10;0.10.1!spark-mongodb_2.10.jar (3130ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/casbah-commons_2.10/2.8.0/casbah-commons_2.10-2.8.0.jar ...\n  [SUCCESSFUL ] org.mongodb#casbah-commons_2.10;2.8.0!casbah-commons_2.10.jar (2812ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/casbah-query_2.10/2.8.0/casbah-query_2.10-2.8.0.jar ...\n  [SUCCESSFUL ] org.mongodb#casbah-query_2.10;2.8.0!casbah-query_2.10.jar (1432ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/casbah-core_2.10/2.8.0/casbah-core_2.10-2.8.0.jar ...\n  [SUCCESSFUL ] org.mongodb#casbah-core_2.10;2.8.0!casbah-core_2.10.jar (2785ms)\ndownloading https://repo1.maven.org/maven2/com/github/nscala-time/nscala-time_2.10/1.0.0/nscala-time_2.10-1.0.0.jar ...\n  [SUCCESSFUL ] com.github.nscala-time#nscala-time_2.10;1.0.0!nscala-time_2.10.jar (2725ms)\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.6.0/slf4j-api-1.6.0.jar ...\n  [SUCCESSFUL ] org.slf4j#slf4j-api;1.6.0!slf4j-api.jar (371ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/2.13.0/mongo-java-driver-2.13.0.jar ...\n  [SUCCESSFUL ] org.mongodb#mongo-java-driver;2.13.0!mongo-java-driver.jar (5259ms)\ndownloading https://repo1.maven.org/maven2/joda-time/joda-time/2.3/joda-time-2.3.jar ...\n  [SUCCESSFUL ] joda-time#joda-time;2.3!joda-time.jar (6949ms)\ndownloading https://repo1.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar ...\n  [SUCCESSFUL ] org.joda#joda-convert;1.2!joda-convert.jar (548ms)\n:: resolution report :: resolve 11850ms :: artifacts dl 26075ms\n  :: modules in use:\n  com.github.nscala-time#nscala-time_2.10;1.0.0 from central in [default]\n  com.stratio.datasource#spark-mongodb_2.10;0.10.1 from central in [default]\n  joda-time#joda-time;2.3 from central in [default]\n  org.joda#joda-convert;1.2 from central in [default]\n  org.mongodb#casbah-commons_2.10;2.8.0 from central in [default]\n  org.mongodb#casbah-core_2.10;2.8.0 from central in [default]\n  org.mongodb#casbah-query_2.10;2.8.0 from central in [default]\n  org.mongodb#mongo-java-driver;2.13.0 from central in [default]\n  org.slf4j#slf4j-api;1.6.0 from central in [default]\n  ---------------------------------------------------------------------\n  |                  |            modules            ||   artifacts   |\n  |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n  ---------------------------------------------------------------------\n  |      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n  ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n  confs: [default]\n  9 artifacts copied, 0 already retrieved (2335kB/51ms)\n... (snip) ... \n```", "```py\nIn [5]:\nfrom pyspark.sql import SQLContext\nsqlContext.sql(\"CREATE TEMPORARY TABLE tweet_table USING com.stratio.datasource.mongodb OPTIONS (host 'localhost:27017', database 'twtr01_db', collection 'twtr01_coll')\")\nsqlContext.sql(\"SELECT * FROM tweet_table where id=598830778269769728 \").collect()\n```", "```py\nOut[5]:\n[Row(text=u'@spark_io is now @particle - awesome news - now I can enjoy my Particle Cores/Photons + @sparkfun sensors + @ApacheSpark analytics :-)', _id=u'55aa640fd770871cba74cb88', contributors=None, retweeted=False, user=Row(contributors_enabled=False, created_at=u'Mon Aug 25 14:01:26 +0000 2008', default_profile=True, default_profile_image=False, description=u'Building open source tools for and teaching enterprise software developers', entities=Row(description=Row(urls=[]), url=Row(urls=[Row(url=u'http://t.co/TSHp13EWeu', indices=[0, 22], \n\n... (snip) ...\n\n 9], name=u'Spark is Particle', screen_name=u'spark_io'), Row(id=487010011, id_str=u'487010011', indices=[17, 26], name=u'Particle', screen_name=u'particle'), Row(id=17877351, id_str=u'17877351', indices=[88, 97], name=u'SparkFun Electronics', screen_name=u'sparkfun'), Row(id=1551361069, id_str=u'1551361069', indices=[108, 120], name=u'Apache Spark', screen_name=u'ApacheSpark')]), is_quote_status=None, lang=u'en', quoted_status_id_str=None, quoted_status_id=None, created_at=u'Thu May 14 12:42:37 +0000 2015', retweeted_status=None, truncated=False, place=None, id=598830778269769728, in_reply_to_user_id=3187046084, retweet_count=0, in_reply_to_status_id=None, in_reply_to_screen_name=u'spark_io', in_reply_to_user_id_str=u'3187046084', source=u'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>', id_str=u'598830778269769728', coordinates=None, metadata=Row(iso_language_code=u'en', result_type=u'recent'), quoted_status=None)]\n#\n```"]
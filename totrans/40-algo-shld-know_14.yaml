- en: Data Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is about data-centric algorithms and, in particular, it focuses
    on three aspects of data-centric algorithms: storage, streaming, and compression.
    This chapter starts with a brief overview of data-centric algorithms, then we
    will discuss various strategies that can be used in data storage. Next, how to
    apply algorithms to streaming data is described, then different methodologies
    to compress data are discussed. Finally, we will learn how we can use the concepts
    developed in this chapter to monitor the speeds of cars traveling on a highway
    using a state-of-the-art sensor network.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should be able to understand the concepts and
    trade-offs involved in the design of various data-centric algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter discusses the  following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Data classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use algorithms to compress data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use algorithms to stream data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's first introduce the basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to data algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether we realize it or not, we are living in an era of big data. Just to get
    an idea about how much data is constantly being generated, just look into some
    of the numbers published by Google for 2019\. As we know, Google Photos is the
    multimedia repository for storing photos created by Google. In 2019, an average
    of 1.2 billion photos and videos were uploaded to Google Photos every day. Also,
    an average of 400 hours of video (amounting to 1 PB of data) were uploaded every
    minute each day to YouTube. We can safely say the amount of data that is being
    generated has simply exploded.
  prefs: []
  type: TYPE_NORMAL
- en: The current interest in data-driven algorithms is driven by the fact that data
    contains valuable information and patterns. If used in the right way, data can
    become the basis of policy-making decisions, marketing, governance, and trend
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: For obvious reasons, algorithms that deal with data are becoming more and more
    important. Designing algorithms that can process data is an active area of research.  There
    is no doubt that exploring the best ways to use data to provide some quantifiable
    benefit is the focus of various organizations, businesses, and governments all
    over the world. But data in its raw form is seldom useful. To mine the information
    from the raw data, it needs to be processed, prepared, and analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: For that, we first need to store it somewhere. Efficient methodologies to store
    the data are becoming more and more important. Note that due to the physical storage
    limitations of single-node systems, big data can only be stored in distributed
    storage consisting of more than one node connected by high-speed communication
    links. So, it makes sense that, for learning data algorithms, we start by looking
    at different data storage algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's classify data into various categories.
  prefs: []
  type: TYPE_NORMAL
- en: Data classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look into how we can classify data in the context of designing data algorithms.
    As discussed in [Chapter 2](04672393-683c-406b-8dd1-4dab5b5d9c4f.xhtml), *Data
    Structures Used in Algorithms*, quantifying the volume, variety, and velocity
    of the data can be used to classify it. This classification can become a basis
    to design data algorithms that can be used for its storage and processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look into these characteristics one by one in the context of data algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**  quantifies the amount of data that needs to be stored and processed
    in an algorithm. As the volume increases, the task becomes data-intensive and
    requires provisioning enough resources to store, cache, and process data. Big
    data is a term that vaguely defines a large volume of data that cannot be handled
    by a single node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**  defines the rate at which new data is being generated. Usually,
    high-velocity data is called "hot data" or a "hot stream" and low-velocity data
    is called a "cold stream" or simply "cold data". In many applications, data will
    be a mix of hot and cold streams that will first need to be prepared and combined
    into a single table before it can be used with the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**  refers to different types of structured and unstructured data
    that needs to be combined into a single table before it can be used by the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section will help us to understand the trade-offs involved and will
    present various design choices when designing storage algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting data storage algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A reliable and efficient data repository is the heart of a distributed system.
    If this data repository is created for analytics, then it is also called a data
    lake. A data repository brings together data from different domains into a single
    location. Let's start with first understanding different issues related to the
    storage of data in a distributed repository.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data storage strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the initial years of digital computing, the usual way of designing a data
    repository was by using a single node architecture. With the ever-increasing sizes
    of datasets, distributed storage of data has now become mainstream. The right
    strategy to store data in a distributed environment depends on the type of data
    and its expected usage pattern as well as its non-functional requirements. To
    further analyze the requirements of a distributed data store, let's start with
    the  **Consistency Availability Partition-Tolerance (CAP)**  theorem, which provides
    us with the basis of devising a data storage strategy for a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the CAP theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1998, Eric Brewer proposed a theorem that later became famous as the CAP
    theorem. It highlights the various trade-offs involved in designing a distributed
    storage system.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the CAP theorem, first, let''s define the following three characteristics
    of distributed storage systems: consistency, availability, and partition tolerance.
    CAP is, in fact, an acronym made up of these three characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency** (or simply C): The distributed storage consists of various
    nodes. Any of these nodes can be used to read, write, or update records in the
    data repository. Consistency guarantees that at a certain time, *t[1]*, independent
    of which node we use to read the data, we will get the same result. Every *read*
    operation either returns the latest data that is consistent across the distributed
    repository or gives an error message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability** (or simply A):Availability guarantees that any node in the
    distributed storage system will be able to immediately handle the request with
    or without consistency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition Tolerance** (or simply P): In a distributed system, multiple nodes
    are connected via a communication network. Partition tolerance guarantees that,
    in the event of communication failure between a small subset of nodes (one or
    more), the system remains operational. Note that to guarantee partition tolerance,
    data needs to be replicated across a sufficient number of nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these three characteristics, the CAP theorem carefully summarizes the
    trade-offs involved in the architecture and design of a distributed system. Specifically,
    the CAP theorem states that, in a storage system, we can only have two of the
    following characteristics: consistency or C, availability or A, and partition
    tolerance or P.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aed77c3c-5f64-4ccd-9799-36306f2bb941.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The CAP theorem also means that we can have three types of distributed storage
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: A CA system (implementing Consistency-Availability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AP system (implementing Availability-Partition Tolerance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A CP system (implementing Consistency-Partition Tolerance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look into them, one by one.
  prefs: []
  type: TYPE_NORMAL
- en: CA systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional single-node systems are CA systems. This is because if we do not
    have a distributed system, then we do not need to worry about partition tolerance.
    In that case, we can have a system that has both consistency and availability,
    that is, a CA system.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional single-node databases such as Oracle or MySQL are all examples of
    CA systems.
  prefs: []
  type: TYPE_NORMAL
- en: AP systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AP systems are the distributed storage systems tuned for availability. Designed
    as highly-responsive systems, they can sacrifice consistency, if needed, to accommodate
    high-velocity data. This means that these are distributed storage systems that
    are designed to immediately handle requests from users. Typical user requests
    are to read or write fast-changing data. Typical AP systems are used in real-time
    monitoring systems such as sensor networks.
  prefs: []
  type: TYPE_NORMAL
- en: High-velocity distributed databases, such as Cassandra, are good examples of
    AP systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look into where an AP system can be used. If Transport Canada wants to
    monitor traffic on one of the highways in Ottawa through a network of sensors
    installed at different locations on the highway, an AP system is recommended for
    implementing distributed data storage.
  prefs: []
  type: TYPE_NORMAL
- en: CP systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CP systems have both consistency and partition tolerance. This means that these
    are the distributed storage systems that are tuned to guarantee consistency before
    a read process can fetch a value.
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case for CP systems is when we want to store document files in
    JSON format. Document datastores such as MongoDB are CP systems tuned for consistency
    in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed data storage is increasingly becoming the most important part of
    a modern IT infrastructure. Distributed data storage should be carefully designed
    based on the characteristics of the data and the requirements of the problem we
    want to solve. Classifying data storage into CA, AP, and CP systems help us to
    understand the various trade-offs involved when designing data storage systems.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look into streaming data algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting streaming data algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be categorized as bounded or unbounded. Bounded data is data at rest
    and is usually processed through a batch process. Streaming is basically data
    processing on unbounded data. Let's look into an example. Let's assume that we
    are analyzing fraudulent transactions at a bank. If we want to look for fraud
    transactions 7 days ago, we have to look at the data at rest; this is an example
    of a batch process.
  prefs: []
  type: TYPE_NORMAL
- en: n the other hand, if we want to detect fraud in real-time, that is an example
    of streaming. Hence, streaming data algorithms are those algorithms that deal
    with processing data streams. The fundamental idea is to divide the input data
    stream into batches, which are then processed by the processing node. Streaming
    algorithms need to be fault-tolerant and should be able to handle the incoming
    velocity of data. As the demand for real-time trend analysis is increasing, the
    demand for stream processing is also increasing these days. Note that, for streaming
    to work, data has to be processed fast and while designing algorithms, this needs
    to be always kept in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many applications of streaming data and its utilization in a meaningful
    way. Some of the applications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart order routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Live dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic sensors along highways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit card transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User moves in multi-user online gaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's see how we can implement streaming using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting data compression algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data compression algorithms are involved in the process of reducing the size
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve into a specific data compression algorithm named
    the lossless compression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Lossless compression algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are algorithms that are capable of compressing data in such a way that
    it can be decompressed without any loss of information. They are used when it
    is important to retrieve the exact original files after decompression. Typical
    uses of lossless compression algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To compress documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To compress and package source code and  executable files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To convert a large number of small files into a small number of large files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the basic techniques of lossless compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data compression is based on the principle that the majority of data is known
    to be using more bits than its entropy indicates is optimal. Recall that entropy
    is a term used to specify the information that the data carries. It means that
    a more optimal bit representation of the same information is possible. Exploring
    and formulating more efficient bit representation becomes the basis for devising
    compression algorithms. Lossless data compression takes advantage of this redundancy
    to compress data without losing any information. In the late '80s, Ziv and Lempel
    proposed dictionary-based data compression techniques that can be used to implement
    lossless data compression. These techniques were an instant hit due to their speed
    and good compression rate. These techniques were used to create the popular Unix-based
    *compress* tool. Also, the ubiquitous `gif` image format uses these compression
    techniques, which proved to be popular as they could be used to represent the
    same information in a lesser number of bits, saving space and communication bandwidth.
    These techniques later became the basis of developing the `zip` utility and its
    variants. The compression standard, V.44, used in modems is also based on it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the techniques one by one in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Huffman coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Huffman coding is one of the oldest methods of compressing data and is based
    on creating a Huffman tree, which is used to both encode and decode data. Huffman
    coding can represent data content in a more compact form by exploiting the fact
    that some data (for instance certain characters of the alphabet) appears more
    frequently in a data stream. By using encodings of different length (shorter for
    the most frequent characters and longer for the least frequent ones), the data
    consumes less space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn a few terminologies related to Huffman coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coding**: Coding in the context of data represents the method of representing
    data from one form to another. We would like the resultant form to be concise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Codeword**:  A particular character in encoded form is called a codeword.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fixed-length coding**:  This is when each  encoded  character, that is, codeword,
    uses the same number of bits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable-length coding**: Codewords can use a different number of bits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation of code**:  This is the expected number of bits per codeword.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prefix free codes**: This means that no codeword is a prefix of any other
    codeword.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoding**: This means that a variable-length code must be free from any
    prefix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For understanding the last two terms, you need to have a look at this table
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **character** | **frequency** | **fixed length code** | **variable length
    code** |'
  prefs: []
  type: TYPE_TB
- en: '| L | .45 | 000 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| M | .13 | 001 | 101 |'
  prefs: []
  type: TYPE_TB
- en: '| N | .12 | 010 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| X | .16 | 011 | 111 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | .09 | 100 | 1101 |'
  prefs: []
  type: TYPE_TB
- en: '| Z | .05 | 101 | 1100 |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we can infer the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed length code:** The fixed-length code for this table is 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable length code:** The variable-length code for this table is *45(1)
    + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) = 2.24*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the Huffman tree created from the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/87919c43-e9b2-4df9-8d42-adf6b96c29d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that Huffman encoding is about converting data into a Huffman tree that
    enables compression. Decoding or decompression brings the data back to the original
    format.
  prefs: []
  type: TYPE_NORMAL
- en: A practical example â€“ Twitter real-time sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Twitter is said to have almost 7,000 tweets every second on a wide variety
    of topics. Let''s try to build a sentiment analyzer that can capture the emotions
    of the news from different news sources in real time. We will start by importing
    the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are using the following two packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VADER** sentiment analysis, which stands for **Valence Aware Dictionary and
    Sentiment Reasoner**. It is one of the popular rule-based sentiment analysis tools
    that is developed for social media. If you have never used it before, then you
    will first have to run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`Tweepy`, which is a Python-based API to access Twitter. Again, if you have
    never used it before, you need to first run this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is a bit tricky. You need to make a request to create a developer
    account with Twitter to get access to the live stream of tweets. Once you have
    the API keys, you can represent them with the following variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s then configure the `Tweepy` API authentication. For that, we need to
    provide the previously created variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the interesting part. We will choose the Twitter handles of the news
    sources that we want to monitor for sentiment analysis. For this example, we have
    chosen the following news sources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the main loop. This loop will start with an empty array
    called `array_sentiments` to hold the sentiments. Then, we will loop through all
    five news sources and collect 100 tweets each. Then, for each tweet, we will calculate
    its polarity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/0270c5ec-21f7-4aa3-b229-4544795df1c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s create a graph that shows the polarity of the news from these individual
    news sources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/85779482-ee6d-485a-b4f2-d432095bffba.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that each of the news sources is represented by a different color.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/8f9fb7d1-bc46-4e29-8c1e-3aa6d7390809.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding numbers summarize the trends of the sentiments. For example, the
    sentiments of BBC are found to be the most positive, and the Canadian news channel,
    CTVnews, seems to be carrying the most negative emotions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at the design of data-centric algorithms. We focused
    on three aspects of data-centric algorithms: storage, compression, and streaming.'
  prefs: []
  type: TYPE_NORMAL
- en: We looked into how the characteristics of data can dictate the data storage
    design. We looked into two different types of data compression algorithms. Then,
    we studied a practical example of how data streaming algorithms can be used to
    count words from a stream of textual data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at cryptographic algorithms. We will learn
    how we can use the power of these algorithms to secure  exchanged and stored messages.
  prefs: []
  type: TYPE_NORMAL

- en: Chapter 8. Handling Input Devices and Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Android is all about interaction. Admittedly, that means feedback, through
    graphics, audio, vibrations, and so on. But there is no interaction without input!
    The success of today''s smartphones takes its root in their multiple and modern
    input possibilities: touchscreens, keyboard, mouse, GPS, accelerometer, light
    detector, sound recorder, and so on. Handling and combining them properly is a
    key to enrich your application and to make it successful.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Although Android handles many input peripherals, the Android NDK has long been
    very limited in its support (not to say the very least), until the release of
    R5! We can now access it directly through a native API. Examples of available
    devices are:'
  prefs: []
  type: TYPE_NORMAL
- en: Keyboard, either physical (with a slide-out keyboard) or virtual (which appears
    on screen)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directional pad (up, down, left, right, and action buttons), often abbreviated
    as D-Pad.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trackball, optical ones included
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Touchscreen, which has made modern smart-phones successful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mouse or Track Pad (since NDK R5, but available on Honeycomb devices only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also access hardware sensors, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Accelerometer, which measures the linear acceleration applied to a device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gyroscope, which measures the angular velocity. It is often combined with the
    magnetometer to compute orientation accurately and quickly. Gyroscope has been
    introduced recently and is not available on most devices yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magnetometer, which gives the ambient magnetic field and consequently the cardinal
    direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light sensor, for example, to automatically adapt to screen luminosity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proximity sensor, for example, to detect ear distance during a call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to hardware sensors, "software sensors" have been introduced with
    Gingerbread. These sensors are derived from the hardware sensor''s data:'
  prefs: []
  type: TYPE_NORMAL
- en: Gravity sensor, to measure the gravity direction and magnitude
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear acceleration sensor, which measures device "movement" excluding gravity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotation vector, which indicates device orientation in space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gravity sensor and the linear acceleration sensor are derived from the accelerometer.
    On the other hand, rotation vector is derived from the magnetometer and the accelerometer.
    Because these sensors are generally computed over time, they usually incur a slight
    delay in getting up-to-date values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To familiarize ourselves more deeply with input devices and sensors, this chapter
    teaches how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Handle screen touches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect keyboard, D-Pad, and trackball events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn the accelerometer sensor into a joypad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with touch events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most emblematic innovation of today's smart phones is the touchscreen, which
    has replaced the now antique mice. A touchscreen detects, as its name suggests,
    touches made with fingers or styluses on a device's surface. Depending on the
    quality of the screen, several touches (also referred to as cursors in Android)
    can be handled, de-multiplying interaction possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s start this chapter by handling touch events in `DroidBlaster`. To
    keep the example simple, we will only handle a single "touch". The goal is to
    move the ship in the direction of touch. The farther the touch, the faster the
    ship goes. Beyond a predefined range `TOUCH_MAX_RANGE`, the ship''s speed reaches
    its speed limit, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Interacting with touch events](img/9645OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The resulting project is provided with this book under the name `DroidBlaster_Part13`.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action – handling touch events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s intercept touch events in `DroidBlaster`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way that we created `ActivityHandler` to process application events
    in [Chapter 5](ch05.html "Chapter 5. Writing a Fully Native Application"), *Writing
    a Fully Native Application*, create `jni/InputHandler.hpp` to process input events.
    The input API is declared in `android/input.h`. Create `onTouchEvent()` to handle
    touch events. These events are packaged in an `AInputEvent` structure. Other input
    peripherals will be described later in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Modify the `jni/EventLoop.hpp` header file to include and handle an `InputHandler`
    instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a similar way, to activity events, define an internal method `processInputEvent()`,
    which is triggered by a static callback `callback_input()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We need to process input events in the `jni/EventLoop.cpp` source file and notify
    the associated `InputHandler`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, connect the Android input queue to `callback_input()`. The `EventLoop`
    itself (that is, `this`) is passed anonymously through the `userData` member of
    the `android_app` structure. That way, callback is able to delegate input processing
    back to our own object, that is, to `processInputEvent()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Touchscreen events are of the type `MotionEvent` (as opposed to key events).
    They can be discriminated according to their source (`AINPUT_SOURCE_TOUCHSCREEN`)
    thanks to the Android native input API (here, `AinputEvent_getSource()`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note how `callback_input()` and by extension `processInputEvent()` return an
    integer value (which is intrinsically a Boolean value). This value indicates that
    an input event (for example, a pressed button) has been processed by the application
    and does not need to be processed further by the system. For example, `1` is returned
    when the back button is pressed to stop event processing and prevent the activity
    from getting terminated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Create `jni/InputManager.hpp` to handle touch events and implement our new `InputHandler`
    interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the methods as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start()` to perform the necessary initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onTouchEvent()` to update the manager state when a new event is triggered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getDirectionX()` and `getDirectionY()` to indicate the ship direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setRefPoint()` refers to the ship position. Indeed, the direction is defined
    as the vector between the touch point and the ship location (that is, the reference
    point).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, declare the necessary members and more specifically `mScaleFactor`, which
    contains the proper ratio to convert the input event from screen coordinates to
    game coordinates (remember that we use a fixed size).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `jni/InputManager.cpp`, starting with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the `start()` method to clear members and compute the scale factor. The
    scale factor is necessary because, as seen in [Chapter 6](ch06.html "Chapter 6. Rendering
    Graphics with OpenGL ES"), *Rendering Graphics with OpenGL ES*, we need to convert
    screen coordinates provided in input events (which depends on the device) into
    game coordinates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The effective event processing comes in `onTouchEvent()`. Horizontal and vertical
    directions are computed according to the distance between the reference point
    and the touch point. This distance is restricted by `TOUCH_MAX_RANGE` to an arbitrary
    range of `65` units. Thus, a ship's maximum speed is reached when the reference-to-touch
    point distance is beyond `TOUCH_MAX_RANGE` pixels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Touch coordinates are retrieved thanks to `AMotionEvent_getX()` and `AMotionEvent_getY()`
    when you move your finger. The direction vector is reset to `0` when no more touch
    is detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a simple component `jni/MoveableBody.hpp`, whose role is to move `PhysicsBody`
    according to input events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Implement this component in `jni/MoveableBody.cpp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`InputManager` and the body are bound in `registerMoveableBody()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Initially, the body has no velocity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, each time it is updated, the velocity mirrors the current input state.
    This velocity is taken in input by `PhysicsManager` created in [Chapter 5](ch05.html
    "Chapter 5. Writing a Fully Native Application"), *Writing a Fully Native Application*,
    to update the entity''s position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Reference the new `InputManager` and `MoveableComponent` in `jni/DroidBlaster.hpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Finally, adapt the `jni/DroidBlaster.cpp` constructor to instantiate `InputManager`
    and `MoveableComponent`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append `InputManager` to `EventLoop`, which dispatches input events, at construction
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The spaceship is the entity being moved. So, pass a reference to its location
    to the `MoveableBody` component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize and update `MoveableBody` and `InputManager` in the corresponding
    methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*What just happened?*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created a simple example of an input system, based on touch events. The ship
    flies toward the touch point at a speed dependent on the touch distance. The touch
    event coordinates are absolute. Their origin is in the upper-left corner of the
    screen, on the opposite of OpenGL, which is on the lower-left corner. If screen
    rotation is permitted by an application, then the screen origin remains on the
    upper-left corner from the user's point of view, whether the device is in portrait
    or landscape mode.
  prefs: []
  type: TYPE_NORMAL
- en: '![What just happened?](img/9645OS_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To implement this new feature, we connected our event loop to the input event
    queue provided by the `native_app_glue` module. This queue is internally represented
    as a UNIX pipe, like the activity event queue. Touchscreen events are embedded
    in an `AInputEvent` structure, which stores other kinds of input events. Input
    events are handled with the `AInputEvent` and `AMotionEvent` API declared in `android/input.h`.
    The `AInputEvent` API is necessary to discriminate input event types using `AInputEvent_getType()`
    and `AInputEvent_getSource()` methods. The `AMotionEvent` API provides methods
    to handle touch events only.
  prefs: []
  type: TYPE_NORMAL
- en: 'The touch API is rather rich. Many details can be requested as shown in the
    following table (non-exhaustively):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '| To detect whether a finger makes contact with the screen, leaving it, or
    moving over the surface.The result is an integer value composed of the event type
    (on byte `1`, for example, `AMOTION_EVENT_ACTION_DOWN`) and a pointer index (on
    byte `2`, to know which finger the event refers to). |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '| To retrieve touch coordinates on screen, expressed in pixels as a float (sub-pixel
    values are possible). |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '| To retrieve how much time a finger has been sliding over the screen and when
    the event was generated in nanoseconds. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '| To detect the pressure intensity and zone. Values usually range between `0.0`
    and `1.0` (but may exceed it). Size and pressure are generally closely related.
    The behavior can vary greatly and be noisy, depending on hardware. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '| Touch events of type `AMOTION_EVENT_ACTION_MOVE` can be grouped together
    for efficiency purposes. These methods give access to these *historical points*
    that occurred between previous and current events. |'
  prefs: []
  type: TYPE_TB
- en: Have a look at `android/input.h` for an exhaustive list of methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look more deeply at the `AMotionEvent` API, you will notice that some
    events have a second parameter `pointer_index`, which ranges between `0` and the
    number of active pointers. Indeed, most touchscreens today are multi-touch! Two
    or more fingers on a screen (if hardware supports it) are translated in Android
    by two or more pointers. To manipulate them, look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '| To know how many fingers touch the screen. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '| To get a pointer unique identifier from a pointer index. This is the only
    way to track a particular pointer (that is, *finger*) over time, as its index
    may change when fingers touch or leave the screen. |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you followed the story of the (*now prehistoric!*) Nexus One, then you know
    that it came out with a hardware defect. Pointers were often getting mixed up,
    two of them exchanging one of their coordinates. So always be prepared to handle
    hardware specificities or hardware that behaves incorrectly!
  prefs: []
  type: TYPE_NORMAL
- en: Detecting keyboard, D-Pad, and Trackball events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most common input device among all is the keyboard. This is true for Android
    too. An Android keyboard can be physical: in the device front face (like traditional
    Blackberries) or on a slide-out screen. However, a keyboard is more commonly virtual,
    that is, emulated on the screen at the cost of a large portion of space taken.
    In addition to the keyboard itself, every Android device must include a few physical
    or emulated buttons such as **Menu**, **Home**, and **Tasks**.'
  prefs: []
  type: TYPE_NORMAL
- en: A much less common type of input device is the Directional-Pad. A D-Pad is a
    set of physical buttons to move up, down, left, or right and a specific action/confirmation
    button. Although they often disappear from recent phones and tablets, D-Pads remain
    one of the most convenient ways to move across text or UI widgets. D-Pads are
    often replaced by trackballs. Trackballs behave similarly to a mouse (the one
    with a ball inside) that would be upside down. Some trackballs are analogical,
    but others (for example, optical ones) behave as a D-Pad (that is, all or nothing).
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting keyboard, D-Pad, and Trackball events](img/9645OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To see how they work, let's use these peripherals to move our space ship in
    `DroidBlaster`. The Android NDK now allows handling all these input peripherals
    on the native side. So, let's try them!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The resulting project is provided with this book under the name `DroidBlaster_Part14`.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action – handling keyboard, D-Pad, and trackball events natively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s extend our new Input system with more event types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `jni/InputHandler.hpp` and add the keyboard and trackball event handlers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Update the method `processInputEvent()` inside the existing file `jni/EventLoop.cpp`
    to redirect the keyboard and trackball events to `InputHandler`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Trackballs and touch events are assimilated to motion events and can be discriminated
    according to their source. On the opposite side, key events are discriminated
    according to their type. Indeed, there exists two dedicated APIs for `MotionEvents`
    (the same for trackballs and touch events) and for `KeyEvents` (identical for
    keyboard, D-Pad, and so on):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify the `jni/InputManager.hpp` file to override these new methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In `jni/InputManager.cpp`, process the keyboard events in `onKeyboardEvent()`
    using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AKeyEvent_getAction()` to get the event type (that is, pressed or not).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AKeyEvent_getKeyCode()` to get the button identity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following code, when left, right, up, or down buttons are pressed, `InputManager`
    calculates the direction and saves it into `mDirectionX` and `mDirectionY`. The
    movement starts when the button is down and stops when it is up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return `true` when the key has been consumed and `false` when it has not. Indeed,
    if a user has pressed, for example, the back button (`AKEYCODE_BACK`) or volume
    buttons (`AKEYCODE_VOLUME_UP`, `AKEYCODE_VOLUME_DOWN`), then we let the system
    react appropriately for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, process trackball events in a new method `onTrackballEvent()`. Retrieve
    the trackball magnitude with `AMotionEvent_getX()` and `AMotionEvent_getY()`.
    Because some trackballs do not offer a gradated magnitude, the movements are quantified
    with plain constants. The possible noise is ignored with an arbitrary trigger
    threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When using a trackball that way, the ship moves until a "counter-movement"
    (for example, requesting to go to the right when going left) or action button
    is pressed (the last `else` section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*What just happened?*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We extended our input system to handle the keyboard, D-Pad, and trackball events.
    D-Pad can be considered as a keyboard extension and is processed the same way.
    Indeed, D-Pad and keyboard events are transported in the same structure (`AInputEvent`)
    and handled by the same API (prefixed with `AKeyEvent`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the main key event methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getAction()` | Indicates whether the button is down (`AKEY_EVENT_ACTION_DOWN`)
    or released (`AKEY_EVENT_ACTION_UP`). Note that multiple key actions can be emitted
    in batch (`AKEY_EVENT_ACTION_MULTIPLE`). |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getKeyCode()` | To retrieve the actual button being pressed (defined
    in `android/keycodes.h`), for example, `AKEYCODE_DPAD_LEFT` for the left button.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getFlags()` | Key events can be associated with one or more flags
    that give various kinds of information on the event, such as `AKEY_EVENT_LONG_PRESS`,
    `AKEY_EVENT_FLAG_SOFT_KEYBOARD` for the event originated from an emulated keyboard.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getScanCode()` | Is similar to a key code except that this is
    the raw key ID, dependent and different from device to device. |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getMetaState()` | Meta states are flags that indicate whether
    some modifier keys, such as Alt or Shift, are pressed simultaneously (for example,
    `AMETA_SHIFT_ON`, `AMETA_NONE`, and so on). |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getRepeatCount()` | Indicates how many times the button event
    occurred, usually when you leave the button down. |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getDownTime()` | To know when a button was pressed. |'
  prefs: []
  type: TYPE_TB
- en: 'Although some of them (especially optical ones) behave like a D-Pad, trackballs
    do not use the same API. Actually, trackballs are handled through the `AMotionEvent`
    API (such as touch events). Of course, some information provided for touch events
    is not always available on trackballs. The most important functions to look at
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `AMotionEvent_getAction()` | To know whether an event represents a move action
    (as opposed to a press action). |'
  prefs: []
  type: TYPE_TB
- en: '| `AMotionEvent_getX()``AMotionEvent_getY()` | To get trackball movement. |'
  prefs: []
  type: TYPE_TB
- en: '| `AKeyEvent_getDownTime()` | To know whether the trackball is pressed (such
    as the D-Pad action button). Currently, most trackballs use an all-or-nothing
    pressure to indicate the press event. |'
  prefs: []
  type: TYPE_TB
- en: A tricky point to keep in mind when dealing with trackballs is that no event
    is generated to indicate that the trackball is not moving. Moreover, trackball
    events are generated as a "burst", which makes it harder to detect when the movement
    is finished. There is no easy way to handle this, except using a manual timer
    and checking regularly that no event has happened for a sufficient amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Never expect peripherals to behave exactly the same on all phones. Trackballs
    are a very good example; they can either indicate a direction like an analogical
    pad or a straight direction like a D-Pad (for example, optical trackballs). There
    is currently no way to differentiate device characteristics from the available
    APIs. The only solutions are to either calibrate the device or configure it at
    runtime or save a kind of device database.
  prefs: []
  type: TYPE_NORMAL
- en: Probing device sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling input devices is essential to any application, but probing sensors
    is important for the smartest one! The most spread sensor among Android game applications
    is the accelerometer.
  prefs: []
  type: TYPE_NORMAL
- en: 'An accelerometer, as its name suggests, measures the linear acceleration applied
    to a device. When moving a device up, down, left, or right, the accelerometer
    gets excited and indicates an acceleration vector in 3D space. Vector is expressed
    in relation to the screen''s default orientation. The coordinate system is relative
    to the device''s natural orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: X axis points to the right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y points up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z points from back to front
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axes become inverted if the device is rotated (for example, Y points left if
    the device is rotated 90 degrees clockwise).
  prefs: []
  type: TYPE_NORMAL
- en: 'A very interesting feature of accelerometers is that they undergo a constant
    acceleration: gravity, around 9.8m/s2 on earth. For example, when lying flat on
    a table, acceleration vector indicates -9.8 on the Z-axis. When straight, it indicates
    the same value on Y axis. So assuming the device position is fixed, device orientation
    on two axes in space can be deduced from the gravity acceleration vector. A magnetometer
    is still required to get full device orientation in 3D space.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that accelerometers work with linear acceleration. They allow detecting
    the translation when the device is not rotating and partial orientation when the
    device is fixed. However, both movements cannot be combined without a magnetometer
    and/or gyroscope.
  prefs: []
  type: TYPE_NORMAL
- en: So we can use the device orientation deduced from the accelerometer to compute
    a direction. Let's now see how to apply this process in `DroidBlaster`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The resulting project is provided with this book under the name `DroidBlaster_Part15`.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action – handling accelerometer events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s handle accelerometer events in `DroidBlaster`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `jni/InputHandler.hpp` and add a new method `onAccelerometerEvent()`.
    Include the `android/sensor.h` official header for sensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create new methods in `jni/EventLoop.hpp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`activateAccelerometer()` and `deactivateAccelerometer()` to enable/disable
    the accelerometer sensor when the activity starts and stops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`processSensorEvent()` retrieves and dispatches sensor events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The callback `callback_input()` static method is bound to the Looper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, define the following members:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mSensorManager`, of type `ASensorManager`, is the main "object" to interact
    with sensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mSensorEventQueue` is `ASensorEventQueue`, which is a structure defined by
    the Sensor API to retrieve occurring events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mSensorPollSource` is `android_poll_source` defined in the Native Glue. This
    structure describes how to bind the native thread Looper to the sensor callback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mAccelerometer`, declared as an `ASensor` structure, represents the sensor
    used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Update constructor initialization list in `jni/EventLoop.cpp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Create the sensor event queue, through which all `sensor` events are notified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bind it to `callback_sensor()`. Note here that we use the `LOOPER_ID_USER` constant
    provided by the Native App Glue to attach a user-defined queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, call `activateAccelerometer()` to initialize the accelerometer sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When an activity is disabled or terminated, disable the running accelerometer
    to avoid consuming battery needlessly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, destroy the `sensor` event queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`callback_sensor()` is triggered when event loop is polled. It dispatches events
    to `processSensorEvent()` on the `EventLoop` instance. We only care about `ASENSOR_TYPE_ACCELEROMETER`
    events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Activate the sensor in `activateAccelerometer()` in three main steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get a sensor of a specific type with `AsensorManager_getDefaultSensor()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, enable it with `ASensorEventQueue_enableSensor()` so that the sensor event
    queue gets filled with related events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the desired event rate with `ASensorEventQueue_setEventRate()`. For a game,
    we typically want measures close to real time. The minimum delay can be queried
    with `ASensor_getMinDelay()` (setting it to a lower value might result in a failure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Obviously, we should perform this setup only when the sensor event queue is
    ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Sensor deactivation is easier and only requires a call to the method `AsensorEventQueue_disableSensor()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '*What just happened?*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We created an event queue to listen to sensor events. Events are wrapped in
    an `ASensorEvent` structure, defined in `android/sensor.h`. This structure provides
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensor event origin, that is, which sensor produced this event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor event occurrence time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor output value. This value is stored in a union structure, that is, you
    can use either one of the inside structures (here, we are interested in the `acceleration`
    vector).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The same `ASensorEvent` structure is used for any Android sensor. In the case
    of the accelerometer, we retrieve a vector with three coordinates `x`, `y`, and
    `z`, one for each axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In our example, the accelerometer is set up with the lowest event rate possible,
    which may vary between devices. It is important to note that the sensor event
    rate has a direct impact on battery saving! So, use a rate that is sufficient
    for your application. The `ASensor` API offers some methods to query the available
    sensors and their capabilities, such as `ASensor_getName()`, `ASensor_getVendor()`,
    `ASensor_getMinDelay()`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we can retrieve sensor events, let's use them to compute a ship's direction.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action – turning an Android device into a Joypad
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's find the device orientation and properly determine the direction.
  prefs: []
  type: TYPE_NORMAL
- en: Write a new file `jni/Configuration.hpp` to help us get device information,
    and more specifically device rotation (defined as `screen_rot`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Declare `findRotation()` to discover the device orientation with the help of
    JNI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve configuration details in `jni/Configuration.cpp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, in the constructor, use the `AConfiguration` API to dump configuration
    properties, such as the current language, country, screen size, screen orientation.
    This information may be interesting, but is not sufficient to properly analyze
    accelerometer events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Then, attach the current native thread to the Android VM.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have carefully read [Chapter 4](ch04.html "Chapter 4. Calling Java Back
    from Native Code"), *Calling Java Back from Native Code*, you know that this step
    is necessary to get access to the `JNIEnv` object (which is thread-specific).
    The `JavaVM` itself can be retrieved from the `android_app` structure.
  prefs: []
  type: TYPE_NORMAL
- en: After that, call `findRotation()` to retrieve the current device rotation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we can detach the thread from Dalvik as we will not use JNI any more.
    Remember that an attached thread should always be detached before terminating
    the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement `findRotation()`, which basically executes the following Java code
    through JNI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, this is slightly more complex to write in JNI.
  prefs: []
  type: TYPE_NORMAL
- en: First, retrieve JNI classes, then methods, and finally, fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, perform the JNI calls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, release the allocated JNI references
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code has been voluntarily simplified to avoid extra checks (that
    is, `FindClass()` and `GetMethodID()` return value and exception checks for each
    method call):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Manage the new accelerometer sensor in `jni/InputManager.hpp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accelerometer axes are transformed in `toScreenCoord()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This transformation implies that we keep track of device rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In `jni/InputManager.hpp`, read the current screen rotation settings with the
    help of the new `Configuration` class. Since `DroidBlaster` forces portrait mode,
    we can store rotation once and for all:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Let's compute a direction from the accelerometer sensor values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, convert accelerometer values from canonical to screen coordinates to
    handle portrait and landscape devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute a direction from the captured accelerometer values. In the following
    code, the `X` and `Z` axis express the roll and pitch, respectively. Check for
    both axes whether the device is in a neutral orientation (that is, `CENTER_X`
    and `CENTER_Z`) or is sloping (`MIN_X`, `MIN_Z`, `MAX_X`, and `MAX_Z`). Note that
    Z values need to be inverted for our needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `toScreenCoord()` helper, swap or invert accelerometer axes depending
    on screen rotation, so that `X` and `Z` axes point toward the same direction,
    whatever device you use when playing `DroidBlaster` in portrait mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '*What just happened?*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The accelerometer is now a Joypad! Android devices can be naturally portrait-oriented
    (mainly smartphones and smaller tablets) or landscape-oriented (mainly tablets).
    This has an impact on applications, which receive accelerometer events. Axes are
    not aligned the same way between these types of devices and depending on the way
    they are rotated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, the screen can be oriented in four different ways: `0`, `90`, `180`,
    and `270` degrees. 0 degree is the device''s natural orientation. Accelerometer
    X axis always points right, Y points up, and Z points towards the front. On a
    phone, Y points up in portrait mode, whereas on most tables, Y points up in landscape
    mode. When the device is oriented at 90 degrees, the axes orientation obviously
    changes (X points up, and so on). This situation may also happen with a tablet
    (where 0 degree corresponds to landscape mode) that is used in portrait mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '![What just happened?](img/9645OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is sadly no way to get device rotation relative to a screen's natural
    orientation with native APIs. Thus, we need to rely on JNI to get accurate device
    rotation. Then, we can easily deduce a direction vector from this like done in
    `onAccelerometerEvent()`.
  prefs: []
  type: TYPE_NORMAL
- en: More on sensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each Android sensor has a unique identifier, defined in `android/sensor.h`.
    These identifiers are the same across all Android devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ASENSOR_TYPE_ACCELEROMETER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASENSOR_TYPE_MAGNETIC_FIELD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASENSOR_TYPE_GYRISCOPE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASENSOR_TYPE_LIGHT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASENSOR_TYPE_PROXIMITY`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additional sensors may exist and be available, even if they are not named in
    the `android/sensor.h` header. On Gingerbread, we have the same case with:'
  prefs: []
  type: TYPE_NORMAL
- en: Gravity sensor (identifier `9`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear acceleration sensor (identifier `10`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotation vector (identifier `11`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rotation vector sensor, successor of the now deprecated orientation vector,
    is essential in the `Augmented Reality` application. It gives you device orientation
    in 3D space. Combined with the GPS, it allows locating any object through the
    eye of your device. The rotation sensor provides a data vector, which can be translated
    to an OpenGL view matrix, thanks to the `android.hardware.SensorManager` class
    (see its source code). That way, you can directly materialize device orientation
    into screen content, linking together real and virtual life.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered multiple ways to interact with Android from native
    code. More precisely, we discovered how to attach an input queue to the `Native
    App Glue` event loop. Then, we handled touch events and processed key events from
    keyboards and D-Pads or motion events from trackballs. Finally, we turned the
    accelerometer into a Joypad.
  prefs: []
  type: TYPE_NORMAL
- en: Because of Android fragmentation, expect specificities in an input device's
    behavior and be prepared to tweak your code. We have already been far in the capabilities
    of Android NDK in terms of application structure, graphics, sound, input, and
    sensors. However, reinventing the wheel is not a solution!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will unleash the real power of the NDK by porting existing
    C/C++ libraries to Android.
  prefs: []
  type: TYPE_NORMAL

- en: '*Chapter 5*: Threads, Memory, and Concurrency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have studied the performance of a single CPU executing one program,
    one instruction sequence. In the introduction of [*Chapter 1*](B16229_01_Epub_AM.xhtml#_idTextAnchor014),
    *Introduction to Performance and Concurrency*, we mentioned that this is not the
    world we live in anymore, and we never touched the subject again. Instead, we
    studied every aspect of the performance of a single-threaded program running on
    one CPU. We have now learned all we need to know about the performance of one
    thread and are ready to study the performance of concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-threaded and multi-core memory accesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data races and memory access synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locks and atomic operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory order and memory barriers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
  prefs: []
  type: TYPE_NORMAL
- en: The code for the chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter05](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter05)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding threads and concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All high-performance computers today have multiple CPUs or multiple CPU cores
    (independent processors in a single package). Even most laptop computers have
    at least two, often four, cores. As we have said many times, in the context of
    performance, efficiency is not leaving any hardware idle; a program cannot be
    efficient or high-performing if it uses only a fraction of the computing power,
    such as one of many CPU cores. There is only one way for a program to use more
    than one processor at a time: we have to run multiple threads or processes. As
    a side note, this isn''t the only way to use multiple processors for the benefit
    of the user: very few laptops, for example, are used for high-performance computing.
    Instead, they use multiple CPUs to better run different and independent programs
    at the same time. It is a perfectly good use model, just not the one we are interested
    in in the context of high-performance computing. HPC systems usually run one program
    on each computer at any time, even one program on many computers in case of distributed
    computations. How does one program use many CPUs? Usually, the program runs multiple
    threads.'
  prefs: []
  type: TYPE_NORMAL
- en: What is a thread?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **thread** is a sequence of instructions that can be executed independently
    of other threads. Multiple threads are running concurrently within the same program.
    All threads share the same memory, so, by definition, threads of the same process
    run on the same machine. We have mentioned that an HPC program can also consist
    of multiple processes. A distributed program runs on multiple machines and utilizes
    many separate processes. The subject of distributed computing is outside the scope
    of this book: we are learning how to maximize the performance of each of these
    processes.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what can we say about the performance of multiple threads? First of all,
    having multiple instruction sequences execute at the same time is beneficial only
    if the system has enough resources actually to execute them *at the same time*.
    Otherwise, the operating system is just switching between different threads to
    allow each one a time slice to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a single processor, a thread that is busy computing provides as much work
    as the processor can handle. This is true even if the thread is not using all
    of the computing units or is waiting on memory accesses: the processor can execute
    only one instruction sequence at a time – it has a single program counter. Now,
    if the thread is waiting on something, such as user input or network traffic,
    the CPU is idle and could execute another thread without impacting the performance
    of the first one. Again, the operating system handles the switching between the
    threads. Note that waiting on memory does not count as waiting in this sense:
    when a thread is waiting on memory, it just takes longer to execute one instruction.
    When a thread is waiting on I/O, it has to make an operating system call, then
    it''s blocked by the OS and isn''t executing anything at all until the OS wakes
    it up to process the data.'
  prefs: []
  type: TYPE_NORMAL
- en: All threads that do heavy computing require adequate resources if the goal is
    to make the program more efficient overall. Usually, when we think about resources
    for threads, we have in mind multiple processors or processor cores. But there
    are other ways to increase resource utilization through concurrency as well, as
    we are about to see.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric multi-threading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have mentioned several times throughout the book that a processor has a
    lot of computing hardware, and most programs rarely, if ever, use all of it: the
    data dependencies in the program limit how much computation the processor can
    do at any time. If the processor has spare computing units, can''t it execute
    another thread at the same time to improve efficiency? This is the idea behind
    **Symmetric Multi-Threading** (**SMT**), also known as **hyper-threading**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An SMT-capable processor has a single set of registers and computing units,
    but two (or more) program counters and an extra copy of whatever additional hardware
    it uses to maintain the state of a running thread (the exact implementation varies
    from one processor to another). The end result is: a single processor appears
    to the operating system and the program as two (usually) or more separate processors,
    each capable of running one thread. In reality, all threads running on one CPU
    compete for the shared internal resources such as registers. The SMT can offer
    significant performance gains if each thread does not make full use of these shared
    resources. In other words, it compensates for the inefficiency of one thread by
    running several such threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, most SMT-capable processors can run two threads, and the performance
    gains vary widely. It is rare to see 100% speedup (two threads both run at full
    speed). Usually, the practical speedup is between 25% and 50% (the second thread
    is effectively running at quarter-speed to half-speed), but some programs get
    no speedup at all. For the purposes of this book, we will not treat the SMT threads
    in any special way: to the program, an SMT processor appears as two processors,
    and anything we say about the performance of two *real* threads running on separate
    cores applies equally to the performance of two threads that happen to run on
    the same core. At the end of the day, you have to measure whether running more
    threads than you have physical cores provides any speedup to your program and,
    based on that, decide how many threads you want to run.'
  prefs: []
  type: TYPE_NORMAL
- en: Whether we're sharing entire physical cores or the logical cores created by
    the SMT hardware, the performance of a concurrent program largely depends on how
    independently the threads can work. This is determined, first and foremost, by
    the algorithm and the partitioning of work between threads; both matters have
    hundreds of books dedicated to them but lie outside of the scope of this book.
    Instead, we now focus on the fundamental factors that affect thread interaction
    and determine the success or failure of a particular implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Threads and memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since there is no performance benefit to time-slice a CPU between multiple
    computing threads, we can assume for the rest of this chapter that we run one
    HPC thread on every processor core (or one thread on every *logical core* presented
    by an SMT processor). As long as these threads do not compete for any resources,
    they run entirely independently of each other, and we enjoy *perfect speedup*:
    two threads will do twice as much work in the same time as could be done by one
    thread. If the work can be divided perfectly between two threads in a way that
    does not require any interaction between them, two threads will solve the problem
    in half the time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This ideal situation does happen, but not often; more importantly, if it happens,
    you are already prepared to get the best performance from your program: you know
    how to optimize the performance of a single thread.'
  prefs: []
  type: TYPE_NORMAL
- en: The hard part of writing efficient concurrent programs begins when the work
    done by different threads is not entirely independent, and the threads start to
    compete for resources. But if each thread has full use of its CPU, what else is
    there left to compete for? What is left is the memory, which is shared between
    all threads and is, therefore, a common resource. This is why any exploration
    of the performance of multi-threaded programs focuses almost exclusively on the
    issues arising from the interaction between threads through memory.
  prefs: []
  type: TYPE_NORMAL
- en: There is another aspect of writing high-performance concurrent programs, and
    that is dividing work between threads and processes that together comprise the
    program. But to learn about that, you have to find a book on parallel programming.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the memory, which was already the *long pole* of performance,
    is even more of a problem when we add concurrency. While the fundamental limits
    imposed by the hardware cannot be overcome, most programs aren't performing even
    close to these limits, and there is much room for a skillful programmer to improve
    the efficiency of their code; this chapter gives the reader the necessary knowledge
    and tools.
  prefs: []
  type: TYPE_NORMAL
- en: Let us first examine the performance of the memory system in the presence of
    threads. We do it the same way as in the last chapter, by measuring the speed
    of reading or writing into memory, only now we use several threads to read or
    write at the same time. We start with the case where each thread has its own memory
    region to access. We are not sharing any data between threads, but we are sharing
    the hardware resources, such as memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory benchmark itself is almost the same as the one we used earlier.
    In fact, the benchmark function itself is exactly the same. For example, to benchmark
    sequential reading, we use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the memory is allocated inside the benchmark function. If this function
    is called from multiple threads, each thread has its own memory region to read.
    This is exactly what the Google Benchmark library does when it runs multi-threaded
    benchmarks. To run a benchmark on more than one thread, you just need to use the
    right arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can specify as many runs as you want for different thread counts or use
    the `ThreadRange()` argument to generate a range of 1, 2, 4, 8, … threads. You
    have to decide how many threads you want to use; for an HPC benchmark, in general,
    there is no reason to go over the number of CPUs you have (accounting for SMT).
    The benchmarking of other memory access modes, such as random access, is done
    the same way; you have already seen the code in the last chapter. For writing,
    we would need something to write; any value will do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it is time to show the results. For example, here is the memory throughput
    for sequential writes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Memory throughput (words per nanosecond) for sequential writing
    of 64-bit integers as a function of memory range for 1 through 16 threads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Memory throughput (words per nanosecond) for sequential writing
    of 64-bit integers as a function of memory range for 1 through 16 threads
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall trend is already familiar to us: we see the speed jumps corresponding
    to the cache sizes. Now we focus on the differences between the curves for a different
    number of threads. We have the results for 1 through 16 threads (the machine used
    to collect these measurements does indeed have at least 16 physical CPU cores).
    Let us start from the left side of the plot. Here, the speed is limited by the
    L1 cache (up to 32 KB) then by the L2 cache (256 KB). This processor has separate
    L1 and L2 caches for each core, so, as long as the data fits into the L2 cache,
    there should not be any interaction between the threads since they don''t share
    any resources: each thread has its own cache. In reality, this is not quite true,
    there are other CPU components that are still shared even for small memory ranges,
    but it''s almost true: the throughput for 2 threads is twice as large as that
    for 1 thread, 4 threads write to memory twice as fast again, and 16 threads are
    almost 4 times faster than 4 threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The picture changes drastically as we exceed the size of the L2 cache and cross
    into the L3 cache and then the main memory: on this system, the L3 cache is shared
    between all the CPU cores. The main memory is shared too, although different memory
    banks are *closer* to different CPUs (the non-uniform memory architecture). For
    1, 2, and even 4 threads, the throughput continues to scale with the number of
    threads: the main memory appears to have enough bandwidth for up to 4 processors
    writing into it at full speed. Then things take a turn for the worse: the throughput
    almost doesn''t increase when we go from 6 to 16 threads. We have saturated the
    memory bus: it can''t write the data any faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If this wasn''t bad enough, consider that these results were obtained on the
    latest hardware at the time of writing (2020). In 2018, the same chart presented
    by the author in one of his classes looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Memory throughput of an older (2018) CPU'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Memory throughput of an older (2018) CPU
  prefs: []
  type: TYPE_NORMAL
- en: This system has a memory bus that can be completely saturated by just two threads.
    Let us see what the implications of this fact for the performance of a concurrent
    program are.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-bound programs and concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The same results can be presented in a different way: by plotting the memory
    speed per thread versus the number of threads relative to that for one thread,
    we focus exclusively on the effect of concurrency on the memory speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Memory throughput, relative to the throughput for a single thread,
    vs. thread count'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Memory throughput, relative to the throughput for a single thread,
    vs. thread count
  prefs: []
  type: TYPE_NORMAL
- en: With the memory speed normalized, so it's always 1 for the single thread, it
    is much easier to see that for small data sets that fit into L1 or L2 cache, the
    memory speed per thread remains almost the same even for 16 threads (each thread
    is writing at 80% of its single-threaded speed). However, as soon as we cross
    into the L3 cache or exceed its size, the speed goes down after 4 threads. Going
    from 8 to 16 threads provides only minimal improvement. There just isn't enough
    bandwidth in the system to write data to memory fast enough.
  prefs: []
  type: TYPE_NORMAL
- en: The results for different memory access patterns look similar, although the
    bandwidth for reading memory often scales slightly better than that for writing.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that if our program was memory-bound in the single-threaded case,
    so its performance was limited by the speed of moving the data to and from the
    main memory, there is a fairly hard limit on the performance improvement we can
    expect to gain from concurrency. If you think that this does not apply to you
    because you don't have an expensive 16-core processor, remember that cheaper processors
    come with a cheaper memory bus, so most 4-core systems don't have enough memory
    bandwidth for all cores either.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multi-threaded programs, it is even more important to avoid becoming memory-bound.
    The implementation techniques that are useful here are splitting computations
    so more work can be done on smaller data sets that fit into L1 or L2 caches; rearranging
    the computations so more work can be done with fewer memory accesses, often at
    the expense of repeating some computations; optimizing the memory access patterns
    so the memory is accessed sequentially instead of randomly (even though you can
    saturate both access patterns, the total bandwidth of sequential accesses is much
    larger, so for the same amount of data your program may be memory-bound if you
    use random access and not limited by memory speed at all if you use sequential
    access). If the implementation techniques alone are insufficient and do not yield
    the desired performance improvements, the next step is to adapt the algorithm
    to the realities of concurrent programming: many problems have multiple algorithms
    that differ in their memory requirements. The fastest algorithm for a single-threaded
    program can often be outperformed by another algorithm that is better suited for
    concurrency: what we lose in the single-threaded execution speed, we make up for
    by brute force of scalable execution.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have assumed that each thread does its own work completely independently
    from all other threads. The only interaction between threads was indirect, due
    to contention for a limited resource such as memory bandwidth. This is the easiest
    kind of program to write, but most real-life programs do not allow such limitations.
    This brings with it a whole new set of performance problems, and it is time for
    us to learn about them.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the cost of memory synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last section was all about running multiple threads on the same machine
    without any interaction between these threads. If you can split the work your
    program does between threads in a way that makes such implementation possible,
    by all means, do it. You cannot beat the performance of such an *embarrassingly
    parallel* program.
  prefs: []
  type: TYPE_NORMAL
- en: More often than not, threads must interact with each other because they are
    contributing work to a common result. Such interactions happen by means of threads
    communicating with each other through the one resource they share, the memory.
    We must now understand the performance implications of this.
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with a trivial example. Say we want to compute a sum of many values.
    We have many numbers to add, but, in the end, only one result. We have so many
    numbers to add that we want to split the work of adding them between several threads.
    But there is only one result value, so the threads have to interact with each
    other as they add to this value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reproduce this problem in a micro-benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, we always increment the result by 1 (the cost of adding integers
    does not depend on the values, and we don't want to benchmark the generation of
    different values, just the addition itself). Since the benchmark function is called
    by each thread, any variable declared inside this function exists independently
    on the stack of each thread; these variables are not shared at all. To have a
    common result that both threads contribute to, the variable must be declared outside
    of the benchmark function, at the file scope (bad idea in general, but necessary
    and acceptable in the very limited context of the micro-benchmark).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this program has a much bigger problem than the global variable:
    the program is simply wrong, and its results are undefined. The problem is that
    we have two threads incrementing the same value. Incrementing a value is a 3-step
    process: the program reads the value from memory, increments it in the register,
    and writes the new value back into memory. It is entirely possible for both threads
    to read the same value (0) at the same time, increment it separately on each processor
    (1), and write it back. The thread that writes second simply overwrites the result
    of the first thread, and, after two increments, the result is 1 instead of 2\.
    Such *competition* of two threads for writing into the same memory location is
    called a **data race**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understand why such unguarded concurrent accesses are a problem,
    you may as well forget it; instead, follow this general rule: any program has
    undefined results if it accesses the same memory location from multiple threads
    without synchronization and at least one of these accesses is a write. This is
    very important: it is not necessary for you to figure out exactly what sequence
    of operations must happen for the result to be incorrect. In fact, there is nothing
    to be gained in this line of reasoning at all. Any time you have two or more threads
    accessing the same memory location, you have a data race unless you can guarantee
    one of two things: either all accesses are read-only, or all accesses use the
    correct memory synchronization (which we are yet to learn about).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our problem of computing the sum requires that we write the answer into the
    result variable, so the access is definitely not read-only. The synchronization
    of memory accesses is, in general, provided by a mutex: every access to a variable
    shared between threads must be guarded by a mutex (it must, of course, be the
    same mutex for all threads).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The lock guard locks the mutex in its constructor and unlocks it in the destructor.
    Only one thread at a time can have the lock and, thus, increment the shared result
    variable. The other threads are blocked on the lock until the first thread releases
    it. Note that *all* accesses must be locked, *both* reads and writes, as long
    as at least one thread is modifying the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Locks are the simplest way to ensure the correctness of a multi-threaded program,
    but they are not the easiest thing to study in terms of performance. They are
    fairly complex entities and often involve a system call. We will start with a
    synchronization option that is, in this particular case, easier to analyze: the
    atomic variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'C++ gives us an option to declare a variable to be atomic. It means that all
    supported operations on this variable are performed as single, non-interruptible,
    atomic transactions: any other thread observing this variable will see its state
    either before the atomic operation or after it, but never in the middle of the
    operation. For example, all integer atomic variables in C++ support atomic increment
    operations: if one thread is executing the operation, no other thread can access
    this variable until the first operation is complete. These operations require
    certain hardware support: for example, the atomic increment is a special hardware
    instruction that reads the old value, increments it, and writes the new value
    all as a single hardware operation.'
  prefs: []
  type: TYPE_NORMAL
- en: For our example, an atomic increment is all we need. It must be stressed that,
    whatever synchronization mechanism we decided to use, all threads must use the
    same mechanism for concurrent accesses to a particular memory location. If we
    use atomic operations on one thread, it guarantees no data races as long as all
    threads use atomic operations. If another thread uses a mutex or non-atomic access,
    all guarantees are void, and the result is undefined again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us rewrite our benchmark to use C++ atomic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The program is now correct: there are no data races here. It is not necessarily
    accurate since a single increment is a very short time interval to measure; we
    really should unroll the loop manually or with a macro and do several increments
    per loop iteration (we have done that in the last chapter, so you can see the
    macro there). Let us see how well it performs. If there were no interaction between
    the threads, two threads would compute the sum in half the time it takes one thread
    to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Atomic increment time in a multi-threaded program'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Atomic increment time in a multi-threaded program
  prefs: []
  type: TYPE_NORMAL
- en: 'We have normalized the results to show the average time of a single increment,
    that is, the time to compute the sum divided by the total number of additions.
    The performance of this program is very disappointing: not only is there no improvement,
    but, in fact, it takes longer to compute the sum on two threads than on one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are even worse if we use the more conventional mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Increment time in a multi-threaded program with a mutex'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Increment time in a multi-threaded program with a mutex
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, as we expected, locking the mutex is a fairly expensive operation
    even on one thread: 23 nanoseconds for a mutex-guarded increment versus 7 nanoseconds
    for the atomic increment. The performance degrades much faster as the number of
    threads increases.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a very important lesson to be learned from these experiments. The portion
    of the program that accesses the shared data will never scale. The best performance
    you can have for accessing the shared data is single-threaded performance. As
    soon as you have two or more threads accessing the same data at the same time,
    the performance can only get worse. Of course, if two threads are accessing the
    same data at different times, they don't really interact with each other, so you
    get the single-threaded performance both times. The performance advantage of a
    multi-threaded program comes from the computations that the threads do independently,
    without synchronization. By definition, such computations are done on data that
    is not shared (if you want your program to be correct, anyway). But why are concurrent
    accesses to the shared data so expensive? In the next section, we will learn the
    reason. We will also learn a very important lesson on carefully interpreting measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Why data sharing is expensive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have just seen, concurrent (simultaneous) access of the shared data is
    a real performance killer. Intuitively, it makes sense: in order to avoid a data
    race, only one thread can operate on the shared data at any given time. We can
    accomplish this with a mutex or use an atomic operation if one is available. Either
    way, when one thread is, say, incrementing the shared variable, all other threads
    have to wait. Our measurements in the last section confirm it.'
  prefs: []
  type: TYPE_NORMAL
- en: However, before taking any action based on observations and experiments, it
    is critically important to understand precisely what we measured and what can
    be concluded with certainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to describe what was observed: incrementing a shared variable from
    multiple threads at the same time does not scale at all and, in fact, is slower
    than using just one thread. This is true for both atomic shared variables and
    non-atomic variables guarded by a mutex. We have not tried to measure unguarded
    access to a non-atomic variable because such an operation leads to undefined behavior
    and incorrect results. We also know that unguarded access to variables that are
    thread-specific (not shared) scales very well with the number of threads, at least
    until we saturate the aggregate memory bandwidth (which can only happen if we
    write large amounts of data; for a single variable this is not an issue). Analyzing
    your experimental results critically and without unjustified preconceptions is
    a very important skill, so let us state again what we know: guarded access to
    shared data is slow and unguarded access to non-shared data is fast. If we conclude
    from this that data sharing makes your program slow, we are making an assumption:
    **shared data** is what''s important, **guarded access** is not. This brings up
    another very important point you should remember when doing performance measurements:
    when comparing two versions of the program, try to change only one thing at a
    time and measure the result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The measurement we are missing is this one: non-shared access to guarded data.
    Of course, we don''t really need to protect accesses to data that is accessed
    by only one thread, but we are trying to understand exactly what makes shared
    data access so expensive: the fact that it is shared or the fact that it is atomic
    (or protected by the lock). We have to make one change at a time, so let us keep
    the atomic access and remove data sharing. There are at least two simple ways
    to do this. The first one is to create a global array of atomic variables and
    have each thread access its own array element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The thread index in Google Benchmark is unique for each thread, the numbers
    start from 0 and are compact (0, 1, 2…). The other simple way is to declare the
    variable in the `benchmark` function itself as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are incrementing the same atomic integer as we did when we collected
    the measurements for *Figure 5.4*, only it is no longer shared between threads.
    This will tell us whether it is the sharing or the atomic variable that makes
    the increment slow. Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Atomic increment time for shared and not shared variables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.6_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Atomic increment time for shared and not shared variables
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Shared** curve is the one from *Figure 5.4*, while the other two are
    from the benchmarks without data sharing. The benchmark with a local variable
    on each thread is labeled **Not shared** and behaves as such: the computation
    on two threads takes half the time compared to that on one thread, going to four
    threads cuts the time by half again, and so on. Remember that this is the average
    time of one increment operation: we do, say, 1 million increments in total, measure
    the total time it takes, and divide by a million. Since the variables that we
    increment are not shared between threads, we expect two threads to run twice as
    fast as one thread, so the **Not shared** result is exactly what we expected.
    The other benchmark, the one where we use an array of atomic variables, but each
    thread uses its own array element, also has no shared data. However, it performs
    as if the data was shared between threads, at least for a small number of threads,
    so we call it **False sharing**: nothing is really shared, but the program behaves
    as if it was.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This result shows that the reason for the high cost of data sharing is more
    complex than what we assumed previously: in the case of false sharing, only one
    thread is operating on each array element, so it does not have to wait for any
    other thread to complete its increment. And yet, threads clearly wait for each
    other. To understand this anomaly, we have to learn more about the way caches
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: The way the data moves between the processors and the memory in a multi-core
    or a multi-processor system is illustrated in *Figure 5.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Data transfer between CPUs and memory in a multi-core system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Data transfer between CPUs and memory in a multi-core system
  prefs: []
  type: TYPE_NORMAL
- en: 'The processor operates on the data in individual bytes, or in words that depend
    on the type of the variable; in our case, an `unsigned long` is an 8-byte word.
    An atomic increment reads a single word at the specified address, increments it,
    and writes it back. But reads from where? The CPU has direct access only to the
    L1 cache, so it gets the data from there. How does the data get from the main
    memory into the cache? It''s copied over the memory bus, which is much wider.
    The minimum amount of data that can be copied from memory to cache and back is
    called a **cache line**. On all x86 CPUs, one cache line is 64 bytes. When a CPU
    needs to lock a memory location for an atomic transaction, such as an atomic increment,
    it may be writing a single word, but it has to lock the entire cache line: if
    two CPUs are allowed to write the same cache line into memory at the same time,
    one of them will overwrite the other. Note that, for simplicity, we show only
    one level of cache hierarchy in *Figure 5.7*, but it makes no difference: data
    travels through all cache levels in chunks of cache line length.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can explain the false sharing we observed: even though the adjacent
    array elements are not really shared between threads, they do occupy the same
    cache line. When a CPU requests exclusive access to one array element for the
    duration of the atomic increment operation, it locks the entire cache line and
    prevents any other CPU from accessing any data in it. Incidentally, this explains
    why the false sharing in *Figure 5.7* appears equivalent to the true data sharing
    for up to 8 threads but becomes faster for more threads: we are writing 8-byte
    words, so 8 of them fit into the same cache line. If we have only 8 threads (or
    fewer), only one thread can increment its value at any given time, the same as
    for true sharing. But with more than 8 threads, the array occupies at least two
    cache lines, and they can be locked by two CPUs independently from each other.
    So, if we have, say, 16 threads at any time, there are two threads that can move
    forward, one for each half of the array.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the real no-sharing benchmark allocates the atomic variables
    on the stack of each thread. These are completely independent memory allocations,
    separated by many cache lines. With no interaction through memory, these threads
    run completely independently of each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our analysis shows that the real reason for the high cost of accessing the
    shared data is the work that must be done to maintain the exclusive access to
    a cache line and to make sure all CPUs have consistent data in their caches: after
    one CPU has obtained exclusive access and updated even one bit in the cache line,
    the copy of that line in all caches of all other CPUs is out of date. Before these
    other CPUs can access any data in the same cache line, they must fetch the updated
    content from the main memory, which, as we have seen, takes a relatively long
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, it doesn't really matter whether two threads try to access
    the same memory location or not, as long as they are competing for access to the
    same cache line. That exclusive cache line access is the origin of the high cost
    of shared variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'One may wonder whether the reason locks are expensive is also found in the
    shared data they contain (all locks must have some amount of shared data, that''s
    the only way one thread can let another thread know that the lock is taken). A
    mutex lock is much more expensive than single atomic access, even on one thread,
    as we have seen in *Figures 5.4* and *5.5*. We can assume, correctly, that locking
    a mutex involves more work than just modifying one atomic variable. But why does
    this work take more time when we have more than one thread? Is it because the
    data is shared and needs exclusive access to the cache line? We leave it as an
    exercise to the reader to confirm that this is indeed so. The key to this experiment
    is to set up false sharing of locks: an array of locks such that each thread operates
    on its own lock, but they compete for the same cache line (of course, such per-thread
    locks don''t actually protect anything from concurrent access, but all we want
    is the time it takes to lock and unlock them). The experiment is slightly more
    complex than you might think: the standard C++ mutex, `std::mutex`, is usually
    quite large, between 40 and 80 bytes depending on the OS. This means you can''t
    fit even two of them into the same cache line. You have to do this experiment
    with a smaller lock, such as a **spinlock** or a **futex**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now understand why the cost of accessing the shared data concurrently is
    so high. This understanding gives us two important lessons. The first one is to
    avoid false data sharing when we attempt to create non-shared data. How can the
    unintended *false sharing* creep into our program? Consider the simple example
    we have studied throughout this chapter: accumulating a sum concurrently. Some
    of our approaches were slower than others, but they were all very slow (slower
    than the single-threaded program, or, at best, no faster). We understand that
    accessing shared data is expensive. So, what is less expensive? Not accessing
    the shared data, of course! Or at least not accessing it as often. There is no
    reason for us to access the shared sum value every time we want to add something
    to it: we can make all the additions locally, on the thread, and add them to the
    shared accumulator value once, at the very end. The code would look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the global result, `sum`, that is shared between all threads and must
    be atomic (or protected by a lock). But each thread accesses this variable exactly
    once after all the work is done. Each thread uses another variable to hold the
    partial sum, only the values added on this thread (increments of 1 in our trivial
    case, but the performance is the same regardless of the values being added). We
    can create a large array to store these per-thread partial sums and give each
    thread a unique array element to work on. Of course, in this trivial example,
    we could just use a local variable, but in a real program, the partial results
    often need to be kept after the worker threads are done, and the final processing
    of these results is done elsewhere, perhaps by another thread. To simulate this
    kind of implementation, we use an array of per-thread variables. Note that these
    variables are just plain integers, not atomic: there is no concurrent access to
    them. Unfortunately, in the process, we fell into the trap of false sharing: the
    adjacent elements of the array are (usually) on the same cache line and, thus,
    cannot be accessed concurrently. This is reflected in the performance of our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Sum accumulation with and without false sharing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – Sum accumulation with and without false sharing
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 5.8*, our program scales very poorly until we get
    to a very large number of threads. On the other hand, it scales perfectly, as
    expected, if we eliminate the false sharing by making sure the per-thread partial
    sums are at least 64 bytes apart (or simply using local variables in our case).
    While both programs become faster when we use more threads, the implementation
    that is not burdened by the false sharing remains approximately twice as fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second lesson will become more important in the later chapters: since accessing
    shared variables concurrently is, comparatively, very expensive, an algorithm
    or an implementation that uses fewer shared variables will, in general, perform
    faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This statement may be confusing at the moment: by nature of the problem, we
    have some amount of the data that must be shared. We can do optimizations like
    the one we just did and eliminate unnecessary accesses to this data. But once
    this is done, the rest is the data we need to access to produce the desired results.
    How can there be more, or fewer, shared variables, then? To understand this, we
    have to realize that there is more to writing concurrent programs than protecting
    access to all shared data.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about concurrency and order
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the reader was reminded earlier in this chapter, any program that accesses
    any shared data without access synchronization (mutexes or atomic accesses, usually)
    has undefined behavior that is usually called a data race. This seems simple enough,
    at least in theory. But our motivational example was too simple: it had just one
    variable shared between threads. There is more to concurrency than locking shared
    variables, as we are about to see.'
  prefs: []
  type: TYPE_NORMAL
- en: The need for order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now consider this example known as the **producer-consumer queue**. Let us
    say that we have two threads. The first thread, the producer, prepares some data
    by constructing objects. The second thread, the consumer, processes the data (does
    work on each object). For simplicity, let us say that we have a large memory buffer
    that is initially uninitialized and the producer thread constructs new objects
    in the buffer as if they were array elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to produce (construct) an object, the producer thread calls the constructor
    via the placement of the `new` operator on each element of the array, starting
    with `N==0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the array element `buffer[N]` is initialized and is available to the consumer
    thread. The producer signals this by advancing the counter `N` then moves on to
    initialize the next object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer thread must not access an array element `buffer[i]` until the
    counter `N` has been incremented so it is greater than `i`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, let us ignore the problem of running out of memory and assume
    that the buffer is large enough. Also, we are not concerned with the termination
    condition (how does the consumer know when to keep consuming?) right now. At the
    moment, we are interested in the producer-consumer handshake protocol: how does
    the consumer access the data without any races?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general rule states that any access to the shared data must be protected.
    Obviously, the counter `N` is a shared variable, so accessing it needs more care:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'But is this enough? Look carefully: there is more shared data in our program.
    The entire array of objects `T` is shared between the two threads: each thread
    needs to access every element. But if we need to lock the entire array, we might
    as well go back to a single-threaded implementation: one of the two threads is
    always going to be locked out. From experience, every programmer who has ever
    written any multi-threaded code knows that, in this case, we do not need to lock
    the array, only the counter. In fact, it is the whole point of locking the counter
    that we don''t need to lock the array this way: any particular element of the
    array is never accessed concurrently. First, it is accessed only by the producer
    before the counter is incremented. Then, it is accessed only by the consumer after
    the counter is incremented. This is known. But the goal of this book is to teach
    you how to understand why things work the way they do, and so, why is locking
    the counter enough? What guarantees that the events really happen in the order
    we imagine?'
  prefs: []
  type: TYPE_NORMAL
- en: 'By the way, even this trivial example has just become not so trivial. The naïve
    way to protect the consumer''s access to the counter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a guaranteed deadlock: once the consumer acquires the lock, it waits
    for the element `i` to be initialized before releasing the lock. The producer
    cannot make any progress because it is waiting to acquire the lock before it can
    increment the counter `N`. Both threads are now waiting forever. It is easy to
    notice that our code would be so much simpler if we just used an atomic variable
    for the counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now every read of the counter `N` by the consumer is atomic, but in between
    the two reads the producer is not blocked and can keep working. This approach
    to concurrency is known as `buffer[i]` concurrently?
  prefs: []
  type: TYPE_NORMAL
- en: Memory order and memory barriers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have realized, being able to access shared variables safely is not enough
    to write any non-trivial concurrent program. We also have to be able to reason
    about the order in which events happen. In our producer and consumer example,
    the entire program rests on a single assumption: that we can guarantee that the
    construction of the Nth array element, incrementing the counter to N + 1, and
    the access to the Nth element by the consumer thread happen in that order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But the problem is really even more complex than that once we realize that
    we are dealing not just with multiple threads but with multiple processors that
    are executing these threads truly at the same time. The key concept we have to
    remember here is **visibility**. A thread is executing on one CPU and is making
    changes to the memory when the CPU assigns values to variables. In reality, the
    CPU is only changing the content of its cache; the cache and memory hardware eventually
    propagate these changes to the main memory or the shared higher-level cache, at
    which point these changes may become visible to other CPUs. We say "*may*" because
    the other CPUs have different values for the same variables in their caches, and
    we do not know when these differences are reconciled. We do know that, once a
    CPU begins an operation on an atomic variable, no other CPU can access the same
    variable until this operation is done, and that once this operation completes,
    all other CPUs will see the latest updated value of this variable (but only if
    all CPUs treat the variable as atomic). We know that the same applies to a variable
    guarded by a lock. But these guarantees are not sufficient for our producer-consumer
    program: based on what we know so far, we cannot be sure it is correct. This is
    because, until now, we were concerned with only one aspect of accessing a shared
    variable: the atomic or transactional nature of this access. We wanted to make
    sure that the entire operation, whether simple or complex, is executed as a single
    transaction without the possibility of being interrupted.'
  prefs: []
  type: TYPE_NORMAL
- en: But there is another aspect of accessing shared data, that of **memory order**.
    Just like the atomicity of the access itself, it is a feature of the hardware
    that is activated using a particular machine instruction (often an attribute or
    a flag on the atomic instruction itself).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several forms of memory order. The least restricted one is the relaxed
    memory order. When an atomic operation is executed with relaxed order, the only
    guarantee we have is that the operation itself is executed atomically. What does
    this mean? Let us first consider the CPU that is executing the atomic operation.
    It runs a thread that contains other operations, both non-atomic and atomic. Some
    of these operations modify the memory; the results of these operations can be
    seen by other CPUs. Other operations read the memory; they observe the results
    of operations executed by other CPUs. The CPU that runs our thread executes these
    operations in a certain order. It may not be the order in which they are written
    in the program: both the compiler and the hardware can reorder instructions, usually
    to improve the performance. But it is a well-defined order. Now let us look at
    it from the point of view of another CPU that is executing a different thread.
    That second CPU can see the content of the memory changing as the first CPU does
    its work. But it does not necessarily see them in the same order with respect
    to each other or to the atomic operation that we have been focused on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Visibility of operations with relaxed memory order'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.9_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Visibility of operations with relaxed memory order
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the visibility that we were talking about earlier: one CPU executes
    operations in a certain order, but their results are visible to other CPUs in
    a very different order. For brevity, we usually talk about the visibility of the
    operations and do not mention *results* every time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If our operations on the shared counter `N` were executed with relaxed memory
    order, we would be in deep trouble: the only way to make our program correct would
    be to lock it so only one thread, the producer or the consumer, can run at any
    time, and we get no performance improvement from concurrency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there are other memory order guarantees we can use. The most important
    one is the acquire-release memory order. When an atomic operation is executed
    with this order, we have a guarantee that any operation that accesses the memory
    and was executed before the atomic operation becomes visible to another thread
    before that thread executes an atomic operation on the same atomic variable. Similarly,
    all operations that are executed after the atomic operation become visible only
    after an atomic operation on the same variable. Again, remember that when we talk
    about the visibility of operations, we really mean that their results become observable
    to other CPUs. This is evident in *Figure 5.10*: on the left, we have the operations
    as they are executed by **CPU0**. On the right, we have the same operations as
    they are seen by **CPU1**. Note, in particular, that the atomic operation shown
    on the right is *Atomic Write*. But **CPU1** does not execute atomic write: it
    executes an atomic read to see the results of the atomic write executed by **CPU0**.
    The same goes for all other operations: on the left, the order is as executed
    by **CPU0**. On the right, the order is as seen by **CPU1**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Visibility of operations with acquire-release memory order'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.10_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – Visibility of operations with acquire-release memory order
  prefs: []
  type: TYPE_NORMAL
- en: The acquire-release order guarantee is a terse statement packed with a lot of
    important information, so let us elaborate on a few distinct points. First of
    all, the order is defined relative to the operations both threads execute on the
    same atomic variable. Until two threads access the same variable atomically, their
    *clocks* remain entirely arbitrary with respect to each other, and we cannot reason
    about what happens before or after something else, there is no meaning to these
    words. It is only when one thread has observed the results of an atomic operation
    executed by another thread that we can talk about *before* and *after*. In our
    producer-consumer example, the producer atomically increments the counter `N`.
    The consumer atomically reads the same counter. If the counter has not changed,
    we don't know anything about the state of the producer. But if the consumer sees
    that the counter has changed from N to N+1 and both threads use the acquire-release
    memory order, we know that all operations executed by the producer prior to incrementing
    the counter are now visible to the consumer. These operations include all the
    work necessary to construct the object that now resides in the array element `buffer[N]`,
    and, thus, the consumer can safely access it.
  prefs: []
  type: TYPE_NORMAL
- en: The second salient point is that both threads must use the acquire-release memory
    order when accessing the atomic variable. If the producer uses this order to increment
    the count, but the consumer reads it with relaxed memory order, there are no guarantees
    on the visibility of any operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final point is that all order guarantees are given in terms of *before*
    and *after* the operation on the atomic variable. Again, in our producer-consumer
    example, we know that the results of the operations executed by the producer to
    construct the Nth object are all visible by the consumer when it sees the counter
    change. There are no guarantees on the order in which these operations become
    visible. You can see this in *Figure 5.10*. Of course, it should not matter to
    us: we can''t touch any part of the object until it''s constructed, and, once
    the construction is finished, we don''t care about the order in which it was done.
    The atomic operations with memory order guarantees act as barriers across which
    other operations cannot move. You can imagine such a barrier in *Figure 5.10*,
    dividing the entire program into two distinct parts: everything that happened
    before the count was incremented and everything that happened after. For that
    reason, it is often convenient to talk about such atomic operations as memory
    barriers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us assume, for a moment, that in our program, all atomic operations on
    the counter `N` have acquire-release barriers. That would certainly guarantee
    that the program is correct. Note, however, that the acquire-release order is
    overkill for our needs. For the producer, it gives us the guarantee that all objects
    `buffer[0]` through `buffer[N]` that were constructed before we incremented the
    count to N+1 will be visible to the consumer when it sees the counter change from
    N to N+1\. We need that guarantee. But we also have the guarantee that none of
    the operations executed for the purpose of constructing the remaining objects,
    `buffer[N+1]` and beyond, have become visible yet. We don''t care about that:
    the consumer is not going to access these objects until it sees the next value
    of the counter. Similarly, on the consumer side, we have the guarantee that all
    the operations executed after the consumer sees the counter change to N+1 will
    have their effects (memory accesses) happen after that atomic operation. We need
    that guarantee: we do not want the CPU to reorder our consumer operations and
    execute some of the instructions that access the object `buffer[N]` before it
    is ready. But we also have the guarantee that the work done by the consumer to
    process the previous objects like `buffer[N-1]` is done and made visible to all
    threads before the consumer moves to the next object. Again, we don''t need that
    guarantee: nothing depends on it.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the harm in having stronger guarantees than what is strictly necessary?
    In terms of correctness, none. But this is a book about writing fast programs
    (also, correct ones). Why are the ordering guarantees necessary in the first place?
    Because when left to their own devices, the compilers and the processors can reorder
    our program instructions almost arbitrarily. Why would they do that? Usually,
    to improve performance. Thus, it stands to reason that the more restrictions we
    impose on the ability to reorder the execution, the stronger the adverse impact
    on performance is. Therefore, in general, we want to use the memory order that
    is restrictive enough for the correctness of our program but no more strict than
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory order that gives us exactly what we need for our producer-consumer
    program is as follows. On the producer side, we need one-half of the guarantee
    given by the acquire-release memory barrier: all operations executed before the
    atomic operation with the barrier must become visible to other threads before
    they execute the corresponding atomic operation. This is known as the release
    memory order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Release memory order'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.11_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – Release memory order
  prefs: []
  type: TYPE_NORMAL
- en: 'When **CPU1** sees the result of the atomic write operation executed by **CPU0**
    with the release memory order, it is guaranteed that the state of the memory,
    as seen by **CPU1**, already reflects all operations executed by **CPU0** before
    this atomic operation. Note that we said nothing about the operations executed
    by **CPU0** after the atomic operation. As we see in *Figure 5.11*, these operations
    may become visible in any order. The memory barrier created by the atomic operation
    is effective only in one direction: any operation that is executed before the
    barrier cannot cross it and be seen after the barrier. But the barrier is permeable
    in the other direction. For this reason, the release memory barrier and the corresponding
    acquire memory barrier are sometimes called **half-barriers**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The acquire memory order is what we need to use on the consumer side. It guarantees
    that all operations executed after the barrier become visible to other threads
    after the barrier, as shown in *Figure 5.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Acquire memory order'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.12_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – Acquire memory order
  prefs: []
  type: TYPE_NORMAL
- en: 'The acquire and release memory barriers are always used as a pair: if one thread
    (in our case, the producer) uses the release memory order with an atomic operation,
    the other thread (the consumer) must use the acquire memory order on the same
    atomic variable. Why do we need both barriers? On the one hand, we have the guarantee
    that everything that is done by the producer to build the new object before it
    increments the count is already visible to the consumer as soon as this increment
    is seen. But this is not enough, so, on the other hand, we have the guarantee
    that the operations executed by the consumer to process this new object cannot
    be moved backward in time, to a moment before the barrier when they could have
    seen the object in an unfinished state.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand that it is not enough to just operate atomically on the
    shared data, you may ask whether our producer-consumer program actually works.
    As it turns out, both the lock version and the lock-free version are correct,
    even though we did not say anything explicitly about the memory order. So, how
    is the memory order controlled in C++?
  prefs: []
  type: TYPE_NORMAL
- en: Memory order in C++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First of all, let us think back to the lock-free version of our producer-consumer
    program, the one with the atomic counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The counter `N` is an atomic variable, an object of a type generated by the
    template `std::atomic` with the type parameter `size_t`. All atomic types support
    atomic read and write operations, that is, they can appear in assignment operations.
    In addition, the integer atomics have the regular integer operations defined and
    implemented atomically, so `++N` is an atomic increment (not all operations are
    defined, for example, there is no operator `*=`). None of these operations explicitly
    specify the memory order, so what guarantees do we have? As it turns out, by default,
    we get the strongest possible guarantee, the bidirectional memory barrier with
    each atomic operation (the actual guarantee is even a little stricter, as you
    will see in the next section). This is why our program is correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you think this is overkill, you can reduce the guarantees to be just the
    ones you need, but you have to be explicit about it. The atomic operations can
    also be executed by calling the member functions of the `std::atomic` type, and
    that is where you can specify the memory order. The consumer thread needs a load
    operation with the acquire barrier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The producer thread needs an increment operation with the release barrier (just
    like the increment operator, the member function also returns the value before
    the increment was done):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Before we go any further, we must realize that we have jumped over one critically
    important step in our optimization. The right way to start the previous paragraph
    is, *If you think this is overkill, you have to prove it by performance measurements,
    and only then can you reduce the guarantees to be just the ones you need*. Concurrent
    programs are hard enough to write even when using locks; the use of lock-free
    code and especially explicit memory orders has to be justified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of locks, what memory order guarantees do they give? We know that
    any operation protected by the lock will be seen by any other thread that acquires
    the lock later, but what about the rest of the memory? The memory order enforced
    by the use of the lock is shown in *Figure 5.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Memory order guarantees of a mutex'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.13_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – Memory order guarantees of a mutex
  prefs: []
  type: TYPE_NORMAL
- en: 'The mutexes have (at least) two atomic operations inside. Locking the mutex
    is an equivalent of a read operation with the acquire memory order (which explains
    the name: this is the memory order we use when we *acquire* the lock). The operation
    creates a half-barrier any operation executed earlier can be seen after the barrier,
    but any operation executed after the lock is acquired cannot be observed earlier.
    When we unlock the mutex or *release* the lock, the release memory order is guaranteed.
    Any operation executed before this barrier will become visible before the barrier.
    You can see that the pair of barriers, acquire and release, act as borders for
    the section of the code sandwiched between them. This is known as the critical
    section: any operation executed inside the critical section, that is, executed
    while the thread was holding the lock, will become visible to any other thread
    when it enters the critical section. No operation can leave the critical section
    (become visible earlier or later), but other operations from the outside can enter
    the critical section. Crucially, no such operation can cross the critical section:
    if an outside operation enters the critical section, it cannot leave. So, anything
    that was done by **CPU0** before its critical section is guaranteed to be visible
    by **CPU1** after its critical section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our producer-consumer program, this translates into the following guarantee:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: All operations executed by the producer to construct the Nth object are done
    before the producer enters the critical section. They will be visible to the consumer
    before it leaves its critical section and begins consuming the Nth object. Therefore,
    the program is correct.
  prefs: []
  type: TYPE_NORMAL
- en: The section you just read introduced the concept of memory order and illustrated
    it with examples. But, as you try to use this knowledge in your code, you will
    find the results wildly inconsistent. To better understand performance, what you
    should expect from the different ways you can use to synchronize your multi-threaded
    programs and avoid data races, we need to have a less hand-waving way to describe
    the memory order and related concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need a more systematic and rigorous way to describe the interaction of threads
    through memory, their use of the shared data, and its effect on concurrent applications.
    This description is known as the memory model. The memory model describes what
    guarantees and restrictions exist when threads access the same memory location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to the C++11 standard, the C++ language had no memory model at all (the
    word *thread* was not mentioned in the standard). Why is that a problem? Consider
    our producer-consumer example again (let us focus on the producer side):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lock_guard` is just an RAII wrapper around the mutex, so we can''t forget
    to unlock it, so the code boils down to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that each line of this code uses either the variable `N` or the object
    `nM`, but they are never used together in one operation. From the C++ point of
    view, this code is similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the order of the operations does not matter, and the compiler
    is free to reorder them as long as the observable behavior does not change (observable
    behavior is things like input and output, changing a value in memory is not an
    observable behavior). Going back to our original example, why doesn't the compiler
    reorder the operations there?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This would be very bad, and yet, nothing in the C++ standard (until C++11) prevents
    the compiler from doing so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we were writing multi-threaded programs in C++ long before 2011,
    so how did they work? Obviously, the compilers did not do such *optimizations*,
    but why? The answer is found in the memory model: the compilers provided certain
    guarantees that went beyond the C++ standard and supplied a certain memory model
    even when the standard required none. The Windows-based compilers followed the
    Windows memory model, while most Unix- and Linux-based compilers provided the
    POSIX memory model and the corresponding guarantees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The C++11 standard changed that and gave C++ its own memory model. We have
    already taken advantage of it in the previous section: the memory order guarantees
    that accompany the atomic operations, and the locks are part of this memory model.
    The C++ memory model now guarantees portability across platforms that previously
    offered a different set of guarantees, each according to its memory model. In
    addition, the C++ memory model offers some language-specific guarantees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen these guarantees in the form of the different memory order specifications:
    relaxed, acquire, release, and acquire-release. C++ has an even stricter memory
    order called `std::memory_order_seq_cst`), which is the default order you get
    when you don''t specify one: not only there is a bidirectional memory barrier
    associated with each atomic operation that specifies this order, but the entire
    program satisfies the sequential consistency requirement. This requirement states
    that the program behaves as if all operations executed by all processors were
    executed in a single global order. Furthermore, this global order has an important
    property: consider any two operations A and B that are executed on one processor
    such that A executed before B. These two operations must appear in the global
    order with A preceding B as well. You can think of a sequentially consistent program
    like this: imagine a deck of cards for every processor, where the cards are operations.
    Then we slide these decks together without shuffling them; cards from one deck
    slide between cards from the other deck, but the order of the cards from the same
    deck never changes. The one combined deck of cards is the apparent global order
    of the operations in the program. Sequential consistency is a desirable property
    because it makes it much easier to reason about the correctness of a concurrent
    program. It does, however, often come at a cost in performance. We can demonstrate
    this cost in a very simple benchmark that compares different memory orders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run this benchmark using different memory orders. The results will,
    of course, depend on the hardware, but the following result is not uncommon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Performance of acquire-release vs. sequential consistency memory
    order'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.14_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – Performance of acquire-release vs. sequential consistency memory
    order
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot more to the C++ memory model than just the atomic operations
    and the memory order. For example, when we studied false sharing earlier, we have
    assumed that it is safe to access adjacent elements of an array from multiple
    threads concurrently. It makes sense: these are different variables. And yet,
    it was not guaranteed by the language or even by the additional restrictions adopted
    by the compiler. On most hardware platforms, accessing adjacent elements of an
    array of integers is indeed thread-safe. But it is definitely not the case for
    data types of smaller size, for example, an array of `bool`. Many processors write
    a single byte using a *masked* integer write: they load the entire 4-byte word
    containing this byte, change the byte to the new value, and write the word back.
    Obviously, if two processors do this at the same time for two bytes that share
    the same 4-byte word, the second write will overwrite the first one. The C++11
    memory model requires that writing into any distinct variables, such as array
    elements, is thread-safe if no two threads access the same variable. Prior to
    C++11, it was easy to write a program that would demonstrate that writing into
    two adjacent `bool` or `char` variables from two threads is not thread-safe. The
    only reason we don''t have this demonstration in this book is that the compilers
    available today don''t fall back to this aspect of C++03 behavior even if you
    specify the standard level as C++03 (this is not guaranteed, and a compiler could
    use masked writes to write single bytes in C++03 mode, but most compilers use
    the same instructions as in C++11 mode).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last example of the importance of the C++ memory model also contains a
    valuable observation: the language and the compiler are not all that defined the
    memory model. The hardware has a memory model, the OS and the runtime environment
    have their memory models, and each component of the hardware/software system the
    program runs on has a memory model. The overall memory model, the total set of
    guarantees and restrictions available to the program, is a superposition of all
    of these memory models. Sometimes you can take advantage of that, for example,
    when writing processor-specific code. However, any portable C++ code can rely
    only on the memory model of the language itself, and, more often than not, other
    underlying memory models are a complication.'
  prefs: []
  type: TYPE_NORMAL
- en: Two kinds of problems arise because of the differences in the memory model of
    the language and that of the hardware. First of all, there may be bugs in your
    program that cannot be detected on particular hardware. Consider the acquire-release
    protocol we used for our producer-consumer program. If we made a mistake and used
    the release memory order on the producer side but relaxed memory order (no barrier
    at all) on the consumer side, we would expect the program to produce incorrect
    results intermittently. However, if you run this program on an x86 CPU, it would
    appear to be correct. This is because the memory model of the x86 architecture
    is such that every store is accompanied by a release barrier, and every load has
    an implicit acquire barrier. Our program still has a bug, and it would trip us
    up if we ported it to, say, an ARM-based processor (like the one in an iPad).
    But the only way to find this bug on x86 hardware is with the help of a tool like
    the **Thread Sanitizer** (**TSAN**) available in GCC and Clang.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second problem is the flip-side of the first one: reducing the restrictions
    on the memory order does not always result in better performance. As you can expect
    from what you have just learned, going from release to relaxed memory order on
    write operations does not yield any benefit on an x86 processor because the overall
    memory model still guarantees the release order (in theory, the compiler might
    do more optimizations with the relaxed memory order than with the release one,
    however, most compilers do not optimize the code across atomic operations at all).'
  prefs: []
  type: TYPE_NORMAL
- en: The memory model provides both the scientific foundation and the common language
    for discussing how programs interact with the memory system. The memory barriers
    are the actual tools the programmer utilizes, in code, to control the memory model
    features. Often, these barriers are invoked implicitly by using locks, but they
    are always there. The optimal use of memory barriers can make a large difference
    in the efficiency of certain high-performance concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the C++ memory model and the guarantees
    it gives to the programmer. The result is a thorough understanding of the low
    level of what happens when multiple threads interact through shared data.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-threaded programs, unsynchronized and unordered access to memory leads
    to undefined behavior and must be avoided at any cost. The cost, however, is usually
    paid in performance. While we always value a correct program over an incorrect
    but fast one, when it comes to memory synchronization, it is easy to overpay for
    correctness. We have seen different ways to manage concurrent memory accesses,
    their advantages, and tradeoffs. The simplest option is to lock all accesses to
    the shared data. The most elaborate implementation, on the other hand, uses atomic
    operations and restricts memory order as little as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first rule of performance is in full force here: performance must be measured,
    not guessed. This is even more important for concurrent programs where clever
    optimizations can fail to yield measurable results for a multitude of reasons.
    On the other hand, the one guarantee you always have is that a simple program
    with locks is easier to write and is more likely to be correct.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with the understanding of the fundamental factors affecting the performance
    of data sharing, you can better understand the results of your measurements, as
    well as developing some sense for when it makes sense to even try to optimize
    the concurrent memory accesses: the larger the part of your code affected by the
    memory order restrictions, the more likely it is that relaxing these restrictions
    will improve the performance. Also, keep in mind that some of the restrictions
    come from the hardware itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this is much more complex material than anything you had to deal with
    in the earlier chapters (not surprising, concurrency is hard in general). The
    next chapter shows some of the ways you can manage this complexity in your program
    without giving up the performance benefits. You will also see the practical applications
    of the knowledge you have learned here.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the memory model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is access to the shared data so important to understand?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What determines the overall memory model for a program?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What constrains the performance gain from concurrency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

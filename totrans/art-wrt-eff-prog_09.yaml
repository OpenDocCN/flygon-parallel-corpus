- en: '*Chapter 7*: Data Structures for Concurrency'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：并发数据结构'
- en: 'In the last chapter, we explored, in detail, the synchronization primitives
    that can be used to ensure the correctness of concurrent programs. We also studied
    the simplest but useful building blocks for these programs: **thread-safe counters**
    and **pointers**.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们详细探讨了可以用来确保并发程序正确性的同步原语。我们还研究了这些程序的最简单但有用的构建块：**线程安全计数器**和**指针**。
- en: 'In this chapter, we are going to continue the study of data structures for
    concurrent programs. The aim of this chapter is two-fold: on the one hand, you
    will learn how to design thread-safe variants of several fundamental data structures.
    On the other hand, we will point out several general principles and observations
    that are important for designing your own data structures to be used in concurrent
    programs, as well as for evaluating the best approaches to organize and store
    your data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续研究并发程序的数据结构。本章的目的是双重的：一方面，你将学习如何设计几种基本数据结构的线程安全变体。另一方面，我们将指出一些对于设计自己的数据结构用于并发程序以及评估组织和存储数据的最佳方法的一般原则和观察是重要的。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding thread-safe data structures, including sequential containers,
    stack and queue, node-based containers, and lists
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线程安全的数据结构，包括顺序容器、栈和队列、基于节点的容器和列表
- en: Improving concurrency, performance, and order guarantees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进并发性能和顺序保证
- en: Recommendations for designing thread-safe data structures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计线程安全数据结构的建议
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter07](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter07).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你将需要一个C++编译器和一个微基准测试工具，比如我们在上一章中使用的Google基准库（在[https://github.com/google/benchmark](https://github.com/google/benchmark)找到）。本章的代码可以在[https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter07](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter07)找到。
- en: What is a thread-safe data structure?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是线程安全的数据结构？
- en: 'Before we begin learning about thread-safe data structures, we have to know
    what they are. If this seems like a simple question – *data structures that can
    be used by multiple threads at once* – you have not given the question enough
    thought. I cannot overstate how important it is to ask this question every time
    you start designing a new data structure or an algorithm to be used in a concurrent
    program. If this sentence puts you on guard and gives you pause, there is a good
    reason for it: I have just implied that the *thread-safe data structure* has no
    single definition that suits every need and every application. This is indeed
    the case, and is a very important point to understand.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始学习线程安全数据结构之前，我们必须知道它们是什么。如果这似乎是一个简单的问题 – *可以同时被多个线程使用的数据结构* – 你还没有认真思考这个问题。我无法过分强调每次开始设计新的数据结构或用于并发程序中的算法时都要问这个问题有多么重要。如果这句话让你警惕并让你停下来，那是有充分理由的：我刚刚暗示*线程安全数据结构*没有适合每个需求和每个应用的单一定义。这确实是这样，也是一个非常重要的观点。
- en: The best kind of thread safety
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程安全的最佳类型
- en: 'Let''s start with something that should be obvious but is often forgotten in
    practice: a very general principle of designing for high performance is that *doing
    zero work is always faster than doing some work*. For the subject at hand, this
    general principle can be narrowed down to *do you need any kind of thread-safety
    for this data structure?* Ensuring thread safety, whatever form it takes, implies
    some amount of work that will need to be done by the computer. Ask yourself, *do
    I really need it? Can I arrange the computation so that each thread has its own
    set of data to operate on?*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个显而易见但在实践中经常被忘记的事情开始：高性能设计的一个非常普遍的原则是*不做任何工作总是比做一些工作更快*。对于这个主题，这个一般原则可以缩小到*你是否需要任何形式的线程安全来处理这个数据结构？*确保线程安全，无论采取什么形式，都意味着计算机需要做一定量的工作。问问自己，*我真的需要吗？我能安排计算，让每个线程都有自己的数据集来操作吗？*
- en: 'A simple example is the thread-safe counter we used in the previous chapter.
    If you need all threads to see the current value of the counter at all times,
    then it was the right solution. However, let''s say that all we need is to count
    some event that happens on multiple threads, such as searching for something in
    a large set of data that has been divided between the threads. A thread does not
    need to know the current value of the count to do the search. Of course, it would
    need to know the latest value of the count to increment it, but that is true only
    if we try to increment the single shared count on all threads, like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子是我们在上一章中使用的线程安全计数器。如果需要所有线程始终看到计数器的当前值，那么这就是正确的解决方案。然而，假设我们只需要计算在多个线程中发生的某个事件，比如在被线程之间分割的大量数据集中搜索某些内容。一个线程不需要知道计数的当前值来进行搜索。当然，它需要知道计数的最新值来递增它，但只有在我们尝试在所有线程上递增单个共享计数时才是如此：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The performance of the counting itself is dismal, as can be seen in a benchmark
    where we do nothing but count (no *search*):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 计数本身的性能是令人沮丧的，如在一个基准测试中可以看到，我们除了计数什么也不做（没有*搜索*）：
- en: '![Figure 7.1 – Counting on multiple threads does not scale if the count is
    shared'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.1 – 如果计数是共享的，则多个线程计数不会扩展'
- en: '](img/Figure_7.1_B16229.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.1_B16229.jpg)'
- en: Figure 7.1 – Counting on multiple threads does not scale if the count is shared
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 如果计数是共享的，则多个线程计数不会扩展
- en: 'The scaling of the counting is actually negative: it takes longer to get to
    the same value of the count on two threads than on one, despite our best efforts
    to use a wait-free count with the minimal memory order requirements. Of course,
    if the search is very long compared to the counting, then the performance of the
    count is irrelevant (but the search code itself may present the same choice of
    doing some work on a global data or a per-thread copy, so consider this an instructive
    example).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we only care about the value of the count at the very end of the computation,
    a much better solution is, of course, to maintain local counts on each thread
    and increment the shared count only once:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To highlight just how unimportant the shared count increment is now, we are
    going to use the basic mutex; usually, a lock is a safer choice as it is easier
    to understand (so, harder to make bugs), although, in the case of a count, an
    atomic integer actually yields simpler code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'If each thread increments the local count many times before it reaches the
    end and has to increment the shared count, the scaling is near-perfect:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Counting on multiple threads scales perfectly with per-thread
    counts'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.2_B16229.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Counting on multiple threads scales perfectly with per-thread counts
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'So the best kind of thread safety is the one that is guaranteed by the fact
    that you don''t access the data structure from multiple threads. Often, this arrangement
    comes at the cost of some overhead: for example, each thread maintains a container
    or a memory allocator whose size grows and shrinks repeatedly. You can avoid any
    locking whatsoever if you don''t release the memory to the main allocator until
    the end of the program. The price will be that the unused memory on one thread
    is not made available to other threads, so the total memory use will be the sum
    of the peak uses of all threads, even if these moments of peak use occur at different
    times. Whether or not this is acceptable depends on the details of the problem
    and the implementation: it is something you have to consider for every program.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: You could say that this entire section is a cop-out when it comes to thread
    safety. It is, from a certain point of view, but it happens so often in practice
    that a shared data structure is used where it is not necessary, and the performance
    gain can be so significant that this point needs to be made. Now it is time to
    move on to the *real* thread safety, where a data structure must be shared between
    threads.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The real thread safety
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s assume that we really need to access a particular data structure from
    multiple threads at the same time. Now we have to talk about thread safety. But
    there is still not enough information to determine what this *thread safety* means.
    We have already discussed in the previous chapter the strong and weak thread-safety
    guarantees. We will see in this chapter that even that partitioning is not enough,
    but it puts us on the right track: instead of talking about general *thread safety*,
    we should be describing the set of guarantees provided by the data structure with
    regard to concurrent access.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, the weak (but usually easy to provide) guarantee is that multiple
    threads can read the same data structure as long as it remains unchanged. The
    strongest guarantee is, obviously, that any operation can be done by any number
    of threads at any time, and the data structure remains in a well-defined state.
    This guarantee is often both expensive and unnecessary. Your program may require
    such a guarantee from some but not all operations supported by the data structure.
    There may be other simplifications, such as the number of threads accessing the
    data structure at once may be limited.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'As a rule, you want to provide as few guarantees as necessary to make your
    program correct and no more: additional thread-safety features are often very
    expensive and create overhead even when they are not used.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let's start exploring concrete data structures and see what
    it takes to provide different levels of thread-safety guarantees.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们开始探索具体的数据结构，看看提供不同级别的线程安全保证需要做些什么。
- en: The thread-safe stack
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程安全的堆栈
- en: One of the simplest data structures from the point of view of concurrency is
    the **stack**. All operations on the stack deal with the top element, so there
    is (conceptually, at least) a single location that needs to be guarded against
    races.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从并发性的角度来看，最简单的数据结构之一是**堆栈**。堆栈上的所有操作都涉及顶部元素，因此（至少在概念上）需要针对竞争进行保护的单个位置。
- en: 'The C++ standard library offers us the `std::stack` container, so it makes
    a good starting point. All C++ containers, including the stack, offer the weak
    thread-safety guarantee: a read-only container can be safely accessed by many
    threads. In other words, any number of threads can call any `const` methods at
    the same time as long as no thread calls any non-`const` methods. While this sounds
    easy, almost simplistic, there is a subtle point here: there must be some kind
    of synchronization event accompanied by a memory barrier between the last modification
    of the object and the portion of the program where it is considered read-only.
    In other words, write access is not really *done* until all threads execute a
    memory barrier: the writer must, as a minimum, do a release, while all readers
    must acquire. Any stronger barrier will work as well, and so will a lock, but
    every thread must take this step.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: C++标准库为我们提供了`std::stack`容器，因此它是一个很好的起点。所有C++容器，包括堆栈，都提供了弱线程安全保证：只读容器可以被多个线程安全地访问。换句话说，只要没有线程调用任何非`const`方法，任意数量的线程可以同时调用任何`const`方法。虽然这听起来很容易，几乎是简单的，但这里有一个微妙的地方：在对象的最后修改和被认为是只读的程序部分之间必须有某种同步事件和内存屏障。换句话说，写访问实际上并没有*完成*，直到所有线程执行内存屏障：写入者必须至少执行一个释放，而所有读取者必须获取。任何更强的屏障也可以工作，锁也可以，但每个线程都必须采取这一步。
- en: Interface design for thread safety
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接口设计的线程安全性
- en: Now, what if at least one thread is modifying the stack, and we need a stronger
    guarantee? The most straightforward way to provide one is by guarding every member
    function of the class with a mutex. This can be done at the application level,
    but such implementation does not enforce the thread safety and is, therefore,
    error-prone. It is also hard to debug and analyze because the lock is not associated
    with the container.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果至少有一个线程正在修改堆栈，我们需要更强的保证怎么办？提供一个最直接的方法是用互斥锁保护类的每个成员函数。这可以在应用程序级别完成，但这样的实现并不强制执行线程安全，因此容易出错。它也很难调试和分析，因为锁与容器没有关联。
- en: 'A better option is to *wrap* the stack class with our own, like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的选择是用我们自己的类来*包装*堆栈类，就像这样：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we could use inheritance instead of encapsulation. Doing so would
    make it easier to write the constructors of `mt_stack`: we would need only one
    `using` statement. However, using public inheritance exposes every member function
    of the base class `std::stack`, so if we forget to wrap one of them, the code
    will compile but will call the unguarded member function directly. Private (or
    protected) inheritance avoids this problem but presents other dangers. Some of
    the constructors would need to be reimplemented: for example, the move constructor
    would need to lock the stack that is being moved from, so it needs a custom implementation
    anyway. Several other constructors would be dangerous to expose without a wrapper
    because they read or modify their arguments. Overall, it is safer if we have to
    write every constructor we want to provide. This is consistent with the very general
    rule of C++; *prefer composition over inheritance*.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以使用继承而不是封装。这样做将使得编写`mt_stack`的构造函数更容易：我们只需要一个`using`语句。然而，使用公共继承会暴露基类`std::stack`的每个成员函数，因此如果我们忘记包装其中的一个，代码将编译但会直接调用未受保护的成员函数。私有（或受保护的）继承可以避免这个问题，但会带来其他危险。一些构造函数需要重新实现：例如，移动构造函数需要锁定正在移动的堆栈，因此它需要自定义实现。还有几个构造函数在没有包装的情况下暴露会很危险，因为它们读取或修改它们的参数。总的来说，如果我们想要提供每个构造函数，最好是安全的。这与C++的一个非常普遍的规则一致；*优先使用组合而不是继承*。
- en: 'Our thread-safe or multi-threaded stack (that''s what *mt* stands for) now
    has the *push* functionality and is ready to receive data. We just need the other
    half of the interface, the *pop*. We can certainly follow the preceding example
    and wrap the `pop()` method, but this is not enough: the STL stack uses three
    separate member functions to remove elements from the stack. `pop()` removes the
    top element but returns nothing, so if you want to know what''s on top of the
    stack, you have to call `top()` first. It is undefined behavior to call either
    of those if the stack is empty, so you have to call `empty()` first and check
    the result. OK, we can wrap all three methods, but this gives us nothing at all.
    In the following code, assume that all member functions of the stack are guarded
    by a lock:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的线程安全或多线程堆栈（这就是*mt*的含义）现在具有*push*功能，并且已经准备好接收数据。我们只需要接口的另一半，*pop*。我们当然可以按照前面的例子包装`pop()`方法，但这还不够：STL堆栈使用三个单独的成员函数来从堆栈中移除元素。`pop()`移除顶部元素但不返回任何内容，所以如果你想知道堆栈顶部是什么，你必须先调用`top()`。如果堆栈为空，调用这两个方法是未定义行为，所以你必须先调用`empty()`并检查结果。好吧，我们可以包装所有三个方法，但这对我们来说毫无意义。在下面的代码中，假设堆栈的所有成员函数都受到锁的保护：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each member function is perfectly thread-safe and perfectly useless in a multi-threaded
    context: the stack may be non-empty one moment – the moment we happen to call
    `s.empty()` – but become empty the next, before we call `s.top()`, because another
    thread could remove the top element in the meantime.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个成员函数在多线程环境中都是完全线程安全的，但在多线程环境中完全无用：堆栈可能在某一时刻非空 - 我们碰巧调用`s.empty()`的时候 - 但在下一时刻变为空，在我们调用`s.top()`之前，因为另一个线程可能在此期间移除了顶部元素。
- en: 'This may very well be the most important lesson from the entire book: *in order
    to provide usable thread-safe functionality, the interface must be chosen with
    thread safety in mind*. More generally, it is not possible to *add* thread safety
    on top of an existing design. Instead, the design must be done with thread safety
    in mind. The reason is this: you may choose to provide certain guarantees and
    invariants in your design that are impossible to maintain in a concurrent program.
    For example, `std::stack` provides the guarantee that if you call `empty()` and
    it returns `false`, you can safely call `top()` as long as you don''t do anything
    else to the stack between these two calls. There is no practically useful way
    to maintain this guarantee in a multi-threaded program.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这很可能是整本书中最重要的一课：*为了提供可用的线程安全功能，接口必须考虑线程安全*。更一般地说，不可能在现有设计的基础上*添加*线程安全。相反，设计必须考虑线程安全。原因是：您可能选择在设计中提供某些保证和不变量，这些保证和不变量在并发程序中是不可能维护的。例如，`std::stack`提供了这样的保证，如果您调用`empty()`并且它返回`false`，则只要在这两次调用之间不对栈进行其他操作，您就可以安全地调用`top()`。在多线程程序中，几乎没有实用的方法来维护这个保证。
- en: 'Fortunately, since we are writing our own wrapper class anyway, we are not
    constrained to use the interface of the wrapped class verbatim. So, what should
    we do instead? Clearly, the entire *pop* operation should be a single member function:
    it should remove the top element from the stack and return it to the caller. One
    complication is what to do when the stack is empty. We have multiple options here.
    We could return a pair of the value and a Boolean flag that indicates whether
    the stack was empty (the value would have to be default-constructed in this case).
    We could return the Boolean alone and pass the value by reference (it remains
    unchanged if the stack is empty). In C++17, the natural solution is to return
    `std::optional`, as shown in the following code. It''s a perfect fit for the job
    of holding a value that may not exist:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于我们无论如何都要编写自己的包装器类，我们并不受约束，必须逐字使用包装类的接口。那么，我们应该做什么呢？显然，整个*pop*操作应该是一个单一的成员函数：它应该从栈中移除顶部元素并将其返回给调用者。一个复杂之处在于当栈为空时该怎么办。在这里我们有多个选择。我们可以返回值和一个布尔标志的对，指示栈是否为空（在这种情况下，值必须是默认构造的）。我们可以仅返回布尔值，并通过引用传递值（如果栈为空，则值保持不变）。在C++17中，自然的解决方案是返回`std::optional`，如下面的代码所示。它非常适合保存可能不存在的值的工作：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see, the entire operation of popping the element from the stack
    is now protected by a lock. The key property of this interface is that it is transactional:
    each member function takes the object from one known state to another known state.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，从栈中弹出元素的整个操作现在受到锁的保护。这个接口的关键特性是它是事务性的：每个成员函数将对象从一个已知状态转换到另一个已知状态。
- en: 'If the object has to transition through some intermediate states that are not
    sufficiently defined, such as the state after calling `empty()` but before calling
    `pop()`, then these states must be hidden from the caller. The caller is instead
    presented with a single atomic transaction: either the top element is returned,
    or the caller is informed that there isn''t one. This ensures the correctness
    of the program; now, we can look at the performance.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对象必须经过一些不够定义的中间状态的转换，比如在调用`empty()`之后但在调用`pop()`之前的状态，那么这些状态必须对调用者隐藏。调用者将被呈现一个单一的原子事务：要么返回顶部元素，要么通知调用者没有顶部元素。这确保了程序的正确性；现在，我们可以看看性能。
- en: Performance of mutex-guarded data structures
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互斥保护数据结构的性能
- en: How well does our stack perform? Given that every operation is locked from start
    to finish, we should not expect the calls to the stack member function to scale
    at all. At best, all threads will execute their stack operations serially, but,
    in reality, we should expect some overhead from the locking. We can measure this
    overhead in a benchmark if we compare the performance of the multi-threaded stack
    with that of `std::stack` on a single thread.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的栈的性能如何？考虑到每个操作从头到尾都被锁定，我们不应该期望栈成员函数的调用会有任何扩展。最好的情况是，所有线程将按顺序执行它们的栈操作，但实际上，我们应该期望锁定会带来一些开销。如果我们将多线程栈的性能与单线程上的`std::stack`的性能进行比较，我们可以在基准测试中测量这种开销。
- en: 'To simplify the benchmark, you may choose to implement a single-threaded non-blocking
    wrapper around `std::stack` that presents the same interface as our `mt_stack`.
    Beware that you cannot benchmark just by pushing on the stack: your benchmark
    will probably run out of memory. Similarly, you cannot reliably benchmark the
    pop operation unless you want to measure the cost of popping from an empty stack.
    If the benchmark runs long enough, you have to combine both push and pop. The
    simplest benchmark may look like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化基准测试，您可以选择在`std::stack`周围实现一个单线程非阻塞的包装器，它呈现与我们的`mt_stack`相同的接口。请注意，您不能仅通过将元素推送到栈上来进行基准测试：您的基准测试可能会耗尽内存。同样，除非要测量从空栈中弹出的成本，否则您不能可靠地对弹出操作进行基准测试。如果基准测试运行时间足够长，您必须同时进行推送和弹出。最简单的基准测试可能如下所示：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When running multi-threaded, there is a chance that some of the `pop()` operations
    will happen while the stack is empty. This may be realistic for the application
    for which you are designing the stack. Also, since the benchmark gives us only
    an approximation of the performance of the data structure in the real application,
    it may not matter. For a more accurate measurement, you would probably have to
    emulate the realistic sequence of push and pop operations produced by your application.
    Anyway, the results should look something like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行多线程时，有可能会发生一些`pop()`操作在栈为空时发生。这可能是您设计栈的应用程序的现实情况。此外，由于基准测试只能给出数据结构在实际应用中性能的近似值，这可能并不重要。要进行更准确的测量，您可能需要模拟应用程序产生的推送和弹出操作的实际序列。无论如何，结果应该看起来像这样：
- en: '![Figure 7.3 – Performance of a mutex-guarded stack'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3 – 互斥保护栈的性能'
- en: '](img/Figure_7.3_B16229.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.3_B16229.jpg)'
- en: Figure 7.3 – Performance of a mutex-guarded stack
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 - 互斥保护的堆栈的性能
- en: 'Note that the "item" here is a push followed by a pop, so the value of "items
    per second" shows how many data elements we can send through the stack every second.
    For comparison, the same stack without any locks performs more than 10 times faster
    on a single thread:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“项”在这里是推送后跟弹出，因此“每秒项数”的值显示了我们可以每秒通过堆栈发送多少数据元素。作为比较，同样的堆栈在单个线程上的性能比没有任何锁的情况下快了10多倍：
- en: '![Figure 7.4 – Performance of std::stack (compare with Figure 7.3)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 - std::stack的性能（与图7.3进行比较）'
- en: '](img/Figure_7.4_B16229.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.4_B16229.jpg)'
- en: Figure 7.4 – Performance of std::stack (compare with Figure 7.3)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 - std::stack的性能（与图7.3进行比较）
- en: As we can see, the simplest implementation of the stack using a mutex has rather
    poor performance. However, you should not be in a rush to find or design some
    clever thread-safe stack, at least not yet. The first question you should ask
    is, *does it matter?* What does the application do with the data on the stack?
    If, say, each data element is a parameter for a simulation that takes several
    seconds, it probably doesn't matter how fast the stack is. On the other hand,
    if the stack is at the heart of some real-time transaction processing system,
    its speed is likely the key to the performance of the entire system.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，使用互斥锁实现的堆栈的性能相当差。然而，你不应该急于寻找或设计一些聪明的线程安全堆栈，至少现在还不是。你应该首先问的问题是，“这重要吗？”应用程序对堆栈上的数据做什么？比如说，每个数据元素是一个需要几秒钟的模拟参数，堆栈的速度可能并不重要。另一方面，如果堆栈是某个实时事务处理系统的核心，它的速度很可能是整个系统性能的关键。
- en: By the way, the results will likely be similar for any other data structure
    such as list, deque, queue, and tree, where the individual operations are much
    faster than the operations on the mutex. But before we can try to improve the
    performance, we have to consider exactly what kind of performance our application
    requires.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，对于任何其他数据结构，如列表、双端队列、队列和树，其结果可能会类似，其中单个操作比互斥锁的操作快得多。但在我们尝试提高性能之前，我们必须考虑我们的应用程序需要什么样的性能。
- en: Performance requirements for different uses
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同用途的性能要求
- en: For the rest of this chapter, let's assume that the performance of the data
    structures matters in your application. Now, can we see the fastest stack implementation
    already? Again, not yet. We also need to consider the use model; in other words,
    what do we do with the stack and what exactly needs to be fast.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，让我们假设数据结构的性能对你的应用程序很重要。现在，我们能看到最快的堆栈实现了吗？同样，还没有。我们还需要考虑使用模型；换句话说，我们对堆栈做什么，什么需要快。
- en: For example, as we have just seen, the key reason for the poor performance of
    the mutex-guarded stack is that its speed is essentially limited by the mutex
    itself. Benchmarking the stack operations is almost the same as benchmarking locking
    and unlocking the mutex. One way to improve the performance would be to improve
    the implementation of the mutex or use another synchronization scheme. Another
    way is to use the mutex less often; this way requires that we redesign the client
    code.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正如我们刚刚看到的，互斥保护的堆栈性能不佳的主要原因是其速度基本上受到互斥本身的限制。对堆栈操作进行基准测试几乎与对互斥锁进行基准测试相同。提高性能的一种方法是改进互斥锁的实现或使用另一种同步方案。另一种方法是更少地使用互斥锁；这种方式需要我们重新设计客户端代码。
- en: 'For example, very often, the caller has multiple items that must be pushed
    onto the stack. Similarly, the caller may be able to pop several elements at once
    from the stack and process them. In this case, we can implement a batch push or
    a batch pop using an array or another container to copy multiple elements to and
    from the stack at once. Since the overhead of locking is large, we can expect
    that pushing, say, 1,024 elements on the stack with one lock/unlock operation
    is faster than pushing each one under a separate lock. Indeed, the benchmark shows
    this to be the case:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，很多时候，调用者有多个项目必须推送到堆栈上。同样，调用者可能能够一次从堆栈中弹出多个元素并处理它们。在这种情况下，我们可以使用数组或另一个容器实现批量推送或批量弹出，一次复制多个元素到堆栈中或从堆栈中。由于锁的开销很大，我们可以期望使用一个锁/解锁操作将1,024个元素推送到堆栈上比分别在单独的锁下推送每个元素更快。事实上，基准测试显示情况是这样的：
- en: '![Figure 7.5 – Performance of the batch stack operations (1,024 elements per
    lock)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.5 - 批处理堆栈操作的性能（每个锁1,024个元素）'
- en: '](img/Figure_7.5_B16229.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.5_B16229.jpg)'
- en: Figure 7.5 – Performance of the batch stack operations (1,024 elements per lock)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 - 批处理堆栈操作的性能（每个锁1,024个元素）
- en: 'We should be very clear about what this technique does and does not accomplish:
    it reduces the overhead of the locking if the critical section is much faster
    than the lock operations themselves. It does not make the locked operations scale.
    Furthermore, by making the critical section longer, we force the threads to wait
    longer on the lock. This is fine if all threads are mostly trying to access the
    stack (this is why the benchmark is getting faster). But if, in our application,
    the threads are mostly doing other computations and only occasionally access the
    stack, the longer wait will likely degrade the overall performance. To answer
    definitively whether batch push and batch pop are beneficial, we would have to
    profile them in a more realistic context.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该非常清楚这种技术能做什么，以及不能做什么：如果关键部分比锁操作本身快得多，它可以减少锁的开销。它并不能使锁定的操作扩展。此外，通过延长关键部分，我们迫使线程在锁上等待更长时间。如果所有线程大部分时间都在尝试访问堆栈（这就是为什么基准测试变得更快的原因），这是可以接受的。但是，如果在我们的应用程序中，线程大部分时间都在做其他计算，只偶尔访问堆栈，那么更长的等待时间可能会降低整体性能。要明确回答批量推送和批量弹出是否有益，我们需要在更真实的环境中对它们进行分析。
- en: 'There are other scenarios where the search for a more limited, application-specific
    solution can yield performance gains far above what any improved implementation
    of a general solution can do. For example, this scenario is common in some applications:
    a single thread pushed a lot of data on the stack upfront, and then multiple threads
    remove the data from the stack and process it, and maybe push more data onto the
    stack. In this case, we can implement an unlocked push to be used only in the
    single-threaded context for the upfront push. While the responsibility is on the
    caller to never use this method in a multi-threaded context, the unlocked stack
    is so much faster than the locked one that it may be worth the complexity.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他情景，寻找更有限的、特定于应用程序的解决方案可以获得远高于任何改进的通用解决方案的性能增益。例如，在一些应用程序中，一个单独的线程预先将大量数据推送到堆栈上，然后多个线程从堆栈中移除数据并处理它，可能还会将更多数据推送到堆栈上。在这种情况下，我们可以实现一个未锁定的推送，仅在单线程上下文中使用。虽然责任在于调用者永远不要在多线程上下文中使用这种方法，但未锁定的堆栈比锁定的堆栈快得多，可能值得复杂化。
- en: 'More complex data structures offer a variety of use models, but even the stack
    can be used by more than simple push and pop. We can also look at the top element
    without deleting it. The `std::stack` provides the `top()` member function, but,
    once again, it is not transactional, so we have to create our own. It is very
    similar to the transactional `pop()` function, only without removing the top element:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的数据结构提供了各种使用模型，但即使堆栈也可以用于更多的简单推送和弹出。我们还可以查看顶部元素而不删除它。`std::stack`提供了`top()`成员函数，但同样，它不是事务性的，所以我们必须创建自己的。它与事务性的`pop()`函数非常相似，只是不删除顶部元素：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that, to allow the lookup-only function, `top()`, to be declared `const`,
    we had to declare the mutex as `mutable`. This should be done with caution: the
    convention for multi-threaded programs is that, following the STL, all `const`
    member functions are safe to call on multiple threads as long as no non-`const`
    member functions are called. This generally implies that `const` methods do not
    modify the object, that they are truly read-only. The mutable data members violate
    this assumption. As a minimum, they should not represent the logical state of
    the object: they are only implementation details. Then, care should be taken to
    avoid any race conditions when modifying them. The mutex satisfies both of these
    requirements.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了允许只查找的函数`top()`被声明为`const`，我们必须将互斥锁声明为`mutable`。这应该谨慎处理：多线程程序的约定是，遵循STL，只要不调用非`const`成员函数，所有`const`成员函数都可以安全地在多个线程上调用。这通常意味着`const`方法不修改对象，它们确实是只读的。可变数据成员违反了这一假设。至少在修改它们时应该避免任何竞争条件。互斥锁满足这两个要求。
- en: 'Now we can consider different use patterns. In some applications, the data
    is pushed on the stack and popped from it. In others, the top stack element may
    need to be examined many times between each push and pop. Let''s focus on the
    latter case first. Examine the code for the `top()` method again. There is an
    obvious inefficiency here: because of the lock, only one thread can read the top
    element of the stack at any moment. But reading the top element is a non-modifying
    (read-only) operation. If all threads did that and no thread tried to modify the
    stack at the same time, we would not need the lock at all, and the `top()` operation
    would scale perfectly. Instead, it has a performance similar to that of the `pop()`
    method.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以考虑不同的使用模式。在一些应用程序中，数据被推送到堆栈上并从中弹出。在其他情况下，顶部堆栈元素可能需要在每次推送和弹出之间被多次检查。让我们首先关注后一种情况。再次检查`top()`方法的代码。这里显然存在一个低效：由于锁的存在，只有一个线程可以在任何时刻读取堆栈的顶部元素。但是读取顶部元素是一个非修改（只读）操作。如果所有线程都这样做，而且没有线程同时尝试修改堆栈，我们根本不需要锁，`top()`操作将会完美扩展。相反，它的性能与`pop()`方法相似。
- en: 'The reason we cannot omit lock in `top()` is that we cannot be sure that another
    thread is not calling `push()` or `pop()` at the same time. But even then, we
    do not need to lock two calls to `top()` against each other; they can proceed
    simultaneously. Only the operations that modify the stack need to be locked. There
    is a type of lock that provides such functionality; it is most commonly called
    a `top()` method uses the shared lock, so any number of threads can execute it
    simultaneously, but the `push()` and `pop()` methods require the unique lock:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在`top()`中省略锁的原因是我们无法确定另一个线程是否同时调用`push()`或`pop()`。但即使如此，我们也不需要锁定两次对`top()`的调用；它们可以同时进行。只有修改堆栈的操作需要被锁定。有一种锁提供了这样的功能；它通常被称为`top()`方法使用共享锁，因此任意数量的线程可以同时执行它，但`push()`和`pop()`方法需要唯一锁：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Unfortunately, our benchmark shows that the performance of the call to `top()`
    by itself does not scale even with the read-write lock:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们的基准测试显示，即使使用读写锁，`top()`的调用性能也无法扩展：
- en: '![Figure 7.6 – Performance of the stack with std::shared_mutex; read-only operations'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.6 – 使用std::shared_mutex的堆栈性能；只读操作'
- en: '](img/Figure_7.6_B16229.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.6_B16229.jpg)'
- en: Figure 7.6 – Performance of the stack with std::shared_mutex; read-only operations
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 使用std::shared_mutex的堆栈性能；只读操作
- en: 'Even worse, the performance of the operations that need the unique lock is
    degraded even more compared to the regular mutex:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至更糟的是，需要唯一锁的操作的性能与常规互斥锁相比会进一步下降：
- en: '![Figure 7.7 – Performance of the stack with std::shared_mutex; write operations'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 – 使用std::shared_mutex的堆栈性能；写操作'
- en: '](img/Figure_7.7_B16229.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.7_B16229.jpg)'
- en: Figure 7.7 – Performance of the stack with std::shared_mutex; write operations
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 使用std::shared_mutex的堆栈性能；写操作
- en: 'Comparing *Figures 7.6* and *7.7* with the earlier measurements in *Figure
    7.4*, we can see that the read-write lock did not give us any improvement at all.
    This conclusion is far from universal: the performance of different mutexes depends
    on the implementation and the hardware. However, in general, the more complex
    locks, such as the shared mutex, will have more overhead than the simple locks.
    Their target application is different: if the critical section itself took much
    longer (say, milliseconds instead of microseconds) and most threads executed read-only
    code, there would be great value in not locking the read-only threads against
    each other, and the overhead of a few microseconds would be much less noticeable.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将*图7.6*和*7.7*与*图7.4*中的早期测量进行比较，我们可以看到读写锁根本没有给我们带来任何改进。这个结论远非普遍适用：不同互斥锁的性能取决于实现和硬件。然而，一般来说，更复杂的锁，如共享互斥锁，会比简单的锁有更多的开销。它们的目标应用是不同的：如果临界区本身花费了更长的时间（比如毫秒而不是微秒），并且大多数线程执行只读代码，那么不锁定只读线程之间的互斥将具有很大的价值，几微秒的开销将不太明显。
- en: 'The longer critical section observation is of great importance: if our stack
    elements were much larger and very expensive to copy, the performance of the locks
    would matter less compared to the cost of copying the large objects, and we would
    start to see scaling. However, assuming our overall goal is to make the program
    fast, rather than showing off a scalable stack implementation, we would optimize
    the entire application by eliminating the expensive copying altogether and using
    a stack of pointers instead.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 更长的临界区观察非常重要：如果我们的栈元素更大，并且复制起来非常昂贵，那么锁的性能就不那么重要了，与复制大对象的成本相比，我们会开始看到扩展。然而，假设我们的总体目标是使程序快速，而不是展示可扩展的栈实现，我们将通过完全消除昂贵的复制并使用指针栈来优化整个应用程序。
- en: Despite the setback we have suffered with the read-write lock, we are on the
    right track with the idea of a more efficient implementation. But before we can
    design one, we have to understand in more detail what exactly each of the stack
    operations does and what are the possible data races at each step that we must
    guard against.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在读写锁方面遭受了挫折，但我们对更高效的实现思路是正确的。但在我们设计之前，我们必须更详细地了解栈操作的确切内容以及在每一步可能发生的数据竞争。
- en: Stack performance in detail
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 栈性能详解
- en: 'As we try to improve the performance of the thread-safe stack (or any other
    data structure) beyond that of the simple lock-guarded implementation, we have
    to first understand in detail the steps involved in each operation and how they
    may interact with other operations executed on different threads. The main value
    of this section is not the faster stack but this analysis: it turns out that these
    low-level steps are common to many data structures. Let''s start with the push
    operation. Most stack implementations are built on top of some array-like container,
    so let''s view the top of the stack as a contiguous block of memory:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图改进线程安全栈（或任何其他数据结构）的性能超出简单的锁保护实现时，我们首先必须详细了解每个操作涉及的步骤以及它们如何与在不同线程上执行的其他操作交互。这一部分的主要价值不在于更快的栈，而在于这种分析：事实证明，这些低级步骤对许多数据结构都是共同的。让我们从推送操作开始。大多数栈实现都是建立在某种类似数组的容器之上，因此让我们将栈顶视为连续的内存块：
- en: '![Figure 7.8 – Top of the stack for push operation'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.8 - 推送操作的栈顶'
- en: '](img/Figure_7.8_B16229.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.8_B16229.jpg)'
- en: Figure 7.8 – Top of the stack for push operation
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 - 推送操作的栈顶
- en: 'There are `N` elements on the stack, so the element count is also the index
    of the first free slot where the next element would go. The push operation must
    increment the top index (which is also the element count) from `N` to `N+1` to
    reserve its slot and then construct the new element in the slot `N`. Note that
    this top index is the only part of the data structure where the threads doing
    push can interact with each other: as long as the index increment operation is
    thread-safe, only one thread can see each value of the index. The first thread
    to execute the push advances the top index to `N+1` and reserves the `N`th slot,
    the next thread increments the index to `N+2` and reserves the `N+1`st slot, and
    so on. The key point here is that there is no race for the slots themselves: only
    one thread can get a particular slot, so it can construct the object there without
    any danger of another thread interfering with it.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 栈上有`N`个元素，因此元素计数也是下一个元素将放置的第一个空槽的索引。推送操作必须将顶部索引（也是元素计数）从`N`增加到`N+1`来保留其槽，然后在槽`N`中构造新元素。请注意，这个顶部索引是数据结构的唯一部分，其中进行推送的线程可以相互交互：只要索引增量操作是线程安全的，只有一个线程可以看到索引的每个值。执行推送的第一个线程将顶部索引提升到`N+1`并保留第`N`个槽，下一个线程将索引增加到`N+2`并保留第`N+1`个槽，依此类推。关键点在于这里对槽本身没有竞争：只有一个线程可以获得特定的槽，因此它可以在那里构造对象，而不会有其他线程干扰。
- en: 'This suggests a very simple synchronization scheme for the push operations:
    all we need is a single atomic value for the top index:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明推送操作的非常简单的同步方案：我们只需要一个用于顶部索引的原子值：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A push operation atomically increments this index and then constructs the new
    element in the array slot indexed by the old value of the index:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 推送操作会原子地增加这个索引，然后在由索引的旧值索引的数组槽中构造新元素：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Again, there is no need to protect the construction step from other threads.
    The atomic index is all we need to make the push operations thread-safe. By the
    way, this is true if we use an array as the stack memory. If we use a container
    such as `std::deque`, we cannot simply construct a new element over its memory:
    we have to call `push_back` to update the size of the container, and that call
    is not thread-safe even if the deque does not need to allocate more memory. For
    this reason, data structure implementations that go beyond basic locks usually
    also have to manage their own memory. Speaking of memory, we have assumed so far
    that the array has space to add more elements, and we do not run out of memory.
    Let''s stick with this assumption for now.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have so far is a very efficient way to implement a thread-safe push
    operation in a particular case: multiple threads may be pushing data onto the
    stack, but nobody is reading it until all push operations are done.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The same idea works if we have a stack with elements already pushed onto it,
    and we need to pop them (and no more new elements are added). *Figure 7.8* works
    for this scenario as well: a thread atomically decrements the top count and then
    returns the top element to the caller:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The atomic decrement guarantees that only one thread can access each array slot
    as the top element. Of course, this works only as long as the stack is not empty.
    We could change the top element index from an unsigned to a signed integer; then,
    we would know that the stack is empty when the index becomes negative.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'This is, again, a very efficient way to implement thread-safe pop operation
    under very special conditions: the stack is already populated, and no new elements
    are added. In this case, we also know how many elements are on the stack, so it
    is fairly easy to avoid an attempt to pop the empty stack.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'In some specific applications, this may be of some value: if the stack is first
    populated by multiple threads without any pops and there is a clearly defined
    point in the program where it switches from adding data to removing it, then we
    have a great solution for each half of the problem. But let''s continue to a more
    general case.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Our very efficient push operation is, unfortunately, of no help when it comes
    to reading from the stack. Let''s consider again how we would implement the operation
    that pops the top element. We have the top index, but all it tells us is how many
    elements are currently being constructed; it says nothing about the location of
    the last element whose construction is completed (element `N-3` in *Figure 7.9*):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Top of the stack for push and pop operations'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.9_B16229.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Top of the stack for push and pop operations
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the thread that does the push and, therefore, the construction, knows
    when it's done. Perhaps what we need is another count that shows how many elements
    are fully constructed. Alas, if only it was that simple. In *Figure 7.9*, let's
    assume that thread A is constructing the element `N-2` and that thread B is constructing
    the element `N-1`. Obviously, thread A was the first to increment the top index.
    But it doesn't mean it will also be the first to complete the push. Thread B may
    finish the construction first. Now, the last constructed element on the stack
    has the index `N-1`, so we could advance the *constructed count* to `N-1` (note
    that we *jumped* over element `N-2`, which is still in the middle of the construction).
    Now we want to pop the top element; no problem, the element `N-1` is ready, and
    we can return it to the caller and remove it from the stack; the *constructed
    count* is now decremented to `N-2`. Which element should we pop next? The element
    `N-2` is still not ready, but nothing in our stack warns us about it. We have
    only one count for *completed* elements, and its value is `N-1`. Now we have a
    data race between the thread that constructs a new element on the stack and the
    thread that tried to pop it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，进行推送和因此构建的线程知道何时完成。也许我们需要另一个计数，显示有多少元素完全构建了。遗憾的是，如果只是那么简单就好了。在*图7.9*中，假设线程A正在构建元素`N-2`，线程B正在构建元素`N-1`。显然，线程A首先增加了顶部索引。但这并不意味着它也会首先完成推送。线程B可能会先完成构建。现在，堆栈上最后构建的元素的索引是`N-1`，所以我们可以将*构建计数*提高到`N-1`（注意我们*跳过*了仍在构建中的元素`N-2`）。现在我们想弹出顶部元素；没问题，元素`N-1`已经准备好了，我们可以将其返回给调用者并从堆栈中删除它；*构建计数*现在减少到`N-2`。接下来应该弹出哪个元素？元素`N-2`仍然没有准备好，但我们的堆栈中没有任何内容告诉我们。我们只有一个用于*完成*元素的计数，它的值是`N-1`。现在我们在构建新元素的线程和尝试弹出它的线程之间存在数据竞争。
- en: 'Even without this race, there is another problem: we just popped the element
    `N-1`, which was the right thing to do at the time. But while that was happening,
    a push was requested on thread C. Which slot should be used? If we use slot `N-1`,
    we risk overwriting the same element that is currently being accessed by thread
    A. If we use slot `N`, then, once all the operations are completed, we have a
    *hole* in the array: the top element is `N`, but the next one is not `N-1`: it
    was already popped, and we have to jump over it. Nothing in this data structure
    tells us that we must do so.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有这场竞赛，还有另一个问题：我们刚刚弹出了元素`N-1`，这在当时是正确的。但与此同时，线程C请求了一个推送。应该使用哪个槽？如果我们使用槽`N-1`，我们就有可能覆盖线程A当前正在访问的相同元素。如果我们使用槽`N`，那么一旦所有操作完成，数组中就会有一个*空洞*：顶部元素是`N`，但下一个元素不是`N-1`：它已经被弹出，我们必须跳过它。这个数据结构中没有任何内容告诉我们我们必须这样做。
- en: We could keep track of which elements are *real* and which ones are *holes*,
    but this is becoming more and more complex (and doing it in a thread-safe manner
    will require additional synchronization that will reduce performance). Also, leaving
    many array slots unused wastes memory. We could attempt to reuse the *holes* for
    new elements pushed on the stack, but at this point, the elements are no longer
    stored consecutively, the atomic top count no longer works, and the whole structure
    begins to resemble a list. By the way, if you think that a list would be a great
    way to implement a thread-safe stack, wait until you see what it takes to implement
    a thread-safe list later in this chapter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以跟踪哪些元素是*真实*的，哪些是*空洞*的，但这变得越来越复杂（以线程安全的方式进行将需要额外的同步，这将降低性能）。此外，留下许多未使用的数组槽会浪费内存。我们可以尝试重用*空洞*来存放推送到堆栈上的新元素，但在这一点上，元素不再按顺序存储，原子顶部计数不再起作用，整个结构开始变得像一个列表。顺便说一句，如果你认为列表是实现线程安全堆栈的好方法，等到你看到本章后面实现线程安全列表需要付出的努力时再说吧。
- en: 'At this point in our design, we must pause the deep dive into the implementation
    details and again review the more general approach to the problem. There are two
    steps that we must do: generalize the conclusions from our deeper understanding
    of the details of the stack implementations and do some performance estimates
    to get a general idea about what solutions are likely to yield performance improvements.
    We will start with the latter.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设计中，我们必须暂停对实现细节的深入探讨，并再次审视问题的更一般方法。我们必须做两步：从我们对堆栈实现细节的更深入理解中得出结论，并进行一些性能估算，以对可能产生性能改进的解决方案有一个大致的了解。我们将从后者开始。
- en: Performance estimates for synchronization schemes
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步方案的性能估算
- en: Our first attempt at a very simple stack implementation without a lock yielded
    some interesting solutions for special cases but no general solution. Before we
    spend much more time building a complex design, we should try to estimate how
    likely is it that it is going to be more efficient than the simple lock-based
    one.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次尝试了一个非常简单的堆栈实现，没有锁定，为特殊情况提供了一些有趣的解决方案，但没有一般解决方案。在我们花费更多时间构建复杂设计之前，我们应该尝试估计它比简单基于锁的解决方案更有效的可能性有多大。
- en: 'Of course, this may seem like circular reasoning: in order to estimate the
    performance, we must first have something to estimate. But we don''t want to do
    the complex design without at least some assurances that the effort will pay off,
    the assurances that require a performance estimate.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这可能看起来像循环推理：为了估计性能，我们必须首先有一些东西来估计。但我们不希望在至少有一些保证努力会有所回报的情况下进行复杂的设计，这些保证需要性能估计。
- en: 'Fortunately, we can fall back on the general observations we learned earlier:
    the performance of concurrent data structures depends largely on how many shared
    variables are accessed concurrently. Let''s assume that we can come up with a
    clever way to implement the stack with a single atomic counter. It is reasonable
    to assume that every push and pop will have to do at least one atomic increment
    or decrement of this counter (unless we are doing batch operations, but we already
    know that they are faster). We can get a reasonable performance estimate if we
    make a benchmark that combines push and pop on the single-threaded stack with
    an atomic operation on a shared atomic counter. There is no synchronization going
    on, so we have to use a separate stack for every thread to avoid race conditions:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以回到我们之前学到的一般观察：并发数据结构的性能在很大程度上取决于有多少共享变量同时访问。让我们假设我们可以想出一个巧妙的方法来使用单个原子计数器实现堆栈。假设每次推送和弹出都至少要对这个计数器进行一次原子递增或递减（除非我们正在进行批量操作，但我们已经知道它们更快）。如果我们进行一个基准测试，将单线程堆栈上的推送和弹出与共享原子计数器上的原子操作相结合，我们可以得到一个合理的性能估计。由于没有同步进行，因此我们必须为每个线程使用一个单独的堆栈，以避免竞争条件：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, `st_stack` is a stack wrapper that presents the same interface as our
    lock-based `mt_stack` but without any locks. The real implementation is going
    to be somewhat slower because the stack top is also shared between threads, but
    this will give us an estimate from above: it is highly unlikely that any implementation
    that is actually thread-safe will outperform this artificial benchmark. What do
    we compare the results to? The benchmark of the lock-based stack in *Figure 7.3*
    shows the performance of the lock-based stack to be between 30M push/pop operations
    per second on one thread and 3.1M on 8 threads. We also know the baseline performance
    of the stack without any locks to be about 485M operations per second (*Figure
    7.4*). On the same machine, our performance estimate with a single atomic counter
    yields these results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`st_stack`是一个堆栈包装器，它提供与我们基于锁的`mt_stack`相同的接口，但没有任何锁。实际实现会稍慢一些，因为堆栈顶部也在线程之间共享，但这将给我们一个从上面估计出来的结果：实际上是线程安全的任何实现都不太可能胜过这个人工基准测试。我们将结果与什么进行比较？*图7.3*中基于锁的堆栈的基准测试显示，在一个线程上每秒30M次推送/弹出操作，8个线程上为3.1M次。我们还知道没有任何锁的堆栈的基准性能约为每秒485M次操作（*图7.4*）。在同一台机器上，我们使用单个原子计数器进行的性能估计得出这些结果：
- en: '![Figure 7.10 – Performance estimate of a hypothetical stack with a single
    atomic counter'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.10 - 具有单个原子计数器的假设堆栈的性能估计'
- en: '](img/Figure_7.10_B16229.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.10_B16229.jpg)'
- en: Figure 7.10 – Performance estimate of a hypothetical stack with a single atomic
    counter
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 - 具有单个原子计数器的假设堆栈的性能估计
- en: 'The result seems like a mixed bag: even under optimal conditions, our stack
    is not going to scale. Again, this is primarily because we are testing a stack
    of small elements; if the elements were large and expensive to copy, we would
    see scaling because multiple threads can copy data at the same time. But the earlier
    observation stands: if copying data becomes so expensive that we need many threads
    to do it, we are better off using a stack of pointers and not copying any data
    at all.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来有点复杂：即使在最佳条件下，我们的堆栈也无法扩展。这主要是因为我们正在测试一个小元素的堆栈；如果元素很大且复制成本很高，我们会看到扩展，因为多个线程可以同时复制数据。但前面的观察仍然成立：如果复制数据变得如此昂贵，以至于我们需要许多线程来执行它，我们最好使用指针堆栈，根本不复制任何数据。
- en: 'On the other hand, the atomic counter is much faster than the mutex-based stack.
    Of course, this is an estimate from above, but it suggests that a lock-free stack
    has some possibilities. However, so does the lock-based stack: there are more
    efficient locks than `std::mutex` when we need to lock very short critical sections.
    We had already seen one such lock in [*Chapter 6*](B16229_06_Epub_AM.xhtml#_idTextAnchor103),
    *Concurrency and Performance*, when we implemented a spinlock. If we use this
    spinlock in our lock-based stack, then, instead of *Figure 7.2*, we get these
    results:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，原子计数器比基于互斥体的堆栈快得多。当然，这只是一个从上面估计出来的结果，但它表明无锁堆栈有一些可能性。然而，基于锁的堆栈也有：当我们需要锁定非常短的临界区时，有比`std::mutex`更有效的锁。在[*第6章*](B16229_06_Epub_AM.xhtml#_idTextAnchor103)中我们已经看到了这样一种锁，*并发和性能*，当我们实现了自旋锁。如果我们在基于锁的堆栈中使用这个自旋锁，那么，我们得到的结果不是*图7.2*，而是这些结果：
- en: '![Figure 7.11 – Performance of the spinlock-based stack'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.11 - 基于自旋锁的堆栈的性能'
- en: '](img/Figure_7.11_B16229.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.11_B16229.jpg)'
- en: Figure 7.11 – Performance of the spinlock-based stack
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 - 基于自旋锁的堆栈的性能
- en: 'Comparing this result with *Figure 7.10* paints a very depressing picture:
    we are not going to come up with a lock-free design that can outperform a simple
    spinlock. The reason that the spinlock can outperform an atomic increment in some
    cases has to do with the relative performance of different atomic instructions
    on this particular hardware; we should not read too much into it.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个结果与*图7.10*进行比较，得出一个非常沮丧的结论：我们不可能设计出一个无锁设计，它能胜过一个简单的自旋锁。自旋锁之所以能在某些情况下胜过原子递增，是因为在这个特定硬件上不同原子指令的相对性能；我们不应该对此过分解读。
- en: 'We could try to do the same estimate with an atomic exchange or compare-and-swap
    instead of the atomic increment. As you learn more about designing thread-safe
    data structures, you will get a sense of which synchronization protocol is likely
    to be useful and what operations should go into the estimate. Also, if you work
    with particular hardware, you should run simple benchmarks to determine which
    operations are more efficient on it. All results so far were obtained on X86-based
    hardware. If we run the same estimates on a large ARM-based server designed specifically
    for HPC applications, we get a very different outcome. The benchmark of a lock-based
    stack yields these results:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用原子交换或比较和交换来进行相同的估计，而不是原子增量。当您了解更多关于设计线程安全数据结构的知识时，您将对哪种同步协议可能有用以及哪些操作应该包括在估计中有所了解。此外，如果您使用特定的硬件，您应该运行简单的基准测试来确定哪些操作在其上更有效。到目前为止，所有结果都是在基于X86的硬件上获得的。如果我们在专门设计用于HPC应用的大型ARM服务器上运行相同的估计，我们将得到一个非常不同的结果。基于锁的栈的基准测试产生了这些结果：
- en: '![Figure 7.12 – Performance of the lock-based stack on an ARM HPC system'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.12 - 在ARM HPC系统上基于锁的栈的性能'
- en: '](img/Figure_7.12_B16229.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.12_B16229.jpg)'
- en: Figure 7.12 – Performance of the lock-based stack on an ARM HPC system
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 - 在ARM HPC系统上基于锁的栈的性能
- en: The ARM systems typically have a much larger number of cores than X86 systems,
    while the performance of a single core is lower. This particular system has 160
    cores on two physical processors, and the performance of the lock drops significantly
    when the program runs on both CPUs. The estimate for the upper limit of the lock-free
    stack performance should be done with a compare-and-swap instruction instead of
    the atomic increment (the latter is particularly inefficient on these processors).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ARM系统通常比X86系统具有更多的核心，而单个核心的性能较低。这个特定系统有两个物理处理器上的160个核心，当程序在两个CPU上运行时，锁的性能显著下降。对无锁栈性能的上限估计应该使用比原子增量更有效的比较和交换指令（后者在这些处理器上特别低效）。
- en: '![Figure 7.13 – Performance estimate for a hypothetical stack with a single
    CAS operation (ARM processors)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.13 - 具有单个CAS操作的假设栈的性能估计（ARM处理器）'
- en: '](img/Figure_7.13_B16229.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.13_B16229.jpg)'
- en: Figure 7.13 – Performance estimate for a hypothetical stack with a single CAS
    operation (ARM processors)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 - 具有单个CAS操作的假设栈的性能估计（ARM处理器）
- en: 'Based on the estimates in *Figure 7.13*, there is a chance that, for a large
    number of threads, we can come up with something better than a simple lock-based
    stack. We are going to continue with our efforts to develop a lock-free stack.
    There are two reasons for it: first of all, this effort is ultimately going to
    pay off on some hardware. Second, the basic elements of this design will be seen
    later in many other data structures, and the stack offers us a simple test case
    for learning about them.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于*图7.13*中的估计，对于大量的线程，我们有可能会得到比基于简单锁的栈更好的东西。我们将继续努力开发无锁栈。有两个原因：首先，这一努力最终将在某些硬件上得到回报。其次，这种设计的基本元素将在以后的许多其他数据结构中看到，而栈为我们提供了一个简单的测试案例来学习它们。
- en: Lock-free stack
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无锁栈
- en: 'Now that we have decided to try and outperform a simple lock-based implementation,
    we need to consider the lessons we have learned from our exploration of the push
    and pop operations by themselves. Each operation is very simple by itself, but
    the interaction of the two is what creates complexity. This is a very common situation:
    it is much harder to correctly synchronize producer and consumer operations running
    on multiple threads than it is to handle only producers or only consumers. Remember
    this when designing your own data structures: if your application allows for any
    kind of limitation on the operations you need to support, such as producers and
    consumers are separate in time, or there is a single producer (or consumer) thread,
    you can almost certainly design a faster data structure for these limited operations.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们决定尝试超越简单的基于锁的实现，我们需要考虑我们从对推入和弹出操作的探索中学到的教训。每个操作本身非常简单，但两者的交互才会产生复杂性。这是一个非常常见的情况：在多个线程上正确同步生产者和消费者操作要比仅处理生产者或仅处理消费者要困难得多。在设计自己的数据结构时请记住这一点：如果您的应用程序允许对您需要支持的操作进行任何形式的限制，比如生产者和消费者在时间上是分开的，或者只有一个生产者（或消费者）线程，那么您几乎可以肯定地为这些有限操作设计一个更快的数据结构。
- en: 'Assuming that we need a fully generic stack, the essence of the problem of
    the producer-consumer interaction can be understood on a very simple example.
    Again, we assume that the stack is implemented on top of an array or an array-like
    container, and the elements are stored consecutively. Let''s say that we have
    `N` elements currently on the stack. The producer thread P is executing the push
    operation, and the consumer thread C is executing the pop operation at the same
    time. What should be the outcome? While it is tempting to try to come up with
    a wait-free design (like we did for only consumers or only producers), any design
    that allows both threads to proceed without waiting is going to break our fundamental
    assumption about how the elements are stored: the thread C has to either wait
    for the thread P to complete the push or return the current top element, `N`.
    Similarly, the thread P has to either wait for the thread C to complete or construct
    a new element in the slot `N+1`. If neither thread waits, the result is a *hole*
    in the array: the last element has the index `N+1`, but there is nothing stored
    in the slot `N`, so we must somehow skip it when we pop data from the stack.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要一个完全通用的堆栈，生产者-消费者交互的问题的本质可以通过一个非常简单的例子来理解。同样，我们假设堆栈是在数组或类似数组的容器之上实现的，并且元素是连续存储的。假设我们当前有`N`个元素在堆栈上。生产者线程P正在执行推送操作，消费者线程C同时正在执行弹出操作。结果应该是什么？虽然诱人的是尝试设计一个无等待的设计（就像我们为仅消费者或仅生产者所做的那样），但是任何允许两个线程在不等待的情况下继续进行的设计都将破坏我们关于元素存储方式的基本假设：线程C必须等待线程P完成推送或返回当前顶部元素`N`。同样，线程P必须等待线程C完成或在槽`N+1`中构造一个新元素。如果两个线程都不等待，结果就是数组中的一个*空洞*：最后一个元素的索引为`N+1`，但在槽`N`中没有存储任何东西，因此我们在从堆栈中弹出数据时必须以某种方式跳过它。
- en: It looks like we have to give up the idea of the wait-free stack implementation
    and make one of the threads wait for the other one to complete its operation.
    We also have to deal with the possibility of the empty stack when the top index
    is zero and a consumer thread attempts to further decrement it. A similar problem
    occurs at the upper bound of the array when the top index points to the last element
    and a producer thread needs another slot.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们必须放弃无等待堆栈实现的想法，并让其中一个线程等待另一个线程完成其操作。当顶部索引为零且消费者线程尝试进一步减少它时，我们还必须处理空堆栈的可能性。当顶部索引指向最后一个元素且生产者线程需要另一个槽时，也会出现类似的问题。
- en: 'Both of these problems require a bounded atomic increment operation: perform
    the increment (or decrement) unless the value equals the specified bound. There
    is no ready-made atomic operation for this in C++ (or on any mainstream hardware
    available today), but we can implement it using **compare-and-swap** (**CAS**)
    as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题都需要有界的原子递增操作：执行递增（或递减），除非值等于指定的边界。在C++中没有现成的原子操作（或者在当今任何主流硬件上都没有），但我们可以使用**比较和交换**（**CAS**）来实现它，如下所示：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is a typical example of how CAS operation is used to implement a complex
    lock-free atomic operation:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是CAS操作用于实现复杂的无锁原子操作的典型示例：
- en: Read the current value of the variable.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取变量的当前值。
- en: Check the necessary conditions. In our case, we verify that the increment would
    not give us the value outside of the specified bounds `[0, maxn)`. If the bounded
    increment fails, we signal it to the caller by returning `-1` (this is an arbitrary
    choice; usually, there is a specific action to be performed for the out-of-bounds
    case).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查必要的条件。在我们的情况下，我们验证递增不会给我们带来指定边界`[0，maxn)`之外的值。如果有界递增失败，我们通过返回`-1`向调用者发出信号（这是一个任意选择；通常，对于超出边界的情况，有特定的操作要执行）。
- en: Atomically replace the value with the desired result if the current value is
    still equal to what we read earlier.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前值仍然等于我们之前读取的值，则用所需的结果原子替换该值。
- en: If *step 3* failed, the current value has been updated, check it again, and
    repeat *steps 3* and *4* until we succeed.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*步骤3*失败，当前值已被更新，再次检查它，并重复*步骤3*和*4*，直到成功。
- en: 'While this may seem to be a kind of lock, there is a fundamental difference:
    the only way the CAS comparison can fail on one thread is if it succeeded (and
    the atomic variable was incremented) on another thread, so any time there is a
    contention for the shared resource, at least one thread is guaranteed to make
    forward progress.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可能看起来像是一种锁，但有一个根本的区别：CAS比较在一个线程上失败的唯一方式是如果它在另一个线程上成功（并且原子变量被递增），所以每当共享资源存在争用时，至少一个线程保证能够取得进展。
- en: 'There is one more important observation that often makes all the difference
    between a scalable implementation and a very inefficient one. The CAS loop, as
    written, is very hostile to the scheduling algorithms of most modern operating
    systems: the thread that loops unsuccessfully also consumes more CPU time and
    will be given higher priority. This is the exact opposite of what we want: we
    want the thread that is currently doing the useful work to run faster. The solution
    is for a thread to yield the scheduler after a few unsuccessful CAS attempts.
    This is accomplished by a system call that is OS-dependent, but C++ has a system-independent
    API via the call to `std::this_thread::yield()`. On Linux, usually one can get
    better performance by calling the `nanosleep()` function to sleep for the minimum
    possible time (1 nanosecond) every few iterations of the loop:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个重要的观察结果，通常是实现可扩展性和非常低效实现之间的关键区别。如所写的CAS循环对大多数现代操作系统的调度算法非常不友好：循环失败的线程还会消耗更多的CPU时间，并且会被赋予更高的优先级。这与我们想要的正好相反：我们希望当前正在执行有用工作的线程运行得更快。解决方案是在几次不成功的CAS尝试后让线程让出调度器。这可以通过一个依赖于操作系统的系统调用来实现，但C++通过调用`std::this_thread::yield()`具有一个与系统无关的API。在Linux上，通常可以通过调用`nanosleep()`函数来睡眠最短可能的时间（1纳秒）来获得更好的性能，每次循环迭代都这样做：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The same approach can be used to implement much more complex atomic transactions,
    such as stack push and pop operations. But first, we have to figure out what atomic
    variables are needed. For the producer threads, we need the index of the first
    free slot in the array. For the consumer threads, we need the index of the last
    fully constructed element. This is all the information we need about the current
    state of the stack, assuming we do not allow "*holes*" in the array:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的方法可以用来实现更复杂的原子事务，比如栈的推送和弹出操作。但首先，我们必须弄清楚需要哪些原子变量。对于生产者线程，我们需要数组中第一个空闲插槽的索引。对于消费者线程，我们需要最后一个完全构造的元素的索引。这是我们关于栈当前状态的所有信息，假设我们不允许数组中的“空洞”：
- en: '![Figure 7.14 – Lock-free stack: c_ is the index of the last fully constructed
    element, and p_ is the index of the first free slot in the array'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.14 – 无锁栈：`c_`是最后一个完全构造的元素的索引，`p_`是数组中第一个空闲插槽的索引'
- en: '](img/Figure_7.14_B16229.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.14_B16229.jpg)'
- en: 'Figure 7.14 – Lock-free stack: `c_` is the index of the last fully constructed
    element, and `p_` is the index of the first free slot in the array'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 无锁栈：`c_`是最后一个完全构造的元素的索引，`p_`是数组中第一个空闲插槽的索引
- en: 'First of all, neither push nor pop can proceed if the two indices are currently
    not equal: different counts imply that either a new element is being constructed
    or the current top element is being copied out. Any stack modification in this
    state may lead to the creation of *holes* in the array.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果两个索引当前不相等，那么推送和弹出都无法进行：不同的计数意味着要么正在构造新元素，要么正在复制当前顶部元素。在这种状态下对栈进行修改可能导致数组中的*空洞*的创建。
- en: 'If the two indices are equal, then we can proceed. To do the push, we need
    to atomically increment the producer index `p_` (bounded by the current capacity
    of the array). Then we can construct the new element in the slot we just reserved
    (indexed by the old value of `p_`). Then we increment the consumer index `c_`
    to indicate that the new element is available to the consumer threads. Note that
    another producer thread could grab the next slot even before the construction
    is completed, but we would have to wait until all new elements are constructed
    before we allow any consumer thread to pop an element. Such an implementation
    is possible, but it is more complex, and it tends to favor the currently executed
    operation: if a push is currently in progress, a pop has to wait, but another
    push can proceed without delay. The result is likely to be a *swarm* of push operations
    executing while all consumer threads are waiting (the effect is similar if a pop
    operation is in progress; it favors another pop).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个索引相等，那么我们可以继续。要进行推送，我们需要原子地增加生产者索引`p_`（受数组当前容量的限制）。然后我们可以在刚刚保留的插槽中构造新元素（由旧值`p_`索引）。然后我们增加消费者索引`c_`，表示新元素已经可供消费者线程使用。请注意，另一个生产者线程甚至可以在构造完成之前抢占下一个插槽，但在允许任何消费者线程弹出元素之前，我们必须等待所有新元素都被构造。这样的实现是可能的，但它更加复杂，而且倾向于当前执行的操作：如果推送当前正在进行，弹出必须等待，但另一个推送可以立即进行。结果很可能是一堆推送操作在执行，而所有消费者线程都在等待（如果弹出操作正在进行，效果类似；它会倾向于另一个弹出）。
- en: The pop is implemented similarly, only we first decrement the consumer index
    `c_` to reserve the top slot, and then decrement `p_` after the object is copied
    or moved from the stack.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出的实现方式类似，只是我们首先将消费者索引`c_`减少到保留顶部插槽，然后在从栈中复制或移动对象之后再减少`p_`。
- en: 'There is just one more trick we have to learn, and that is how to manipulate
    both counts atomically. For example, we said earlier that a thread has to wait
    for the two indices to become equal. How can this be accomplished? If we read
    one index atomically and then the other index, also atomically, there is a chance
    that the first index has changed since we read it. We have to read both indices
    in a single atomic operation. The same is true for other operations on the indices.
    C++ allows us to declare an atomic struct of two integers; however, we must be
    careful: very few hardware platforms have a *double CAS* instruction that operates
    on two long integers atomically, and even then, it is usually very slow. The better
    solution is to pack both values into a single 64-bit word (on a 64-bit processor).
    The hardware atomic instructions such as load or compare-and-swap do not really
    care how you are going to interpret the data they read or write: they just copy
    and compare 64-bit words. You can later treat these bits as a long or a double
    or a pair of ints (the atomic increment is, of course, different, which is why
    you cannot use it on a double value).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要学习一个技巧，那就是如何原子地操作这两个计数。例如，我们之前说过，线程必须等待两个索引变得相等。这怎么实现呢？如果我们原子地读取一个索引，然后再原子地读取另一个索引，那么第一个索引自从我们读取它以来可能已经发生了变化。我们必须在一个原子操作中读取两个索引。对于索引的其他操作也是如此。C++允许我们声明一个包含两个整数的原子结构；但是，我们必须小心：很少有硬件平台有一个*双CAS*指令，可以原子地操作两个长整数，即使有，它通常也非常慢。更好的解决方案是将两个值打包到一个64位字中（在64位处理器上）。硬件原子指令（如加载或比较和交换）实际上并不关心你将如何解释它们读取或写入的数据：它们只是复制和比较64位字。你以后可以将这些位视为长整数、双精度浮点数或一对整数（原子增量当然是不同的，这就是为什么你不能在双精度值上使用它）。
- en: 'Now, all that is left is to convert the preceding algorithm into code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要将前面的算法转换成代码：
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The two indices are 32-bit integers packed into a 64-bit atomic value. The
    method `equal()` may look strange, but its purpose will become evident in a moment.
    It returns true if the two indices are equal; otherwise, it updates the stored
    index values from the specified atomic variable. This follows the CAS pattern
    we have seen earlier: if the desired condition is not met, read the atomic variable
    again.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个索引是打包成64位原子值的32位整数。`equal()`方法可能看起来很奇怪，但它的目的很快就会变得明显。如果两个索引相等，则返回true；否则，它会从指定的原子变量中更新存储的索引值。这遵循了我们之前看到的CAS模式：如果条件不满足，再次读取原子变量。
- en: 'Note that we can no longer build our thread-safe stack on top of the STL stack:
    the container itself is shared between threads, and the `push()` and `pop()` operations
    on it are not thread-safe without locking even if the container is not growing.
    For simplicity, in our example, we used a deque that was initialized with a *large
    enough* number of default-constructed elements. As long as we don''t call any
    container member functions, we can operate on different elements of the container
    from different threads independently. Remember that this is just a shortcut to
    avoid dealing with memory management and thread safety at the same time: in any
    practical implementation, you don''t want to default-construct all the elements
    upfront (and the element type may not even have a default constructor). Often,
    high-performance concurrent software systems have their own custom memory allocators
    anyway. Otherwise, you can also use an STL container of a dummy type of the same
    size and alignment as the stack element type, but with a simple constructor and
    destructor (the implementation is simple enough and is left as an exercise to
    the reader).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不能再在STL堆栈的基础上构建我们的线程安全堆栈：容器本身在线程之间是共享的，即使容器不再增长，对其进行`push()`和`pop()`操作也不是线程安全的。为简单起见，在我们的示例中，我们使用了一个deque，它初始化了足够大数量的默认构造元素。只要我们不调用任何容器成员函数，我们就可以独立地在不同的线程中操作容器的不同元素。请记住，这只是一个快捷方式，可以避免同时处理内存管理和线程安全：在任何实际实现中，您不希望预先默认构造所有元素（而且元素类型甚至可能没有默认构造函数）。通常，高性能的并发软件系统都有自己的自定义内存分配器。否则，您也可以使用一个与堆栈元素类型大小和对齐方式相同的虚拟类型的STL容器，但具有简单的构造函数和析构函数（实现足够简单，留给读者作为练习）。
- en: 'The push operation implements the algorithm we discussed earlier: wait for
    the indices to become equal, advance the producer index `p_`, construct the new
    object, and advance the consumer index `c_` when done:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 推送操作实现了我们之前讨论的算法：等待索引变得相等，推进生产者索引`p_`，构造新对象，完成后推进消费者索引`c_`：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The last CAS operation should never fail unless there is a bug in our code:
    once the calling thread successfully advanced `p_`, no other thread can change
    either value until the same thread advanced `c_` to match (as we already discussed,
    there is an inefficiency in that, but fixing it comes at the cost of much higher
    complexity). Also, note that, for brevity, we omitted the call to `nanosleep()`
    or `yield()` inside the loop, but it is essential in any practical implementation.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们的代码中有错误，否则最后的CAS操作不应该失败：一旦调用线程成功推进了`p_`，没有其他线程可以改变任何一个值，直到相同的线程推进了`c_`以匹配（正如我们已经讨论过的，这里存在一个低效性，但修复它会带来更高的复杂性成本）。另外，请注意，为了简洁起见，我们省略了循环内的`nanosleep()`或`yield()`调用，但在任何实际实现中都是必不可少的。
- en: 'The pop operation is similar, only it first decrements the consumer index `c_`
    and then, when it is done removing the top element from the stack, decrements
    `p_` to match `c_`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出操作类似，只是首先减少消费者索引`c_`，然后在从堆栈中移除顶部元素时，减少`p_`以匹配`c_`：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, the last compare-and-swap should not fail if the program is correct.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果程序正确，最后的比较和交换操作不应该失败。
- en: 'The lock-free stack is one of the simplest lock-free data structures possible,
    and it is already fairly complex. The testing required to validate that our implementation
    is correct is not straightforward: in addition to all the single-threaded unit
    tests, we have to validate that there are no race conditions. This task is made
    much easier by the sanitizer tools such as the **Thread Sanitizer** (**TSAN**)
    available in recent GCC and CLANG compilers. The advantage of these sanitizers
    is that they detect potential data races, not just the data races that actually
    happen during the test (in a small test, the chances to observe two threads accessing
    the same memory incorrectly at the same time are rather slim).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 无锁堆栈是可能的最简单的无锁数据结构之一，而且它已经相当复杂。验证我们的实现是否正确所需的测试并不简单：除了所有单线程单元测试之外，我们还必须验证是否存在竞争条件。这项任务得到了最近GCC和CLANG编译器中可用的**线程检测器**（**TSAN**）等消毒工具的大大简化。这些消毒工具的优势在于它们可以检测潜在的数据竞争，而不仅仅是在测试期间实际发生的数据竞争（在小型测试中，观察到两个线程同时不正确地访问相同内存的机会相当渺茫）。
- en: 'After all our effort, what is the performance of the lock-free stack? As expected,
    on X86 processors, it does not outperform the spinlock-based version:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 经过我们所有的努力，无锁堆栈的性能如何？如预期的那样，在X86处理器上，它并没有超越基于自旋锁的版本：
- en: '![Figure 7.15 – Performance of the lock-free stack on X86 CPU (compare with
    Figure 7.11)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.15 - X86 CPU上无锁堆栈的性能（与图7.11进行比较）'
- en: '](img/Figure_7.15_B16229.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.15_B16229.jpg)'
- en: Figure 7.15 – Performance of the lock-free stack on X86 CPU (compare with Figure
    7.11)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 - X86 CPU上无锁堆栈的性能（与图7.11进行比较）
- en: 'For comparison, the spinlock-guarded stack can execute about 70M operations
    per second on the same machine. This is consistent with the expectations we had
    after the performance estimates in the previous section. The same estimates, however,
    suggested that the lock-free stack may be superior on ARM processors. The benchmark
    confirms that our efforts were not wasted:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 作为比较，受自旋锁保护的堆栈可以在同一台机器上每秒执行约70M次操作。这与我们在上一节性能估计后的预期一致。然而，相同的估计表明，无锁堆栈在ARM处理器上可能更优秀。基准测试证实了我们的努力没有白费：
- en: '![Figure 7.16 – Performance of the lock-free stack on ARM CPU (compare with
    Figure 7.12)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.16 - ARM CPU上无锁堆栈的性能（与图7.12进行比较）'
- en: '](img/Figure_7.16_B16229.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.16_B16229.jpg)'
- en: Figure 7.16 – Performance of the lock-free stack on ARM CPU (compare with Figure
    7.12)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 - ARM CPU上无锁堆栈的性能（与图7.12进行比较）
- en: While the single-threaded performance of the lock-based stack is superior, the
    lock-free stack is much faster if the number of threads is large. The advantage
    of the lock-free stack becomes even greater if the benchmark includes a large
    fraction of `top()` calls (that is, many threads read the top element before one
    thread pops it) or if the producer and consumer threads are distinct (some threads
    call only `push()`, while other threads call only `pop()`).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于锁的栈的单线程性能优越，但是如果线程数量很大，无锁栈的速度要快得多。如果基准测试包括大量的`top()`调用（即许多线程在一个线程弹出之前读取顶部元素）或者生产者和消费者线程是不同的（一些线程只调用`push()`，而其他线程只调用`pop()`），无锁栈的优势甚至更大。
- en: 'To conclude this section, we have explored the different implementations of
    a thread-safe stack data structure. To understand what is required for thread
    safety, we had to analyze each operation separately, as well as the interaction
    of multiple concurrent operations. The following are the lessons that we learned:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分，我们已经探讨了线程安全栈数据结构的不同实现。为了理解线程安全所需的内容，我们必须分析每个操作，以及多个并发操作的交互。以下是我们学到的教训：
- en: With a good lock implementation, a lock-guarded stack offers reasonable performance
    and is much simpler than the alternatives.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用良好的锁实现，锁保护的栈提供了合理的性能，并且比其他选择更简单。
- en: 'Any application-specific knowledge about the limitations on the use of the
    data structure should be exploited to gain performance cheaply. This is not the
    place to develop generic solutions, quite the opposite: implement as few features
    as you can and try to gain performance advantages from the restrictions.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于数据结构使用限制的任何特定应用知识都应该被利用来廉价地获得性能。这不是开发通用解决方案的地方，恰恰相反：尽量实现尽可能少的功能，并尝试从限制中获得性能优势。
- en: A generic lock-free implementation is possible but, even for a data structure,
    that is as simple as a stack, it is quite complex. Sometimes, this complexity
    may even be justified.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通用的无锁实现是可能的，但即使对于像栈这样简单的数据结构，它也是相当复杂的。有时，这种复杂性甚至是合理的。
- en: 'So far, we have skirted the issue of memory management: it is hidden behind
    the vague *allocate more memory* when the stack runs out of capacity. We will
    need to come back to that later. But first, let''s explore more different data
    structures.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回避了内存管理的问题：当栈的容量用完时，它被隐藏在模糊的*分配更多内存*之后。我们需要稍后回到这个问题。但首先，让我们探索更多不同的数据结构。
- en: The thread-safe queue
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程安全队列
- en: 'The next data structure we are going to consider is the queue. It is again
    a very simple data structure, conceptually an array that is accessible from both
    ends: the data is added to the end of the array and removed from the beginning
    of it. There are some very important differences between the queue and the stack
    when it comes to implementation. There are also many similarities, and we will
    refer to the previous section frequently.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要考虑的数据结构是队列。它是一个非常简单的数据结构，概念上是一个可以从两端访问的数组：数据被添加到数组的末尾，并从开头移除。在实现方面，队列和栈之间有一些非常重要的区别。也有许多相似之处，我们将经常参考前一节。
- en: 'Just like the stack, the STL has a queue container, `std::queue`, and it has
    the exact same problem when it comes to concurrency: the interface for removing
    elements is not transactional, it requires three separate member function calls.
    If we wanted to use `std::queue` with a lock to create a thread-safe queue, we
    would have to wrap it just like we did with the stack:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 就像栈一样，STL有一个队列容器`std::queue`，在并发性方面存在相同的问题：删除元素的接口不是事务性的，它需要三个单独的成员函数调用。如果我们想要使用带锁的`std::queue`创建线程安全队列，我们将不得不像处理栈一样对其进行包装：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We decided to use the spinlock right away (a simple benchmark can confirm that
    it is again faster than a mutex). The `front()` method, if desired, can be implemented
    similarly to the `pop()` method, only without removing the front element. The
    basic benchmark again measured the time it takes to push an element onto the queue
    and pop it back. Using the same X86 machine we did in the last section, we can
    obtain these numbers:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定立即使用自旋锁（一个简单的基准测试可以证实它再次比互斥锁更快）。如果需要，`front()`方法可以类似于`pop()`方法实现，只是不移除前面的元素。基本基准测试再次测量将元素推送到队列并将其弹出所需的时间。使用与上一节相同的X86机器，我们可以得到以下数字：
- en: '![Figure 7.17 – Performance of a spinlock-guarded std::queue'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.17 - 自旋锁保护的std::queue的性能'
- en: '](img/Figure_7.17_B16229.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.17_B16229.jpg)'
- en: Figure 7.17 – Performance of a spinlock-guarded std::queue
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 - 自旋锁保护的std::queue的性能
- en: For comparison, on the same hardware, `std::queue` without any locks delivers
    about 280M items per second (an *item* is a push and a pop, so we measure how
    many elements we can send through the queue per second). So far, the picture is
    very similar to what we have seen earlier for the stack. To do better than the
    lock-guarded version, we have to try to come up with a lock-free implementation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 作为比较，在相同的硬件上，没有任何锁的`std::queue`每秒可以传递大约280M个项目（*项目*是推送和弹出，因此我们测量每秒可以通过队列发送多少元素）。到目前为止，这个情况与我们之前在栈中看到的非常相似。为了比锁保护的版本更好，我们必须尝试提出一个无锁实现。
- en: Lock-free queue
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无锁队列
- en: 'Before we dive into designing a lock-free queue, it is important to do a detailed
    analysis of each transaction, just like we did for the stack. Again, we will assume
    that the queue is built on top of an array or an array-like container (and we
    will defer the questions about what happens when the array is full). Pushing elements
    onto the queue looks just like it does for the stack:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入设计无锁队列之前，重要的是对每个事务进行详细分析，就像我们为栈所做的那样。同样，我们将假设队列是建立在数组或类似数组的容器之上的（并且我们将推迟关于数组满时会发生什么的问题）。将元素推送到队列看起来就像为栈做的那样：
- en: '![Figure 7.18 – Adding elements to the back of the queue (producer''s view)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.18 - 将元素添加到队列后面（生产者视图）'
- en: '](img/Figure_7.18_B16229.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.18_B16229.jpg)'
- en: Figure 7.18 – Adding elements to the back of the queue (producer's view)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 向队列后端添加元素（生产者视图）
- en: 'All we need is the index of the first empty slot in the array. Removing elements
    from the queue, however, is quite different from the same operation on the stack.
    You can see this in *Figure 7.19* (compare it with *Figure 7.9*):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所需要的只是数组中第一个空槽的索引。然而，从队列中移除元素与从栈中进行相同操作是完全不同的。您可以在*图7.19*中看到这一点（与*图7.9*进行比较）：
- en: '![Figure 7.19 – Removing elements from the front of the queue (consumer''s
    view)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.19 – 从队列前端移除元素（消费者视图）'
- en: '](img/Figure_7.19_B16229.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.19_B16229.jpg)'
- en: Figure 7.19 – Removing elements from the front of the queue (consumer's view)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 从队列前端移除元素（消费者视图）
- en: The elements are removed from the front of the queue, so we need the index of
    the first element that has not been removed yet (the current front of the queue),
    and that index is also advanced.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 元素从队列的前端移除，因此我们需要第一个尚未被移除的元素的索引（队列的当前前端），并且该索引也会被增加。
- en: 'Now we come to the crucial difference between the queue and the stack: in the
    stack, both producer and consumer operate on the same location: the top of the
    stack. We have seen the consequences of this: once the producer started to construct
    a new element at the top of the stack, the consumer has to wait for it to complete.
    The pop operation cannot return the last constructed element without leaving a
    *hole* in the array, and it can''t return the element being constructed until
    the construction is done.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到队列和栈之间的关键区别：在栈中，生产者和消费者都在同一位置操作：栈的顶部。我们已经看到了这样做的后果：一旦生产者开始在栈顶构造新元素，消费者就必须等待它完成。弹出操作不能返回最后构造的元素而不在数组中留下*空洞*，也不能在构造完成之前返回正在构造的元素。
- en: The situation is very different for the queue. As long as the queue is not empty,
    the producers and the consumers do not interact at all. The push operation does
    not need to know what the front index is, and the pop operation does not care
    where the back index is as long as it's somewhere ahead of the front. The producers
    and the consumers are not competing for access to the same memory location.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于队列，情况则大不相同。只要队列不为空，生产者和消费者根本不会相互交互。推送操作不需要知道前端索引在哪里，弹出操作也不关心后端索引在哪里，只要它在前端之前的某个位置。生产者和消费者不会竞争访问同一内存位置。
- en: Whenever we have the case that there are several different ways to access the
    data structure and they (mostly) do not interact with each other, the general
    suggestion is to first consider the scenario where these roles are assigned to
    different threads. The further simplification can be to start with the case of
    one thread of each kind; in our case, it means one producer thread and one consumer
    thread.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们有多种不同的方式来访问数据结构，并且它们（大多数情况下）不相互交互时，一般建议首先考虑这些角色分配给不同线程的情况。进一步简化可以从每种类型的一个线程开始；在我们的情况下，这意味着一个生产者线程和一个消费者线程。
- en: 'Since only the producer needs access to the back index, and there is only one
    producer thread, we don''t even need an atomic integer for this index. Similarly,
    the front index is just a regular integer. The only time the two threads interact
    with each other is when the queue becomes empty. For that, we need an atomic variable:
    the size of the queue. The producer constructs the new element in the first empty
    slot and advances the back index (in any order, there is only one producer thread).
    Then, it increments the size of the queue to reflect the fact that the queue now
    has one more element ready to be taken from it.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只有生产者需要访问后端索引，并且只有一个生产者线程，因此我们甚至不需要原子整数来表示此索引。同样，前端索引只是一个常规整数。这两个线程相互交互的唯一时间是队列变为空时。为此，我们需要一个原子变量：队列的大小。生产者在第一个空槽中构造新元素并增加后端索引（以任何顺序，只有一个生产者线程）。然后，它增加队列的大小，以反映队列现在有一个更多的元素可以从中取出。
- en: 'The consumer must operate in reverse order: first, check the size to make sure
    the queue is not empty. Then the consumer can take the first element from the
    queue and advance the front index. Of course, there is no guarantee that the size
    does not change between the time it is checked and the time the front element
    is accessed. But it does not cause any problems: there is only one consumer thread,
    and the producer thread can only increment the size.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者必须以相反的顺序操作：首先，检查大小以确保队列不为空。然后消费者可以从队列中取出第一个元素并增加前端索引。当然，在检查大小和访问前端元素之间大小可能会发生变化，但这不会造成任何问题：只有一个消费者线程，生产者线程只能增加大小。
- en: 'While exploring the stack, we deferred the issue of adding more memory to the
    array and assumed that we somehow know the maximum capacity of the stack and will
    not exceed it (we could also make the push operation fail if that capacity is
    exceeded). For the queue, the same assumption is not enough: as the elements are
    added and removed from the queue, both the front and the back indices advance
    and will eventually reach the end of the array. Of course, at this point, we have
    the first elements of the array unused, so the simplest solution is to treat the
    array as a circular buffer and use modulo arithmetic for array indices:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索栈时，我们推迟了向数组添加更多内存的问题，并假设我们以某种方式知道栈的最大容量，并且不会超过它（如果超过了，我们也可以使推送操作失败）。对于队列，同样的假设是不够的：因为元素被添加和移除，前端和后端索引都会前进，并最终到达数组的末尾。当然，在这一点上，数组的第一个元素是未使用的，因此最简单的解决方案是将数组视为循环缓冲区，并对数组索引使用模运算：
- en: '[PRE18]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This queue requires a special benchmark because of the constraints we accepted
    on its design: one producer thread and one consumer thread:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在设计上接受了的约束条件，这个队列需要一个特殊的基准：一个生产者线程和一个消费者线程：
- en: '[PRE19]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For comparison, we should benchmark our lock-guarded queue under the same conditions
    (performance of the locks is generally sensitive to the exact nature of the contention
    between threads). On the same X86 machine, the two queues perform at roughly the
    same throughput of 100M integer elements per second. On the ARM processor, the
    locks are relatively more expensive, in general, and our queue is no exception:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们应该在相同条件下对我们的锁保护队列进行基准测试（锁的性能通常对线程之间的竞争情况非常敏感）。在相同的X86机器上，两个队列的吞吐量大约为每秒100M个整数元素。在ARM处理器上，锁的成本通常更高，我们的队列也不例外：
- en: '![Figure 7.20 – Performance of a lock-based versus a lock-free queue of integers
    on ARM'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.20 - 在ARM上整数的基于锁和无锁队列的性能'
- en: '](img/Figure_7.20_B16229.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.20_B16229.jpg)'
- en: Figure 7.20 – Performance of a lock-based versus a lock-free queue of integers
    on ARM
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 - 在ARM上整数的基于锁和无锁队列的性能
- en: 'However, even on X86, our analysis is not yet complete. In the previous section,
    we mentioned that if the stack elements are large, copying them takes relatively
    longer than the thread synchronization (locking or atomic operations). We could
    not make much use of it because most of the time, one thread still had to wait
    for the other thread to complete the copy, so the alternative was suggested: a
    stack of pointers, with the actual data stored elsewhere. The downside is that
    we need another thread-safe container to store this data (although often, the
    program needs to store it somewhere anyway). This is still a viable suggestion
    for the queue, but now we have another alternative. As we have already mentioned,
    the producer and consumer threads in the queue do not wait for each other: their
    interaction ends after the size is checked. It stands to reason that, if the data
    elements are large, the lock-free queue will have an advantage because both threads
    can copy the data at the same time and the contention between the threads, or
    the time when two threads are competing for access to the same memory location
    (the lock or the atomic value), is much shorter. To do such a benchmark, we just
    need to create a queue of large objects, such as a struct with a large array in
    it. As expected, the lock-free queue now performs faster, even on the X86 hardware:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在X86上，我们的分析还没有完成。在前一节中，我们提到如果栈元素很大，复制它们所需的时间相对于线程同步（锁定或原子操作）要长。我们无法充分利用它，因为大多数情况下，一个线程仍然需要等待另一个线程完成复制，因此建议另一种方法：使用指针栈，实际数据存储在其他地方。缺点是我们需要另一个线程安全的容器来存储这些数据（尽管通常，程序无论如何都需要将其存储在某个地方）。这仍然是队列的一个可行建议，但现在我们有另一种选择。正如我们已经提到的，队列中的生产者和消费者线程不会互相等待：它们的交互在大小检查后就结束了。可以推断，如果数据元素很大，无锁队列将具有优势，因为两个线程可以同时复制数据，线程之间的竞争，或者两个线程争夺对同一内存位置的访问（锁或原子值）的时间要短得多。要进行这样的基准测试，我们只需要创建一个大对象的队列，比如一个包含大数组的结构体。正如预期的那样，即使在X86硬件上，无锁队列现在也表现得更快：
- en: '![Figure 7.21 – Performance of a lock-based versus a lock-free queue of large
    elements on X86'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.21 - 在X86上大型元素的基于锁和无锁队列的性能'
- en: '](img/Figure_7.21_B16229.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.21_B16229.jpg)'
- en: Figure 7.21 – Performance of a lock-based versus a lock-free queue of large
    elements on X86
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 - 在X86上大型元素的基于锁和无锁队列的性能
- en: 'Even with the restrictions we have imposed, this is a very useful data structure:
    this queue can be used for transferring data between a producer and a consumer
    thread when we know an upper bound on the number of elements we can enqueue or
    can handle the situation when the producer has to wait before pushing more data.
    The queue is very efficient; even more important for some applications is the
    fact that it has very low and predictable latency: the queue itself is not just
    lock-free but wait-free. One thread never has to wait for the other unless the
    queue is full. By the way, if the consumer has to do certain processing on each
    data element it takes from the queue and starts falling behind until the queue
    fills up, one common approach is to have the producer process the elements it
    could not enqueue. This serves to delay the producer thread until the consumer
    can catch up (this method is not suitable for every application since it can process
    data out of order, but quite often, it works).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们施加了限制的情况下，这仍然是一个非常有用的数据结构：当我们知道可以入队的元素数量的上限，或者可以处理生产者在推送更多数据之前必须等待的情况时，这个队列可以用于在生产者和消费者线程之间传输数据。这个队列非常高效；对于一些应用程序来说更重要的是，它具有非常低且可预测的延迟：队列本身不仅是无锁的，而且是无等待的。一个线程永远不必等待另一个线程，除非队列已满。顺便说一句，如果消费者必须对从队列中取出的每个数据元素进行某些处理，并且开始落后直到队列填满，一个常见的方法是让生产者处理它无法入队的元素。这有助于延迟生产者线程，直到消费者赶上（这种方法并不适用于每个应用程序，因为它可能会无序处理数据，但通常情况下是有效的）。
- en: 'The generalization of our queue for the case of many producer or consumer threads
    is going to make the implementation more complex. The simple wait-free algorithm
    based on atomic size no longer works even if we make the front and back indices
    atomic: if multiple consumer threads read a non-zero value of size, this is no
    longer sufficient for all of them to proceed. With multiple consumers, the size
    can decrease and become zero after it was checked by one thread and found to be
    non-zero (it just means that the other threads popped all remaining elements after
    the first thread tested the size, but before it tried to access the front of the
    queue).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的队列在有多个生产者或消费者线程的情况下的泛化将使实现更加复杂。基于原子大小的简单无等待算法即使我们将前后索引设为原子，也不再适用：如果多个消费者线程读取了一个非零大小的值，这对于所有这些线程来说已经不再足够让它们继续进行。对于多个消费者，大小可以在一个线程检查并发现非零值后减小并变为零（这只是意味着其他线程在第一个线程测试大小后，但在它尝试访问队列前弹出了所有剩余元素）。
- en: 'One general solution is to use the same technique we used for the stack: pack
    the front and back indices into a single 64-bit atomic word and access them both
    atomically using compare-and-swap. The implementation is similar to that of the
    stack; the reader who understood the code in the previous section is well-prepared
    to implement this queue. There are other lock-free queue solutions that can be
    found in the literature; this chapter should give you sufficient background to
    understand, compare, and benchmark these implementations.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的解决方案是使用我们为栈使用的相同技术：将前端和后端索引打包到一个64位原子字中，并使用比较和交换原子地访问它们两个。实现类似于栈的实现；在前一节理解了代码的读者已经准备好实现这个队列。在文献中还可以找到其他无锁队列解决方案；本章应该为您提供足够的背景来理解、比较和基准测试这些实现。
- en: 'Implementing a complex lock-free data structure correctly is a time-consuming
    project that requires skill and attention. It is good to have some performance
    estimates before the implementation is complete, so we can know whether the effort
    is likely to pay off. We have already seen one approach to benchmarking the code
    that does not yet exist: a simulated benchmark that combines the operations on
    a non-thread-safe data structure (local to each thread) with the operations on
    shared variables (locks or atomic data). The goal is to come up with a computationally
    equivalent code fragment that can be benchmarked; it is never going to be perfect,
    but if we have an idea for a lock-free queue with three atomic variables and a
    compare-and-swap operation on each one, and we discover that the estimated benchmark
    is several times slower than the spinlock-guarded queue, the work of implementing
    the real queue is unlikely to pay off.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个复杂的无锁数据结构是一个耗时的项目，需要技巧和注意力。在实现完成之前，最好能有一些性能估计，这样我们就可以知道努力是否有可能得到回报。我们已经看到了一种基准测试代码的方法，这个代码还不存在：一个模拟基准测试，结合了对非线程安全数据结构（每个线程本地的）的操作和对共享变量（锁或原子数据）的操作。目标是提出一个可以进行基准测试的计算等效代码片段；它永远不会完美，但是如果我们有一个关于一个具有三个原子变量和每个变量上的比较和交换操作的无锁队列的想法，并且我们发现估计的基准测试比自旋锁保护的队列慢几倍，那么实现真正的队列的工作可能不会得到回报。
- en: 'The second way to benchmark partially implemented code is to construct benchmarks
    that avoid certain corner cases that we have not yet implemented. For example,
    if you expect the queue to not be empty most of the time, and your initial implementation
    does not handle the case of the empty queue, you should benchmark that implementation
    and restrict the benchmark so the queue never gets empty. This benchmark will
    tell you if you are on the right track: it will show what performance you can
    expect in the typical case of the non-empty queue. We had actually taken this
    approach already when we deferred handling of the case when the stack or the queue
    runs out of memory. We simply assumed that it''s not going to happen very often
    and constructed the benchmark to avoid this case.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 部分实现代码的第二种基准测试方法是构建基准测试，避免我们尚未实现的某些边缘情况。例如，如果您期望队列大部分时间不为空，并且您的初始实现没有处理空队列的情况，那么您应该对该实现进行基准测试，并限制基准测试，使队列永远不会为空。这个基准测试将告诉您是否走在正确的轨道上：它将展示您在非空队列的典型情况下可以期望的性能。当我们推迟处理栈或队列耗尽内存的情况时，我们实际上已经采取了这种方法。我们简单地假设这种情况不会经常发生，并构建了基准测试来避免这种情况。
- en: There is yet another type of concurrent data structure implementation that can
    often be very efficient. We are going to learn about this technique next.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种并发数据结构实现类型，通常可以非常高效。我们接下来要学习这种技术。
- en: Non-sequentially consistent data structures
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非顺序一致的数据结构
- en: 'Let''s first revisit the simple question, *what is a queue?* Of course, we
    know what a queue is: it''s a data structure such that the element added first
    is also retrieved first. Conceptually, and in many implementations, this is guaranteed
    by the order in which the elements are added to the underlying array: we have
    an array of queued elements, new entries are added to the front, while the oldest
    ones are read from the back.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先重新审视一个简单的问题，*队列是什么？*当然，我们知道队列是什么：它是一种数据结构，使得首先添加的元素也是首先检索到的。在概念上和许多实现中，这是由元素添加到底层数组的顺序来保证的：我们有一个排队元素的数组，新条目添加到前面，而最老的条目从后面读取。
- en: 'But let''s examine closely if this definition still holds for a concurrent
    queue. The code that is executed when an element is read from the queue looks
    something like this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 但是让我们仔细检查一下这个定义是否仍然适用于并发队列。当从队列中读取一个元素时执行的代码看起来像这样：
- en: '[PRE20]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The return value may be wrapped in `std::optional` or passed by reference; it
    doesn't matter. The point is, the value is read from the queue, the back index
    is decremented, and the control returns to the caller. In a multi-threaded program,
    the thread can be preempted at any moment. It is entirely possible that if we
    have two threads, A and B, and thread A reads the oldest element from the queue,
    it is thread B that completes execution of `pop()` first and returns its value
    to the caller. Thus, if we enqueue two elements X and Y, in that order, and have
    multiple threads dequeue them and print their values, the program prints Y then
    X. The same kind of reordering can happen when multiple threads push elements
    onto the queue. The end result is that even if the queue itself maintains a strict
    order (if you were to pause the program and examine the array in memory, the elements
    are in the right order), the order of dequeued elements as observed by the rest
    of the program is not guaranteed to be exactly the order in which they were enqueued.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the order is not entirely random either: even in a concurrent program,
    a stack looks very different from a queue. The order of the data retrieved from
    a queue is approximately the order in which the values were added; significant
    rearrangements are rare (they happen when one thread is, for some reason, delayed
    for a significant time).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another very important property that is still preserved by our queue:
    **sequential consistency**. A sequentially consistent program produces the output
    that is identical to the output of a program where operations from all threads
    are executed one at a time (without any concurrency), and the order of the operations
    executed by any particular thread is not changed. In other words, the equivalent
    program takes the sequences of operations executed by all threads and interleaves
    them but does not reshuffle them.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential consistency is a convenient property to have: it is much easier
    to analyze the behavior of such programs. For example, in the case of the queue,
    we have the guarantee that if two elements X and Y were enqueued by thread A,
    X first, then Y, and they happen to be both dequeued by thread B, they will come
    out in the correct order. On the other hand, we can argue that, in practice, it
    doesn''t really matter: the two elements may be dequeued by two different threads,
    in which case they can appear in any order, so the program has to be able to handle
    it.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are willing to give up sequential consistency, this opens up a whole
    new approach to designing concurrent data structures. Let''s explore it on the
    example of a queue. The basic idea is this: instead of a single queue thread-safe
    queue, we can have several single-threaded sub-queues. Each thread must atomically
    acquire exclusive ownership of one of these sub-queues. The simplest way to implement
    this is with an array of atomic pointers to the sub-queues, as shown in *Figure
    7.22*. To acquire the ownership and, at the same time, prevent any other thread
    from getting access to the queue, we atomically exchange the sub-queue pointer
    with null.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Non-sequentially-consistent queue based on an array sub-queue
    accessed via atomic pointers'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.22_B16229.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.22 – Non-sequentially-consistent queue based on an array sub-queue
    accessed via atomic pointers
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: A thread that needs to access the queue must first acquire a sub-queue. We can
    start from any element of the pointer array; if it's null, that sub-queue is currently
    busy, and we try the next element, and so on until we reserve a sub-queue. At
    this point, there is only one thread operating on the sub-queue, so there is no
    need for thread safety (the sub-queue can even be `std::queue`). After the operation
    (push or pop) is completed, the thread returns the ownership of the sub-queue
    to the queue by atomically writing the sub-queue pointer back into the array.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The push operation must continue to try to reserve the sub-queue until it finds
    one (alternatively, we can allow the push to fail after a certain number of tries
    and signal the caller that the queue is too busy). The pop operation may reserve
    a sub-queue only to find that it's empty. In this case, it has to try to pop from
    another sub-queue (we can keep an atomic count of elements in the queue to optimize
    the fast return if the queue is empty).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, pop may fail on one thread and report that the queue is empty when
    in fact, it isn''t because another thread has pushed new data onto the queue.
    But this could happen with any concurrent queue: one thread checks the queue size,
    finds that the queue is empty, but before the control is returned to the caller,
    the queue becomes non-empty. Again, the sequential consistency puts some limits
    on what kind of inconsistencies can be observed by multiple threads, while our
    non-sequentially consistent queue makes the order of outgoing elements much less
    certain. Still, the order is maintained *on average*.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not the right data structure for every problem, but when the *mostly
    queue-like most of the time* order is acceptable, it can lead to significant performance
    improvements, especially in systems with many threads. Observe the scaling of
    the non-sequentially consistent queue on a large X86 server running many threads:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Performance of the non-sequentially-consistent queue'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.23_B16229.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.23 – Performance of the non-sequentially-consistent queue
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: In this benchmark, all threads do both push and pop operations, and the elements
    are fairly large (copying each element requires copying 1KB of data). For comparison,
    the spinlock-guarded `std::queue` delivers the same performance (about 170k elements
    per second) on a single thread but does not scale at all (the entire operation
    is locked), and the performance drops slowly (due to the overhead of locking)
    to about 130k elements per second for the maximum number of threads.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Of course, many other data structures can benefit from this approach if you're
    willing to embrace the chaos of the non-sequentially-consistent programs for the
    sake of performance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The last subject we need to cover when it comes to concurrent sequential containers
    such as stack and queue is how to handle the situation when they need more memory.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Memory management for concurrent data structures
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we persisted in pushing back on the issue of memory management and
    assumed that the initial memory allocation for the data structure would suffice,
    at least for lock-free data structures that do not make the entire operation single-threaded.
    The lock-guarded and the non-sequentially-consistent data structures we have seen
    throughout this chapter do not have this problem: under the lock or exclusive
    ownership, there is only one thread operating on the particular data structure,
    so the memory is allocated in the usual way.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: For a lock-free data structure, memory allocation is a significant challenge.
    It is usually a relatively long operation, especially if the data must be copied
    to the new location. Even though multiple threads may detect that the data structure
    ran out of memory, usually only one thread can add new memory (it is very hard
    to make that part multi-threaded as well), the remaining threads must wait. There
    is no good general solution to this problem, but we will present several recommendations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the best option is to avoid the problem altogether. In many situations,
    when a lock-free data structure is needed, it is possible to estimate its maximum
    capacity and preallocate the memory. For example, we may know the total number
    of data elements we are going to enqueue. Alternatively, it may be possible to
    push the problem back to the caller: instead of adding memory, we can tell the
    caller that the data structure is out of capacity; in some problems, this may
    be an acceptable trade-off for the performance of the lock-free data structure.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，最好的选择是完全避免问题。在许多情况下，当需要无锁数据结构时，可以估计其最大容量并预先分配内存。例如，我们可能知道要入队的数据元素的总数。或者，可能可以将问题推迟给调用者：而不是添加内存，我们可以告诉调用者数据结构已经达到容量上限；在某些问题中，这可能是无锁数据结构的性能的可接受折衷。
- en: If the memory needs to be added, it is highly desirable that adding memory should
    not require copying of the entire existing data structure. This implies that we
    can't simply allocate more memory and copy everything to the new location. Instead,
    we must store the data in memory blocks of a fixed size, the way `std::deque`
    does it. When more memory is required, another block is allocated, and there are
    usually a few pointers that need to be changed, but no data is copied.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要添加内存，非常希望添加内存不需要复制整个现有数据结构。这意味着我们不能简单地分配更多内存并将所有内容复制到新位置。相反，我们必须以固定大小的内存块存储数据，就像`std::deque`所做的那样。当需要更多内存时，将分配另一个块，并且通常有一些指针需要更改，但不会复制数据。
- en: In all cases where memory allocation is done, this must be an infrequent event.
    If this is not so, then we are almost certainly better off with a single-threaded
    data structure protected by a lock or temporary exclusive ownership. The performance
    of this rare event is not critical, and we can simply lock the entire data structure
    and have one thread do the memory allocation and all the necessary updates. The
    key requirement is to make the common execution path, the one where we do not
    need more memory, as fast as possible.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有进行内存分配的情况下，这必须是一个不经常发生的事件。如果不是这样，那么我们几乎肯定最好使用由锁或临时独占所有权保护的单线程数据结构。这种罕见事件的性能并不重要，我们可以简单地锁定整个数据结构，并让一个线程进行内存分配和所有必要的更新。关键要求是使常见的执行路径，即我们不需要更多内存的路径，尽可能快。
- en: 'The idea is very simple: we certainly do not want to acquire the memory lock
    on every thread every time, which would serialize the whole program. We also don''t
    need to do this: most of the time, we are not out of memory, and there is no need
    for this lock. So instead, we are going to check an atomic flag. The flag is set
    only if memory allocation is currently in progress, and all threads must wait:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常简单：我们肯定不希望每次都在每个线程上获取内存锁，这会使整个程序串行化。我们也不需要这样做：大多数情况下，我们并不缺内存，也不需要这个锁。因此，我们将检查一个原子标志。只有在内存分配当前正在进行时，标志才会被设置，所有线程都必须等待。
- en: '[PRE21]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The problem here is that multiple threads may detect the out-of-memory condition
    at the same time before one of them sets the wait flag; they would then all try
    to add more memory to the data structure. This usually creates a race (reallocating
    the underlying storage is rarely thread-safe). However, there is a simple solution
    known as the **double-checked locking**. It uses both a mutex (or another lock)
    and an atomic flag. If the flag is not set, all is well, and we can proceed as
    usual. If the flag is set, we must acquire the lock and check the flag again:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，多个线程可能在一个设置等待标志之前同时检测到内存不足的情况；然后它们都会尝试向数据结构添加更多内存。这通常会产生竞争（重新分配底层存储很少是线程安全的）。然而，有一个简单的解决方案，称为**双重检查锁定**。它使用互斥锁（或另一个锁）和原子标志。如果标志未设置，一切正常，我们可以像往常一样继续。如果标志已设置，我们必须获取锁并再次检查标志：
- en: '[PRE22]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first time, we check the out-of-memory condition without any locking. It
    is fast and, most of the time, we are not out of memory. The second time, we check
    it under the lock, where we have the guarantee that only one thread is executing
    at a time. Multiple threads may detect that we are out of memory; however, the
    first one to get the lock is the thread that handles this case. All remaining
    threads wait for the lock; when they acquire the lock, they do the second check
    (hence, double-checked locking) and discover that we are no longer out of memory.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次，我们在没有任何锁定的情况下检查内存不足的情况。这很快，而且大多数情况下，我们并不缺内存。第二次，在锁定状态下检查，我们保证只有一个线程在执行。多个线程可能会检测到我们内存不足；然而，第一个获得锁的线程是处理这种情况的线程。所有剩余的线程等待锁；当它们获得锁时，它们进行第二次检查（因此，双重检查锁定），并发现我们不再缺内存。
- en: 'This approach can be generalized to handle any special case that happens very
    infrequently but is much more difficult to implement in a lock-free manner than
    the rest of the code. In some cases, it may even be useful for situations such
    as the empty queue: as we have seen, the handling of multiple producers or multiple
    consumers would require a simple atomically incremented index if the two groups
    of threads never had to interact with each other. If, in a particular application,
    we have a guarantee that the queue rarely, if ever, becomes empty, we could favor
    an implementation that is very fast (wait-free) for the non-empty queue but falls
    back on a global lock if the queue might be empty.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以推广到处理任何特殊情况，这些情况发生得非常少，但是在无锁方式下实现起来比代码的其他部分要困难得多。在某些情况下，甚至可能对空队列等情况有用：正如我们所见，如果两组线程永远不必相互交互，那么处理多个生产者或多个消费者将需要一个简单的原子递增索引。如果在特定应用程序中，我们保证队列很少或几乎不会变为空，那么我们可以偏向于实现对非空队列非常快（无等待），但如果队列可能为空，则退回到全局锁的实现。
- en: We have covered the sequential data structures in enough detail now. It is time
    to study the nodal data structures next.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经详细介绍了顺序数据结构。现在是时候学习下一个节点数据结构了。
- en: The thread-safe list
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程安全的列表
- en: In the sequential data structures we have studied so far, the data is stored
    in an array (or at least a conceptual array made up of memory blocks). Now we
    will consider a very different type of data structure where the data is linked
    together by pointers. The simplest example is a list where each element is allocated
    separately, but everything we learn here applies to other nodal containers such
    as trees, graphs, or any other data structure where each element is allocated
    separately, and the data is linked together by pointers.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止研究的顺序数据结构中，数据存储在数组中（或者至少是由内存块组成的概念数组）。现在我们将考虑一种非常不同的数据结构类型，其中数据由指针连接在一起。最简单的例子是一个列表，其中每个元素都是单独分配的，但我们在这里学到的一切都适用于其他节点容器，如树、图或任何其他数据结构，其中每个元素都是单独分配的，并且数据由指针连接在一起。
- en: 'For simplicity, we will consider a singly linked list; in STL, it is available
    as `std::forward_list`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们将考虑一个单链表；在STL中，它可以作为`std::forward_list`使用：
- en: '![Figure 7.24 – Singly-linked list with iterators'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.24 - 带迭代器的单链表'
- en: '](img/Figure_7.24_B16229.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.24_B16229.jpg)'
- en: Figure 7.24 – Singly-linked list with iterators
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 - 带迭代器的单链表
- en: Because each element is allocated separately, it can also be deallocated individually.
    Often, a lightweight allocator is used for these data structures, where the memory
    is allocated in large blocks that are partitioned into node-sized fragments. When
    a node is deallocated, the memory is not returned to the OS but is put on a free
    list for the next allocation request. For our purposes, it is largely irrelevant
    whether the memory is allocated directly from the OS or handled by a specialized
    allocator (although the latter can often be much more efficient).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个元素都是单独分配的，所以它也可以单独释放。通常，这些数据结构使用轻量级分配器，其中内存是在大块中分配的，然后被分成节点大小的片段。当一个节点被释放时，内存不会被返回给操作系统，而是被放在一个空闲列表中，以供下一个分配请求使用。对于我们的目的来说，内存是直接从操作系统分配还是由专门的分配器处理（尽管后者通常更有效）在很大程度上并不重要。
- en: The list iterators present an additional challenge in concurrent programs. As
    we see in *Figure 7.24*, these iterators can point anywhere in the list. If an
    element is removed from the list, we expect its memory to eventually become available
    for constructing and inserting another element (if we do not do this and hold
    all memory until the entire list is deleted, adding and removing a few elements
    repeatedly can waste a lot of memory). However, we cannot delete the list node
    if there is an iterator pointing to it. This is true in single-threaded programs
    as well, but it is often much harder to manage in concurrent programs. With multiple
    threads possibly working with iterators, we often cannot guarantee by the execution
    flow of the operations that no iterators are pointing to the element we are about
    to delete. In this case, we need the iterators to extend the lifetime of the list
    nodes that they point to. This, of course, is a job for reference-counted smart
    pointers such as `std::shared_ptr`. Let's assume from now on that all the pointers
    in the list, both the ones linking the nodes together and the ones inside the
    iterators, are smart pointers (`std::shared_ptr` or a similar pointer with stronger
    thread-safety guarantees).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表迭代器在并发程序中提出了额外的挑战。正如我们在*图7.24*中看到的，这些迭代器可以指向列表中的任何位置。如果从列表中删除一个元素，我们希望它的内存最终可以用于构造和插入另一个元素（如果我们不这样做，并且一直保留所有内存直到整个列表被删除，重复添加和删除几个元素可能会浪费大量内存）。然而，如果有一个迭代器指向它，我们就不能删除列表节点。这在单线程程序中也是如此，但在并发程序中管理起来通常要困难得多。由于可能有多个线程可能使用迭代器，我们通常无法通过操作的执行流来保证没有迭代器指向我们即将删除的元素。在这种情况下，我们需要迭代器来延长它们所指向的列表节点的生命周期。当然，这是引用计数智能指针（如`std::shared_ptr`）的工作。从现在开始，让我们假设列表中的所有指针，无论是将节点链接在一起的指针还是迭代器中的指针，都是智能指针（`std::shared_ptr`或具有更强线程安全性保证的类似指针）。
- en: 'Just like we did with the sequential data structures, our first attempt at
    a thread-safe data structure should be a lock-guarded implementation. In general,
    you should never design a lock-free data structure until you know that you need
    one: developing lock-free code may be *cool*, but trying to find bugs in it is
    most definitely not.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在顺序数据结构中所做的那样，我们对于线程安全数据结构的第一次尝试应该是一个带锁的实现。一般来说，除非你知道你需要一个，否则你不应该设计一个无锁的数据结构：开发无锁代码可能很*酷*，但尝试在其中找到错误绝对不是。
- en: 'Just like we did earlier, we have to redesign parts of the interface, so all
    operations are transactional: `pop_front()` should work whether the list is empty
    or not, for example. We can then protect all operations with a lock. For operations
    such as `push_front()` and `pop_front()`, we can expect a performance similar
    to what we have observed for the stack or the queue earlier. But the list presents
    additional challenges we did not have to face until now.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前做的那样，我们必须重新设计接口的部分，以便所有操作都是事务性的：例如，`pop_front()`应该在列表为空或不为空时都能工作。然后我们可以用锁来保护所有操作。对于`push_front()`和`pop_front()`等操作，我们可以期望与之前观察到的堆栈或队列类似的性能。但是列表提出了我们直到现在都没有不得不面对的额外挑战。
- en: First, the list supports insertions at arbitrary locations; in the case of `std::forward_list`,
    it is `insert_after()` to insert a new element after the one pointed to by an
    iterator. If we insert two elements on two threads simultaneously, we would like
    the insertions to proceed concurrently unless the two locations are close to each
    other and affect the same list node. But we cannot get that with a single lock
    guarding the entire list.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，列表支持在任意位置插入；在`std::forward_list`的情况下，可以使用`insert_after()`在迭代器指向的元素之后插入一个新元素。如果我们在两个线程上同时插入两个元素，我们希望插入可以同时进行，除非两个位置靠近并影响同一个列表节点。但是我们无法通过一个单一的锁来保护整个列表来实现这一点。
- en: 'The situation is even worse if we consider long-running operations such as
    searching the list for an element that has the desired value (or satisfies some
    other condition). We would have to lock the list for the entire search operation,
    so no adding or removing elements to the list while the list is traversed. Of
    course, if we search frequently, the list is not the right data structure, but
    trees and other nodal data structures have the same problem: if we need to traverse
    large parts of the data structure, the lock is held for the duration of the entire
    operation, preventing all other threads from accessing even the nodes unrelated
    to the ones we''re currently operating on.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, these problems are not your concern if you never encounter them:
    if your list is accessed from the front and backends only, then a lock-guarded
    list may be perfectly sufficient. As we have seen many times, when it comes to
    designing concurrent data structures, unnecessary generality is your enemy. Build
    only what you need.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, however, nodal data structures are accessed not just from
    the ends or, in the case of trees or graphs, there aren''t really any *ends*.
    Locking the entire data structure so that it can be accessed by only one thread
    at a time is not acceptable if the program spends most of the time operating on
    this data structure. The next idea you may consider is locking each node separately;
    in the case of the list, we could add a spinlock to every node and lock the node
    if we need to change it. Unfortunately, this approach runs into the problem that
    is the bane of all lock-based solutions: the deadlocks. Any thread that needs
    to operate on more than one node will have to acquire multiple locks. Let''s say
    that thread A holds the lock on node 1, and now it needs to insert a new node
    after node 2, so it tries to get that lock too. At the same time, thread B holds
    the lock on node 2, and it wants to erase the node after node 1, so it tries to
    get that lock. Both threads will now wait forever. This problem is not avoidable
    with so many locks that can be acquired in arbitrary order unless we enforce very
    strict limitations on how the threads may access the list (hold only one lock
    at any time), and then we run the risk of livelocks as many threads constantly
    release and reacquire locks.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: If we truly need a list or another nodal data structure that is accessed concurrently,
    we have to come up with a lock-free implementation. As we have seen already, lock-free
    code is not easy to write and even harder to write correctly. Quite often, the
    better option is to come up with a different algorithm that does not require a
    thread-safe nodal data structure. Often, this can be done by copying parts of
    the global data structure into a thread-specific one that is then accessed by
    a single thread; at the end of the computation, the fragments from all threads
    are put together again. Sometimes, it is easier to partition the data structure
    so no nodes are accessed concurrently (for example, it may be possible to partition
    the graph and process each subgraph on one thread and then handle the boundary
    nodes). But if you really need a thread-safe nodal data structure, the next section
    will explain the challenges and give you some options for the implementation.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Lock-free list
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic idea behind a **lock-free list**, or any other nodal container, is
    quite simple and is based on using compare-and-swap to manipulate the pointers
    to the nodes. Let''s start with the simpler operation: the insertion. We are going
    to describe the insertion at the head of the list, but the insertion after any
    other node works the same way.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Insertion of a new node at the head of a singly-linked list'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.25_B16229.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.25 – Insertion of a new node at the head of a singly-linked list
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we want to insert a new node at the head of the list shown
    in *Figure 7.25a*. The first step is to read the current head pointer, that is,
    the pointer to the first node. Then we create the new node with the desired value;
    its next pointer is the same as the current head pointer, so this node is linked
    into the list before the current first node (*Figure 7.25b*). At this point, the
    new node is not yet accessible to any other thread, so the data structure can
    be accessed concurrently. Finally, we execute the CAS: if the current head pointer
    is still unchanged, we atomically replace it with the pointer to the new node
    (*Figure 7.25c*). If the head pointer no longer has the value it had when we first
    read it, we read the new value, write it as the next pointer of our new node,
    and try the atomic CAS again.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple and reliable algorithm. It is the generalization of the publishing
    protocol we saw in the previous chapter: the new data is created on a thread with
    no concern for thread safety because it is not yet accessible to other threads.
    As the final action, the thread publishes the data by atomically changing the
    root pointer from which all the data can be accessed (in our case, the head of
    the list). If we were inserting the new node after another node, we would atomically
    change that node''s next pointer instead. The only difference is that multiple
    threads may be trying to publish new data at the same time; to avoid data races,
    we have to use compare-and-swap.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider the opposite operation, erasing the front node of the
    list. This is also done in three steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Lock-free removal at the head of a singly-linked list'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.26_B16229.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.26 – Lock-free removal at the head of a singly-linked list
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: First, we read the head pointer, use it to access the first node of the list,
    and read its next pointer (*Figure 7.26a*). Then we atomically write the value
    of that next pointer into the head pointer (*Figure 7.26b*), but only if the head
    pointer has not changed (CAS). At this point, the former first node is not accessible
    to any other thread, but our thread still has the original value of the head pointer
    and can use it to delete the node we had removed (*Figure 7.26c*). This is, again,
    simple and reliable. But the trouble arises when we try to combine both of these
    operations.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that two threads operate on the list at the same time. Thread
    A is trying to remove the first node of the list. The first step is to read the
    head pointer and the pointer to the next node; this pointer is about to become
    the new head of the list, but the compare-and-swap hasn''t happened yet. For now,
    the head is unchanged, and the new head is a value head'' that exists only in
    some local variable of thread A. This moment is captured in *Figure 7.27a*:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Lock-free insertion and removal at the head of a singly-linked
    list'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.27_B16229.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.27 – Lock-free insertion and removal at the head of a singly-linked
    list
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Just at this moment, thread B successfully removes the first node of the list.
    Then it removes the next node also, leaving the list in the state shown in *Figure
    7.27b* (thread A has not made any more progress). Thread B then inserts a new
    node at the head of the list (*Figure 7.27c*); however, since the memory of the
    two deleted nodes was deallocated, the new allocation for the node T4 reuses the
    old allocation, so the node T4 is allocated at the same address as the original
    node T1 used to have. This can easily happen as long as the memory of the deleted
    nodes is available for new allocations; in fact, most memory allocators prefer
    to return the most recently released memory on the assumption that it is still
    *hot* in the cache of the CPU.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, thread A is finally running again, and the operation it is about to do
    is compare-and-swap: if the head pointer has not changed since the last time thread
    A read it, the new head becomes `head''`. Unfortunately, the value of the head
    pointer is still the same, as far as thread A can see (it could not observe the
    entire history of the changes). The CAS operation succeeds, and the new head pointer
    now points to the unused memory where the node T2 used to be, while the node T4
    is no longer accessible (*Figure 7.27d*). The entire list is corrupted.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，线程A终于再次运行，它即将执行的操作是比较和交换：如果头指针自上次线程A读取以来没有改变，新的头就变成`head'`。不幸的是，就线程A所能看到的情况而言，头指针的值仍然是相同的（它无法观察到所有的变化历史）。CAS操作成功，新的头指针现在指向了曾经是节点T2的未使用内存，而节点T4不再可访问（*图7.27d*）。整个列表已经损坏。
- en: 'This failure mechanism is so common in lock-free data structures that it has
    a name: the **A-B-A problem**. **A** and **B** here refer to memory locations:
    the problem is that some pointer in the data structure changes its value from
    A to B and then back to A. Another thread observes only the initial and the final
    values and sees no change at all; the compare-and-swap operation succeeds, and
    the execution takes the path where the programmer has assumed that the data structure
    is unchanged. Unfortunately, this assumption is not true: the data structure may
    have changed almost arbitrarily, except that the value of the observed pointer
    was restored to what it once was.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这种失败机制在无锁数据结构中非常常见，它有一个名字：**A-B-A问题**。这里的**A**和**B**指的是内存位置：问题是数据结构中的某个指针从A变为B，然后再变回A。另一个线程只观察到初始和最终值，并没有看到任何变化；比较和交换操作成功，执行了程序员假定数据结构未改变的路径。不幸的是，这个假设是不正确的：数据结构几乎可以任意改变，除了观察到的指针的值被恢复到它曾经的值。
- en: 'The root of the problem is that if the memory is deallocated and reallocated,
    pointers, or addresses in memory, do not uniquely identify the data stored at
    that address. There are multiple solutions to this problem, but they all accomplish
    the same thing by different means: you have to make sure that once you read a
    pointer that will eventually be used by compare-and-swap, the memory at that address
    cannot be deallocated until the compare-and-swap is done (successfully or otherwise).
    If the memory is not deallocated, then another allocation cannot happen at the
    same address, and you are safe from the A-B-A problem. Note that *not deallocating
    memory* is not the same as *not deleting nodes*: you can certainly make the node
    inaccessible from the rest of the data structure (remove the node), and you can
    even call the destructor for the data stored in the node; you just cannot free
    the memory occupied by the node.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的根源在于，如果内存被释放和重新分配，指针或内存中的地址不能唯一标识存储在该地址的数据。对于这个问题有多种解决方案，但它们都通过不同的方式实现了同样的目标：确保一旦读取了将被比较和交换使用的指针，该地址的内存在比较和交换完成（成功或不成功）之前不能被释放。如果内存没有被释放，那么另一个分配就不能发生在同一个地址上，您就不会遇到A-B-A问题。请注意，*不释放内存*并不等同于*不删除节点*：您当然可以使节点对于数据结构的其余部分不可访问（删除节点），甚至可以调用节点中存储的数据的析构函数；您只是不能释放节点占用的内存。
- en: 'There are many ways to solve the A-B-A problem by delaying memory deallocation.
    The application-specific options are usually the simplest if they are possible.
    If you know that the algorithm does not remove many nodes over the lifetime of
    the data structure, you may simply keep all removed nodes on a list of deferred
    deallocations, to be deleted when the entire data structure is deleted. A more
    general version of this approach can be described as application-driven garbage
    collection: all deallocated memory goes on a *garbage* list first. The garbage
    memory is periodically returned to the main memory allocator, but during this
    garbage collection, all operations on the data structure are suspended: the operations
    in progress must complete before the collection starts, and all new operations
    are blocked until the collection is done. This ensures that no compare-and-swap
    operation can span the time interval of the garbage collection and, thus, the
    recycled memory is never encountered by any operation. The popular and often very
    efficient **RCU** (**read-copy-update**) technique is a variant of this method
    as well. Another common approach is the use of hazard pointers.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以通过延迟内存释放来解决A-B-A问题。如果可能的话，特定于应用程序的选项通常是最简单的。如果您知道算法在数据结构的生命周期内不会删除许多节点，您可以简单地将所有已删除的节点保留在延迟释放列表中，在整个数据结构被删除时再删除。这种方法的更一般版本可以被描述为应用驱动的垃圾收集：所有释放的内存首先放在*垃圾*列表上。垃圾内存定期返回给主内存分配器，但在此期间，数据结构上的所有操作都被暂停：正在进行的操作必须在收集开始之前完成，所有新操作都被阻塞，直到收集完成。这确保了没有比较和交换操作可以跨越垃圾收集的时间间隔，因此，回收的内存永远不会被任何操作遇到。流行且通常非常高效的**RCU**（**读-复制-更新**）技术也是这种方法的变体。另一种常见的方法是使用危险指针。
- en: In this book, we will present yet another approach that employs atomic shared
    pointers (`std::shared_ptr` is not atomic by itself, but the standard included
    the necessary functions for atomic operations on shared pointers, or you can write
    your own for this specific application and make it faster). Let's revisit *Figure
    7.27b*, but now let all pointers be atomic shared pointers. As long as there is
    at least one such pointer to a node, that node cannot be deallocated. In the same
    sequence of events, thread A still has the old head pointer that points to the
    original node T1, as well as the intended new head pointer, `head'`, that points
    to the node T2\.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将介绍另一种方法，它使用原子共享指针（`std::shared_ptr`本身不是原子的，但标准包含了对共享指针进行原子操作的必要函数，或者您可以为特定应用程序编写自己的函数并使其更快）。让我们重新审视*图7.27b*，但现在让所有指针都是原子共享指针。只要有至少一个这样的指针指向一个节点，该节点就不能被释放。在相同的事件序列中，线程A仍然拥有指向原始节点T1的旧头指针，以及指向节点T2的新头指针`head'`。
- en: '![Figure 7.28 – Lock-free insertion and removal at the head of a singly-linked
    list with shared pointers'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.28_B16229.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.28 – Lock-free insertion and removal at the head of a singly-linked
    list with shared pointers
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Thread B has removed both nodes from the list (*Figure 7.28*), but the memory
    has not been released. The new node T4 is allocated at some other address, different
    from the addresses of all currently allocated nodes. Thus, when thread A resumes
    execution, it will find the new list head different from the old head value; the
    compare-and-swap will fail, and thread A will attempt the operation again. At
    this point, it will re-read the head pointer (and get the address of the node
    T3). The old value of the head pointer is now gone; since it was the last shared
    pointer pointing to the node T1, this node has no more references and is deleted.
    Similarly, node T2 is deleted as soon as the shared pointer `head'` is reset to
    its new intended value (the next pointer of the node T3). Both nodes T1 and T2
    have no shared pointers pointing to them, so they are finally deleted.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this takes care of the insertion at the front. To allow insertion
    and removal anywhere, we have to make all pointers to the nodes into shared pointers.
    This includes the *next* pointers of all nodes as well as the pointers to nodes
    that are hidden inside list iterators. Such a design has another major advantage:
    it takes care of the problems with list traversals (such as search operations)
    that happen concurrently with insertions and deletions.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: If a list node was removed while there is an iterator pointing to this list
    (*Figure 7.29*), the node remains allocated, and the iterator is valid. Even if
    we remove the next node (T3), it will not be deallocated because there is a shared
    pointer pointing to it (the *next* pointer of node T2). The iterator can traverse
    the entire list.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Thread-safe traversal of a lock-free list with atomic shared
    pointers'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.29_B16229.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.29 – Thread-safe traversal of a lock-free list with atomic shared pointers
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this traversal may include nodes that are no longer in the list,
    that is, no longer reachable from the head of the list. This is the nature of
    the concurrent data structures: there is no meaningful way to talk about the *current
    content of the list*: the only way to know the content of the list is to iterate
    over it from the head to the last node, but, by the time the iterator reached
    the end of the list, the previous nodes might have changed, and the result of
    the traversal is no longer *current*. This way of thinking takes some getting
    used to.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to show any benchmarks of the lock-free list versus a lock-guarded
    list because these benchmarks must be specific to the application. If you benchmark
    only insertions and deletions at the head of the list (`push_front()` and `pop_front()`),
    the spinlock-guarded list will be faster (atomic shared pointers are not cheap).
    On the other hand, if you benchmark simultaneous insertions and searches, you
    can make the lock-free list faster by as much as you want: do a traversal of a
    list of 1M elements with the lock-guarded list locked the entire time while the
    lock-free list can do simultaneous iterations on every thread, along with insertions
    and deletions. No matter how slow the atomic pointers are, the lock-free list
    will be faster if you just make it long enough. This is not a gratuitous observation:
    your application may need to do the operations that would require locking the
    list for a very long time unless you can somehow partition the list in a way that
    avoids deadlocks. If this is what you need to do, the lock-free list is the fastest
    by far. On the other hand, if you need to iterate over just a few elements and
    never in many different locations at the same time, a lock-guarded list will do
    fine.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'The A-B-A problem and the solutions we have listed apply not just to the lists
    but to all nodal data structures: doubly-linked list, tree, and graph. In data
    structures linked by multiple pointers, you may encounter additional problems.
    First of all, even if all pointers are atomic, changing two atomic pointers one
    after the other is not an atomic operation. This leads to temporary inconsistencies
    in the data structure: for example, you may expect that going from a node to the
    next node and back to the previous node will get you back to the original node.
    This is not always true in the case of concurrency: if a node is inserted or removed
    at this location, one of the pointers may be updated before the other. The second
    problem is specific to shared pointers or any other implementation that uses reference
    counting: if the data structure has pointer loops, the nodes in the loop do not
    get deleted even when there are no more external references to them. The simplest
    example is the doubly-linked list, where two adjacent nodes always have pointers
    to each other. The way we solve this problem in single-threaded programs is by
    using weak pointers (in a doubly-linked list, all *next* pointers could be shared,
    and all *previous* pointers would then be weak). This does not work as well for
    concurrent programs: the whole point is to delay the deallocation of memory until
    there are no more references to it, and the weak pointers do not do that. For
    these cases, additional garbage collection may be necessary: after the last external
    pointer to a node is deleted, we have to traverse the linked nodes and check whether
    there are any external pointers to them (we can do it by checking the reference
    counts). List fragments with no external pointers can be safely deleted. For such
    data structures, alternative approaches such as hazard pointers or explicit garbage
    collection may be preferred. The reader should refer to specialized publications
    on lock-free programming for more information on these methods.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our exploration of high-performance data structures for concurrent
    programming. Let's now summarize what we have learned.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important lesson of this chapter is that *designing data structures
    for concurrency is hard, and you should take every opportunity to simplify it*.
    Application-specific restrictions on the use of the data structures can be used
    to make them both simpler and faster.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The first decision you must make is which parts of your code need thread safety
    and which do not. Often, the best solution is to give each thread its own data
    to work on: any data used by a single thread needs no thread-safety concerns at
    all. When that is not an option, look for other application-specific restrictions:
    do you have multiple threads modifying a particular data structure? The implementation
    is often simpler if there is only one writer thread. Are there any application-specific
    guarantees you can exploit? Do you know the maximum size of the data structure
    upfront? Do you need to delete data from the data structure as well as add it
    at the same time, or can you separate these operations in time? Are there well-defined
    periods where some data structures are not changing? If so, you do not need any
    synchronization to read them. These and many other application-specific restrictions
    can be used to greatly improve the performance of the data structures.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'The second important decision is: what operations on the data structures are
    you going to support? Another way to restate the last paragraph is "implement
    the minimal necessary interface." Any interface you do implement must be transactional:
    each operation must have well-defined behavior for any state of the data structure.
    Any operation that is valid only if the data structure is in a certain state cannot
    be safely invoked in a concurrent program unless the caller uses client-side locking
    to combine multiple operations into a single transaction (in which case, these
    should probably be one operation in the first place).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also teaches several ways to implement data structures of different
    types, as well as the ways to estimate and evaluate their performance. Ultimately,
    accurate performance measurement can be obtained only in the context of the real
    application and with the actual data. However, useful approximate benchmarks can
    save a lot of time during the development and evaluation of potential alternatives.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes our exploration of concurrency. Next, we go on to learn
    how the use of the C++ language itself influences the performance of our programs.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the most critical feature of the interface of data structures designed
    for thread safety?
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are the data structures with limited functionality often more efficient
    than their generic variants?
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are lock-free data structures always faster than lock-based ones?
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the challenges of managing memory in concurrent applications?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the A-B-A problem?.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

- en: '*Chapter 12*: Design for Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter reviews all the performance-related factors and features we have
    learned in this book and explores the subject of how the knowledge and the understanding
    we have gained should influence the design decisions we make when developing a
    new software system or rearchitecting an existing one. We will see how the design
    decisions impact the performance of the software systems, learn how to make performance-related
    design decisions in the absence of detailed data, as well as reviewing the best
    practices for designing APIs, concurrent data structures, and high-performance
    data structures to avoid inefficiencies. We will explore the following subjects:'
  prefs: []
  type: TYPE_NORMAL
- en: Interaction between the design and performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API design considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design for optimal data access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance trade-offs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making informed design decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn how to treat good performance as one of the design goals from
    the beginning and how to design high-performance software systems in ways that
    ensure that efficient implementation does not become a struggle against the fundamental
    architecture of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need a C++ compiler and a micro-benchmarking tool, such as the Google
    Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter12](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter12).
  prefs: []
  type: TYPE_NORMAL
- en: Interaction between the design and performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does good design help to achieve good performance, or do you have to occasionally
    compromise best design practices to achieve the best performance? These issues
    are hotly debated in the programming community. Usually, the design evangelists
    will argue that if you think that you need to choose between good design and good
    performance, your design is not good enough. On the other hand, hackers (we're
    using this term in the classic sense, programmers who hack together solutions,
    nothing to do with the criminal aspect) often view design guidelines as constraints
    on the best possible optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to show that both points of view are valid, to a
    degree. They are also mistaken if viewed as "the whole truth." It would be dishonest
    to deny that many design practices, when applied to a specific software system,
    can constrain performance. On the other hand, many guidelines for achieving and
    maintaining efficient code are also solid design recommendations and improve both
    the performance and the design quality.
  prefs: []
  type: TYPE_NORMAL
- en: We take a more nuanced view of the tension between design and performance. For
    a particular system (and you are most interested in *your* system, the one you
    are working on *right now*), some design guidelines and practices can indeed cause
    inefficiencies and poor performance. We would be hard-pressed to name a design
    rule that is always antithetical to efficiency, but for a particular system, and
    maybe in some specific context, such rules and practices are quite common. If
    you embrace a design that follows such rules, you may indeed end up embedding
    inefficiencies into the core architecture of your software system, and it will
    be very hard to remedy by "optimizations" short of a total rewrite of the critical
    parts of the program. Anyone who dismisses or sugar-coats the potential severity
    of this pitfall does not have your best interests in mind. On the other hand,
    anyone who claims that this justifies abandoning solid design practices presents
    a false, oversimplified choice.
  prefs: []
  type: TYPE_NORMAL
- en: If you realize that a particular design approach follows good practices, improves
    clarity and maintainability, but degrades performance, the correct response is
    to choose a different but also good design approach. In other words, while it
    is common to discover that some good designs produce poor performance, it is highly
    unlikely that, for a given software system, every good design would cause inefficiencies.
    "All" you need to do is select from several possible good-quality designs the
    one that also allows for a good performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now, this is easier said than done, of course, but hopefully, this book will
    help. In the rest of the chapter, we will focus on two sides of the problem. First,
    what design practices are suggested when performance is a concern? Second, how
    can we evaluate the likely performance impact when we don't have a program we
    could run and measure, but all we have is a (possibly incomplete) design?
  prefs: []
  type: TYPE_NORMAL
- en: 'If you read the last two paragraphs carefully, you cannot escape the observation
    that performance is a design consideration: just like we factor into the design
    our requirements such as "support many users" or "store terabytes of data on disk,"
    the performance targets are a part of the requirements and should be explicitly
    considered at the design stage. This leads us to the key concept of designing
    high-performance systems, which is…'
  prefs: []
  type: TYPE_NORMAL
- en: Design for performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said, performance is one of the design goals, equal in importance to other
    constraints and requirements. Thus, the answer to the problem of "this design
    results in poor performance" is the same as what we would do if the issue was
    "this design does not provide the features we need." In both cases, we need a
    different design, not a worse design. We are just more used to evaluating designs
    based on what they do rather than how fast they do it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help you choose performance-promoting design practices on the first try,
    we will now go over several design guidelines that specifically target good performance.
    They are also solid design principles with good reasons to embrace them: following
    these guidelines will not make your design worse.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two such guidelines deal with the interaction of different components
    of a design (functions, classes, modules, processes, any components). First, we
    recommend that these interactions convey as little information as possible for
    the overall system to still function. Second, we suggest that different components
    provide each other with as much information as they have about the expected outcome
    of the interaction. If you think this is a contradiction, you are absolutely correct.
    Design is often the art of resolving contradictions, and the way you do it is
    this: both contradicting statements are true, just not at the same time or in
    the same place. What follows is a good illustration of this (much more general)
    technique of managing contradictions in design.'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum information principle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us start with the first guideline: communicate as little information as
    possible. Context is of utmost importance here: specifically, we recommend that
    a component reveals as little information as possible about how it handles a particular
    request. The interaction between components is governed by a contract. We are
    used to this idea when we talk about the interfaces of classes and functions,
    but it is a much broader concept. For example, the protocol used to communicate
    between two processes is a contract.'
  prefs: []
  type: TYPE_NORMAL
- en: In any such interface or interaction, the party that makes and fulfills a promise
    must not volunteer any additional information. Let us look at some specific examples.
    We will start with a class that implements a basic queue and ask ourselves, what
    is a good interface from the point of view of efficiency?
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the methods allows us to check whether the queue is empty. Note that
    the caller didn''t ask the queue how many elements it has, just whether or not
    it''s empty. While some implementations of the queue may cache the size and compare
    it with zero to resolve this request, for other implementations, it may be more
    efficient to determine whether the queue is empty than to count elements. The
    contract says, "I will return true if the queue is empty." Even if you think you
    know the size, don''t make any additional promises: do not volunteer any information
    that wasn''t requested. This way, you are free to change your implementation later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the methods to enqueue and dequeue should guarantee only that a
    new element is added to or removed from the queue. For popping an element from
    the queue, we have to handle the case of an empty queue or declare the result
    of such an attempt undefined (the approach chosen by the STL). You might notice
    that the STL queue, so far, exhibits an excellent interface from the efficiency
    point of view: it fulfills the contract for a queue data structure without revealing
    any unnecessary details. In particular, a `std::queue` is an adapter that can
    be implemented on top of one of several containers. The fact that the queue can
    be implemented as a vector, deque, or a list tells us that the interface is doing
    a great job concealing the details of the implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an opposite example of an interface leaking too much implementation information,
    consider another STL container, the unordered set (or map). The `std::unordered_set`
    container has an interface that allows us to insert new elements and check if
    a given value is already in the set (so far, so good). By definition, it lacks
    an internal order of elements, and the performance guarantees provided by the
    standard make it clear that the data structure uses hashing. Perforce, the section
    of the interface that explicitly refers to hashing cannot be considered gratuitous:
    in particular, it is necessary to specify a user-given hash function. But the
    interface goes further and, through methods such as `bucket_count()`, exposes
    the fact that the underlying implementation *must* be a separate-chaining hash
    table with buckets for resolving hash conflicts. It is, therefore, impossible
    to create a fully STL-compliant unordered set using, for example, an open addressing
    hash table. This interface constrains the implementation and may prevent you from
    using a more efficient implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we used class design for simple examples, the same principle can be applied
    to APIs of larger modules, client-server protocols, and other interactions between
    components of the system: *when designing a component that responds to a request
    or provides a service, offer a terse contract and reveal the information needed
    by the requestor and nothing else*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The design guideline of revealing minimum information, or minimum promise,
    is essentially a generalization of a popular guideline for class interfaces: the
    interface should not reveal the implementation. Also, consider that correcting
    a violation of this guideline is going to be quite difficult: if your design leaks
    implementation details, the clients will come to rely on them and will break once
    you change the implementation. Thus, so far, designing for performance is consistent
    with general good design practices. With the next guideline, we start to expose
    the tension between different design goals and the corresponding best practices.'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum information principle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the component that fulfills the request should avoid unnecessarily disclosing
    anything that might constrain the implementation, the opposite is true for the
    component that makes the request. The requestor or the caller should be able to
    provide specific information about what exactly is needed. Of course, the caller
    supplies the information only if there is an appropriate interface for it, and,
    thus, what we are really saying is the interface should be designed to allow such
    "complete" requests.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, to provide the best performance, it is often important to know
    the intent behind the request. Again, an example should make it easier to understand
    the concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start with a random access sequence container. Random access means that
    we can access an arbitrary i-th element of the container without the need to access
    any other elements. The usual way this is done is with the index operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With this operator, we can, for example, iterate over the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From the point of view of efficiency, this is not the best way: we are using
    a random access iterator for sequential iteration. Generally, when you use a more
    powerful or more capable interface but utilize only a fraction of its capabilities,
    you should be concerned about efficiency: the extra flexibility of this interface
    may have come at the cost of some performance, which you are wasting if you do
    not use these features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not have to go far for an example. Let us consider `std::deque`: it is
    a block-allocated container that supports random access. In order to access arbitrary
    element `i`, we have to first calculate which block contains this element (generally,
    a modulo operation) and the index of the element within the block, then find the
    address of the block in the auxiliary data structure (block pointer table) and
    index into the block. This process has to be repeated for the next element, even
    though, in most cases, the element will reside in the same block, and we already
    know its address. This happens because the request for an arbitrary element does
    not contain enough information: there is no way to express that we are going to
    ask for the next element soon. Consequently, the deque cannot handle the traversal
    in the most efficient manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative way for scanning the entire container is to use the iterator
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementer of the deque can assume that incrementing (or decrementing)
    an iterator is an operation that is done frequently. Therefore, if you have an
    iterator `it` and access the corresponding element `*it`, odds are good that you
    are going to ask for the next element. The deque iterator can store the block
    pointer or the index of the right entry in the block pointer table, which would
    make accessing all elements within one block much cheaper. With the help of a
    simple benchmark, we can verify that indeed it is much faster to traverse the
    deque using an iterator than using an index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The results show very impressive performance differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Traversal of std::deque using index versus iterator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Traversal of std::deque using index versus iterator
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very important to point out the key difference between designing for
    performance and optimizing for performance. There is no guarantee that the iterator
    access to a deque is faster: a particular implementation may, in fact, use the
    index operator to implement the iterator. Such a guarantee may come only from
    an optimized implementation. In this chapter, we are interested in the design.
    The design can''t really be "optimized," although, if you talk about an "efficient
    design," others will likely understand what you mean. The design can allow or
    prevent certain optimizations, so it is more accurate to talk about "performance-hostile"
    and "performance-friendly" design (the latter is often called an efficient design).'
  prefs: []
  type: TYPE_NORMAL
- en: In our deque example, the index operator interface is as efficient as it can
    be for random access, and it treats sequential iteration as a particular case
    of random access. There is no way for the caller to say, "I will probably ask
    for the adjacent element next." Conversely, from the existence of the iterator,
    we can infer that it is likely to be incremented or decremented. The implementation
    is free to make this increment operation more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take our container example one step further. This time, we consider
    a custom container that functions essentially as a tree, but, unlike `std::set`,
    we do not store the values in the tree nodes. Instead, we store the values in
    a sequence container (data store), while the tree nodes contain pointers to elements
    of this container. The tree is essentially an index into the data store, so it
    needs a custom comparison function: we want to compare the values, not the pointers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When a new element is inserted, it is added to the end of the data store, while
    the pointer is added to the appropriate place in the index as determined by the
    comparison of the elements. Why would we choose such implementation over `std::set`?
    In some cases, we may have requirements that force our hand: for example, the
    data store may be a memory-mapped file on a disk. In other cases, we may choose
    this implementation for performance benefits, even though at first glance, the
    extra memory use and the indirect access to elements through the pointer should
    degrade the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the performance advantage of this indexed tree container, let us examine
    the operation that does the search for an element that satisfies a given predicate.
    We can do this search easily, assuming our container provides iterators that simply
    iterate over the index set; the dereference operator should return the indexed
    element, not the pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine whether a value that meets certain requirements has been stored
    in the container, we can simply iterate over the entire container and check the
    predicate for every value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'What information do we supply to the container when we access it using the
    iterators, as we did just now? Just like before, we tell it that we intend to
    access the next element, every time. We do not tell it anything about the reason
    we are doing this. Does the intent matter? In this case, very much so. Look carefully
    at what we really need to do: we need to access every element in the container
    until we find one that meets the given condition. If this seems like restating
    the same thing, you are not being pedantic enough. Nowhere in this statement of
    requirement did we say that we want to access container elements *in order*, only
    that we need to iterate over all of them. If we had an API call that tells the
    container to check all elements but does not require any particular order, the
    container implementation would be free to optimize the access order. For our indexed
    container, the optimal access order is to iterate over the data store vector itself:
    this provides the best memory access pattern (sequential access). The actual order
    of the elements in the store is, in our case, the order in which they were added,
    but it does not matter: all we are asking to return is a Boolean value; we do
    not even ask where the matching element is located. To put it another way, while
    there may be multiple elements that satisfy the condition, the caller wants to
    know whether at least one such element exists. We did not ask for the value of
    the element or for any specific element: this is the request to "find any," not
    "find first."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the version of the interface that allows the caller to supply all the
    relevant information and a possible implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Is it faster? Again, a benchmark can answer. The difference is more pronounced
    if the value is not found or found rarely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Search in an indexed data store using iterators vs. find()
    member function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Search in an indexed data store using iterators vs. find() member
    function
  prefs: []
  type: TYPE_NORMAL
- en: Once again, it is very important to take a step back and reevaluate this example
    as a lesson for software design instead of a particular optimization technique.
    In this context, it is not important that our `find()` member function is so much
    faster than the iterator-based search. What is important at the design stage is
    that it might be faster with the appropriate implementation. The reason it might
    be faster is the knowledge of the caller's intent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the information supplied by the caller using non-member vs. member
    `find()`. When the non-member `find()` function calls the container interface,
    we tell the container, "let me see the values of all container elements, one by
    one, in order." We don''t actually need most of this, but that''s the information
    we give the container because that''s the only information we can channel through
    the iterator interface. On the other hand, the member `find()` allows us to make
    the following request: "examine all elements in any order and tell me if there
    is at least one that matches this condition." This request imposes much fewer
    restrictions: it is a high-level request that leaves the details to the container
    itself. In our example, the implementer took advantage of this freedom to provide
    much better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: At the design stage, you will likely not know that such optimized implementation
    is possible. The very first implementation of the member `find()` might as well
    run the iterator loop or call `std::find_if`. It is also possible that you will
    never get to optimizing this function because, in your applications, it is rarely
    called and is not a performance bottleneck. But software systems tend to live
    longer than you expect, and fundamental redesign is difficult and time-consuming.
    A good system architecture should not restrict the evolution of the system for
    years, sometimes decades, even as new features are added and performance requirements
    change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we have seen the difference between a performance-friendly and a performance-hostile
    design. The same principle applies, of course, to interactions between system
    components and is not limited to the classes: *when designing a component that
    responds to a request or provides a service, allow the requestor to supply all
    relevant information, in particular, to express the intent behind the request.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a more controversial guideline for several reasons. First of all, it
    explicitly goes against the popular approach to class design: never implement
    a (public) member function for a task that does not require privileged access
    and can be implemented entirely through an existing public API. There are several
    ways we can reason about this. First of all, one can say that "can be implemented
    ten times slower" does not really qualify as "can be implemented," so the guideline
    does not apply. The counterpoint is that at the design stage, you may not even
    know that you need this performance. The other important rule that we may be violating
    is "do not optimize prematurely," although this rule should not be taken simplistically:
    in particular, a reasonable proponent of this rule will often add, "but do not
    pessimize prematurely either." The latter, in the context of design, means making
    design decisions that cut off future optimization opportunities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of the maximum information principle (or information-rich interface)
    is, thus, a matter of balance and sound judgment. Consider that, in general, violating
    this guideline is not nearly as maleficent as not following the previous rule:
    if your interface or contract exposes unnecessary information, it is very hard
    to take it back from all the clients who came to rely on it. On the other hand,
    if your interface does not allow the client to supply the relevant intent information,
    the client may be forced into an inefficient implementation. But nothing will
    break after you add the more information-rich interface later, and the clients
    can transition to this interface as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision about whether to provide a more information-rich interface upfront,
    thus, hinges on several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How likely is it that this component or this interaction between components
    is going to be performance-critical? While guessing about the performance of a
    particular code is to be discouraged, you usually know the general requirements
    for the components in question: a database that is accessed millions of times
    per second is likely to be a performance bottleneck somewhere, while the system
    that serves employee addresses for paychecks twice a month can be designed conservatively
    and optimized later if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How broad is the impact of this design decision? In particular, if the inefficient
    implementation proliferates, how entrenched would it be by the time we add a new,
    higher-level interface? A class that is used once or twice can be easily updated
    along with its clients; a communication protocol that will become the standard
    for the entire system and will be used in a restful API that stores messages on
    disk for weeks and months should have extensibility built into it from the start,
    including an option for future information-rich requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, these choices are not clear-cut and rely on the designer's intuition
    tempered with knowledge and experience. This book can help with the former, and
    practice takes care of the latter.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen throughout this section, we often focus on the interfaces and
    data organization when considering the performance implications of different design
    decisions. In the following two sections, we will turn explicitly to these two
    subjects, starting with the interface design.
  prefs: []
  type: TYPE_NORMAL
- en: API design considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many books and articles that present best practices for API design.
    They usually focus on usability, clarity, and flexibility. The common guidelines,
    such as "make the interfaces clear and easy to use correctly" and "make it difficult
    to misuse the interfaces," do not directly address performance but also do not
    interfere with the practices that promote good performance and efficiency. In
    the previous section, we have addressed two important guidelines that should be
    remembered when designing interfaces for performance. In this section, we will
    explore some more specific guidelines that target performance explicitly. Many
    high-performance programs rely on concurrent execution, so it makes sense to address
    design for concurrency first.
  prefs: []
  type: TYPE_NORMAL
- en: API design for concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important rule when designing concurrent components and their interfaces
    is to provide clear thread-safety guarantees. Note that "clear" does not mean
    "strong": in fact, for optimum performance, it is often better to provide weaker
    guarantees on low-level interfaces. The approach chosen by the STL is a fine example
    to follow: all methods that may change the state of the object offer the weak
    guarantee: the program is well-defined as long as only one thread is using the
    container at any time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want a stronger guarantee, you can use locks at the application level.
    A much better practice is to create your own locking classes that offer a strong
    guarantee on the interfaces you want. Sometimes, these classes are just locking
    decorators: they wrap every member function of the decorated object in a lock.
    More often, there are multiple operations that must be protected by a single lock.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Why? Because it makes no sense to allow the clients to see a particular data
    structure after "half" of the operation is done. This leads us to a more general
    observation: as a rule, the thread-safe interfaces should also be transactional.
    The state of the component (class, server, database, and so on) should be valid
    before an API call is made and after it is made. All invariants promised by the
    interface contract should be maintained. It is highly likely that, during the
    execution of the requested member function (for classes), the object went through
    one or more states that would not be considered as valid by the clients: it does
    not maintain the specified invariants. The interface should make it impossible
    for another thread to observe the object in such an invalid state. Let us illustrate
    with an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall our index tree from the previous section. If we want to make this tree
    thread-safe (which is a short-hand for offering the strong guarantee), we should
    make inserting new elements safe even when called from multiple threads at the
    same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, other methods have to be protected as well. It is obvious that we
    do not want to lock the `push_back()` and the `insert()` calls separately: what
    would the client do with an object that has the new element in the data store
    but not in the index? According to our interface, it is not even defined whether
    or not this new element is in the container: if we scan the index using the iterators,
    it is not, but if we scan the data store using `find()`, then it is. This inconsistency
    tells us that the invariants of the index tree container are maintained before
    and after but not in the middle of the insertion. Therefore, it is very important
    that no other thread can see such an ill-defined state. We accomplish this by
    making sure that the interface is both thread-safe and transactional. It is safe
    to call multiple member functions concurrently; some threads will block and wait
    for other threads to complete their work, but there is no undefined behavior.
    Each member function moves the object from one well-defined state to another well-defined
    state (in other words, it executes a transaction such as adding a new element).
    The combination of these two factors makes the object safe to use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need a counter-example (what not to do when designing interfaces for
    concurrency), recall the discussion of `std::queue` in [*Chapter 7*](B16229_07_Epub_AM.xhtml#_idTextAnchor117),
    *Data Structures for Concurrency*. The interface for removing elements from the
    queue is not transactional: `front()` returns the front element but does not remove
    it, while `pop()` removes the front element but returns nothing, and both yield
    undefined behavior if the queue is empty. Locking these methods individually does
    us no good, so a thread-safe API has to use one of the approaches we considered
    in [*Chapter 7*](B16229_07_Epub_AM.xhtml#_idTextAnchor117), *Data Structures for
    Concurrency*, to construct a transaction and guard it with a lock.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we turn to efficiency: as you can see, it would do us no good if the individual
    objects that serve as building blocks of our container did their own locking.
    Imagine if `std::deque<T>::push_back()` was itself guarded by a lock. It would
    make the deque thread-safe (assuming other relevant methods were locked too, of
    course). But it would not do us any good since we still need to guard the entire
    transaction with a lock. All it does is wastes some time acquiring and releasing
    a lock that we do not need.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that not all data is being accessed concurrently. In a well-designed
    program that minimizes the amount of shared state, most work is done on thread-specific
    data (objects and other data that is exclusive to one thread) and updates to the
    shared data are relatively infrequent. The objects that are exclusive to one thread
    should not incur the overhead of locking or other synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems that we now have a contradiction: on the one hand, we should design
    our classes and other components with thread-safe transactional interfaces. On
    the other hand, we should not burden these interfaces with locks or other synchronization
    mechanisms because we might be building higher-level components that do their
    own locking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general approach to resolving this contradiction is to do both: provide
    non-locking interfaces that can be used as building blocks of higher-level components
    and provide thread-safe interfaces where it makes sense. Often, the latter is
    accomplished by decorating the non-locking interface with a lock guard. Of course,
    this has to be done within reason. First of all, any non-transactional interfaces
    are there exclusively for single-threaded use or for building higher-level interfaces.
    Either way, they do not need to be locked. Second, there are some components and
    interfaces that, in a particular design, are used in a narrow context. Maybe a
    data structure is designed specifically for the work that is being done on each
    thread separately; again, there is no reason to add the overhead of concurrency
    to it. Some components may be, by design, intended for concurrent use only and
    are top-level components – they should have thread-safe transactional interfaces.
    This still leaves many classes and other components that are likely to be used
    both ways and need locking and non-locking variants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, fundamentally, two ways to go about it. The first is to design a
    single component that can use locking if requested, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For this to work, we need a conditional `lock_guard`. It is possible to construct
    one using `std::optional` or `std::unique_ptr`, but it''s inelegant and inefficient.
    It is much easier to write our own RAII class similar to `std::lock_guard`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to being non-copyable, `std::lock_guard` is also non-movable. You
    can follow the same design or make your class movable. For classes, you can often
    handle the locking condition at compile time instead of runtime. This approach
    uses a policy-based design with a locking policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We should have at least two versions of the locking policy `LP`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create `index_tree` objects with weak or strong thread-safety guarantees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this compile-time approach works well for classes but may not be
    applicable to other types of components and interfaces. For example, when communicating
    with a remote server, you may want to notify it at runtime whether the current
    session is shared or exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is the one we discussed earlier, a locking decorator. In
    this version, the original class (`index_tree`) offers only the weak thread-safety
    guarantee. The strong guarantee is provided by this wrapper class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that, while encapsulation is generally preferred to inheritance, the advantage
    of the inheritance here is that we can avoid copying all the constructors of the
    decorated class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same approaches can be applied to other APIs: an explicit parameter to
    control locking vs. a decorator. Which one to use depends largely on the particulars
    of your design – they both have their pros and cons. Note that, even if the overhead
    of locking is insignificant compared to the work done by a particular API call,
    there may be good reasons to avoid gratuitous locking: in particular, such locking
    greatly increases the amount of code that should be vetted for possible deadlocks.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is a lot of overlap between the guideline that all thread-safe
    interfaces should be transactional and the best practices for designing exception-safe,
    or, more generally, error-safe interfaces. The latter is more complex because
    not only do we have to guarantee a valid state before and after the call to an
    interface but also that the system remains in a well-defined state after an error
    is detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the point of view of performance, error handling is essentially overhead:
    we do not expect errors to be frequent (otherwise, they are not really errors
    but regularly occurring situations we have to deal with). Fortunately, the best
    practices for writing error-safe code, such as using RAII objects for cleanup,
    are also quite efficient and rarely impose significant overhead. Nonetheless,
    some error conditions are quite difficult to detect reliably, as we have seen
    in [*Chapter 11*](B16229_11_Epub_AM.xhtml#_idTextAnchor176), *Undefined Behavior
    and Performance*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several guidelines for designing efficient concurrent APIs that we
    have learned in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Interfaces intended for concurrent use should be **transactional**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interfaces should **provide the minimum necessary thread-safety guarantee**
    (weak guarantee for interfaces that are not intended to be used concurrently).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For interfaces that are used both as a client-visible API and as building blocks
    for higher-level components that create their own, more complex transactions and
    provide the appropriate locking, it is **often desirable to have two versions:
    one with the strong thread-safety guarantee and another with the weak one** (or,
    locking and non-locking). This can be done with conditional locking or using decorators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These guidelines are in general agreement with other best practices for designing
    robust and clear APIs. Thus, it is rare that we have to make design trade-offs
    to allow better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now leave behind the issues of concurrency and turn to other areas of
    design for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Copying and sending data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This discussion is going to be a generalization of the matters we covered in
    [*Chapter 9*](B16229_09_Epub_AM.xhtml#_idTextAnchor149), *High-Performance C++*,
    when we talked about unnecessary copying. Using any interface, not just a C++
    function call, usually involves sending or receiving some data. This is a very
    general notion, and we won't be able to offer any specific guidelines that are
    universally applicable beyond the equally general "be mindful of the cost of data
    transfer." We can elaborate this a little for some common types of interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already discussed the overhead of copying memory in C++ and the resulting
    considerations for the interfaces. We covered the implementation techniques in
    [*Chapter 9*](B16229_09_Epub_AM.xhtml#_idTextAnchor149), *High-Performance C++*.
    For the design, we can emphasize the generally important guideline: **have a well-defined
    data ownership and lifetime management**. The reason it comes up in the context
    of performance is that often excessive copying is a side effect of muddled ownership,
    a workaround for data disappearing while it''s still being used because the lifetime
    of many pieces of the complex system is not well-understood.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A very different set of issues needs to be managed in distributed programs,
    client-server applications, or, generally, any interface between components where
    bandwidth constraints matter. In these situations, data compression is often used:
    we trade CPU time for bandwidth because it costs processing time to compress and
    decompress the data, but the transmission is going to be faster. Often, the decision
    of whether to compress the data in a particular channel cannot be made at the
    design time: we simply don''t know enough to make an informed trade-off. Thus,
    it is important to design the system to allow for the possibility of compression.
    This has some non-trivial implications for designing the interfaces of the data
    structures that may be converted to a compressed format. If your design calls
    for compressing the entire set of data, transmitting it, then converting it back
    to the decompressed format, then the interfaces you use to work with the data
    do not change, but the memory requirements grow because you will have both compressed
    and uncompressed representations stored in memory at some point. The alternative
    is a data structure that stores compressed data internally, which takes some forethought
    when it comes to designing its interfaces.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, imagine that we have a simple struct for storing three-dimensional
    locations and maybe some attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'A very popular guideline says that we should avoid getter and setter methods
    that do nothing but access the corresponding data member; we are advised against
    doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We store these objects in a collection of points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This design served us fine for a while, but the requirements evolved, and now
    we have to store and transmit millions of points. It is hard to imagine how we
    might introduce internal compression with this interface: the index operator returns
    a reference to an object that must have three `double` data members accessible
    directly. If we had getters and setters, we might have been able to implement
    the point as a proxy to a compressed set of points inside the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The collection stores compressed data and can decompress parts of it on the
    fly to get access to the point identified by the `point_id_`.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, an even more compression-friendly interface would be one that requires
    us to iterate over the entire collection of points sequentially. Now you should
    realize that we just revisited the guideline that instructs us to reveal as little
    information as possible about the internal workings of our collection. The focus
    on compression serves to provide us with a particular point of view. If you think
    about the possibility of data compression, or, generally, alternative data representations
    for storage and transmission, you have to also think about restricting access
    to this data. Maybe you can come up with algorithms that do all the required computations
    without using random access to the data? If you limit access by design, you preserve
    the possibility of compressing the data (or taking advantage of the limited access
    pattern in some other way).
  prefs: []
  type: TYPE_NORMAL
- en: There are other types of interfaces, of course, and they all have their own
    runtime, memory, and storage space costs associated with transmitting large volumes
    of data. When designing for performance, consider the possibility that these costs
    will become performance-critical and try to **limit the interfaces for maximum
    freedom of internal data representation**. Of course, this, like anything else,
    should be practiced within reason; it is highly unlikely that a hand-written configuration
    file will ever become a performance bottleneck (computers read faster than you
    write, in any format).
  prefs: []
  type: TYPE_NORMAL
- en: We have touched on the matter of data layout as it affects the interface design.
    Let us now focus directly on the performance impact of data organization.
  prefs: []
  type: TYPE_NORMAL
- en: Design for optimal data access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed the impact of data organization on performance in detail in [*Chapter
    4*](B16229_04_Epub_AM.xhtml#_idTextAnchor064), *Memory Architecture and Performance*.
    There, we observed that whenever you have no "hot code," you will usually find
    "hot data." In other words, if the runtime is spread over a large part of the
    code and nothing stands out as a good optimization opportunity, it is likely that
    there is some data (one or more data structures) that is being accessed throughout
    the program, and it is these accesses that limit the overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be a very unpleasant situation to find oneself in: the profiler shows
    no low-hanging fruit for optimization, you may find some sub-optimal code, but
    the measurements show that you can save at most a percent or two of total runtime
    from each of these places. Unless you know what to look for, it is very hard to
    find ways to improve the performance of such code.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know that you need to look for "hot data," how do you do it? First
    of all, it is much easier if all data accesses are done through function calls
    and not by directly reading and writing public data members. Even if these accessor
    functions do not take much time themselves, you can instrument them to count the
    access operations, which will directly show which data is hot. This approach is
    similar to code profiling, only instead of finding instructions that are executed
    many times, you find memory locations that are accessed many times (some profiles
    will do such measurements for you without the need to instrument the code). Once
    again, we come back to the design guideline that prescribes clearly defined interfaces
    that do not expose internal details such as data layout in memory – the ability
    to easily monitor data access is another benefit of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should point out that every design concerns itself with both the organization
    of code (components, interfaces, and so on) and the organization of data. You
    may not be thinking about the specific data structures yet, but you absolutely
    must consider data flows: every component needs some information to do its work.
    Which parts of the system generate this information, who owns it, who is responsible
    for delivering it to the component or module where it is needed? The computations
    usually produce some new information. Again, where should it be delivered, and
    who will own it? Every design includes such data flow analysis: if you think that
    you don''t have it, you are doing it implicitly through the documentation of the
    interfaces. The information flow and its ownership can be inferred from the totality
    of the API contracts, but this is a rather convoluted way of going about it.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you explicitly describe the information flow, you know what data is present
    at every step of the execution and is accessed by every component. You also know
    what data must be transferred between components. You can now think about ways
    to organize this data.
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches you can take at the design stage when it comes to data
    organization. One approach is to rely on the interfaces to provide an abstract
    view of the data while concealing all details about its true organization. This
    is our very first guideline from the beginning of this chapter, the minimum information
    principle, taken to the extreme. If it works, you can implement optimizing the
    data structures behind the interfaces later as needed. The caveat is that it is
    rarely possible to design an interface that does not restrict the underlying data
    organization in any way, and doing so usually comes at a high cost. For example,
    if you have an ordered collection of data, do you want to allow insertions in
    the middle of the collection? If the answer is yes, the data will not be stored
    in an array-like structure that requires moving half the elements to open up a
    space in the middle (a restriction on the implementation). On the other hand,
    if you steadfastly refuse to allow any interface that limits your implementation,
    you will end up with a very limited interface and may be unable to use the fastest
    algorithms (the cost of not committing to a particular data organization early).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach is to consider at least some of the data organization as
    a part of the design. This will reduce the flexibility of the implementation but
    will relax some of the restrictions on the interface design. For example, you
    may decide that, in order to access the data in a particular order, you will use
    an index that points to the locations where the data elements are stored. You
    will embed the cost of the indirect access into the foundation of your system
    architecture, but you gain the flexibility of data access: the elements can be
    stored optimally, and the right index can be constructed for any kind of random
    or ordered access. Our `index_tree` is a trivial example of such a design.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we had to use some pretty low-level concepts when discussing how the
    data organization is designed for performance. Usually, the details like "access
    through an extra pointer" are seen as implementation matters. But when designing
    high-performance systems, you have to be concerned with things like cache locality
    and indirect references.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best results are usually obtained through combining both approaches: you
    identify the most important data and come up with an efficient organization. Not
    in every detail, of course, but in general, for example, if your program, at its
    basic level, searches a lot of strings many times, you may decide to store all
    strings in a large, contiguous block of memory and use indices for searches and
    other targeted accesses. You would then design a high-level interface to build
    an index and use it through iterators, but the exact organization of such index
    is left to the implementation. Your interface imposes some restrictions: for example,
    you may decide that the caller may request random access or bidirectional iterators
    when building the index, which would, in turn, affect the implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The design of concurrent systems requires extra attention to the sharing of
    data. At the design stage, you should pay particular attention to classifying
    the data as not shared, read-only, or shared for writing. The latter should be
    minimized, of course: as we have seen in [*Chapter 6*](B16229_06_Epub_AM.xhtml#_idTextAnchor103),
    *Concurrency and Performance*, accessing shared data is expensive. On the other
    hand, redesigning a component or a data structure that was intended for exclusive
    single-threaded access to be thread-safe is difficult and often results in poor
    performance (it is hard to graft thread safety on top of a fundamentally unsafe
    design). You should spend time at the design stage during the data flow analysis
    to clearly define data ownership and access restrictions. Since the words "data
    ownership" often refer to very low-level details such as "do we use a smart pointer
    and which class has it?," it may be preferable to talk about information ownership
    and access to information. Identify the pieces of information that must be available
    together, determine which component produces and owns the information, which components
    modify some of the information, and whether or not it is done concurrently. **The**
    **design should include a high-level classification of all data by its access:
    single-threaded (exclusive), read-only, or shared**. Note that these roles could
    change in time: some data could be produced by a single thread but later read,
    without modifications, by multiple threads at once. This should be reflected in
    the design as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The overall guideline to **treat the flow of data, or the flow of knowledge,
    as a part of the design** is often forgotten but is otherwise quite straightforward.
    It is the more specific guideline to **consider the combination of data organization
    restrictions and interfaces that leave significant implementation freedom** during
    the design that is often seen as a premature optimization. Many a programmer will
    insist that the words "cache locality" have no place during the design stage.
    This is, indeed, one of the compromises we have to make when we treat performance
    as one of the design goals. We often have to weigh such competing motivations
    during system design, which brings us to the subject of making trade-offs when
    designing for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance trade-offs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Design is often the art of compromise; there are competing goals and requirements
    that must be balanced. In this section, we are going to talk specifically about
    performance-related trade-offs. You will make many such decisions when designing
    high-performance systems. Here are some to be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Interface design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have witnessed the benefits of exposing implementation as little as possible
    throughout this chapter. But there is a tension between the freedom to optimize
    that we gain in doing so vs. the cost of very abstract interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tension requires making trade-offs between optimizing different components:
    an interface that does not restrict the implementation in any way usually limits
    the client quite severely. For example, let us revisit our collection of points.
    What can we do without restricting its implementation? We cannot allow any insertions
    except at the end (the implementation may be a vector, and copying half the collection
    is unacceptable). We can only append to the end, which means we cannot maintain
    sorted order, for example. There can be no random access (the collection may be
    stored in a list). We may be unable to provide even a reverse iterator if the
    collection is compressed. A point collection that leaves almost unlimited freedom
    to the implementer is restricted to forward iterators (streaming access) and maybe
    append operations. Even the latter is a restriction, some compression schemes
    require finalizing the data before it can be read, so the collection can be in
    a write-only state or a read-only state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not giving this example to demonstrate how rigorous pursuit of implementation-agnostic
    APIs leads to unrealistic restrictions on the clients. Quite the contrary: this
    is a valid design for processing large amounts of data. The collections are written
    by appending to the end; there is no particular order to the data until the writing
    is finalized. Finalization may include sorting and compression. To read the collection,
    we uncompress it on the fly (if our compression algorithm works on several points
    at once, we need a buffer to hold uncompressed data). If the collection must be
    edited, we can use the algorithm we first introduced in [*Chapter 4*](B16229_04_Epub_AM.xhtml#_idTextAnchor064),
    *Memory Architecture and Performance*, for memory-efficient editing or strings:
    we always read the entire collection from the beginning to the end; each point
    is modified as needed, new points are added, etc. We write the results into the
    new collection and eventually delete the original one. This design allows for
    very efficient data storage, both in terms of memory use (high compression) and
    in terms of efficient memory access (cache-friendly sequential accesses only).
    It also requires the clients to implement all their operations in terms of streaming
    access and read-modify-write operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can arrive at the same point from the other end: if you analyze your data
    access patterns and conclude that you can live with streaming access and read-modify-write
    updates, you can make that part of your design. Not a specific compression scheme,
    of course, but the high-level data organization: writing must be finalized before
    anything can be read, and the only way to alter the data is to copy the entire
    collection to a new one, modifying its content during copying as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting observation about this trade-off is that not only may we have
    to balance performance requirements against ease of use or other design considerations,
    but there is usually a decision to be made about which aspect of performance is
    more important. Usually, the low-level components should be given precedence:
    their architecture is more fundamental to the overall design than the choice of
    algorithms in higher-level components. Thus, it is harder to change later, which
    makes it more important to make an informed design decision. Note that, when it
    comes to designing components, there are other trade-offs to be made.'
  prefs: []
  type: TYPE_NORMAL
- en: Component design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just seen that sometimes for one component to have a great performance
    by design, limitations must be imposed on other components whose performance then
    requires careful choice of algorithms and skillful implementation. But this is
    not the only trade-off we have to make.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common balancing acts in design for performance is that of
    choosing the appropriate granularity level for components and modules. Making
    small components is generally a good design practice, particularly in test-driven
    design (but generally in any design that has testability as one of the goals).
    On the other hand, splitting the system into too many pieces with restricted interactions
    between them can be bad for performance. Often, treating larger units of data
    and code as single components allows for more efficient implementation. Again,
    our point collection is an example: it is more efficient if we don''t allow unrestricted
    access to point objects inside the collection.'
  prefs: []
  type: TYPE_NORMAL
- en: In the end, these decisions should be made by considering the conflicting requirements
    and taking advantage of the opportunities to resolve the contradictions. It would
    be good to have a point as a separate unit, testable and reusable in other code.
    But do we really need to expose the point collection as a collection of these
    point units? Perhaps, we can instead treat it as a collection of all the information
    contained in the points it stores, while the point object is created only for
    reading and writing points into the collection, one at a time. This approach allows
    us to retain good modularity and achieve high performance. In general, the interfaces
    are implemented in terms of clear and testable components, while internally, the
    larger components store the data in an entirely different format.
  prefs: []
  type: TYPE_NORMAL
- en: What should be avoided is creating "back doors" in the interfaces that are made
    specifically to work around the restrictions that resulted from following good
    design practices but now lead to performance limitations. This generally compromised
    both competing design goals in an ad hoc manner. Instead, it is better to redesign
    the involved components. If you don't see a way to resolve the contradicting requirements,
    erase the component boundary and make the smaller units into internal, implementation-specific
    subcomponents.
  prefs: []
  type: TYPE_NORMAL
- en: Another design aspect we have not concerned ourselves at all with so far is
    error handling, so a few words are in order.
  prefs: []
  type: TYPE_NORMAL
- en: Errors and undefined behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Error handling is one of those things that are often treated as an afterthought
    but should be an equal and important factor in design decisions. In particular,
    it is very difficult to add exception safety (and, by extension, error safety)
    to a program that was not designed with a particular exception-handling methodology
    in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'Error handling begins with the interfaces: all interfaces are essentially contracts
    that govern the interactions between components. These contracts should include
    any restrictions on the input data: a component will function as specified if
    certain external conditions are met. But the contract should also specify what
    happens if the conditions are not met and the component cannot fulfill the contract
    (or the programmer decided that it is undesirable or too difficult to do so).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Much of this error response should also be covered by the contract: if the
    specified requirements are not met, the component will report an error in a certain
    way. It could be exceptions, error codes, status flags, or a combination of other
    methods. Other books are written on the best practices of error handling. Here
    we focus on performance.'
  prefs: []
  type: TYPE_NORMAL
- en: From the performance point of view, the most important consideration is usually
    the overhead of handling a potential error in the much more common case when the
    inputs and the results are correct and nothing bad happens. It is often expressed
    simply as "error handling must be cheap."
  prefs: []
  type: TYPE_NORMAL
- en: What is meant by this is that error handling must be cheap in the normal, no-error
    case. Conversely, we usually don't care about the expense of processing errors
    when this rare event actually happens. What exactly this entails varies greatly
    from one design to the next one.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in applications that handle transactions, we usually want commit-or-rollback
    semantics: each transaction either succeeds or does nothing at all. The performance
    cost of such a design may be high, however. Often, it is acceptable to have a
    failed transaction still affect some changes, as long as these changes do not
    change the primary invariants of the system. For a disk-based database, it may
    be acceptable to waste some space on disk; then, we can always allocate the space
    for the transaction and write to the disk, but, in case of error, we leave this
    partially written region inaccessible to the user.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In such cases where we "hide" the full consequences of an error to improve
    performance, it is good to design a separate mechanism to clean such aftereffects
    of errors. For our database, such cleanup can proceed in a separate background
    process with low priority to avoid interfering with the primary accesses. Again,
    this is an example of resolving contradictions by separating them in time: if
    we have to recover from errors but it is too expensive to do so, do the expensive
    part later.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have to consider the possibility that even detecting a contract
    violation is too expensive in some cases. [*Chapter 11*](B16229_11_Epub_AM.xhtml#_idTextAnchor176),
    *Undefined Behavior and Performance*, covered this scenario. The interface contract
    should clearly state that if certain restrictions are violated, the results are
    undefined. If you choose this approach, do not make the program spend time making
    the undefined results more "acceptable." Undefined means undefined; anything can
    happen. This should not be done lightly, and you should consider alternatives
    such as lightweight data collection that leaves the expensive work to the code
    path that handles the real errors when they occur. But being clear about the contract
    boundaries and undefined outcomes is preferable to uncertain alternatives along
    the lines of "we will do the best we can, but no promises."
  prefs: []
  type: TYPE_NORMAL
- en: There are many trade-offs that have to be made during the design stage, and
    this chapter is not meant to be a complete list of trade-offs or an all-encompassing
    guide to achieving balance. Instead, we show several commonly occurring contradictions
    and the possible approaches to resolving them.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make informed decisions when balancing performance design goals
    against other targets and motivations, it is important to have some performance
    estimates. But how do we get performance metrics so early in the design stage?
    This is the last and, in some ways, the hardest part of designing for performance
    that we are yet to discuss.
  prefs: []
  type: TYPE_NORMAL
- en: Making informed design decisions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is not only when making decisions about trade-offs that we have to stand
    of the firm foundation of good performance data. After all, how can we make decisions
    about designing data structures for efficient memory access if we do not know
    how much it costs to access data in a cache-optimal order as opposed to some random
    order? This comes back to the first rule of performance, which you should have
    memorized by now: never guess about performance. This is easier said than done
    if our program exists as a scattering of design diagrams on a whiteboard.'
  prefs: []
  type: TYPE_NORMAL
- en: You can't run a design, so how do you get measurements to guide and back up
    your design decisions? Some of the knowledge comes with experience. By this, I
    don't mean the kind of experience that says, "we have always done it this way."
    But you may have designed and implemented similar components and other parts of
    the new system. If they are reusable, they come with reliable performance information.
    But even if you have to modify them or design something similar, you have highly
    relevant performance measurements that likely transfer well to the new design.
  prefs: []
  type: TYPE_NORMAL
- en: 'What should we do, then, if we have no relevant programs that can be used to
    measure performance? This is when we have to fall back on models and prototypes.
    Models are artificial constructs that mimic the expected workload and performance
    of some parts of our future program, to the best of our knowledge. For example,
    if we have to make a decision about organizing large amounts of data in memory
    and we know that we will have to frequently process the entire data corpus, our
    micro-benchmarks from [*Chapter 4*](B16229_04_Epub_AM.xhtml#_idTextAnchor064),
    *Memory Architecture and Performance*, are the kind of model you might use: process
    the same volume of data organized as a list vs. an array. This is a model, not
    an exact measurement of your future program''s performance, but it provides valuable
    insight and gives you good data to support your decisions. Just remember that
    the more approximate the model is, the more inaccurate the predictions are: if
    you model two alternative designs and come up with performance measurements within
    10% of each other, you should probably consider it a wash. By the way, this does
    not make it a waste: you obtained important information, both design options offer
    similar performance, so you are free to choose based on other criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all models are micro-benchmarks. Sometimes you can use existing programs
    to model new behavior. Say you have a distributed program that operates on some
    data similar to what your next program needs to deal with. The new program will
    have much more data, and the similarity is only superficial (maybe both programs
    work on strings), so the old program cannot be used to do any real measurements
    of handling the new data. No matter: we can modify the code to send and receive
    much longer strings. What if our existing program makes no use of them? That''s
    ok, too: we will write some code to generate and consume these strings in a somewhat
    realistic manner and embed it in the program. Now we can fire up the part of the
    program that does the distributed computations and see how long it takes to send
    and receive the expected volumes of data. Let''s assume it takes long enough that
    we are considering compression. We can do better than that, though: add compression
    to the code and compare network transfer speedup with compression and decompression
    costs. If you don''t want to invest a lot of time writing a realistic compression
    algorithm for your specific data, try reusing an existing compression library.
    Comparing several compression algorithms from freely available libraries will
    give you even more valuable data for a later time when you have to decide how
    much compression is optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note carefully what we have just done: we used an existing program as a framework
    to run some new code that approximates the behavior of the future program. In
    other words, we have constructed a prototype. Prototyping is another way to get
    performance estimates for making design decisions. Of course, building prototypes
    for performance is somewhat different from making feature-based prototypes. In
    the latter case, we want to quickly put together a system that demonstrates the
    desired behavior, usually with no regard for the performance or quality of the
    implementation. A performance prototype should give us reasonable performance
    numbers, so the low-level implementation must be efficient. We can neglect corner
    cases and error handling. We can also skip many features as long as the ones we
    prototype do exercise the code we want to benchmark. Sometimes, our prototype
    will have no features at all: instead, somewhere in the code, we will hard-code
    a condition that in a real system happens when certain features are exercised.
    The high-performance code we have to create during such prototyping often forms
    the foundation of our low-level libraries later.'
  prefs: []
  type: TYPE_NORMAL
- en: It should be pointed out that all models are approximate, and they would still
    be approximate even if you had a complete and final implementation for the code
    whose performance you are trying to measure. The micro-benchmarks are, generally,
    less accurate than larger frameworks, which gives rise to catchy titles like "micro-benchmarks
    are lies." The main reason the micro-benchmarks and other performance models do
    not always match the eventual results is that any program's performance is affected
    by its environment. For example, you may benchmark a piece of code for optimal
    memory access, only to find out that it's usually running alongside other threads
    that completely saturate the memory bus.
  prefs: []
  type: TYPE_NORMAL
- en: Just like it's important to understand the limitations of the models, it is
    also important to not over-react. Benchmarks do provide useful information. The
    more complete and realistic the measured software is, the more accurate the results
    are. If the benchmark shows one piece of code several times faster than the other,
    this difference is unlikely to disappear completely once the code is running in
    its final context. But it would be a folly to try to get the last 5% of efficiency
    from anything other than the final version of the code running on the real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prototypes – approximations for the real programs that reproduce with some
    degree of accuracy the properties we are interested in – allow us to get reasonable
    estimates of performance that would follow from different design decisions. They
    can range from micro-benchmarks to experiments on large, preexisting programs,
    but they all serve one goal: move design for performance from the realm of guesswork
    to the foundation of sound measurement-driven decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last chapter of our book reviews everything we learned about the performance
    and what determines it, then uses this knowledge to come up with design guidelines
    for high-performance software systems. We have offered several recommendations
    for designing interfaces, data organization, components, and modules and described
    ways to make design decisions informed with good measurement results before we
    have an implementation whose performance can be measured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we must emphasize that design for performance does not automatically
    yield good performance: it allows for the possibility of a high-performing implementation.
    The alternative is a performance-hostile design that locks in decisions constraining
    and preventing efficient code and data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This book has been a journey: we started by learning about the performance
    of individual hardware components, then studied their interactions with each other
    and how they influence our use of programming languages. This path led us, at
    last, to the idea of design for performance. This is the last chapter in the book,
    but not the last step on your journey: now comes the wide and exciting field of
    applying your knowledge to practical problems that await you.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is design for performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we make sure that the interfaces do not restrict optimal implementation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do interfaces that communicate the client's intent allow for better performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we make informed performance-related design decisions when we have no
    performance measurements?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

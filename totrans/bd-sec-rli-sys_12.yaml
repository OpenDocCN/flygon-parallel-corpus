- en: Chapter 8\. Design for Resilience
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。弹性设计
- en: By Vitaliy Shipitsyn, Mitch Adler, Zoltan Egyed, and Paul Blankinship
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 维塔利·希皮茨因、米奇·阿德勒、佐尔坦·埃吉德和保罗·布兰金希普
- en: with Jesus Climent, Jessie Yang, Douglas Colish, and Christoph Kern
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与耶稣·克利门特、杰西·杨、道格拉斯·科利什和克里斯托夫·科恩一起
- en: As a part of system design, “resilience” describes the system’s ability to hold
    out against a major malfunction or disruption. Resilient systems can recover automatically
    from failures in parts of the system—or possibly the failure of the entire system—and
    return to normal operations after the problems are addressed. Services in a resilient
    system ideally remain running throughout an incident, perhaps in a degraded mode.
    Designing resilience into every layer of a system’s design helps defend that system
    against unanticipated failures and attack scenarios.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为系统设计的一部分，“弹性”描述了系统抵抗重大故障或中断的能力。具有弹性的系统可以自动从系统部分的故障中恢复，甚至可能是整个系统的故障，并在问题得到解决后恢复正常运行。弹性系统中的服务理想情况下在事故期间始终保持运行，可能是以降级模式。在系统设计的每一层中设计弹性有助于防御系统不可预期的故障和攻击场景。
- en: Designing a system for resilience is different from designing for recovery (covered
    in depth in [Chapter 9](ch09.html#design_for_recovery)). Resilience is closely
    tied to recovery, but while recovery focuses on the ability to fix systems *after*
    they break, resilience is about designing systems that *delay* or *withstand*
    breakage. Systems designed with a focus on both resilience and recovery are better
    able to recover from failures, and require minimal human intervention.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为弹性设计系统与为恢复设计系统有所不同（在[第9章](ch09.html#design_for_recovery)中深入讨论）。弹性与恢复密切相关，但是恢复侧重于在系统*发生*故障后修复系统的能力，而弹性是关于设计*延迟*或*承受*故障的系统。注重弹性和恢复的系统更能够从故障中恢复，并且需要最少的人为干预。
- en: Design Principles for Resilience
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性设计原则
- en: A system’s resilience properties are built on the design principles discussed
    earlier in [Part II](part02.html#designing_systems). In order to evaluate a system’s
    resilience, you must have a good understanding of how that system is designed
    and built. You need to align closely with other design qualities covered in this
    book—least privilege, understandability, adaptability, and recovery—to strengthen
    your system’s stability and resilience attributes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的弹性属性建立在本书[第II部分](part02.html#designing_systems)中讨论的设计原则之上。为了评估系统的弹性，您必须对系统的设计和构建有很好的了解。您需要与本书中涵盖的其他设计特性密切配合——最小特权、可理解性、适应性和恢复——以加强系统的稳定性和弹性属性。
- en: 'The following approaches, each of which this chapter explores in depth, characterize
    a resilient system:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法是本章深入探讨的，它们表征了一个具有弹性的系统：
- en: Design each layer in the system to be independently resilient. This approach
    builds defense in depth with each layer.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计系统的每一层都具有独立的弹性。这种方法在每一层中构建了深度防御。
- en: Prioritize each feature and calculate its cost, so you understand which features
    are critical enough to attempt to sustain no matter how much load the system experiences,
    and which features are less important and can be throttled or disabled when problems
    arise or resources are constrained. You can then determine where to apply the
    system’s limited resources most effectively, and how to maximize the system’s
    serving capabilities.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑每个功能并计算其成本，以便了解哪些功能足够关键，可以尝试在系统承受多大负载时维持，哪些功能不那么重要，可以在出现问题或资源受限时进行限制或禁用。然后确定在哪里最有效地应用系统有限的资源，以及如何最大化系统的服务能力。
- en: Compartmentalize the system along clearly defined boundaries to promote the
    independence of the isolated functional parts. This way, it’s also easier to build
    complementary defense behaviors.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将系统分隔成清晰定义的边界，以促进隔离功能部分的独立性。这样，也更容易构建互补的防御行为。
- en: Use compartment redundancy to defend against localized failures. For global
    failures, have some compartments provide different reliability and security properties.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隔舱冗余来防御局部故障。对于全局故障，一些隔舱提供不同的可靠性和安全性属性。
- en: Reduce system reaction time by automating as many of your resilience measures
    as you can safely. Work to discover new failure modes that could benefit either
    from new automation or improvements to existing automation.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自动化尽可能多的弹性措施来减少系统的反应时间。努力发现可能受益于新自动化或改进现有自动化的新故障模式。
- en: Maintain the effectiveness of the system by validating its resilience properties—both
    its automated response and any other resilience attributes of the system.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过验证系统的弹性属性来保持系统的有效性——包括其自动响应和系统的其他弹性属性。
- en: Defense in Depth
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度防御
- en: '*Defense in depth* protects systems by establishing multiple layers of defense
    perimeters. As a result, attackers have limited visibility into the systems, and
    successful exploits are harder to launch.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度防御*通过建立多层防御边界来保护系统。因此，攻击者对系统的可见性有限，成功利用更难发动。'
- en: The Trojan Horse
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特洛伊木马
- en: The story of the Trojan Horse, as told by Virgil in the *Aeneid*, is a cautionary
    tale about the dangers of an inadequate defense. After 10 fruitless years besieging
    the city of Troy, the Greek army constructs a large wooden horse that it presents
    as a gift to the Trojans. The horse is brought within the walls of Troy, and attackers
    hiding inside the horse burst forth, exploit the city’s defenses from the inside,
    and then open the city gates to the entire Greek army, which destroys the city.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 特洛伊木马的故事，由维吉尔在《埃涅阿斯纪》中讲述，是一个关于不足防御危险的警示故事。在围困特洛伊城十年无果之后，希腊军队建造了一匹巨大的木马，作为礼物送给特洛伊人。木马被带进特洛伊城墙内，藏在木马里的攻击者突然冲出来，从内部利用了城市的防御，然后打开城门让整个希腊军队进入，摧毁了城市。
- en: Imagine this story’s ending if the city had planned for defense in depth. First,
    Troy’s defensive forces might have inspected the Trojan Horse more closely and
    discovered the deception. If the attackers had managed to make it inside the city
    gates, they could have been confronted with another layer of defense—for example,
    the horse might have been enclosed in a secure courtyard, with no access to the
    rest of the city.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果这个城市计划了深度防御，这个故事的结局会是什么样子。首先，特洛伊的防御力量可能会更仔细地检查特洛伊木马并发现欺骗。如果攻击者设法进入城门，他们可能会面对另一层防御，例如，木马可能被封闭在一个安全的庭院里，无法进入城市的其他地方。
- en: What does a 3,000-year-old story tell us about security at scale, or even security
    itself? First, if you’re trying to understand the strategies you need to defend
    and contain a system, you must first understand the attack itself. If we consider
    the city of Troy as a system, we can walk through the attackers’ steps (stages
    of the attack) to uncover weaknesses that defense in depth might address.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个3000年前的故事告诉我们关于规模安全甚至安全本身的什么？首先，如果你试图了解你需要防御和遏制系统的策略，你必须首先了解攻击本身。如果我们把特洛伊城看作一个系统，我们可以按照攻击者的步骤（攻击的阶段）来发现深度防御可能解决的弱点。
- en: 'At a high level, we can divide the Trojan attack into four stages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们可以将特洛伊木马攻击分为四个阶段：
- en: '*Threat modeling and vulnerability discovery*—Assess the target and specifically
    look for defenses and weaknesses. The attackers couldn’t open the city gates from
    the outside, but could they open them from the inside?'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*威胁建模和漏洞发现*——评估目标并专门寻找防御和弱点。攻击者无法从外部打开城门，但他们能从内部打开吗？'
- en: '*Deployment*—Set up the conditions for the attack. The attackers constructed
    and delivered an object that Troy eventually brought inside its city walls.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*部署*——为攻击设置条件。攻击者构建并交付了一个特洛伊最终带进城墙内的物体。'
- en: '*Execution*—Carry out the actual attack, which capitalizes on the previous
    stages. Soldiers came out of the Trojan Horse and opened the city gates to let
    in the Greek army.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*执行*——执行实际的攻击，利用之前阶段的攻击。士兵们从特洛伊木马中出来，打开城门让希腊军队进入。'
- en: '*Compromise*—After successful execution of the attack, the damage occurs and
    mitigation begins.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*妥协*——在成功执行攻击后，损害发生并开始减轻。'
- en: The Trojans had opportunities to disrupt the attack at every stage before the
    compromise, and paid a heavy price for missing them. In the same way, your system’s
    defense in depth can reduce the price you might have to pay if your system is
    ever compromised.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 特洛伊人在妥协之前的每个阶段都有机会阻止攻击，并因错过这些机会而付出了沉重的代价。同样，你系统的深度防御可以减少如果你的系统被攻击的话可能需要付出的代价。
- en: Threat modeling and vulnerability discovery
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 威胁建模和漏洞发现
- en: Attackers and defenders can both assess a target for weaknesses. Attackers perform
    reconnaissance against their targets, find weaknesses, and then model attacks.
    Defenders should do what they can to limit the information exposed to attackers
    during reconnaissance. But because defenders can’t completely prevent this reconnaissance,
    they must detect it and use it as a signal. In the case of the Trojan Horse, the
    defenders might have been on the alert because of inquiries from strangers about
    how the gates were defended. In light of that suspicious activity, they would
    have then exercised extra caution when they found a large wooden horse at the
    city gate.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者和防御者都可以评估目标的弱点。攻击者对他们的目标进行侦察，找到弱点，然后模拟攻击。防御者应该尽力限制在侦察期间向攻击者暴露的信息。但是因为防御者无法完全阻止这种侦察，他们必须检测到它并将其用作信号。在特洛伊木马的情况下，防御者可能会因为陌生人询问城门的防御方式而保持警惕。鉴于这种可疑活动，当他们在城门口发现一个大木马时，他们会更加谨慎。
- en: 'Making note of these strangers’ inquiries amounts to gathering intelligence
    on threats. There are many ways to do this for your own systems, and you can even
    choose to outsource some of them. For example, you might do the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些陌生人的询问相当于收集威胁情报。有许多方法可以为你自己的系统做到这一点，你甚至可以选择外包其中的一些。例如，你可以做以下事情：
- en: Monitor your system for port and application scans.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视你的系统进行端口和应用程序扫描。
- en: Keep track of DNS registrations of URLs similar to yours—an attacker might use
    those registrations for spear phishing attacks.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪类似你的URL的DNS注册情况——攻击者可能会利用这些注册进行钓鱼攻击。
- en: Buy threat intelligence data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买威胁情报数据。
- en: Build a threat intelligence team to study and passively monitor the activities
    of known and likely threats to your infrastructure. While we don’t recommend that
    small companies invest resources in this approach, it may become cost-effective
    as your company grows.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个威胁情报团队来研究和被动监视已知和可能对你基础设施构成威胁的活动。虽然我们不建议小公司投入资源进行这种方法，但随着公司的发展，这可能会变得具有成本效益。
- en: 'As a defender with inside knowledge of your system, your assessment can be
    more detailed than the attacker’s reconnaissance. This is a critical point: if
    you understand your system’s weaknesses, you can defend against them more efficiently.
    And the more you understand the methods that attackers are currently using or
    are capable of exploiting, the more you amplify this effect. A word of caution:
    beware of developing blind spots to attack vectors you consider unlikely or irrelevant.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对你系统内部了解的防御者，你的评估可以比攻击者的侦察更详细。这是一个关键点：如果你了解你系统的弱点，你可以更有效地防御它们。而且你了解攻击者目前正在使用或有能力利用的方法越多，你就越能放大这种效果。一个警告：要小心对你认为不太可能或不相关的攻击向量产生盲点。
- en: Deployment of the attack
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击部署
- en: If you know that attackers are performing reconnaissance against your system,
    efforts to detect and stop the attack are critical. Imagine that the Trojans had
    decided not to permit the wooden horse to enter the city gates because it was
    created by someone they did not trust. Instead, they might have thoroughly inspected
    the Trojan Horse before allowing it inside, or perhaps they might have just set
    it on fire.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道攻击者正在对你的系统进行侦察，那么检测和阻止攻击的努力就至关重要。想象一下，如果特洛伊人决定不允许木马进入城门，因为它是由他们不信任的人创建的。相反，他们可能会在允许它进入之前彻底检查特洛伊木马，或者可能会将其点燃。
- en: In modern times, you can detect potential attacks using network traffic inspection,
    virus detection, software execution control, protected sandboxes,^([1](ch08.html#ch08fn1))
    and proper provisioning of privileges for signaling anomalous use.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代，你可以使用网络流量检查、病毒检测、软件执行控制、受保护的沙箱^([1](ch08.html#ch08fn1))和适当的特权配置来检测潜在的攻击。
- en: Execution of the attack
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击的执行
- en: If you can’t prevent all deployments from adversaries, you need to limit the
    blast radius of potential attacks. If the defenders had boxed in the Trojan Horse,
    thereby limiting their exposure, the attackers would have had a much harder time
    advancing from their hiding spot unnoticed. Cyberwarfare refers to this tactic
    (described in more detail in [“Runtime layers”](#runtime_layers)) as *sandboxing*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法阻止对手的所有部署，你需要限制潜在攻击的影响范围。如果防御者将特洛伊木马圈起来，从而限制了他们的暴露，攻击者将会更难从他们的藏身之处不被察觉地前进。网络战将这种策略称为*沙盒化*（在[“运行时层”](#runtime_layers)中有更详细的描述）。
- en: Compromise
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 妥协
- en: When the Trojans woke to find their enemies standing over their beds, they knew
    their city had been compromised. This awareness came well after the actual compromise
    occurred. Many unfortunate banks faced a similar situation in 2018 after their
    infrastructure was polluted by [EternalBlue](https://oreil.ly/wNI2u) and [WannaCry](https://oreil.ly/irovS).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当特洛伊人醒来发现敌人站在他们的床边时，他们知道他们的城市已经被妥协了。这种意识是在实际妥协发生之后才出现的。许多不幸的银行在2018年面临了类似的情况，因为他们的基础设施被[EternalBlue](https://oreil.ly/wNI2u)和[WannaCry](https://oreil.ly/irovS)污染了。
- en: How you respond from this point forward determines how long your infrastructure
    remains compromised.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何从这一点做出回应，将决定你的基础设施被妥协的时间有多长。
- en: Google App Engine Analysis
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google App Engine分析
- en: 'Let’s consider defense in depth as applied to a more modern case: Google App
    Engine. Google App Engine allows users to host application code, and to scale
    as load increases without managing networks, machines, and operating systems.
    [Figure 8-1](#a_simplified_view_of_google_app_engine) shows a simplified architecture
    diagram of App Engine in its early days. Securing the application code is a developer’s
    responsibility, while securing the Python/Java runtime and the base OS is Google’s
    responsibility.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑深度防御如何应用到一个更现代的案例：Google App Engine。Google App Engine允许用户托管应用程序代码，并在负载增加时进行扩展，而无需管理网络、机器和操作系统。[图8-1](#a_simplified_view_of_google_app_engine)显示了App
    Engine早期的简化架构图。保护应用程序代码是开发者的责任，而保护Python/Java运行时和基本操作系统是Google的责任。
- en: '![A simplified view of Google App Engine architecture](assets/bsrs_0801.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Google App Engine架构的简化视图](assets/bsrs_0801.png)'
- en: Figure 8-1\. A simplified view of Google App Engine architecture
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1. Google App Engine架构的简化视图
- en: The original implementation of Google App Engine required special process isolation
    considerations. At that time Google used traditional POSIX user isolation as its
    default strategy (through distinct user processes), but we decided that running
    each user’s code in an independent virtual machine was too inefficient for the
    level of planned adoption. We needed to figure out how to run third-party, untrusted
    code in the same way as any other job within Google’s infrastructure.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Google App Engine的原始实现需要特殊的进程隔离考虑。当时，Google使用传统的POSIX用户隔离作为默认策略（通过不同的用户进程），但我们决定在计划的采用程度上，将每个用户的代码运行在独立的虚拟机中效率太低。我们需要找出如何以与Google基础设施中的任何其他作业相同的方式运行第三方、不受信任的代码。
- en: Risky APIs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险的API
- en: 'Initial threat modeling for App Engine turned up a few worrisome areas:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: App Engine的初始威胁建模发现了一些令人担忧的领域：
- en: Network access was problematic. Up to that point, all applications running within
    the Google production network were assumed to be trusted and authenticated infrastructure
    components. Since we were introducing arbitrary, untrusted third-party code into
    this environment, we needed a strategy to isolate internal APIs and network exposure
    from App Engine. We also needed to bear in mind that App Engine itself was running
    on that same infrastructure, and therefore was dependent on access to those same
    APIs.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络访问存在问题。在那之前，所有在Google生产网络中运行的应用程序都被认为是受信任的和经过身份验证的基础设施组件。由于我们在这个环境中引入了任意的、不受信任的第三方代码，我们需要一种策略来将App
    Engine的内部API和网络暴露与其隔离开。我们还需要记住，App Engine本身是运行在同一基础设施上的，因此依赖于对这些API的访问。
- en: The machines running user code required access to the local filesystem. At least
    this access was limited to the directories belonging to the given user, which
    helped protect the execution environment and reduce the risk of user-provided
    applications interfering with applications of other users on the same machine.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行用户代码的机器需要访问本地文件系统。至少这种访问被限制在属于特定用户的目录中，这有助于保护执行环境，并减少用户提供的应用程序对同一台机器上其他用户的应用程序的干扰的风险。
- en: The Linux kernel meant that App Engine was exposed to a large attack surface,
    which we wanted to minimize. For example, we wanted to prevent as many classes
    of local privilege escalation as possible.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux内核意味着App Engine暴露在了大规模攻击的表面上，我们希望将其最小化。例如，我们希望尽可能防止许多本地权限提升的类别。
- en: To address these challenges, we first examined limiting user access to each
    API. Our team removed built-in APIs for I/O operations for networking and filesystem
    interactions at runtime. We replaced the built-in APIs with “safe” versions that
    made calls to other cloud infrastructure, rather than directly manipulating the
    runtime environment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: To prevent users from reintroducing the intentionally removed capabilities to
    the interpreters, we didn’t allow user-supplied compiled bytecode or shared libraries.
    Users had to depend on the methods and libraries we provided, in addition to a
    variety of permitted runtime-only open source implementations that they might
    need.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Runtime layers
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also extensively audited the runtime base data object implementations for
    features that were likely to produce memory corruption bugs. This audit produced
    a handful of upstream bug fixes in each of the runtime environments we launched.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: We assumed that at least some of these defensive measures would fail, as we
    weren’t likely to find and predict every exploitable condition in the chosen runtimes.
    We decided to specifically adapt the Python runtime to compile down to Native
    Client (NaCL) bitcode. NaCL allowed us to prevent many classes of memory corruption
    and control-flow subversion attacks that our in-depth code auditing and hardening
    missed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: We weren’t completely satisfied that NaCL would contain all risky code breakouts
    and bugs in their entirety, so we added a second layer of `ptrace` sandboxing
    to filter and alert on unexpected system calls and parameters. Any violations
    of these expectations immediately terminated the runtime and dispatched alerts
    at high priority, along with logs of relevant activity.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Over the next five years, the team caught a few cases of anomalous activity
    resulting from exploitable conditions in one of the runtimes. In each case, our
    sandbox layer gave us a significant advantage over attackers (whom we confirmed
    to be security researchers), and our multiple layers of sandboxing contained their
    activities within the design parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Functionally, the Python implementation in App Engine featured the sandboxing
    layers shown in [Figure 8-2](#python_implementation_in_app_engine).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![Sandboxing layers of Python implementation in App Engine](assets/bsrs_0802.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Sandboxing layers of Python implementation in App Engine
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: App Engine’s layers are complementary, with each layer anticipating the weak
    points or likely failures of the previous one. As defense activations move through
    the layers, signals of a compromise become stronger, allowing us to focus efforts
    on probable attacks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Although we took a thorough and layered approach to security for Google App
    Engine, we still benefited from external help in securing the environment.^([2](ch08.html#ch08fn2))
    In addition to our team finding anomalous activity, external researchers discovered
    several cases of exploitable vectors. We’re grateful to the researchers who found
    and disclosed the gaps.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Degradation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing for defense in depth, we assume that system components or even
    entire systems can fail. Failures can happen for many reasons, including physical
    damage, a hardware or network malfunction, a software misconfiguration or bug,
    or a security compromise. When a component fails, the impact may extend to every
    system that depends on it. The global pool of similar resources also becomes smaller—for
    example, disk failures reduce overall storage capacity, network failures reduce
    bandwidth and increase latency, and software failures reduce the computational
    capacity system-wide. The failures might compound—for example, a storage shortage
    could lead to software failures.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Resource shortages like these, or a sudden spike in incoming requests like those
    caused by the [Slashdot effect](https://oreil.ly/Z1UL8), misconfiguration, or
    a denial-of-service attack, could lead to system overload. When a system’s load
    exceeds its capacity, its response inevitably begins to degrade, and that can
    lead to a completely broken system with no availability. Unless you’ve planned
    for this scenario in advance, you don’t know where the system may break—but this
    will most likely be where the system is weakest, and not where it’s safest.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源短缺，或者像[Slashdot效应](https://oreil.ly/Z1UL8)所引起的突然请求激增，错误配置，或者拒绝服务攻击，都可能导致系统超载。当系统负载超过其容量时，其响应必然开始退化，这可能导致一个完全破碎的系统，没有可用性。除非您事先计划了这种情况，否则您不知道系统可能会在哪里崩溃，但这很可能是系统最薄弱的地方，而不是最安全的地方。
- en: 'To control degradation, you must select which system properties to disable
    or adjust when dire circumstances arise, while doing all you can to protect the
    system’s security. If you *deliberately* design multiple response options for
    circumstances like these, the system can make use of controlled breakpoints, rather
    than experiencing a chaotic collapse. Instead of triggering cascading failures
    and dealing with the mayhem that follows, your system can respond by *degrading
    gracefully*. Here are some ways you can make that happen:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制退化，当出现严重情况时，您必须选择禁用或调整哪些系统属性，同时尽一切可能保护系统的安全性。如果您*故意*为这些情况设计多个响应选项，系统可以利用受控的断点，而不是经历混乱的崩溃。您的系统可以通过*优雅地退化*来响应，而不是触发级联故障并处理随之而来的混乱。以下是一些实现这一目标的方法：
- en: Free up resources and decrease the rate of failed operations by disabling infrequently
    used features, the least critical functions, or high-cost service capabilities.
    You can then apply the freed resources to preserving important features and functions.
    For example, most systems that accept TLS connections support both Elliptic Curve
    (ECC) and RSA cryptosystems. Depending on your system’s implementation, one of
    the two will be cheaper while giving you comparable security. In software, ECC
    is less resource-intensive for private key operations.^([3](ch08.html#ch08fn3))
    Disabling support for RSA when systems are resource-constrained will make room
    for more connections at the lower cost of ECC.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过禁用不经常使用的功能、最不重要的功能或高成本的服务功能，释放资源并减少失败操作的频率。然后，您可以将释放的资源应用于保留重要功能和功能。例如，大多数接受TLS连接的系统都支持椭圆曲线（ECC）和RSA加密系统。根据您系统的实现，其中一个将更便宜，同时提供可比较的安全性。在软件中，ECC对私钥操作的资源消耗较少。^([3](ch08.html#ch08fn3))当系统资源受限时，禁用对RSA的支持将为ECC的更低成本提供更多连接空间。
- en: 'Aim for system response measures to take effect quickly and automatically.
    This is easiest with servers under your direct control, where you can arbitrarily
    toggle operational parameters of any scope or granularity. User clients are harder
    to control: they have long rollout cycles because client devices may postpone
    or be unable to receive updates. Additionally, the diversity of client platforms
    increases the chance of rollbacks of response measures due to unanticipated incompatibilities.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是使系统响应措施能够快速自动地生效。这在您直接控制的服务器上最容易，您可以任意切换任何范围或粒度的操作参数。用户客户端更难控制：它们具有较长的发布周期，因为客户端设备可能推迟或无法接收更新。此外，客户端平台的多样性增加了由于意外不兼容性而导致响应措施回滚的机会。
- en: Understand which systems are critical for your company’s mission as well as
    their relative importance and interdependencies. You might have to preserve the
    minimal features of these systems in proportion to their relative value. For example,
    Google’s Gmail has a “simple HTML mode” that disables fancy UI styling and search
    autocompletion but allows users to continue opening mail messages. Network failures
    limiting bandwidth in a region could deprioritize even this mode if that allowed
    network security monitoring to continue to defend user data in the region.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解哪些系统对公司的使命至关重要，以及它们的相对重要性和相互依赖性。您可能需要按照它们的相对价值保留这些系统的最小功能。例如，谷歌的Gmail有一个“简单的HTML模式”，它禁用了花哨的UI样式和搜索自动完成，但允许用户继续打开邮件。如果网络故障限制了某个地区的带宽，甚至可以降低这种模式的优先级，如果这样可以让网络安全监控继续保护该地区的用户数据。
- en: If these adjustments meaningfully improve the system’s capacity to absorb load
    or failure, they provide a critical complement to all other resilience mechanisms—and
    give incident responders more time to respond. It’s better to make the essential
    and difficult choices in advance rather than when under pressure during an incident.
    Once individual systems develop a clear degradation strategy, it becomes easier
    to prioritize degradation at a larger scope, across multiple systems or product
    areas.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些调整能够显著提高系统吸收负载或故障的能力，它们将为所有其他弹性机制提供关键的补充，并为事件响应者提供更多的响应时间。最好是提前做出必要和困难的选择，而不是在事件发生时承受压力。一旦个别系统制定了明确的退化策略，就更容易在更大范围内优先考虑退化，跨多个系统或产品领域。
- en: Differentiate Costs of Failures
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分故障成本
- en: There is some cost to any failed operation—for example, a failed data upload
    from a mobile device to an application backend consumes computing resources and
    network bandwidth to set up an RPC and push some data. If you can refactor your
    flows to fail early or cheaply, you may be able to reduce or avoid some failure-related
    waste.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 任何失败操作都会有一定的成本，例如，从移动设备上传数据到应用后端的失败数据上传会消耗计算资源和网络带宽来设置RPC并推送一些数据。如果您可以重构您的流程以便早期或廉价地失败，您可能能够减少或避免一些与失败相关的浪费。
- en: 'To reason about cost of failures:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于故障成本的推理：
- en: Identify the total costs of individual operations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 识别个别操作的总成本。
- en: For example, you could collect CPU, memory, or bandwidth impact metrics during
    load testing of a particular API. Focus first on the most impactful operations—either
    by criticality or frequency—if pressed for time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以在对特定API进行负载测试期间收集CPU、内存或带宽影响指标。如果时间紧迫，首先专注于最具影响力的操作，无论是通过关键性还是频率。
- en: Determine at what stage in the operation these costs are incurred.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 确定在操作的哪个阶段产生了这些成本。
- en: You could inspect source code or use developer tools to collect introspection
    data (for example, web browsers offer tracking of request stages). You could even
    instrument the code with failure simulations at different stages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查源代码或使用开发人员工具来收集内省数据（例如，Web浏览器提供请求阶段的跟踪）。您甚至可以在不同阶段的代码中加入故障模拟。
- en: Armed with the information you gather about operation costs and failure points,
    you can look for changes that could defer higher-cost operations until the system
    progresses further toward success.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 利用您收集的有关操作成本和故障点的信息，您可以寻找可以推迟高成本操作的变化，直到系统更进一步朝着成功发展。
- en: Computing resources
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算资源
- en: The computing resources that a failing operation consumes—from the beginning
    of the operation until failure—are unavailable to any other operations. This effect
    multiplies if clients retry aggressively on failure, a scenario that might even
    lead to a cascading system failure. You can free up computing resources more quickly
    by checking for error conditions earlier in the execution flows—for example, you
    can check the validity of data access requests before the system allocates memory
    or initiates data reads/writes. [SYN cookies](https://oreil.ly/EaL2N) can let
    you avoid allocating memory to TCP connection requests originating from spoofed
    IP addresses. CAPTCHA can help to protect the most expensive operations from automated
    abuse.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从操作开始到失败期间消耗的计算资源对任何其他操作都是不可用的。如果客户端在失败时进行积极的重试，这种影响会成倍增加，甚至可能导致系统级联故障。通过在执行流程的早期检查错误条件，您可以更快地释放计算资源，例如，您可以在系统分配内存或启动数据读取/写入之前检查数据访问请求的有效性。[SYN
    cookies](https://oreil.ly/EaL2N)可以让您避免为源自伪造IP地址的TCP连接请求分配内存。CAPTCHA可以帮助保护最昂贵的操作免受自动滥用。
- en: More broadly, if a server can learn that its health is declining (for example,
    from a monitoring system’s signals), you can have the server switch into a lame-duck
    mode:^([4](ch08.html#ch08fn4)) it continues to serve, but lets its callers know
    to throttle down or stop sending requests. This approach provides better signals
    to which the overall environment can adapt, and simultaneously minimizes resources
    diverted to serving errors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，如果服务器可以得知其健康状况正在下降（例如，来自监控系统的信号），您可以让服务器切换到“残废鸭”模式：它继续提供服务，但让其调用者知道要减少或停止发送请求。这种方法提供了更好的信号，整体环境可以适应，同时最小化了用于提供错误的资源。
- en: It’s also possible for multiple instances of a server to become unused because
    of external factors. For example, the services they run could be “drained” or
    isolated because of a security compromise. If you monitor for such conditions,
    the server resources could be temporarily released for reuse by other services.
    Before you reallocate resources, however, you should be certain to secure any
    data that can be helpful for a forensic investigation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能由于外部因素，多个服务器实例变得未被使用。例如，它们运行的服务可能因安全妥协而被“排空”或隔离。如果您监视这种情况，服务器资源可以暂时释放以供其他服务重用。然而，在重新分配资源之前，您应该确保保护任何可能对法医调查有帮助的数据。
- en: User experience
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户体验
- en: The system’s interactions with the user should have an acceptable level of behavior
    in degraded conditions. An ideal system informs users that its services might
    be malfunctioning, but lets them continue to interact with parts that remain functional.
    Systems might try different connection, authentication, and authorization protocols
    or endpoints to preserve the functional state. Any data staleness or security
    risks due to failures should be clearly communicated to the users. Features that
    are no longer safe to use should be explicitly disabled.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 系统与用户的交互在降级条件下应具有可接受的行为水平。理想的系统会通知用户其服务可能出现故障，但允许他们继续与保持功能的部分进行交互。系统可能尝试不同的连接、认证和授权协议或端点以保持功能状态。由于故障造成的任何数据陈旧或安全风险应清楚地向用户传达。不再安全使用的功能应明确禁用。
- en: For example, adding an offline mode to an online collaboration application can
    preserve core functionality despite temporary loss of online storage, the ability
    to show updates from others, or integration with chat features. In a chat application
    with end-to-end encryption, users might occasionally change their encryption key
    used for protecting communications. Such an application would keep all previous
    communications accessible, because their authenticity is not affected by this
    change.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，向在线协作应用添加离线模式可以在临时丢失在线存储、显示他人更新或集成聊天功能的情况下保留核心功能。在端到端加密的聊天应用中，用户可能偶尔更改用于保护通信的加密密钥。这样的应用将保持所有先前的通信可访问，因为它们的真实性不受此更改的影响。
- en: In contrast, an example of a poor design would be a situation where the entire
    GUI becomes unresponsive because one of its RPCs to a backend has timed out. Imagine
    a mobile application designed to connect to its backends on startup in order to
    display only the freshest content. The backends could be unreachable simply because
    the device’s user disabled the connectivity intentionally; still, users would
    not see even the previously cached data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，一个糟糕的设计示例是整个GUI变得无响应，因为其后端的RPC之一已超时。想象一下，设计为在启动时连接到后端以仅显示最新内容的移动应用。后端可能无法访问，仅仅是因为设备的用户有意禁用了连接；尽管如此，用户仍然看不到以前缓存的数据。
- en: A user experience (UX) research and design effort may be required to arrive
    at a UX solution that delivers usability and productivity in a degraded mode.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要用户体验（UX）研究和设计工作，以找到在降级模式下提供可用性和生产力的UX解决方案。
- en: Speed of mitigation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减轻速度
- en: The recovery speed of a system after it fails affects the cost of that failure.
    This response time includes the time between when a human or automation makes
    a mitigating change and when the last affected instance of the component is updated
    and recovers. Avoid placing critical points of failure into components like client
    applications, which are harder to control.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 系统在失败后的恢复速度会影响失败的成本。此响应时间包括人类或自动化进行减轻变化的时间以及最后一个受影响的组件实例更新和恢复的时间。避免将关键故障点放入像客户端应用这样更难控制的组件中。
- en: Going back to the earlier example of the mobile application that initiates a
    freshness update on launch, that design choice turns connectivity to the backends
    into a critical dependency. In this situation, the initial problems are amplified
    by the slow and uncontrollable rate of application updates.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回到之前的例子，移动应用在启动时发起新鲜度更新的设计选择将连接性转变为关键依赖。在这种情况下，初始问题会因应用程序更新的缓慢和不可控速度而被放大。
- en: Deploy Response Mechanisms
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署响应机制
- en: Ideally, a system should actively respond to deteriorating conditions with safe,
    preprogrammed measures that maximize the effectiveness of the response while minimizing
    risks to security and reliability. Automated measures can generally perform better
    than humans—humans are slower to respond, may not have sufficient network or security
    access to complete a necessary operation, and aren’t as good at solving for multiple
    variables. However, humans should remain in the loop to provide checks and balances,
    and to make decisions under unforeseen or nontrivial circumstances.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，系统应该通过安全的、预先编程的措施积极应对恶化的条件，以最大限度地提高响应的效果，同时最大限度地减少对安全性和可靠性的风险。自动化措施通常比人类表现更好——人类反应较慢，可能没有足够的网络或安全访问权限来完成必要的操作，并且在解决多个变量时不如机器。然而，人类应该保持在循环中，以提供检查和平衡，并在意外或非平凡情况下做出决策。
- en: 'Let’s consider in detail managing excessive load—whether due to loss of serving
    capacity, benign traffic spikes, or even DoS attacks. Humans might not respond
    fast enough, and traffic could overwhelm servers enough to lead to cascading failures
    and an eventual global service crash. Creating a safeguard by permanently overprovisioning
    servers wastes money and doesn’t guarantee a safe response. Instead, servers should
    adjust how they respond to load based upon current conditions. You can use two
    specific automation strategies here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细考虑管理过度负载的问题——无论是由于服务能力的丧失、良性流量峰值，还是DoS攻击。人类可能反应不够快，流量可能会压倒服务器，导致级联故障和最终全局服务崩溃。通过永久超额配置服务器来创建保障会浪费金钱，并不能保证安全响应。相反，服务器应根据当前条件调整它们对负载的响应方式。在这里可以使用两种具体的自动化策略：
- en: Load shedding is done by returning errors rather than serving requests.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载放弃是通过返回错误而不是提供请求来实现的。
- en: Throttling of clients is done by delaying responses until closer to the request
    deadline.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端的限流是通过延迟响应直到接近请求截止日期来实现的。
- en: '[Figure 8-3](#complete_outage_and_a_possible_cascadin) illustrates a traffic
    spike that exceeds the capacity. [Figure 8-4](#using_load_shedding_and_throttling_to_m)
    illustrates the effects of load shedding and throttling to manage the load spike.
    Note the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#complete_outage_and_a_possible_cascadin)说明了超出容量的流量峰值。[图8-4](#using_load_shedding_and_throttling_to_m)说明了使用负载放弃和限流来管理负载峰值的效果。请注意以下内容：'
- en: The curve represents requests per second, and the area under it represents total
    requests.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线代表每秒请求，曲线下方代表总请求量。
- en: Whitespace represents traffic processed without failure.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空白表示处理流量而没有失败。
- en: The backward-slashed area represents degraded traffic (some requests failed).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反斜线区域代表受损的流量（一些请求失败）。
- en: The crosshatched areas represent rejected traffic (all requests failed).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜线区域代表被拒绝的流量（所有请求失败）。
- en: The forward-slashed area represents traffic subject to prioritization (important
    requests succeeded).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜线区域代表受优先处理的流量（重要请求成功）。
- en: '[Figure 8-3](#complete_outage_and_a_possible_cascadin) shows how the system
    might actually crash, leading to a greater impact in terms of both volume (number
    of requests lost) and time (duration of the outage extends past the traffic spike).
    [Figure 8-3](#complete_outage_and_a_possible_cascadin) also distinguishes the
    uncontrolled nature of degraded traffic (the backward-slashed area) prior to system
    crash. [Figure 8-4](#using_load_shedding_and_throttling_to_m) shows that the system
    with load shedding rejects significantly less traffic than in [Figure 8-3](#complete_outage_and_a_possible_cascadin)
    (the crosshatched area), with the rest of the traffic either processed without
    failure (whitespace area) or rejected if lower priority (forward-slashed area).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#complete_outage_and_a_possible_cascadin)显示了系统可能实际崩溃，导致在请求数量和停机时间方面产生更大的影响。[图8-3](#complete_outage_and_a_possible_cascadin)还区分了系统崩溃前的受控流量的不受控制的性质（反斜线区域）。[图8-4](#using_load_shedding_and_throttling_to_m)显示了负载放弃的系统拒绝的流量明显少于[图8-3](#complete_outage_and_a_possible_cascadin)中的流量（斜线区域），其余流量要么在没有失败的情况下被处理（空白区域），要么如果优先级较低则被拒绝（斜线区域）。'
- en: '![Complete outage and a possible cascading failure from a load spike](assets/bsrs_0803.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![完全停机和负载峰值可能引发级联故障](assets/bsrs_0803.png)'
- en: Figure 8-3\. Complete outage and a possible cascading failure from a load spike
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。完全停机和负载峰值可能引发级联故障
- en: '![Using load shedding and throttling to manage a load spike](assets/bsrs_0804.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![使用负载放弃和限流来管理负载峰值](assets/bsrs_0804.png)'
- en: Figure 8-4\. Using load shedding and throttling to manage a load spike
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。使用负载放弃和限流来管理负载峰值
- en: Load shedding
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载放弃
- en: The primary resilience objective of load shedding (described in [Chapter 22
    of the SRE book](https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/))
    is to stabilize components at maximum load, which can be especially beneficial
    for preserving security-critical functions. When the load on a component starts
    to exceed its capacity, you want the component to serve errors for all excessive
    requests rather than crashing. Crashing makes *all* of the component’s capacity
    unavailable—not just the capacity for the excess requests. When this capacity
    is gone, the load just shifts elsewhere, possibly causing a cascading failure.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 负载分担的主要弹性目标（在[SRE书籍的第22章](https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/)中描述）是将组件稳定在最大负载，这对于保护安全关键功能尤为有益。当组件的负载开始超过其容量时，您希望组件为所有过多的请求提供错误响应，而不是崩溃。崩溃会使*所有*组件的容量不可用——不仅仅是用于过多请求的容量。当这种容量消失时，负载会转移到其他地方，可能导致级联故障。
- en: Load shedding allows you to free server resources even before a server’s load
    reaches capacity, and to make those resources available for more valuable work.
    To select which requests to shed, the server needs to have notions of request
    priority and request cost. You can define a policy that determines how many of
    each request type to shed based upon request priority, request cost, and current
    server utilization. Assign request priorities based on the business criticality
    of the request or its dependents (security-critical functions should get high
    priority). You can either measure or empirically estimate request costs.^([5](ch08.html#ch08fn5))
    Either way, these measurements should be comparable to server utilization measurements,
    such as CPU and (possibly) memory usage. Computing request costs should of course
    be economical.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 负载分担允许您在服务器负载达到容量之前释放服务器资源，并使这些资源可用于更有价值的工作。为了选择要分担的请求，服务器需要具有请求优先级和请求成本的概念。您可以定义一个策略，根据请求优先级、请求成本和当前服务器利用率来确定每种请求类型要分担多少。根据请求的业务关键性或其依赖关系分配请求优先级（安全关键功能应该获得高优先级）。您可以测量或经验估计请求成本。^([5](ch08.html#ch08fn5))无论哪种方式，这些测量应该与服务器利用率测量相当，例如CPU和（可能）内存使用。当然，计算请求成本应该是经济的。
- en: Throttling
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限流
- en: Throttling (described in [Chapter 21 of the SRE book](https://landing.google.com/sre/sre-book/chapters/handling-overload/))
    indirectly modifies the client’s behavior by delaying the present operation in
    order to postpone future operations. After the server receives a request, it may
    wait before processing the request or, once it has finished processing the request,
    wait before sending the response to the client. This approach reduces the rate
    of requests the server receives from clients (if clients send requests sequentially),
    which means that you can redirect the resources saved during wait times.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 限流（在[SRE书籍的第21章](https://landing.google.com/sre/sre-book/chapters/handling-overload/)中描述）通过延迟当前操作以推迟未来操作，间接修改客户端的行为。服务器收到请求后，可能会在处理请求之前等待，或者在处理完请求后，在向客户端发送响应之前等待。这种方法减少了服务器从客户端接收的请求的速率（如果客户端按顺序发送请求），这意味着您可以在等待时间内重定向所节省的资源。
- en: Similar to load shedding, you could define policies to apply throttling to specific
    offending clients, or more generally to all clients. Request priority and cost
    play a role in selecting which requests to throttle.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于负载分担，您可以定义策略将限流应用于特定的有问题的客户端，或者更普遍地应用于所有客户端。请求优先级和成本在选择要限流的请求时起着作用。
- en: Automated response
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动响应
- en: Server utilization statistics can help determine when to consider applying controls
    like load shedding and throttling. The more heavily a server is loaded, the less
    traffic or load it can handle. If controls take too long to activate, higher-priority
    requests may end up being dropped or throttled.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器利用统计数据可以帮助确定何时考虑应用诸如负载分担和限流之类的控制。服务器负载越重，它能处理的流量或负载就越少。如果控制需要太长时间才能激活，优先级较高的请求可能最终会被丢弃或限流。
- en: To effectively manage these degradation controls at scale, you may need a central
    internal service. You can translate business considerations about mission-critical
    features and the costs of failure into policies and signals for this service.
    This internal service can also aggregate heuristics about clients and services
    in order to distribute updated policies to all servers in near real time. Servers
    can then apply these policies according to rules based on server utilization.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地管理这些降级控制，您可能需要一个中央内部服务。您可以将关于使命关键功能和故障成本的业务考虑转化为该服务的策略和信号。这个内部服务还可以聚合关于客户端和服务的启发式信息，以便向几乎实时地所有服务器分发更新的策略。然后服务器可以根据基于服务器利用率的规则应用这些策略。
- en: 'Some possibilities for automated response include the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一些自动响应的可能性包括以下内容：
- en: Implementing load-balancing systems that can respond to throttling signals and
    attempt to shift traffic to servers with lower loads
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施能够响应限流信号并尝试将流量转移到负载较低的服务器的负载平衡系统
- en: Providing DoS protections that can assist in response to malicious clients if
    throttling is ineffective or damaging
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供可以在限流无效或有害时协助应对恶意客户端的DoS保护
- en: Using reports of heavy load shedding for critical services to trigger preparation
    for failover to alternative components (a strategy that we discuss later in this
    chapter)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用关于关键服务的大规模停电报告来触发备用组件的故障转移准备（这是我们在本章后面讨论的一种策略）
- en: 'You can also use automation for self-reliant failure detection: a server that
    determines that it can’t serve some or all classes of requests can degrade itself
    to a full load-shedding mode. Self-contained or self-hosted detection is desirable
    because you don’t want to rely on external signals (possibly simulated by an attacker)
    to force an entire fleet of servers into an outage.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用自动化进行自主故障检测：确定无法为某些或所有类别的请求提供服务的服务器可以将自身降级为完全的负载分担模式。自包含或自托管的检测是可取的，因为您不希望依赖外部信号（可能是攻击者模拟的）来迫使整个服务器群陷入故障。
- en: As you implement graceful degradation, it’s important to determine and record
    levels of system degradation, regardless of what triggered the problem. This information
    is useful for diagnosing and debugging. Reporting the actual load shedding or
    throttling (whether self-imposed or directed) can help you evaluate global health
    and capacity and detect bugs or attacks. You also need this information in order
    to evaluate the current remaining system capacity and user impact. In other words,
    you want to know how degraded the individual components and the entire system
    are, and what manual actions you might need to take. After the event, you’ll want
    to evaluate the effectiveness of your degradation mechanisms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施优雅降级时，重要的是确定和记录系统降级的级别，无论是什么触发了问题。这些信息对诊断和调试很有用。报告实际的负载分担或限制（无论是自我施加的还是指导的）可以帮助您评估全局健康和容量，并检测错误或攻击。您还需要这些信息来评估当前剩余的系统容量和用户影响。换句话说，您想知道各个组件和整个系统的降级程度，以及您可能需要采取的手动操作。事件发生后，您将希望评估您的降级机制的有效性。
- en: Automate Responsibly
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任地自动化
- en: Exercise caution when creating automated response mechanisms so that they do
    not degrade system security and reliability to an unintended degree.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建自动响应机制时要谨慎，以防止它们使系统的安全性和可靠性降低到意外程度。
- en: Failing safe versus failing secure
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全失败与安全失败
- en: When designing a system to handle failure, you must balance between optimizing
    for reliability by failing open (safe) and optimizing for security by failing
    closed (secure):^([6](ch08.html#ch08fn6))
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计处理故障的系统时，您必须在优化可靠性的失败开放（安全）和优化安全性的失败关闭（安全）之间取得平衡：^（[6](ch08.html#ch08fn6)）
- en: To maximize *reliability*, a system should resist failures and serve as much
    as possible in the face of uncertainty. Even if the system’s integrity is not
    intact, as long as its configuration is viable, a system optimized for availability
    will serve what it can. If ACLs failed to load, the assumed default ACL is “allow
    all.”
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最大程度地提高*可靠性*，系统应该抵抗故障，并在面对不确定性时尽可能提供服务。即使系统的完整性没有得到保证，只要其配置是可行的，一个针对可用性进行优化的系统将提供其所能提供的服务。如果ACL加载失败，则假定默认ACL为“允许所有”。
- en: To maximize *security*, a system should lock down fully in the face of uncertainty.
    If the system cannot verify its integrity—regardless of whether a failed disk
    took away a part of its configs or an attacker changed the configs for an exploit—it
    can’t be trusted to operate and should protect itself as much as possible. If
    ACLs failed to load, the assumed default ACL is “deny all.”
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最大程度地提高*安全性*，系统在面对不确定性时应完全封锁。如果系统无法验证其完整性——无论是由于故障的磁盘带走了其配置的一部分，还是攻击者更改了配置以进行利用——它就不能被信任运行，应尽可能保护自己。如果ACL加载失败，则假定默认ACL为“拒绝所有”。
- en: These principles of reliability and security are clearly at odds. To resolve
    this tension, each organization must first determine its minimal nonnegotiable
    security posture, and then find ways to provide the required reliability of critical
    features of security services. For example, a network configured to drop low-QoS
    (quality of service) packets might require that security-oriented RPC traffic
    be tagged for special QoS to prevent packet drops. Security-oriented RPC servers
    might need special tagging to avoid CPU starvation by workload schedulers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可靠性和安全性原则显然是相互矛盾的。为了解决这种紧张局势，每个组织必须首先确定其最低不可妥协的安全姿态，然后找到提供所需安全服务关键特性的可靠性的方法。例如，配置为丢弃低QoS（服务质量）数据包的网络可能需要为安全导向的RPC流量标记特殊的QoS，以防止数据包丢失。安全导向的RPC服务器可能需要特殊标记，以避免被工作负载调度程序耗尽CPU。
- en: A foothold for humans
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类的立足点
- en: Sometimes humans must get involved in service degradation decisions. For example,
    the ability of rule-based systems to make a judgment call is inherently limited
    by predefined rules. Automation doesn’t act when faced with unforeseen circumstances
    that don’t map to any of the system’s predefined responses. An automated response
    might also produce unforeseen circumstances due to a programming error. Allowing
    appropriate human intervention to deal with these and similar situations requires
    some forethought in system design.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有时人类必须参与服务降级决策。例如，基于规则的系统进行判断的能力受到预定义规则的固有限制。当自动化面临不符合系统任何预定义响应的未预见情况时，自动化不会起作用。由于编程错误，自动化响应也可能产生未预见的情况。允许适当的人类干预来处理这些和类似情况需要在系统设计中进行一些事先考虑。
- en: First, you should prevent automation from disabling the services that employees
    use to recover your infrastructure (see [“Emergency Access”](ch09.html#emergency_access)).
    It’s important to design protections for these systems so that even DoS attacks
    cannot completely prevent access. For example, a SYN attack must not stop a responder
    from opening a TCP connection for an SSH session. Be sure to implement low-dependency
    alternatives, and continuously validate the capabilities of those alternatives.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您应该防止自动化禁用员工用于恢复基础设施的服务（请参阅[“紧急访问”](ch09.html#emergency_access)）。重要的是为这些系统设计保护，以便即使是DoS攻击也不能完全阻止访问。例如，SYN攻击不应该阻止响应者为SSH会话打开TCP连接。确保实施低依赖性的替代方案，并持续验证这些替代方案的能力。
- en: In addition, don’t allow automation to make unsupervised policy changes of either
    large magnitude (for example, a single server shedding *all* RPCs) or substantial
    scope (*all* servers shedding some RPC). Consider implementing a change budget
    instead. When automation exhausts that budget, no automatic refresh occurs. Instead,
    a human must increase the budget or make a different judgment call. Note that
    despite this human intervention, automation is still in place.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不要允许自动化进行大规模（例如，单个服务器放弃*所有*RPC）或大范围（*所有*服务器放弃一些RPC）的无监督策略更改。考虑实施变更预算。当自动化耗尽该预算时，不会发生自动刷新。相反，必须由人类增加预算或做出不同的判断。请注意，尽管有人类干预，自动化仍然存在。
- en: Controlling the Blast Radius
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制爆炸半径
- en: You can add another layer to your defense-in-depth strategy by limiting the
    scope of each part of your system. For example, consider network segmentation.
    In the past, it was common for an organization to have a single network that contained
    all of its resources (machines, printers, storage, databases, and so on). These
    resources were visible to any user or service on that network, and access was
    controlled by the resource itself.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过限制系统的每个部分的范围，您可以为您的深度防御策略增加另一层。例如，考虑网络分段。过去，组织通常拥有一个包含所有资源（机器、打印机、存储、数据库等）的单一网络。这些资源对网络上的任何用户或服务都是可见的，并且访问由资源本身控制。
- en: Today, a common way to improve security is to *segment* your network and grant
    access to each segment to specific classes of users and services. You can do this
    by using virtual LANs (VLANs) with network ACLs, which is an easy-to-configure,
    industry-standard solution. You can control traffic into each segment, and control
    which segments are allowed to communicate. You can also limit each segment’s access
    to “need to know” information.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，改善安全性的常见方法是对网络进行分段，并为特定类别的用户和服务授予对每个段的访问权限。您可以通过使用具有网络ACL的虚拟局域网（VLAN）来实现这一点，这是一种易于配置的行业标准解决方案。您可以控制进入每个段的流量，并控制允许通信的段。您还可以限制每个段对“需要知道”的信息的访问。
- en: Network segmentation is a good example of the general idea of compartmentalization,
    which we discussed in [Chapter 6](ch06.html#design_for_understandability). *Compartmentalization*
    involves deliberately creating small individual operational units (compartments)
    and limiting access to and from each one. It’s a good idea to compartmentalize
    most aspects of your systems—servers, applications, storage, and so on. When you
    use a single-network setup, an attacker who compromises a user’s credentials can
    potentially access every device on the network. When you use compartmentalization,
    however, a security breach or traffic overload in one compartment does not jeopardize
    all of the compartments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分段是隔离的一般概念的一个很好的例子，我们在第6章中讨论过。隔离涉及有意地创建小的个体操作单元（隔间），并限制对每个隔间的访问和来自每个隔间的访问。对系统的大多数方面进行隔间化是一个好主意，包括服务器、应用程序、存储等等。当您使用单一网络设置时，入侵者如果窃取了用户的凭据，可能会访问网络上的每个设备。然而，当您使用隔间化时，一个隔间中的安全漏洞或流量过载并不会危及所有隔间。
- en: Controlling the blast radius means compartmentalizing the impact of an event,
    similar to the way compartments on a ship grant resilience against the whole ship
    sinking. Designing for resilience, you should create compartmental barriers that
    constrain both attackers *and* accidental failures. These barriers allow you to
    better tailor and automate your responses. You can also use these boundaries to
    create failure domains that deliver component redundancy and failure isolation,
    as discussed in [“Failure Domains and Redundancies”](#failure_domains_and_redundancies).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 控制爆炸半径意味着将事件的影响分隔开，类似于船舶上的隔舱可以使整艘船免于沉没。在设计弹性时，您应该创建约束攻击者和意外故障的隔舱壁垒。这些隔离壁垒可以让您更好地定制和自动化您的响应。您还可以使用这些边界来创建故障域，提供组件冗余和故障隔离，如在“故障域和冗余”中所讨论的那样。
- en: Compartments also aid in quarantine efforts, reducing the need for responders
    to actively balance defending and preserving evidence. Some compartments can be
    isolated and frozen for analysis while other compartments are recovered. Additionally,
    compartments create natural boundaries for replacement and repair during incident
    response—a compartment may be jettisoned to save the remainder of the system.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 隔间还有助于隔离努力，减少了响应者需要积极平衡防御和保留证据的需求。一些隔间可以被隔离和冻结以进行分析，而其他隔间则可以被恢复。此外，隔间在事件响应期间为更换和修复创建了自然边界，一个隔间可能被舍弃以拯救系统的其余部分。
- en: To control the blast radius of an incursion, you must have a way to establish
    boundaries and to be sure those boundaries are secure. Consider a job running
    in production as one compartment.^([7](ch08.html#ch08fn7)) This job must permit
    some access (you want the compartment to be useful), but not unrestricted access
    (you want to protect the compartment). Restricting who can access the job relies
    on your ability to recognize endpoints in production and confirm their identity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制入侵的爆炸半径，您必须有一种建立边界并确保这些边界安全的方法。将生产中运行的作业视为一个隔间。这个作业必须允许一些访问（您希望这个隔间有用），但不能是无限制的访问（您希望保护这个隔间）。限制谁可以访问作业取决于您识别生产中的端点并确认其身份的能力。
- en: You can do this by using authenticated remote procedure calls, which identify
    both parties within one connection. To protect the parties’ identities from spoofing
    and to conceal their contents from the network, these RPCs use mutually authenticated
    connections, which can certify the identities of both parties connected to the
    service. To permit endpoints to make more informed decisions about other compartments,
    you may add additional information that endpoints publish along with their identity.
    For example, you can add location information to the certificate so that you can
    reject nonlocal requests.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用经过身份验证的远程过程调用来实现这一点，该调用可以在一个连接中识别双方。为了保护各方的身份免受欺骗，并将其内容隐藏在网络中，这些RPC使用相互认证的连接，可以证明连接到服务的双方的身份。为了让端点对其他隔间做出更明智的决定，您可以添加端点与其身份一起发布的附加信息。例如，您可以向证书添加位置信息，以便拒绝非本地请求。
- en: 'Once mechanisms to establish compartments are in place, you face a difficult
    tradeoff: you need to constrain your operations with enough separation to deliver
    useful-sized compartments, but without creating *too much* separation. For example,
    one balanced approach to compartmentalization would be to consider every RPC method
    as a separate compartment. This aligns compartments along logical application
    boundaries, and the count of compartments is linear to the number of system features.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立隔间的机制到位，您将面临一个艰难的抉择：您需要通过足够的分离来限制您的操作，以提供有用大小的隔间，但又不会创建*太多*的分离。例如，隔间化的一个平衡方法是将每个RPC方法视为单独的隔间。这样可以沿着逻辑应用边界对齐隔间，而隔间的数量与系统功能的数量成正比。
- en: Compartment separation that controls the acceptable parameter values of RPC
    methods would warrant more careful consideration. While this would create tighter
    security controls, the number of possible violations per RPC method is proportional
    to the number of RPC clients. This complexity would compound across all of the
    system’s features, and require coordination of changes in client code and server
    policy. On the other hand, compartments that wrap an entire server (regardless
    of its RPC services or their methods) are much easier to manage, but provide comparatively
    much less value. When balancing this tradeoff, it’s necessary to consult with
    the incident management and operations teams to consider your choices of compartment
    types and to validate the utility of your choices.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 控制RPC方法可接受参数值的隔间分离需要更加谨慎的考虑。虽然这会创建更严格的安全控制，但每个RPC方法可能的违规次数与RPC客户端的数量成正比。这种复杂性会在系统的所有功能中累积，并需要协调客户端代码和服务器策略的更改。另一方面，无论RPC服务或其方法如何，包装整个服务器的隔间要容易得多，但提供的价值相对较少。在权衡这种权衡时，有必要与事件管理和运营团队协商，以考虑隔间类型的选择，并验证您的选择的效用。
- en: Imperfect compartments that don’t perfectly cover all edge cases can also provide
    value. For example, the process of finding the edge cases may cause an attacker
    to make a mistake that alerts you to their presence. Any time that it takes such
    an adversary to escape a compartment is additional time that your incident response
    team has to react.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不完美的隔间化并不能完全覆盖所有边缘情况，但也可以提供价值。例如，寻找边缘情况的过程可能会导致攻击者犯错，从而提醒您他们的存在。攻击者逃离隔间所需的任何时间都是您的事件响应团队有机会做出反应的额外时间。
- en: Incident management teams must plan and practice tactics for sealing compartments
    to contain an incursion or a bad actor. Turning off part of your production environment
    is a dramatic step. Well-designed compartments give incident management teams
    the option to perform actions that are proportional to the incidents, so they
    don’t necessarily have to take an entire system offline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 事件管理团队必须计划和实践封锁隔间以遏制入侵或不良行为的策略。关闭生产环境的一部分是一个戏剧性的举措。设计良好的隔间给事件管理团队提供了执行与事件成比例的操作的选项，因此他们不一定要将整个系统下线。
- en: When you implement compartmentalization, you face a tradeoff between having
    all customers share a single instance of a given service,^([8](ch08.html#ch08fn8))
    or running separate service instances that support individual customers or subsets
    of customers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当您实施隔间化时，您面临着一个抉择：让所有客户共享给定服务的单个实例，或者运行支持单个客户或客户子集的单独服务实例。
- en: 'For example, running two virtual machines (VMs)—each controlled by different
    mutually distrustful entities—on the same hardware comes with a certain risk:
    exposure to zero-day vulnerabilities in the virtualization layer perhaps, or subtle
    cross-VM information leaks. Some customers may choose to eliminate these risks
    by compartmentalizing their deployments based on physical hardware. To facilitate
    this approach, many cloud providers offer deployment on per-customer dedicated
    hardware.^([9](ch08.html#ch08fn9)) In this case, the cost of reduced resource
    utilization is reflected in a pricing premium.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在同一硬件上运行由不同互不信任的实体控制的两个虚拟机（VM）存在一定风险：可能会暴露在虚拟化层的零日漏洞，或者存在微妙的跨VM信息泄漏。一些客户可能会选择通过基于物理硬件对其部署进行隔间化来消除这些风险。为了促进这种方法，许多云提供商提供基于每个客户专用硬件的部署。在这种情况下，减少资源利用的成本反映在定价溢价中。
- en: Compartment separation adds resilience to a system as long as the system has
    mechanisms to maintain the separation. The difficult task is tracking those mechanisms
    and ensuring they remain in place. To prevent regressions, it’s valuable to validate
    that operations prohibited across separation boundaries indeed fail (see [“Continuous
    Validation”](#continuous_validation)). Conveniently, because operational redundancy
    relies on compartmentalization (covered in [“Failure Domains and Redundancies”](#failure_domains_and_redundancies)),
    your validation mechanisms can cover both prohibited and expected operations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 隔间分离为系统增加了韧性，只要系统有机制来维持这种分离。困难的任务是跟踪这些机制并确保它们保持在位。为了防止退化，验证跨隔间边界禁止的操作失败是有价值的。方便的是，因为运营冗余依赖于隔间化（在“故障域和冗余”中介绍），您的验证机制可以涵盖被禁止和预期的操作。
- en: Google compartmentalizes by role, location, and time. When an attacker tries
    to compromise a compartmentalized system, the potential scope of any single attack
    is greatly reduced. If the system is compromised, the incident management teams
    have options to disable only parts of it to purge the effects of the compromise
    while leaving other parts operational. The following sections explore the different
    types of compartmentalization in detail.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌通过角色、位置和时间进行隔间化。当攻击者试图破坏隔间化系统时，任何单次攻击的潜在范围都大大减少。如果系统受到攻击，事件管理团队可以选择仅禁用部分系统以清除受攻击的影响，同时保持其他部分运行。接下来的部分将详细探讨不同类型的隔间化。
- en: Role Separation
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 角色分离
- en: Most modern microservices architecture systems allow users to run jobs as particular
    *roles*, sometimes referred to as *service accounts*. The jobs are then provided
    with credentials that allow them to authenticate to other microservices on the
    network in their specific roles. If an adversary compromises a single job, they
    will be able to impersonate the job’s corresponding role across the network. Because
    this allows the adversary to access all data that the other jobs running as that
    role could access, this effectively means that adversary has compromised the other
    jobs as well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: To limit the blast radius of such a compromise, different jobs should typically
    be run as different roles. For example, if you have two microservices that need
    access to two different classes of data (say, photos and text chats), running
    these two microservices as different roles can increase the resilience of your
    system even if the two microservices are developed and run by the same team.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Location Separation
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Location separation* helps to limit an attacker’s impact along an additional
    dimension: the location where the microservice is running. For example, you might
    want to prevent an adversary who has physically compromised a single datacenter
    from being able to read data in all your other datacenters. Similarly, you might
    want your most powerful administrative users to have their access permissions
    limited to only specific regions to mitigate insider risk.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious way to achieve location separation is to run the same microservices
    as different roles in different locations (like datacenters or cloud regions,
    which also typically correspond to different physical locations). You can then
    use your normal access control mechanisms to protect instances of the same service
    in different locations from each other, just as you would protect different services
    running as different roles from each other.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Location separation helps you resist an attack that moves from one location
    to another. Location-based cryptographic compartments let you limit access to
    applications and their stored data to specific locations, containing the blast
    radius of local attacks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Physical location is a natural compartmentalization border, since many adverse
    events are connected to physical locations. For example, natural disasters are
    confined to a region, as are other localized mishaps such as fiber cuts, power
    outages, or fires. Malicious attacks that require the physical presence of the
    attacker are also confined to locations the attacker can actually get to, and
    all but the most capable (state-level attackers, for example) likely don’t have
    the capability to send attackers to many locations all at once.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the degree of risk exposure can depend on the nature of the physical
    location. For example, the risk of specific kinds of natural disasters varies
    with geographical region. Also, the risk of an attacker tailgating into a building
    and finding an open network port to plug into is higher in an office location
    with heavy employee and visitor traffic, as opposed to a datacenter with tightly
    controlled physical access.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, you’ll want to take location into account when designing
    your systems, to ensure that localized impacts stay confined to systems in that
    region, while letting your multiregional infrastructure continue to operate. For
    example, it’s important to ensure that a service provided by servers in several
    regions does not have a critical dependency on a backend that is single-homed
    in one datacenter. Similarly, you’ll want to ensure that physical compromise of
    one location does not allow an attacker to easily compromise other locations:
    tailgating into an office and plugging into an open port in a conference room
    should not give an intruder network access to production servers in your datacenter.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Aligning physical and logical architecture
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When compartmentalizing an architecture into logical failure and security domains,
    it’s valuable to align relevant physical boundaries with logical boundaries. For
    example, it’s useful to segment your network on both network-level risks (such
    as networks exposed to malicious internet traffic versus trusted internal networks)
    and risks of physical attacks. Ideally, you’d have network segregation between
    corporate and production environments housed in physically separate buildings.
    Beyond that, you might further subdivide your corporate network to segregate areas
    with high visitor traffic, such as conference and meeting areas.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, a physical attack, such as stealing or backdooring a server,
    can give an attacker access to important secrets, encryption keys, or credentials
    that then might permit them to further penetrate your systems. With this in mind,
    it’s a good idea to logically compartmentalize distribution of secrets, keys,
    and credentials to physical servers to minimize the risk of a physical compromise.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you operate web servers in several physical datacenter locations,
    it can be advantageous to deploy a separate certificate to each server, or share
    a certificate only across servers in one location, instead of sharing a single
    certificate across *all* your servers. This can make your response to the physical
    compromise of one datacenter more agile: you can drain its traffic, revoke just
    the cert(s) deployed to that datacenter, and take the datacenter offline for incident
    response and recovery, all the while serving traffic from your remaining datacenters.
    If you had a single certificate deployed to all servers, you’d instead have to
    very quickly replace the cert on all of them—even the ones that were not actually
    compromised.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Isolation of trust
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While services may need to communicate across location boundaries to operate
    properly, a service might also want to reject requests from locations it doesn’t
    expect to communicate with. To do this, you can restrict communications by default,
    and allow only the expected communications across location boundaries. It’s also
    unlikely that all APIs on any service will use the same set of location restrictions.
    User-facing APIs are typically open globally, while control plane APIs are usually
    constrained. This makes fine-grained (per API call) control of permitted locations
    necessary. Creating tools that make it easy for any given service to measure,
    define, and enforce location limits on individual APIs enables teams to use their
    per-service knowledge to implement location isolation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: To restrict communications based on location, each identity needs to include
    location metadata. Google’s job control system certifies and runs jobs in production.
    When the system certifies a job to run in a given compartment, it annotates the
    job’s certificate with that compartment’s location metadata. Each location has
    its own copy of the job control system that certifies jobs to run in that location,
    and machines in that location only accept jobs from that system. This is designed
    to prevent an attacker from piercing the compartment boundary and affecting other
    locations. Contrast this approach to a single centralized authority—if there were
    only one job control system for all of Google, its location would be quite valuable
    to an attacker.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Once trust isolation is in place, we can extend ACLs on stored data to include
    location restrictions. This way, we can separate locations for storage (where
    we put the data) from locations for access (who can retrieve or modify the data).
    This also opens up the possibility of trusting physical security versus trusting
    access by API—sometimes the additional requirement of a physical operator is worthwhile,
    as it removes the possibility of remote attacks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: To help control compartment violations, Google has a root of trust in each location
    and distributes the list of trusted roots and the locations they represent to
    all machines in the fleet. This way, each machine can detect spoofing across locations.
    We can also revoke a location’s identity by distributing an updated list to all
    machines declaring the location untrustworthy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of location-based trust
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At Google, we have chosen to design our corporate network infrastructure so
    that location does not imply any trust. Instead, under the the zero trust networking
    paradigm of our BeyondCorp infrastructure (see [Chapter 5](ch05.html#design_for_least_privilege)),
    a workstation is trusted based on a certificate issued to the individual machine,
    and assertions about its configuration (such as up-to-date software). Plugging
    an untrusted machine into an office-floor network port will assign it to an untrusted
    guest VLAN. Only authorized workstations (authenticated via the 802.1x protocol)
    are assigned to the appropriate workstation VLAN.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: We have also chosen to not even rely on physical location to establish trust
    for servers in datacenters. One motivating experience came out of a Red Team assessment
    of a datacenter environment. In this exercise, the Red Team placed a wireless
    device on top of a rack and quickly plugged it into an open port, to allow further
    penetration of the datacenter’s internal network from outside the building. When
    they returned to clean up after the exercise, they found that an attentive datacenter
    technician had neatly zip-tied the access point’s cabling—apparently offended
    by the untidy install job and on the assumption that the device must be legitimate.
    This story illustrates the difficulty of ascribing trust based on physical location,
    even within a physically secure area.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: In Google’s production environment, similarly to the BeyondCorp design, authentication
    between production services is rooted in machine-to-machine trust based on per-machine
    credentials. A malicious implant on an unauthorized device would not be trusted
    by Google’s production environment.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Isolation of confidentiality
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have a system to isolate trust, we need to isolate our encryption keys
    to ensure that data secured through a root of encryption in one location is not
    compromised by exfiltration of encryption keys in another location. For example,
    if one branch of a company is compromised, attackers should not be able to read
    data from the company’s other branches.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Google has base encryption keys that protect key trees. These keys eventually
    protect data at rest through key wrapping and key derivation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: To isolate encryption and key wrapping to a location, we need to ensure that
    the root keys for a location are only available to the correct location. This
    requires a distribution system that only places root keys in the correct locations.
    A key access system should leverage trust isolation to ensure that these keys
    cannot be accessed by entities that aren’t in the appropriate location.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Using these principles, a given location allows the use of ACLs on local keys
    to prevent remote attackers from decrypting data. Decryption is prevented even
    if attackers have access to the encrypted data (through internal compromise or
    exfiltration).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from a global key tree to a local key tree should be gradual.
    While any part of the tree may move from global to local independently, isolation
    isn’t complete for a given leaf or branch of the tree until all keys above it
    have transitioned to local keys.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Time Separation
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, it’s useful to limit the abilities of an adversary over time. The most
    common scenario to consider here is an adversary who has compromised a system
    and stolen a key or credential. If you rotate your keys and credentials over time
    and expire the old ones, the adversary must maintain their presence to reacquire
    the new secrets, which gives you more opportunities to detect the theft. Even
    if you never do detect the theft, rotation is still critical because you might
    close the avenue the adversary used to gain access to the key or credential during
    normal security hygiene work (e.g., by patching the vulnerability).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: As we discuss in [Chapter 9](ch09.html#design_for_recovery), doing key and credential
    rotation and expiration reliably requires careful tradeoffs. For example, using
    wall clock–based expiration for credentials can be problematic if there’s a failure
    that prevents rotation to new credentials before the time the old credentials
    expire. Providing useful time separation requires balancing the frequency of rotation
    against the risk of downtime or loss of data if the rotation mechanism fails.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'DEEP DIVE: Failure Domains and Redundancies'
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we’ve covered how to design systems that adjust their behavior in response
    to attacks and contain attack fallout by using compartmentalization. To address
    complete failures of system components, system design must incorporate redundancies
    and distinct failure domains. These tactics can hopefully limit the impact of
    failures and avert complete collapse. It’s particularly important to mitigate
    failures of critical components, since any system that depends on failed critical
    components is also at risk of complete failure.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than aiming to prevent all failures at all times, you can create a balanced
    solution for your organization by combining the following approaches:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Break up systems into independent failure domains.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aim to reduce the probability of a single root cause affecting elements in multiple
    failure domains.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create redundant resources, components, or procedures that can replace the failed
    ones.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure Domains
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *failure domain* is a type of blast radius control. Instead of structurally
    separating by role, location, or time, failure domains achieve functional isolation
    by partitioning a system into multiple equivalent but completely independent copies.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Functional isolation
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A failure domain looks like a single system to its clients. If necessary, any
    of the individual partitions can take over for the entire system during an outage.
    Because a partition has only a fraction of the system’s resources, it can support
    only a fraction of the system’s capacity. Unlike managing role, location, and
    time separations, operating failure domains and maintaining their isolation requires
    ongoing effort. In exchange, failure domains increase system resilience in ways
    other blast radius controls can’t.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Failure domains help protect systems from global impact because a single event
    doesn’t typically affect all failure domains at once. However, in extreme cases,
    a significant event can disrupt multiple, or even all, failure domains. For example,
    you can think of a storage array’s underlying devices (HDDs or SSDs) as failure
    domains. Although any one device may fail, the entire storage system remains functional
    because it creates a new data replica elsewhere. If a large number of storage
    devices fail and there aren’t sufficient spare devices to maintain data replicas,
    further failures might result in data loss in the storage system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Data isolation
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You need to prepare for the possibility of having bad data at the data source
    or within individual failure domains. Therefore, each failure domain instance
    needs its own data copy in order to be functionally independent of the other failure
    domains. We recommend a twofold approach to achieve data isolation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: First, you can restrict how data updates can enter a failure domain. A system
    accepts new data only after it passes all validation checks for typical and safe
    changes. Some exceptions are escalated for justification, and a breakglass mechanism^([10](ch08.html#ch08fn10))
    may allow new data to enter the failure domain. As a result, you are more likely
    to prevent attackers or software bugs from making disruptive changes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider ACL changes. A human mistake or a bug in ACL-generating
    software could produce an empty ACL, which might result in denying access to everyone.^([11](ch08.html#ch08fn11))
    Such an ACL change could cause system malfunction. Similarly, an attacker might
    try to expand their reach by adding a “permit all” clause to an ACL.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: At Google, individual services generally have an RPC endpoint for intake of
    new data and for signaling. Programming frameworks, such as those presented in
    [Chapter 12](ch12.html#writing_code), include APIs for versioning data snapshots
    and evaluating their validity. Client applications can take advantage of the programming
    framework’s logic for qualifying new data as safe. Centralized data push services
    implement quality controls for data updates. The data push services check where
    to get the data from, how to package it, and when to push the packaged data. To
    prevent automation from causing a widespread outage, Google rate limits global
    changes using per-application quotas. We prohibit actions that change multiple
    applications at once or that change the application capacity too quickly within
    a time period.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, enabling systems to write the last known good configuration to disk
    makes the systems resilient to losing access to configuration APIs: they can use
    the saved config. Many of Google’s systems preserve old data for a limited duration
    of time in case the most recent data becomes corrupted for any reason. This is
    another example of defense in depth, helping provide long-term resilience.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Practical aspects
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Even splitting a system into only two failure domains brings substantial benefits:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Having two failure domains provides A/B regression capabilities and limits the
    blast radius of system changes to a single failure domain. To achieve this functionality,
    use one failure domain as a canary, and have a policy that doesn’t allow updates
    to both failure domains at the same time.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geographically separated failure domains can provide isolation for natural disasters.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use different software versions in different failure domains, thereby
    reducing the chances of a single bug breaking all servers or corrupting all data.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining data and functional isolation enhances overall resilience and incident
    management. This approach limits the risk of data changes that are accepted without
    justification. When issues do arise, isolation delays their propagation to the
    individual functional units. This gives other defense mechanisms more time to
    detect and react, which is especially beneficial during hectic and time-sensitive
    incident response. By pushing multiple candidate fixes to distinct failure domains
    in parallel, you can independently evaluate which fixes have the intended effect.
    That way, you can avoid accidentally pushing a rushed update with a mistaken “fix”
    globally, further degrading your entire system.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Failure domains incur operational costs. Even a simple service with a few failure
    domains requires you to maintain multiple copies of service configurations, keyed
    by failure domain identifiers. Doing so requires the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring configuration consistency
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting all configurations from simultaneous corruption
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiding the separation into failure domains from client systems to prevent accidental
    coupling to a particular failure domain
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potentially partitioning all dependencies, because one shared dependency change
    might accidentally propagate to all failure domains
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that a failure domain may suffer complete failure if even
    one of its critical components fails. After all, you partitioned the original
    system into failure domains in the first place so that the system can stay up
    even when a failure domain’s copies fail completely. However, failure domains
    simply shift the problem one level down. The following section discusses how you
    can use alternative components to mitigate the risk of complete failure of all
    failure domains.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Component Types
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The resilient quality of a failure domain is expressed as the combined reliability
    of both its components and their dependencies. Resilience of the entire system
    increases with the number of failure domains. However, this increased resilience
    is offset by the operational overhead of maintaining more and more failure domains.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: You can achieve further improvements in resilience by slowing down or stopping
    new feature development, gaining more stability in exchange. If you avoid adding
    a new dependency, you also avoid its potential failure modes. If you stop updating
    code, the rate of new bugs decreases. However, even if you halt all new feature
    development, you still need to react to occasional changes in state, like security
    vulnerabilities and increases in user demand.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, halting all new feature development isn’t a viable strategy for
    most organizations. In the following sections, we present a hierarchy of alternative
    approaches to balancing reliability and value. In general, there are three broad
    classes of reliability for services: high capacity, high availability, and low
    dependency.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: High-capacity components
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The components that you build and run in the normal course of business make
    up your *high-capacity* service. That’s because these components make up the main
    fleet serving your users. This is where your service absorbs spikes in user requests
    or resource consumption due to new features. High-capacity components also absorb
    DoS traffic, until DoS mitigation takes effect or graceful degradation kicks in.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Because these components are the most critically important to your service,
    you should focus your efforts here first—for example, by following best practices
    for capacity planning, software and configuration rollouts, and more, as covered
    in Part III of the [SRE book](https://landing.google.com/sre/sre-book/toc/index.html)
    and Part II of the [SRE workbook](https://landing.google.com/sre/workbook/toc/).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: High-availability components
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your system has components whose failures impact all users, or otherwise
    have significant wide-reaching consequences—the high-capacity components discussed
    in the preceding section—you may mitigate these risks by deploying copies of those
    components. These copies of components are *high availability* if they offer a
    provably lower probability of outages.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve lower probability of outages, the copies should be configured with
    fewer dependencies and a limited rate of changes. This approach reduces the chances
    of infrastructure failures or operational errors breaking the components. For
    example, you might do the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Use data cached on local storage to avoid depending on a remote database.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use older code and configs to avoid recent bugs in newer versions.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running high-availability components has little operational overhead, but it
    requires additional resources whose costs scale proportionally to the size of
    the fleet. Determining whether the high-availability components should sustain
    your entire user base or only a portion of that base is a cost/benefit decision.
    Configure graceful degradation capabilities the same way between each high-capacity
    and high-availability component. This allows you to trade fewer resources for
    more aggressive degradation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Low-dependency components
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If failures in the high-availability components are unacceptable, a *low-dependency*
    service is the next level of resilience. Low dependency requires an alternative
    implementation with minimal dependencies. Those minimal dependencies are also
    low dependency. The total set of services, processes, or jobs that may fail is
    as small as the business needs and costs can bear. High-capacity and high-availability
    services can serve large user bases and offer rich features because of layers
    of cooperating platforms (virtualization, containerization, scheduling, application
    frameworks). While these layers help scaling by permitting services to add or
    move nodes rapidly, they also incur higher rates of outages as error budgets across
    the cooperating platforms add up.^([12](ch08.html#ch08fn12)) In contrast, low-dependency
    services have to simplify their serving stack until they can accept the stack’s
    aggregate error budget. In turn, simplifying the serving stack may lead to having
    to remove features.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Low-dependency components require you to determine if it’s possible to build
    an alternative for a critical component, where the critical and alternative components
    do not share any failure domains. After all, the success of redundancy is inversely
    proportional to the probability of the same root cause affecting both components.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Consider storage space as a fundamental building block of a distributed system—you
    might want to store local data copies as a fallback when the RPC backends for
    data storage are unavailable. However, a general approach of storing local data
    copies isn’t always practical. Operational costs increase to support the redundant
    components, while the benefit the extra components provide is typically zero.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you end up with a small set of low-dependency components with limited
    users, features, and costs, but that are confidently available for temporary loads
    or recovery. While most useful features usually rely on multiple dependencies,
    a severely degraded service is better than an unavailable one.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: As a small-scale example, imagine a device for which write-only or read-only
    operations are presumed to be available over the network. In a home security system,
    such operations include recording event logs (write-only) and looking up emergency
    phone numbers (read-only). An intruder’s break-in plan includes disabling the
    home’s internet connectivity, thus disrupting the security system. To counter
    this type of failure, you configure the security system to also use a local server
    that implements the same APIs as the remote service. The local server writes event
    logs to local storage, updates the remote service, and retries failed attempts.
    The local server also responds to emergency phone number lookup requests. The
    phone number list is periodically refreshed from the remote service. From the
    home security console’s perspective, the system is working as expected, writing
    logs and accessing emergency numbers. Additionally, a low-dependency, hidden landline
    may provide dialing capabilities as backup to a disabled wireless connection.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: As a business-scale example, a global network failure is one of the scariest
    types of outages, because it impacts both service functionality and the ability
    of responders to fix the outage. Large networks are managed dynamically and are
    more at risk for global outages. Building an alternative network that fully avoids
    reusing the same network elements as in the main network—links, switches, routers,
    routing domains, or [SDN](https://oreil.ly/Row8c) software—requires careful design.
    This design must target a specific and narrow subset of use cases and operating
    parameters, allowing you to focus on simplicity and understandability. Aiming
    for minimal capital expenditures for this infrequently used network also naturally
    leads to limiting the available features and bandwidth. Despite the limitations,
    the results are sufficient. The goal is to support only the most critical features,
    and only for a fraction of the usual bandwidth.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Redundancies
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Redundant systems are configured to have more than a single option for each
    of their dependencies. Managing the choice between these options is not always
    straightforward, and attackers can potentially exploit the differences between
    the redundant systems—for example, by pushing the system toward the less secure
    option. Remember that a resilient design achieves security *and* reliability without
    sacrificing one for the other. If anything, when low-dependency alternatives have
    stronger security, this can serve as a disincentive to attackers who are considering
    wearing down your system.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Failover strategies
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supplying a set of backends, usually through load-balancing technologies, adds
    resilience in the face of a backend failure. For example, it is impractical to
    rely on a single RPC backend. Whenever that backend needs to restart, the system
    will hang. For simplicity, the system usually treats redundant backends as *interchangeable*,
    as long as all backends provide the same feature behaviors.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: A system that needs different *reliability* behaviors (for the same set of feature
    behaviors) should rely on a distinct set of interchangeable backends that provide
    the desired reliability behaviors. The system itself must implement logic to determine
    which set of behaviors to use and when to use them—for example, through flags.
    This gives you full control over the system’s reliability, especially during recovery.
    Contrast this approach to requesting low-dependency behavior from the same high-availability
    backend. Using an RPC parameter, you might prevent the backend from attempting
    to contact its unavailable runtime dependency. If the runtime dependency is also
    a startup dependency, your system is still one process restart from disaster.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: When to fail over to a component with better stability is situation-specific.
    If automatic failover is a goal, you should address the differences in available
    capacity by using the means covered in [“Controlling Degradation”](#controlling_degradation).
    After failover, such a system switches to using throttling and load-shedding policies
    tuned for the alternative component. If you want the system to fail back after
    the failed component recovers, provide a way to disable that failback—you may
    need to stabilize fluctuations or precisely control failover in some cases.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Common pitfalls
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve observed some common pitfalls with operating alternative components, regardless
    of whether they’re high availability or low dependency.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: For instance, over time you can grow to rely on alternative components for normal
    operation. Any dependent system that begins to treat the alternative systems as
    backup likely overloads them during an outage, making the alternative system an
    unexpected cause for denial of service. The opposite problem occurs when the alternative
    components are not routinely used, resulting in rot and surprise failures whenever
    they are needed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Another pitfall is unchecked growth of dependence on other services or amounts
    of required compute resources. Systems tend to evolve as user demands change and
    developers add features. Over time, dependencies and dependents grow, and systems
    may use resources less efficiently. High-availability copies may fall behind high-capacity
    fleets, or low-dependency services may lose consistency and reproducibility when
    their intended operating constraints are not continuously monitored and validated.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'It is crucial that failover to alternative components does not compromise the
    system’s integrity or security. Consider the following scenarios in which the
    right choice depends on your organization’s circumstances:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'You have a high-availability service that runs six-week-old code for security
    reasons (to defend against recent bugs). However, this same service requires an
    urgent security fix. Which risk would you choose: not applying the fix, or potentially
    breaking the code with the fix?'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A remote key service’s startup dependency for fetching private keys that decrypt
    data may be made low dependency by storing private keys on local storage. Does
    this approach create an unacceptable risk to those keys, or can an increase in
    key rotation frequency sufficiently counteract that risk?
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You determine that you can free up resources by reducing the frequency of updates
    to data that changes infrequently (for example, ACLs, certificate revocation lists,
    or user metadata). Is it worthwhile to free up these resources, even if doing
    so potentially gives an attacker more time to make changes to that data or enables
    those changes to persist undetected for longer?
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you need to make certain to prevent your system from autorecovering
    at the wrong time. If resilience measures automatically throttled the system’s
    performance, it’s OK for those same measures to automatically unthrottle it. However,
    if you applied a manual failover, don’t permit automation to override the failover—the
    drained system might be quarantined because of a security vulnerability, or your
    team might be mitigating a cascading failure.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'DEEP DIVE: Continuous Validation'
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From both a reliability and a security perspective, we want to be sure that
    our systems behave as anticipated under both normal and unexpected circumstances.
    We also want to be sure that new features or bug fixes don’t gradually erode a
    system’s layered resilience mechanisms. There is no substitute for actually exercising
    the system and validating that it works as intended.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Validation focuses on observing the system under *realistic* but *controlled*
    circumstances, targeting workflows within a single system or across multiple systems.^([13](ch08.html#ch08fn13))
    Unlike [chaos engineering](https://oreil.ly/Fvx4L), which is exploratory in nature,
    validation confirms specific system properties and behaviors covered in this chapter
    and Chapters [5](ch05.html#design_for_least_privilege), [6](ch06.html#design_for_understandability),
    and [9](ch09.html#design_for_recovery). When you validate regularly, you ensure
    that the outcomes remain as expected and that the validation practices themselves
    remain functional.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: There’s some art to making validation *meaningful*. To start with, you can use
    some of the concepts and practices covered in [Chapter 15](ch15.html#onefive_investigating_systems)—for
    example, how to choose what to validate, and how to measure effective system attributes.
    Then you can gradually evolve your validation coverage by creating, updating,
    or removing checks. You can also extract useful details from actual incidents—these
    details are the ultimate truths of your system’s behaviors, and often highlight
    needed design changes or gaps in your validation coverage. Finally, it’s important
    to remember that as business factors change, individual services tend to evolve
    and change as well, potentially resulting in incompatible APIs or unanticipated
    dependencies.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'A general validation maintenance strategy includes the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Discovering new failures
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing validators for each failure
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing all validators repeatedly
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phasing out validators when the relevant features or behaviors no longer exist
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To discover relevant failures, rely on the following sources:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Regular bug reports from users and employees
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuzzing and fuzzing-like approaches (described in [Chapter 13](ch13.html#onethree_testing_code))
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure-injection approaches (akin to the [Chaos Monkey tool](https://oreil.ly/fvSKQ))
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytical judgment of subject matter experts who operate your systems
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an automation framework can help you schedule incompatible checks to
    run at different times so that they don’t conflict with each other. You should
    also monitor and establish periodic audits of automation to catch broken or compromised
    behaviors.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Validation Focus Areas
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s beneficial to validate whole systems and the end-to-end cooperation among
    their services. But because validating the failure response of whole systems that
    serve real users is expensive and risky, you have to compromise. Validating smaller
    system replicas is usually more affordable, and still provides insights that are
    impossible to obtain by validating individual system components in isolation.
    For example, you can do the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Tell how callers react to an RPC backend that is responding slowly or becoming
    unreachable.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See what happens when resource shortages occur, and whether it’s feasible to
    obtain an emergency resource quota when resource consumption spikes.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another practical solution is to rely upon logs to analyze interactions between
    systems and/or their components. If your system implements compartmentalization,
    operations that attempt to cross boundaries of role, location, or time separation
    should fail. If your logs record unexpected successes instead, these successes
    should be flagged. Log analysis should be active at all times, letting you observe
    actual system behaviors during validation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'You should validate the attributes of your security design principles: least
    privilege, understandability, adaptability, and recovery. Validating recovery
    is especially critical, because recovery efforts necessarily involve human actions.
    Humans are unpredictable, and unit tests cannot check human skills and habits.
    When validating recovery design, you should review both the readability of recovery
    instructions and the efficacy and interoperability of different recovery workflows.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Validating security attributes means going beyond ensuring correct system responses.
    You should also check that the code or configuration doesn’t have any known vulnerabilities.
    Active penetration testing of a deployed system gives a black-box view of the
    system’s resilience, often highlighting attack vectors the developers did not
    consider.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Interactions with low-dependency components deserve special attention. By definition,
    these components are deployed in the most critical circumstances. There is no
    fallback beyond these components. Fortunately, a well-designed system should have
    a limited number of low-dependency components, which makes the goal of defining
    validators for all critical functions and interactions feasible. You realize the
    return on investment in these low-dependency components *only* if they work when
    needed. Your recovery plans should often rely on the low-dependency components,
    and you should validate their use by humans for the possible situation where the
    system degrades to that level.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Validation in Practice
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section presents a few validation scenarios that have been used at Google
    to demonstrate the wide spectrum of approaches to continuous validation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Inject anticipated changes of behavior
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can validate system response to load shedding and throttling by injecting
    a change of behavior into the server, and then observing whether all affected
    clients and backends respond appropriately.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: For example, Google implements server libraries and control APIs that permit
    us to add arbitrary delays or failures to any RPC server. We use this functionality
    in periodic disaster readiness exercises, and teams may easily run experiments
    at any time. Using this approach, we study isolated RPC methods, whole components,
    or larger systems, and specifically look for signs of cascading failures. Starting
    with a small increase in latency, we build a step function toward simulating a
    full outage. Monitoring graphs clearly reflect changes in response latencies just
    as they would for real problems, at the step points. Correlating these timelines
    with monitoring signals from clients and backend servers, we can observe the propagation
    of effect. If error rates spike disproportionately to the patterns we observed
    at the earlier steps, we know to step back, pause, and investigate whether the
    behavior is unexpected.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to have a reliable mechanism for canceling the injected behaviors
    quickly and safely. If there’s a failure, even if the cause doesn’t seem related
    to the validation, the right decision is to abort experiments first, and then
    evaluate when it is safe to try again.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Exercise emergency components as part of normal workflows
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can be certain that a low-dependency or high-availability system is functioning
    correctly and ready to roll out to production when we observe the system performing
    its intended functions. To test readiness, we push either a small fraction of
    real traffic or a small fraction of real users to the system we are validating.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'High-availability systems (and sometimes low-dependency systems) are validated
    by mirroring requests: clients send two identical requests, one to the high-capacity
    component and one to the high-availability component. By modifying the client
    code or injecting a server that can duplicate one input traffic stream into two
    equivalent output streams,^([14](ch08.html#ch08fn14)) you can compare the responses
    and report the differences. Monitoring services send alerts when the response
    discrepancies exceed anticipated levels. Some discrepancies are expected; for
    example, if the fallback system has older data or features. For that reason, a
    client should use the response from the high-capacity system, unless an error
    occurs or the client was explicitly configured to ignore that system—both of which
    might happen in emergencies. Mirroring the requests requires not only code changes
    at the client, but also the ability to customize the mirroring behavior. Because
    of that, this strategy is easier to deploy on frontend or backend servers rather
    than on end-user devices.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Low-dependency systems (and occasionally high-availability systems) are better
    suited for validation by real users than by request mirroring. This is because
    low-dependency systems differ substantially from their higher-unreliability counterparts
    in terms of features, protocols, and system capacity. At Google, on-call engineers
    use low-dependency systems as an integral part of their on-call duties. We use
    this strategy for a few reasons:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Many engineers participate in on-call rotations, but only a small fraction of
    engineers are on call at once. This naturally restricts the set of people involved
    in validating.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When engineers are on call, they might need to rely on emergency paths. Well-practiced
    use of low-dependency systems reduces the time it takes an on-call engineer to
    switch to using these systems in a true emergency, and avoids the risk of unexpected
    misconfiguration.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transitioning on-call engineers to using only low-dependency systems can be
    implemented gradually and by different means, depending on the business criticality
    of each system.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Split when you cannot mirror traffic
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an alternative to request mirroring, you can split requests between disjoint
    sets of servers. This is appropriate if request mirroring is not feasible—for
    example, when you have no control over client code, but load balancing at the
    level of request routing is feasible. Consequently, splitting requests works only
    when alternative components use the same protocols, as is often the case with
    high-capacity and high-availability versions of a component.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Another application of this strategy is to distribute traffic across a set of
    failure domains. If your load balancing targets a single failure domain, you can
    run focused experiments against that domain. Because a failure domain has lower
    capacity, attacking it and eliciting resilient responses requires less load. You
    can quantify the impact of your experiment by comparing the monitoring signals
    from other failure domains. By adding load shedding and throttling, you further
    increase the quality of output from the experiment.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Oversubscribe but prevent complacency
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quota assigned to customers but not consumed is a waste of resources. Therefore,
    in the interest of maximizing resource utilization, many services oversubscribe
    resources by some sane margin. A margin call on resources may happen at any time.
    A resilient system tracks priorities so it can release lower-priority resources
    to fulfill demand for higher-priority resources. However, you should validate
    whether the system can actually release those resources reliably, and in an acceptable
    amount of time.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Google once had a service that needed a lot of disk space for batch processing.
    User services take priority over batch processing and allocate significant disk
    reserves for usage spikes. We permitted the batch processing service to utilize
    disks unused by the user services, under a specific condition: any disks in a
    particular cluster must be fully released within *X* hours. The validation strategy
    we developed consisted of periodically moving the batch processing service out
    of a cluster, measuring how long the move took, and fixing any new issues uncovered
    at each attempt. This was not a simulation. Our validation ensured that the engineers
    who promised the SLO of *X* hours had both real evidence and real experience.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: These validations are expensive, but most of the costs are absorbed by automation.
    Load balancing limits the costs to managing the resource provisioning at the source
    and target locations. If resource provisioning is mostly automated—for example,
    as is the case with cloud services—it becomes a matter of running scripts or playbooks
    to execute a series of requests to the automation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: For smaller services or companies, the strategy of periodically executing a
    rebalancing applies similarly. The resulting confidence in responding predictably
    to shifts in application load is part of the foundation for software architecture
    that can serve a global user base.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Measure key rotation cycles
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Key rotation is simple in theory, but in practice it may bring unpleasant surprises,
    including full service outages. When validating that key rotation works, you should
    look for at least two distinct outcomes:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Key rotation latency
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The time it takes to complete a single rotation cycle
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Verified loss of access
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Certainty that the old key is fully useless after rotation
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: We recommend periodically rotating keys so they remain ready for nonnegotiable
    emergency key rotations prompted by a security compromise. This means rotating
    keys even if you don’t have to. If the rotation process is expensive, look for
    ways to lower its costs.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'At Google, we’ve experienced that measuring key rotation latency helps with
    multiple objectives:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: You learn whether every service that uses the key is actually able to update
    its configuration. Perhaps a service was not designed for key rotation, or was
    designed for it but was never tested, or a change broke what previously worked.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You learn how long each service takes to rotate your key. Key rotation might
    be as trivial as a file change and server restart, or as involved as a gradual
    rollout across all world regions.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You discover how other system dynamics delay the key rotation process.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring key rotation latency has helped us form a realistic expectation of
    the entire cycle, both in normal and emergency circumstances. Account for possible
    rollbacks (caused by key rotation or other events), change freezes for services
    out of error budget, and serialized rollouts due to failure domains.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: How to verify loss of access via an old key is likely case-specific. It’s not
    easy to prove that all instances of the old key were destroyed, so ideally you
    can demonstrate that attempting to use the old key fails, after which point you
    can destroy the old key. When this approach isn’t practical, you can rely on key
    deny-list mechanisms (for example, CRLs). If you have a central certificate authority
    and good monitoring, you may be able to create alerts if any ACLs list the old
    key’s fingerprint or serial number.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Practical Advice: Where to Begin'
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing resilient systems isn’t a trivial task. It takes time and energy,
    and diverts efforts from other valuable work. You need to consider the tradeoffs
    carefully, according to the degree of resilience you want, and then pick from
    the wide spectrum of options we describe—the few or many solutions that fit your
    needs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'In order of costs:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Failure domains and blast radius controls have the lowest costs because of their
    relatively static nature, yet offer significant improvements.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: High-availability services are the next most cost-effective solution.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider these options next:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Consider deploying load-shedding and throttling capabilities if your organization’s
    scale or risk aversion justifies investing in active automation for resilience.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the effectiveness of your defenses against DoS attacks (see [Chapter 10](ch10.html#mitigating_denial_of_service_attacks)).
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you build a low-dependency solution, introduce a process or a mechanism to
    ensure that it stays low dependency over time.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It can be hard to overcome a resistance to invest in resilience improvements,
    because the benefits manifest as an absence of problems. These arguments might
    help:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Deploying failure domains and blast radius controls will have a lasting effect
    on future systems. The isolation techniques can encourage or enforce good separation
    of operational failure domains. Once in place, they will inevitably make it harder
    to design and deploy unnecessarily coupled or fragile systems.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular key change and rotation techniques and exercises not only ensure preparation
    for security incidents, but also give you general cryptographic agility—for example,
    knowing you can upgrade encryption primitives.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relatively low additional cost of deploying high-availability instances
    of a service provides for a cheap way to examine how much you might be able to
    improve the service’s availability. It’s also cheap to abandon.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load-shedding and throttling capabilities, along with the other approaches covered
    in [“Controlling Degradation”](#controlling_degradation), reduce the cost of the
    resources the company needs to maintain. The resulting user-visible improvements
    often apply to the most valued product features.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling degradation critically contributes to the speed and effectiveness
    of first reaction when defending against DoS attacks.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-dependency solutions are relatively expensive yet rarely used in practice.
    To determine whether they are worth their cost, it can help to know how much time
    it would take to bring up all the dependencies of the business-critical services.
    You can then compare the costs and conclude whether it’s better to invest your
    time elsewhere.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whatever resilience solutions you put together, look for affordable ways to
    keep them continuously validated, and avoid cost cutting that risks their effectiveness.
    The benefit from investing in validation is in locking in, for the long term,
    the compounding value of all other resilience investments. If you automate these
    techniques, the engineering and support teams can focus on delivering new value.
    The cost of automating and monitoring will ideally be amortized across other efforts
    and products your company is pursuing.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: You will probably periodically run out of money or time you can invest into
    resilience. The next time you have the opportunity to spend more of these limited
    resources, consider streamlining the costs of your already deployed resilience
    mechanisms first. Once you are confident in their quality and efficiency, venture
    into more resilience options.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed various ways to build resilience into the security
    and reliability of a system, starting from the design phase. In order to provide
    resilience, humans need to make choices. We can optimize some choices with automation,
    but for others we still need humans.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Resilience of reliability properties helps preserve a system’s most important
    functionality, so the system doesn’t fully succumb under conditions of excessive
    load or extensive failures. If the system does break, this functionality extends
    the time available for responders to organize, prevent more damage, or, if necessary,
    engage in manual recovery. Resilience helps systems withstand attacks and defends
    against attempts to gain long-term access. If an attacker breaks into the system,
    design features like blast radius controls limit the damage.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Ground your design strategies in defense in depth. Examine a system’s security
    the same way you view uptime and reliability. At its core, defense in depth is
    like *N*+1 redundancy for your defenses. You don’t trust all of your network capacity
    to a single router or switch, so why trust a single firewall or other defense
    measure? In designing for defense in depth, always assume and check for failures
    in different layers of security: failures in outer perimeter security, the compromise
    of an endpoint, an insider attack, and so on. Plan for lateral moves with the
    intent of stopping them.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Even when you design your systems to be resilient, it’s possible that resilience
    will fall short at some point and your system will break. The next chapter discusses
    what happens *after* that happens: how do you recover broken systems, and how
    can you minimize the damage caused by breakages?'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#ch08fn1-marker)) Protected sandboxes provide an isolated environment
    for untrusted code and data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.html#ch08fn2-marker)) Google runs bug bounty [reward programs](https://oreil.ly/ZQGNW).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch08.html#ch08fn3-marker)) See Singh, Soram Ranbir, Ajoy Kumar Khan,
    and Soram Rakesh Singh. 2016\. “Performance Evaluation of RSA and Elliptic Curve
    Cryptography.” *Proceedings of the 2nd International Conference on Contemporary
    Computing and Informatics*: 302–306\. doi:10.1109/IC3I.2016.7917979.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.html#ch08fn4-marker)) This is described in [Chapter 20 of the SRE
    book](https://landing.google.com/sre/sre-book/chapters/load-balancing-datacenter/).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.html#ch08fn5-marker)) See [Chapter 21 of the SRE book](https://landing.google.com/sre/sre-book/chapters/handling-overload/).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.html#ch08fn6-marker)) The concepts of “failing open” and “fail closed”
    refer to the service *remaining operational* (being reliable) or *shutting down*
    (being secure), respectively. The terms “fail open” and “fail closed” are often
    used interchangeably with “fail safe” and “fail secure,” as described in [Chapter 1](ch01.html#the_intersection_of_security_and_reliab).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.html#ch08fn7-marker)) See [Chapter 2 of the SRE book](https://landing.google.com/sre/sre-book/chapters/production-environment/)
    for a description of the production environment at Google.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch08.html#ch08fn8-marker)) Typically, this instance of the service would
    still be served by many replicas of the underlying server, but it would function
    as a single logical compartment.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch08.html#ch08fn9-marker)) For example, Google Cloud Platform offers so-called
    [sole-tenant nodes](https://oreil.ly/anLXq).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch08.html#ch08fn10-marker)) A breakglass mechanism is one that can bypass
    policies to allow engineers to quickly resolve outages. See [“Breakglass”](ch05.html#breakglass).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch08.html#ch08fn11-marker)) Systems using ACLs must fail closed (secure),
    with access explicitly granted by ACL entries.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch08.html#ch08fn12-marker)) See [Chapter 3 in the SRE book](https://landing.google.com/sre/sre-book/chapters/embracing-risk/).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch08.html#ch08fn13-marker)) This differs from unit tests, integration
    tests, and load tests, which are covered in [Chapter 13](ch13.html#onethree_testing_code).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch08.html#ch08fn14-marker)) This is similar to what the Unix command
    `tee` does for `stdin`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL

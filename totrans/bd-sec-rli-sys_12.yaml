- en: Chapter 8\. Design for Resilience
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。弹性设计
- en: By Vitaliy Shipitsyn, Mitch Adler, Zoltan Egyed, and Paul Blankinship
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 维塔利·希皮茨因、米奇·阿德勒、佐尔坦·埃吉德和保罗·布兰金希普
- en: with Jesus Climent, Jessie Yang, Douglas Colish, and Christoph Kern
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与耶稣·克利门特、杰西·杨、道格拉斯·科利什和克里斯托夫·科恩一起
- en: As a part of system design, “resilience” describes the system’s ability to hold
    out against a major malfunction or disruption. Resilient systems can recover automatically
    from failures in parts of the system—or possibly the failure of the entire system—and
    return to normal operations after the problems are addressed. Services in a resilient
    system ideally remain running throughout an incident, perhaps in a degraded mode.
    Designing resilience into every layer of a system’s design helps defend that system
    against unanticipated failures and attack scenarios.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为系统设计的一部分，“弹性”描述了系统抵抗重大故障或中断的能力。具有弹性的系统可以自动从系统部分的故障中恢复，甚至可能是整个系统的故障，并在问题得到解决后恢复正常运行。弹性系统中的服务理想情况下在事故期间始终保持运行，可能是以降级模式。在系统设计的每一层中设计弹性有助于防御系统不可预期的故障和攻击场景。
- en: Designing a system for resilience is different from designing for recovery (covered
    in depth in [Chapter 9](ch09.html#design_for_recovery)). Resilience is closely
    tied to recovery, but while recovery focuses on the ability to fix systems *after*
    they break, resilience is about designing systems that *delay* or *withstand*
    breakage. Systems designed with a focus on both resilience and recovery are better
    able to recover from failures, and require minimal human intervention.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为弹性设计系统与为恢复设计系统有所不同（在[第9章](ch09.html#design_for_recovery)中深入讨论）。弹性与恢复密切相关，但是恢复侧重于在系统*发生*故障后修复系统的能力，而弹性是关于设计*延迟*或*承受*故障的系统。注重弹性和恢复的系统更能够从故障中恢复，并且需要最少的人为干预。
- en: Design Principles for Resilience
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性设计原则
- en: A system’s resilience properties are built on the design principles discussed
    earlier in [Part II](part02.html#designing_systems). In order to evaluate a system’s
    resilience, you must have a good understanding of how that system is designed
    and built. You need to align closely with other design qualities covered in this
    book—least privilege, understandability, adaptability, and recovery—to strengthen
    your system’s stability and resilience attributes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的弹性属性建立在本书[第II部分](part02.html#designing_systems)中讨论的设计原则之上。为了评估系统的弹性，您必须对系统的设计和构建有很好的了解。您需要与本书中涵盖的其他设计特性密切配合——最小特权、可理解性、适应性和恢复——以加强系统的稳定性和弹性属性。
- en: 'The following approaches, each of which this chapter explores in depth, characterize
    a resilient system:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法是本章深入探讨的，它们表征了一个具有弹性的系统：
- en: Design each layer in the system to be independently resilient. This approach
    builds defense in depth with each layer.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计系统的每一层都具有独立的弹性。这种方法在每一层中构建了深度防御。
- en: Prioritize each feature and calculate its cost, so you understand which features
    are critical enough to attempt to sustain no matter how much load the system experiences,
    and which features are less important and can be throttled or disabled when problems
    arise or resources are constrained. You can then determine where to apply the
    system’s limited resources most effectively, and how to maximize the system’s
    serving capabilities.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑每个功能并计算其成本，以便了解哪些功能足够关键，可以尝试在系统承受多大负载时维持，哪些功能不那么重要，可以在出现问题或资源受限时进行限制或禁用。然后确定在哪里最有效地应用系统有限的资源，以及如何最大化系统的服务能力。
- en: Compartmentalize the system along clearly defined boundaries to promote the
    independence of the isolated functional parts. This way, it’s also easier to build
    complementary defense behaviors.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将系统分隔成清晰定义的边界，以促进隔离功能部分的独立性。这样，也更容易构建互补的防御行为。
- en: Use compartment redundancy to defend against localized failures. For global
    failures, have some compartments provide different reliability and security properties.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隔舱冗余来防御局部故障。对于全局故障，一些隔舱提供不同的可靠性和安全性属性。
- en: Reduce system reaction time by automating as many of your resilience measures
    as you can safely. Work to discover new failure modes that could benefit either
    from new automation or improvements to existing automation.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自动化尽可能多的弹性措施来减少系统的反应时间。努力发现可能受益于新自动化或改进现有自动化的新故障模式。
- en: Maintain the effectiveness of the system by validating its resilience properties—both
    its automated response and any other resilience attributes of the system.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过验证系统的弹性属性来保持系统的有效性——包括其自动响应和系统的其他弹性属性。
- en: Defense in Depth
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度防御
- en: '*Defense in depth* protects systems by establishing multiple layers of defense
    perimeters. As a result, attackers have limited visibility into the systems, and
    successful exploits are harder to launch.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度防御*通过建立多层防御边界来保护系统。因此，攻击者对系统的可见性有限，成功利用更难发动。'
- en: The Trojan Horse
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特洛伊木马
- en: The story of the Trojan Horse, as told by Virgil in the *Aeneid*, is a cautionary
    tale about the dangers of an inadequate defense. After 10 fruitless years besieging
    the city of Troy, the Greek army constructs a large wooden horse that it presents
    as a gift to the Trojans. The horse is brought within the walls of Troy, and attackers
    hiding inside the horse burst forth, exploit the city’s defenses from the inside,
    and then open the city gates to the entire Greek army, which destroys the city.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 特洛伊木马的故事，由维吉尔在《埃涅阿斯纪》中讲述，是一个关于不足防御危险的警示故事。在围困特洛伊城十年无果之后，希腊军队建造了一匹巨大的木马，作为礼物送给特洛伊人。木马被带进特洛伊城墙内，藏在木马里的攻击者突然冲出来，从内部利用了城市的防御，然后打开城门让整个希腊军队进入，摧毁了城市。
- en: Imagine this story’s ending if the city had planned for defense in depth. First,
    Troy’s defensive forces might have inspected the Trojan Horse more closely and
    discovered the deception. If the attackers had managed to make it inside the city
    gates, they could have been confronted with another layer of defense—for example,
    the horse might have been enclosed in a secure courtyard, with no access to the
    rest of the city.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果这个城市计划了深度防御，这个故事的结局会是什么样子。首先，特洛伊的防御力量可能会更仔细地检查特洛伊木马并发现欺骗。如果攻击者设法进入城门，他们可能会面对另一层防御，例如，木马可能被封闭在一个安全的庭院里，无法进入城市的其他地方。
- en: What does a 3,000-year-old story tell us about security at scale, or even security
    itself? First, if you’re trying to understand the strategies you need to defend
    and contain a system, you must first understand the attack itself. If we consider
    the city of Troy as a system, we can walk through the attackers’ steps (stages
    of the attack) to uncover weaknesses that defense in depth might address.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个3000年前的故事告诉我们关于规模安全甚至安全本身的什么？首先，如果你试图了解你需要防御和遏制系统的策略，你必须首先了解攻击本身。如果我们把特洛伊城看作一个系统，我们可以按照攻击者的步骤（攻击的阶段）来发现深度防御可能解决的弱点。
- en: 'At a high level, we can divide the Trojan attack into four stages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们可以将特洛伊木马攻击分为四个阶段：
- en: '*Threat modeling and vulnerability discovery*—Assess the target and specifically
    look for defenses and weaknesses. The attackers couldn’t open the city gates from
    the outside, but could they open them from the inside?'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*威胁建模和漏洞发现*——评估目标并专门寻找防御和弱点。攻击者无法从外部打开城门，但他们能从内部打开吗？'
- en: '*Deployment*—Set up the conditions for the attack. The attackers constructed
    and delivered an object that Troy eventually brought inside its city walls.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*部署*——为攻击设置条件。攻击者构建并交付了一个特洛伊最终带进城墙内的物体。'
- en: '*Execution*—Carry out the actual attack, which capitalizes on the previous
    stages. Soldiers came out of the Trojan Horse and opened the city gates to let
    in the Greek army.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*执行*——执行实际的攻击，利用之前阶段的攻击。士兵们从特洛伊木马中出来，打开城门让希腊军队进入。'
- en: '*Compromise*—After successful execution of the attack, the damage occurs and
    mitigation begins.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*妥协*——在成功执行攻击后，损害发生并开始减轻。'
- en: The Trojans had opportunities to disrupt the attack at every stage before the
    compromise, and paid a heavy price for missing them. In the same way, your system’s
    defense in depth can reduce the price you might have to pay if your system is
    ever compromised.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 特洛伊人在妥协之前的每个阶段都有机会阻止攻击，并因错过这些机会而付出了沉重的代价。同样，你系统的深度防御可以减少如果你的系统被攻击的话可能需要付出的代价。
- en: Threat modeling and vulnerability discovery
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 威胁建模和漏洞发现
- en: Attackers and defenders can both assess a target for weaknesses. Attackers perform
    reconnaissance against their targets, find weaknesses, and then model attacks.
    Defenders should do what they can to limit the information exposed to attackers
    during reconnaissance. But because defenders can’t completely prevent this reconnaissance,
    they must detect it and use it as a signal. In the case of the Trojan Horse, the
    defenders might have been on the alert because of inquiries from strangers about
    how the gates were defended. In light of that suspicious activity, they would
    have then exercised extra caution when they found a large wooden horse at the
    city gate.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者和防御者都可以评估目标的弱点。攻击者对他们的目标进行侦察，找到弱点，然后模拟攻击。防御者应该尽力限制在侦察期间向攻击者暴露的信息。但是因为防御者无法完全阻止这种侦察，他们必须检测到它并将其用作信号。在特洛伊木马的情况下，防御者可能会因为陌生人询问城门的防御方式而保持警惕。鉴于这种可疑活动，当他们在城门口发现一个大木马时，他们会更加谨慎。
- en: 'Making note of these strangers’ inquiries amounts to gathering intelligence
    on threats. There are many ways to do this for your own systems, and you can even
    choose to outsource some of them. For example, you might do the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些陌生人的询问相当于收集威胁情报。有许多方法可以为你自己的系统做到这一点，你甚至可以选择外包其中的一些。例如，你可以做以下事情：
- en: Monitor your system for port and application scans.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视你的系统进行端口和应用程序扫描。
- en: Keep track of DNS registrations of URLs similar to yours—an attacker might use
    those registrations for spear phishing attacks.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪类似你的URL的DNS注册情况——攻击者可能会利用这些注册进行钓鱼攻击。
- en: Buy threat intelligence data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买威胁情报数据。
- en: Build a threat intelligence team to study and passively monitor the activities
    of known and likely threats to your infrastructure. While we don’t recommend that
    small companies invest resources in this approach, it may become cost-effective
    as your company grows.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个威胁情报团队来研究和被动监视已知和可能对你基础设施构成威胁的活动。虽然我们不建议小公司投入资源进行这种方法，但随着公司的发展，这可能会变得具有成本效益。
- en: 'As a defender with inside knowledge of your system, your assessment can be
    more detailed than the attacker’s reconnaissance. This is a critical point: if
    you understand your system’s weaknesses, you can defend against them more efficiently.
    And the more you understand the methods that attackers are currently using or
    are capable of exploiting, the more you amplify this effect. A word of caution:
    beware of developing blind spots to attack vectors you consider unlikely or irrelevant.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对你系统内部了解的防御者，你的评估可以比攻击者的侦察更详细。这是一个关键点：如果你了解你系统的弱点，你可以更有效地防御它们。而且你了解攻击者目前正在使用或有能力利用的方法越多，你就越能放大这种效果。一个警告：要小心对你认为不太可能或不相关的攻击向量产生盲点。
- en: Deployment of the attack
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击部署
- en: If you know that attackers are performing reconnaissance against your system,
    efforts to detect and stop the attack are critical. Imagine that the Trojans had
    decided not to permit the wooden horse to enter the city gates because it was
    created by someone they did not trust. Instead, they might have thoroughly inspected
    the Trojan Horse before allowing it inside, or perhaps they might have just set
    it on fire.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道攻击者正在对你的系统进行侦察，那么检测和阻止攻击的努力就至关重要。想象一下，如果特洛伊人决定不允许木马进入城门，因为它是由他们不信任的人创建的。相反，他们可能会在允许它进入之前彻底检查特洛伊木马，或者可能会将其点燃。
- en: In modern times, you can detect potential attacks using network traffic inspection,
    virus detection, software execution control, protected sandboxes,^([1](ch08.html#ch08fn1))
    and proper provisioning of privileges for signaling anomalous use.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代，你可以使用网络流量检查、病毒检测、软件执行控制、受保护的沙箱^([1](ch08.html#ch08fn1))和适当的特权配置来检测潜在的攻击。
- en: Execution of the attack
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击的执行
- en: If you can’t prevent all deployments from adversaries, you need to limit the
    blast radius of potential attacks. If the defenders had boxed in the Trojan Horse,
    thereby limiting their exposure, the attackers would have had a much harder time
    advancing from their hiding spot unnoticed. Cyberwarfare refers to this tactic
    (described in more detail in [“Runtime layers”](#runtime_layers)) as *sandboxing*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法阻止对手的所有部署，你需要限制潜在攻击的影响范围。如果防御者将特洛伊木马圈起来，从而限制了他们的暴露，攻击者将会更难从他们的藏身之处不被察觉地前进。网络战将这种策略称为*沙盒化*（在[“运行时层”](#runtime_layers)中有更详细的描述）。
- en: Compromise
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 妥协
- en: When the Trojans woke to find their enemies standing over their beds, they knew
    their city had been compromised. This awareness came well after the actual compromise
    occurred. Many unfortunate banks faced a similar situation in 2018 after their
    infrastructure was polluted by [EternalBlue](https://oreil.ly/wNI2u) and [WannaCry](https://oreil.ly/irovS).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当特洛伊人醒来发现敌人站在他们的床边时，他们知道他们的城市已经被妥协了。这种意识是在实际妥协发生之后才出现的。许多不幸的银行在2018年面临了类似的情况，因为他们的基础设施被[EternalBlue](https://oreil.ly/wNI2u)和[WannaCry](https://oreil.ly/irovS)污染了。
- en: How you respond from this point forward determines how long your infrastructure
    remains compromised.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何从这一点做出回应，将决定你的基础设施被妥协的时间有多长。
- en: Google App Engine Analysis
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google App Engine分析
- en: 'Let’s consider defense in depth as applied to a more modern case: Google App
    Engine. Google App Engine allows users to host application code, and to scale
    as load increases without managing networks, machines, and operating systems.
    [Figure 8-1](#a_simplified_view_of_google_app_engine) shows a simplified architecture
    diagram of App Engine in its early days. Securing the application code is a developer’s
    responsibility, while securing the Python/Java runtime and the base OS is Google’s
    responsibility.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑深度防御如何应用到一个更现代的案例：Google App Engine。Google App Engine允许用户托管应用程序代码，并在负载增加时进行扩展，而无需管理网络、机器和操作系统。[图8-1](#a_simplified_view_of_google_app_engine)显示了App
    Engine早期的简化架构图。保护应用程序代码是开发者的责任，而保护Python/Java运行时和基本操作系统是Google的责任。
- en: '![A simplified view of Google App Engine architecture](assets/bsrs_0801.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Google App Engine架构的简化视图](assets/bsrs_0801.png)'
- en: Figure 8-1\. A simplified view of Google App Engine architecture
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1. Google App Engine架构的简化视图
- en: The original implementation of Google App Engine required special process isolation
    considerations. At that time Google used traditional POSIX user isolation as its
    default strategy (through distinct user processes), but we decided that running
    each user’s code in an independent virtual machine was too inefficient for the
    level of planned adoption. We needed to figure out how to run third-party, untrusted
    code in the same way as any other job within Google’s infrastructure.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Google App Engine的原始实现需要特殊的进程隔离考虑。当时，Google使用传统的POSIX用户隔离作为默认策略（通过不同的用户进程），但我们决定在计划的采用程度上，将每个用户的代码运行在独立的虚拟机中效率太低。我们需要找出如何以与Google基础设施中的任何其他作业相同的方式运行第三方、不受信任的代码。
- en: Risky APIs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险的API
- en: 'Initial threat modeling for App Engine turned up a few worrisome areas:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: App Engine的初始威胁建模发现了一些令人担忧的领域：
- en: Network access was problematic. Up to that point, all applications running within
    the Google production network were assumed to be trusted and authenticated infrastructure
    components. Since we were introducing arbitrary, untrusted third-party code into
    this environment, we needed a strategy to isolate internal APIs and network exposure
    from App Engine. We also needed to bear in mind that App Engine itself was running
    on that same infrastructure, and therefore was dependent on access to those same
    APIs.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络访问存在问题。在那之前，所有在Google生产网络中运行的应用程序都被认为是受信任的和经过身份验证的基础设施组件。由于我们在这个环境中引入了任意的、不受信任的第三方代码，我们需要一种策略来将App
    Engine的内部API和网络暴露与其隔离开。我们还需要记住，App Engine本身是运行在同一基础设施上的，因此依赖于对这些API的访问。
- en: The machines running user code required access to the local filesystem. At least
    this access was limited to the directories belonging to the given user, which
    helped protect the execution environment and reduce the risk of user-provided
    applications interfering with applications of other users on the same machine.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行用户代码的机器需要访问本地文件系统。至少这种访问被限制在属于特定用户的目录中，这有助于保护执行环境，并减少用户提供的应用程序对同一台机器上其他用户的应用程序的干扰的风险。
- en: The Linux kernel meant that App Engine was exposed to a large attack surface,
    which we wanted to minimize. For example, we wanted to prevent as many classes
    of local privilege escalation as possible.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux内核意味着App Engine暴露在了大规模攻击的表面上，我们希望将其最小化。例如，我们希望尽可能防止许多本地权限提升的类别。
- en: To address these challenges, we first examined limiting user access to each
    API. Our team removed built-in APIs for I/O operations for networking and filesystem
    interactions at runtime. We replaced the built-in APIs with “safe” versions that
    made calls to other cloud infrastructure, rather than directly manipulating the
    runtime environment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们首先检查了限制用户对每个 API 的访问。我们的团队在运行时删除了用于网络和文件系统交互的内置 API。我们用“安全”版本替换了内置
    API，这些版本调用其他云基础设施，而不是直接操作运行时环境。
- en: To prevent users from reintroducing the intentionally removed capabilities to
    the interpreters, we didn’t allow user-supplied compiled bytecode or shared libraries.
    Users had to depend on the methods and libraries we provided, in addition to a
    variety of permitted runtime-only open source implementations that they might
    need.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止用户重新引入解释器中故意删除的功能，我们不允许用户提供的编译字节码或共享库。用户必须依赖我们提供的方法和库，以及各种可能需要的允许的仅运行时开源实现。
- en: Runtime layers
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行时层
- en: We also extensively audited the runtime base data object implementations for
    features that were likely to produce memory corruption bugs. This audit produced
    a handful of upstream bug fixes in each of the runtime environments we launched.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对运行时基本数据对象实现进行了广泛的审计，以查找可能导致内存损坏错误的功能。这次审计在我们推出的每个运行时环境中产生了一些上游错误修复。
- en: We assumed that at least some of these defensive measures would fail, as we
    weren’t likely to find and predict every exploitable condition in the chosen runtimes.
    We decided to specifically adapt the Python runtime to compile down to Native
    Client (NaCL) bitcode. NaCL allowed us to prevent many classes of memory corruption
    and control-flow subversion attacks that our in-depth code auditing and hardening
    missed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设至少一些这些防御措施会失败，因为我们不太可能找到和预测所选择运行时中的每个可利用条件。我们决定将 Python 运行时专门适应编译为 Native
    Client (NaCL) 位码。NaCL 允许我们防止许多类内存损坏和控制流颠覆攻击，这些攻击我们深度代码审计和加固都错过了。
- en: We weren’t completely satisfied that NaCL would contain all risky code breakouts
    and bugs in their entirety, so we added a second layer of `ptrace` sandboxing
    to filter and alert on unexpected system calls and parameters. Any violations
    of these expectations immediately terminated the runtime and dispatched alerts
    at high priority, along with logs of relevant activity.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不完全满意 NaCL 能够完全包含所有风险代码突破和错误，因此我们添加了第二层 `ptrace` 沙盒，以过滤和警报意外的系统调用和参数。对这些期望的任何违反立即终止运行时，并以高优先级发送警报，以及相关活动的日志。
- en: Over the next five years, the team caught a few cases of anomalous activity
    resulting from exploitable conditions in one of the runtimes. In each case, our
    sandbox layer gave us a significant advantage over attackers (whom we confirmed
    to be security researchers), and our multiple layers of sandboxing contained their
    activities within the design parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的五年里，团队发现了一些异常活动的案例，这是由于其中一个运行时中的可利用条件。在每种情况下，我们的沙盒层都给我们带来了明显的优势，使我们能够控制他们的活动在设计参数内。
- en: Functionally, the Python implementation in App Engine featured the sandboxing
    layers shown in [Figure 8-2](#python_implementation_in_app_engine).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 功能上，App Engine 中的 Python 实现具有 [图 8-2](#python_implementation_in_app_engine)
    中显示的沙盒层。
- en: '![Sandboxing layers of Python implementation in App Engine](assets/bsrs_0802.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![App Engine 中 Python 实现的沙盒层](assets/bsrs_0802.png)'
- en: Figure 8-2\. Sandboxing layers of Python implementation in App Engine
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2. App Engine 中 Python 实现的沙盒层
- en: App Engine’s layers are complementary, with each layer anticipating the weak
    points or likely failures of the previous one. As defense activations move through
    the layers, signals of a compromise become stronger, allowing us to focus efforts
    on probable attacks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: App Engine 的各层是互补的，每一层都预期了前一层的弱点或可能的失败。随着防御激活穿过各层，对妥协的信号变得更强，使我们能够集中精力应对可能的攻击。
- en: Although we took a thorough and layered approach to security for Google App
    Engine, we still benefited from external help in securing the environment.^([2](ch08.html#ch08fn2))
    In addition to our team finding anomalous activity, external researchers discovered
    several cases of exploitable vectors. We’re grateful to the researchers who found
    and disclosed the gaps.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们对 Google App Engine 的安全性采取了彻底和分层的方法，但我们仍然受益于在保护环境方面的外部帮助。除了我们的团队发现异常活动外，外部研究人员还发现了几种可利用的向量。我们对发现并披露这些漏洞的研究人员表示感激。
- en: Controlling Degradation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制退化
- en: When designing for defense in depth, we assume that system components or even
    entire systems can fail. Failures can happen for many reasons, including physical
    damage, a hardware or network malfunction, a software misconfiguration or bug,
    or a security compromise. When a component fails, the impact may extend to every
    system that depends on it. The global pool of similar resources also becomes smaller—for
    example, disk failures reduce overall storage capacity, network failures reduce
    bandwidth and increase latency, and software failures reduce the computational
    capacity system-wide. The failures might compound—for example, a storage shortage
    could lead to software failures.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度防御设计时，我们假设系统组件甚至整个系统都可能失败。失败可能由许多原因引起，包括物理损坏、硬件或网络故障、软件配置错误或错误，或安全妥协。当组件失败时，影响可能会扩展到依赖它的每个系统。类似资源的全局池也变得更小
    - 例如，磁盘故障会减少整体存储容量，网络故障会减少带宽并增加延迟，软件故障会降低整个系统的计算能力。故障可能会相互叠加 - 例如，存储空间不足可能导致软件故障。
- en: Resource shortages like these, or a sudden spike in incoming requests like those
    caused by the [Slashdot effect](https://oreil.ly/Z1UL8), misconfiguration, or
    a denial-of-service attack, could lead to system overload. When a system’s load
    exceeds its capacity, its response inevitably begins to degrade, and that can
    lead to a completely broken system with no availability. Unless you’ve planned
    for this scenario in advance, you don’t know where the system may break—but this
    will most likely be where the system is weakest, and not where it’s safest.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源短缺，或者像[Slashdot效应](https://oreil.ly/Z1UL8)所引起的突然请求激增，错误配置，或者拒绝服务攻击，都可能导致系统超载。当系统负载超过其容量时，其响应必然开始退化，这可能导致一个完全破碎的系统，没有可用性。除非您事先计划了这种情况，否则您不知道系统可能会在哪里崩溃，但这很可能是系统最薄弱的地方，而不是最安全的地方。
- en: 'To control degradation, you must select which system properties to disable
    or adjust when dire circumstances arise, while doing all you can to protect the
    system’s security. If you *deliberately* design multiple response options for
    circumstances like these, the system can make use of controlled breakpoints, rather
    than experiencing a chaotic collapse. Instead of triggering cascading failures
    and dealing with the mayhem that follows, your system can respond by *degrading
    gracefully*. Here are some ways you can make that happen:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制退化，当出现严重情况时，您必须选择禁用或调整哪些系统属性，同时尽一切可能保护系统的安全性。如果您*故意*为这些情况设计多个响应选项，系统可以利用受控的断点，而不是经历混乱的崩溃。您的系统可以通过*优雅地退化*来响应，而不是触发级联故障并处理随之而来的混乱。以下是一些实现这一目标的方法：
- en: Free up resources and decrease the rate of failed operations by disabling infrequently
    used features, the least critical functions, or high-cost service capabilities.
    You can then apply the freed resources to preserving important features and functions.
    For example, most systems that accept TLS connections support both Elliptic Curve
    (ECC) and RSA cryptosystems. Depending on your system’s implementation, one of
    the two will be cheaper while giving you comparable security. In software, ECC
    is less resource-intensive for private key operations.^([3](ch08.html#ch08fn3))
    Disabling support for RSA when systems are resource-constrained will make room
    for more connections at the lower cost of ECC.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过禁用不经常使用的功能、最不重要的功能或高成本的服务功能，释放资源并减少失败操作的频率。然后，您可以将释放的资源应用于保留重要功能和功能。例如，大多数接受TLS连接的系统都支持椭圆曲线（ECC）和RSA加密系统。根据您系统的实现，其中一个将更便宜，同时提供可比较的安全性。在软件中，ECC对私钥操作的资源消耗较少。^([3](ch08.html#ch08fn3))当系统资源受限时，禁用对RSA的支持将为ECC的更低成本提供更多连接空间。
- en: 'Aim for system response measures to take effect quickly and automatically.
    This is easiest with servers under your direct control, where you can arbitrarily
    toggle operational parameters of any scope or granularity. User clients are harder
    to control: they have long rollout cycles because client devices may postpone
    or be unable to receive updates. Additionally, the diversity of client platforms
    increases the chance of rollbacks of response measures due to unanticipated incompatibilities.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是使系统响应措施能够快速自动地生效。这在您直接控制的服务器上最容易，您可以任意切换任何范围或粒度的操作参数。用户客户端更难控制：它们具有较长的发布周期，因为客户端设备可能推迟或无法接收更新。此外，客户端平台的多样性增加了由于意外不兼容性而导致响应措施回滚的机会。
- en: Understand which systems are critical for your company’s mission as well as
    their relative importance and interdependencies. You might have to preserve the
    minimal features of these systems in proportion to their relative value. For example,
    Google’s Gmail has a “simple HTML mode” that disables fancy UI styling and search
    autocompletion but allows users to continue opening mail messages. Network failures
    limiting bandwidth in a region could deprioritize even this mode if that allowed
    network security monitoring to continue to defend user data in the region.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解哪些系统对公司的使命至关重要，以及它们的相对重要性和相互依赖性。您可能需要按照它们的相对价值保留这些系统的最小功能。例如，谷歌的Gmail有一个“简单的HTML模式”，它禁用了花哨的UI样式和搜索自动完成，但允许用户继续打开邮件。如果网络故障限制了某个地区的带宽，甚至可以降低这种模式的优先级，如果这样可以让网络安全监控继续保护该地区的用户数据。
- en: If these adjustments meaningfully improve the system’s capacity to absorb load
    or failure, they provide a critical complement to all other resilience mechanisms—and
    give incident responders more time to respond. It’s better to make the essential
    and difficult choices in advance rather than when under pressure during an incident.
    Once individual systems develop a clear degradation strategy, it becomes easier
    to prioritize degradation at a larger scope, across multiple systems or product
    areas.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些调整能够显著提高系统吸收负载或故障的能力，它们将为所有其他弹性机制提供关键的补充，并为事件响应者提供更多的响应时间。最好是提前做出必要和困难的选择，而不是在事件发生时承受压力。一旦个别系统制定了明确的退化策略，就更容易在更大范围内优先考虑退化，跨多个系统或产品领域。
- en: Differentiate Costs of Failures
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分故障成本
- en: There is some cost to any failed operation—for example, a failed data upload
    from a mobile device to an application backend consumes computing resources and
    network bandwidth to set up an RPC and push some data. If you can refactor your
    flows to fail early or cheaply, you may be able to reduce or avoid some failure-related
    waste.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 任何失败操作都会有一定的成本，例如，从移动设备上传数据到应用后端的失败数据上传会消耗计算资源和网络带宽来设置RPC并推送一些数据。如果您可以重构您的流程以便早期或廉价地失败，您可能能够减少或避免一些与失败相关的浪费。
- en: 'To reason about cost of failures:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于故障成本的推理：
- en: Identify the total costs of individual operations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 识别个别操作的总成本。
- en: For example, you could collect CPU, memory, or bandwidth impact metrics during
    load testing of a particular API. Focus first on the most impactful operations—either
    by criticality or frequency—if pressed for time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以在对特定API进行负载测试期间收集CPU、内存或带宽影响指标。如果时间紧迫，首先专注于最具影响力的操作，无论是通过关键性还是频率。
- en: Determine at what stage in the operation these costs are incurred.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 确定在操作的哪个阶段产生了这些成本。
- en: You could inspect source code or use developer tools to collect introspection
    data (for example, web browsers offer tracking of request stages). You could even
    instrument the code with failure simulations at different stages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查源代码或使用开发人员工具来收集内省数据（例如，Web浏览器提供请求阶段的跟踪）。您甚至可以在不同阶段的代码中加入故障模拟。
- en: Armed with the information you gather about operation costs and failure points,
    you can look for changes that could defer higher-cost operations until the system
    progresses further toward success.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 利用您收集的有关操作成本和故障点的信息，您可以寻找可以推迟高成本操作的变化，直到系统更进一步朝着成功发展。
- en: Computing resources
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算资源
- en: The computing resources that a failing operation consumes—from the beginning
    of the operation until failure—are unavailable to any other operations. This effect
    multiplies if clients retry aggressively on failure, a scenario that might even
    lead to a cascading system failure. You can free up computing resources more quickly
    by checking for error conditions earlier in the execution flows—for example, you
    can check the validity of data access requests before the system allocates memory
    or initiates data reads/writes. [SYN cookies](https://oreil.ly/EaL2N) can let
    you avoid allocating memory to TCP connection requests originating from spoofed
    IP addresses. CAPTCHA can help to protect the most expensive operations from automated
    abuse.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从操作开始到失败期间消耗的计算资源对任何其他操作都是不可用的。如果客户端在失败时进行积极的重试，这种影响会成倍增加，甚至可能导致系统级联故障。通过在执行流程的早期检查错误条件，您可以更快地释放计算资源，例如，您可以在系统分配内存或启动数据读取/写入之前检查数据访问请求的有效性。[SYN
    cookies](https://oreil.ly/EaL2N)可以让您避免为源自伪造IP地址的TCP连接请求分配内存。CAPTCHA可以帮助保护最昂贵的操作免受自动滥用。
- en: More broadly, if a server can learn that its health is declining (for example,
    from a monitoring system’s signals), you can have the server switch into a lame-duck
    mode:^([4](ch08.html#ch08fn4)) it continues to serve, but lets its callers know
    to throttle down or stop sending requests. This approach provides better signals
    to which the overall environment can adapt, and simultaneously minimizes resources
    diverted to serving errors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，如果服务器可以得知其健康状况正在下降（例如，来自监控系统的信号），您可以让服务器切换到“残废鸭”模式：它继续提供服务，但让其调用者知道要减少或停止发送请求。这种方法提供了更好的信号，整体环境可以适应，同时最小化了用于提供错误的资源。
- en: It’s also possible for multiple instances of a server to become unused because
    of external factors. For example, the services they run could be “drained” or
    isolated because of a security compromise. If you monitor for such conditions,
    the server resources could be temporarily released for reuse by other services.
    Before you reallocate resources, however, you should be certain to secure any
    data that can be helpful for a forensic investigation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能由于外部因素，多个服务器实例变得未被使用。例如，它们运行的服务可能因安全妥协而被“排空”或隔离。如果您监视这种情况，服务器资源可以暂时释放以供其他服务重用。然而，在重新分配资源之前，您应该确保保护任何可能对法医调查有帮助的数据。
- en: User experience
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户体验
- en: The system’s interactions with the user should have an acceptable level of behavior
    in degraded conditions. An ideal system informs users that its services might
    be malfunctioning, but lets them continue to interact with parts that remain functional.
    Systems might try different connection, authentication, and authorization protocols
    or endpoints to preserve the functional state. Any data staleness or security
    risks due to failures should be clearly communicated to the users. Features that
    are no longer safe to use should be explicitly disabled.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 系统与用户的交互在降级条件下应具有可接受的行为水平。理想的系统会通知用户其服务可能出现故障，但允许他们继续与保持功能的部分进行交互。系统可能尝试不同的连接、认证和授权协议或端点以保持功能状态。由于故障造成的任何数据陈旧或安全风险应清楚地向用户传达。不再安全使用的功能应明确禁用。
- en: For example, adding an offline mode to an online collaboration application can
    preserve core functionality despite temporary loss of online storage, the ability
    to show updates from others, or integration with chat features. In a chat application
    with end-to-end encryption, users might occasionally change their encryption key
    used for protecting communications. Such an application would keep all previous
    communications accessible, because their authenticity is not affected by this
    change.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，向在线协作应用添加离线模式可以在临时丢失在线存储、显示他人更新或集成聊天功能的情况下保留核心功能。在端到端加密的聊天应用中，用户可能偶尔更改用于保护通信的加密密钥。这样的应用将保持所有先前的通信可访问，因为它们的真实性不受此更改的影响。
- en: In contrast, an example of a poor design would be a situation where the entire
    GUI becomes unresponsive because one of its RPCs to a backend has timed out. Imagine
    a mobile application designed to connect to its backends on startup in order to
    display only the freshest content. The backends could be unreachable simply because
    the device’s user disabled the connectivity intentionally; still, users would
    not see even the previously cached data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，一个糟糕的设计示例是整个GUI变得无响应，因为其后端的RPC之一已超时。想象一下，设计为在启动时连接到后端以仅显示最新内容的移动应用。后端可能无法访问，仅仅是因为设备的用户有意禁用了连接；尽管如此，用户仍然看不到以前缓存的数据。
- en: A user experience (UX) research and design effort may be required to arrive
    at a UX solution that delivers usability and productivity in a degraded mode.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要用户体验（UX）研究和设计工作，以找到在降级模式下提供可用性和生产力的UX解决方案。
- en: Speed of mitigation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减轻速度
- en: The recovery speed of a system after it fails affects the cost of that failure.
    This response time includes the time between when a human or automation makes
    a mitigating change and when the last affected instance of the component is updated
    and recovers. Avoid placing critical points of failure into components like client
    applications, which are harder to control.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 系统在失败后的恢复速度会影响失败的成本。此响应时间包括人类或自动化进行减轻变化的时间以及最后一个受影响的组件实例更新和恢复的时间。避免将关键故障点放入像客户端应用这样更难控制的组件中。
- en: Going back to the earlier example of the mobile application that initiates a
    freshness update on launch, that design choice turns connectivity to the backends
    into a critical dependency. In this situation, the initial problems are amplified
    by the slow and uncontrollable rate of application updates.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回到之前的例子，移动应用在启动时发起新鲜度更新的设计选择将连接性转变为关键依赖。在这种情况下，初始问题会因应用程序更新的缓慢和不可控速度而被放大。
- en: Deploy Response Mechanisms
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署响应机制
- en: Ideally, a system should actively respond to deteriorating conditions with safe,
    preprogrammed measures that maximize the effectiveness of the response while minimizing
    risks to security and reliability. Automated measures can generally perform better
    than humans—humans are slower to respond, may not have sufficient network or security
    access to complete a necessary operation, and aren’t as good at solving for multiple
    variables. However, humans should remain in the loop to provide checks and balances,
    and to make decisions under unforeseen or nontrivial circumstances.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，系统应该通过安全的、预先编程的措施积极应对恶化的条件，以最大限度地提高响应的效果，同时最大限度地减少对安全性和可靠性的风险。自动化措施通常比人类表现更好——人类反应较慢，可能没有足够的网络或安全访问权限来完成必要的操作，并且在解决多个变量时不如机器。然而，人类应该保持在循环中，以提供检查和平衡，并在意外或非平凡情况下做出决策。
- en: 'Let’s consider in detail managing excessive load—whether due to loss of serving
    capacity, benign traffic spikes, or even DoS attacks. Humans might not respond
    fast enough, and traffic could overwhelm servers enough to lead to cascading failures
    and an eventual global service crash. Creating a safeguard by permanently overprovisioning
    servers wastes money and doesn’t guarantee a safe response. Instead, servers should
    adjust how they respond to load based upon current conditions. You can use two
    specific automation strategies here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细考虑管理过度负载的问题——无论是由于服务能力的丧失、良性流量峰值，还是DoS攻击。人类可能反应不够快，流量可能会压倒服务器，导致级联故障和最终全局服务崩溃。通过永久超额配置服务器来创建保障会浪费金钱，并不能保证安全响应。相反，服务器应根据当前条件调整它们对负载的响应方式。在这里可以使用两种具体的自动化策略：
- en: Load shedding is done by returning errors rather than serving requests.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载放弃是通过返回错误而不是提供请求来实现的。
- en: Throttling of clients is done by delaying responses until closer to the request
    deadline.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端的限流是通过延迟响应直到接近请求截止日期来实现的。
- en: '[Figure 8-3](#complete_outage_and_a_possible_cascadin) illustrates a traffic
    spike that exceeds the capacity. [Figure 8-4](#using_load_shedding_and_throttling_to_m)
    illustrates the effects of load shedding and throttling to manage the load spike.
    Note the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#complete_outage_and_a_possible_cascadin)说明了超出容量的流量峰值。[图8-4](#using_load_shedding_and_throttling_to_m)说明了使用负载放弃和限流来管理负载峰值的效果。请注意以下内容：'
- en: The curve represents requests per second, and the area under it represents total
    requests.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线代表每秒请求，曲线下方代表总请求量。
- en: Whitespace represents traffic processed without failure.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空白表示处理流量而没有失败。
- en: The backward-slashed area represents degraded traffic (some requests failed).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反斜线区域代表受损的流量（一些请求失败）。
- en: The crosshatched areas represent rejected traffic (all requests failed).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜线区域代表被拒绝的流量（所有请求失败）。
- en: The forward-slashed area represents traffic subject to prioritization (important
    requests succeeded).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜线区域代表受优先处理的流量（重要请求成功）。
- en: '[Figure 8-3](#complete_outage_and_a_possible_cascadin) shows how the system
    might actually crash, leading to a greater impact in terms of both volume (number
    of requests lost) and time (duration of the outage extends past the traffic spike).
    [Figure 8-3](#complete_outage_and_a_possible_cascadin) also distinguishes the
    uncontrolled nature of degraded traffic (the backward-slashed area) prior to system
    crash. [Figure 8-4](#using_load_shedding_and_throttling_to_m) shows that the system
    with load shedding rejects significantly less traffic than in [Figure 8-3](#complete_outage_and_a_possible_cascadin)
    (the crosshatched area), with the rest of the traffic either processed without
    failure (whitespace area) or rejected if lower priority (forward-slashed area).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-3](#complete_outage_and_a_possible_cascadin)显示了系统可能实际崩溃，导致在请求数量和停机时间方面产生更大的影响。[图8-3](#complete_outage_and_a_possible_cascadin)还区分了系统崩溃前的受控流量的不受控制的性质（反斜线区域）。[图8-4](#using_load_shedding_and_throttling_to_m)显示了负载放弃的系统拒绝的流量明显少于[图8-3](#complete_outage_and_a_possible_cascadin)中的流量（斜线区域），其余流量要么在没有失败的情况下被处理（空白区域），要么如果优先级较低则被拒绝（斜线区域）。'
- en: '![Complete outage and a possible cascading failure from a load spike](assets/bsrs_0803.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![完全停机和负载峰值可能引发级联故障](assets/bsrs_0803.png)'
- en: Figure 8-3\. Complete outage and a possible cascading failure from a load spike
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。完全停机和负载峰值可能引发级联故障
- en: '![Using load shedding and throttling to manage a load spike](assets/bsrs_0804.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![使用负载放弃和限流来管理负载峰值](assets/bsrs_0804.png)'
- en: Figure 8-4\. Using load shedding and throttling to manage a load spike
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。使用负载放弃和限流来管理负载峰值
- en: Load shedding
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载放弃
- en: The primary resilience objective of load shedding (described in [Chapter 22
    of the SRE book](https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/))
    is to stabilize components at maximum load, which can be especially beneficial
    for preserving security-critical functions. When the load on a component starts
    to exceed its capacity, you want the component to serve errors for all excessive
    requests rather than crashing. Crashing makes *all* of the component’s capacity
    unavailable—not just the capacity for the excess requests. When this capacity
    is gone, the load just shifts elsewhere, possibly causing a cascading failure.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 负载分担的主要弹性目标（在[SRE书籍的第22章](https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/)中描述）是将组件稳定在最大负载，这对于保护安全关键功能尤为有益。当组件的负载开始超过其容量时，您希望组件为所有过多的请求提供错误响应，而不是崩溃。崩溃会使*所有*组件的容量不可用——不仅仅是用于过多请求的容量。当这种容量消失时，负载会转移到其他地方，可能导致级联故障。
- en: Load shedding allows you to free server resources even before a server’s load
    reaches capacity, and to make those resources available for more valuable work.
    To select which requests to shed, the server needs to have notions of request
    priority and request cost. You can define a policy that determines how many of
    each request type to shed based upon request priority, request cost, and current
    server utilization. Assign request priorities based on the business criticality
    of the request or its dependents (security-critical functions should get high
    priority). You can either measure or empirically estimate request costs.^([5](ch08.html#ch08fn5))
    Either way, these measurements should be comparable to server utilization measurements,
    such as CPU and (possibly) memory usage. Computing request costs should of course
    be economical.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 负载分担允许您在服务器负载达到容量之前释放服务器资源，并使这些资源可用于更有价值的工作。为了选择要分担的请求，服务器需要具有请求优先级和请求成本的概念。您可以定义一个策略，根据请求优先级、请求成本和当前服务器利用率来确定每种请求类型要分担多少。根据请求的业务关键性或其依赖关系分配请求优先级（安全关键功能应该获得高优先级）。您可以测量或经验估计请求成本。^([5](ch08.html#ch08fn5))无论哪种方式，这些测量应该与服务器利用率测量相当，例如CPU和（可能）内存使用。当然，计算请求成本应该是经济的。
- en: Throttling
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限流
- en: Throttling (described in [Chapter 21 of the SRE book](https://landing.google.com/sre/sre-book/chapters/handling-overload/))
    indirectly modifies the client’s behavior by delaying the present operation in
    order to postpone future operations. After the server receives a request, it may
    wait before processing the request or, once it has finished processing the request,
    wait before sending the response to the client. This approach reduces the rate
    of requests the server receives from clients (if clients send requests sequentially),
    which means that you can redirect the resources saved during wait times.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 限流（在[SRE书籍的第21章](https://landing.google.com/sre/sre-book/chapters/handling-overload/)中描述）通过延迟当前操作以推迟未来操作，间接修改客户端的行为。服务器收到请求后，可能会在处理请求之前等待，或者在处理完请求后，在向客户端发送响应之前等待。这种方法减少了服务器从客户端接收的请求的速率（如果客户端按顺序发送请求），这意味着您可以在等待时间内重定向所节省的资源。
- en: Similar to load shedding, you could define policies to apply throttling to specific
    offending clients, or more generally to all clients. Request priority and cost
    play a role in selecting which requests to throttle.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于负载分担，您可以定义策略将限流应用于特定的有问题的客户端，或者更普遍地应用于所有客户端。请求优先级和成本在选择要限流的请求时起着作用。
- en: Automated response
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动响应
- en: Server utilization statistics can help determine when to consider applying controls
    like load shedding and throttling. The more heavily a server is loaded, the less
    traffic or load it can handle. If controls take too long to activate, higher-priority
    requests may end up being dropped or throttled.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器利用统计数据可以帮助确定何时考虑应用诸如负载分担和限流之类的控制。服务器负载越重，它能处理的流量或负载就越少。如果控制需要太长时间才能激活，优先级较高的请求可能最终会被丢弃或限流。
- en: To effectively manage these degradation controls at scale, you may need a central
    internal service. You can translate business considerations about mission-critical
    features and the costs of failure into policies and signals for this service.
    This internal service can also aggregate heuristics about clients and services
    in order to distribute updated policies to all servers in near real time. Servers
    can then apply these policies according to rules based on server utilization.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地管理这些降级控制，您可能需要一个中央内部服务。您可以将关于使命关键功能和故障成本的业务考虑转化为该服务的策略和信号。这个内部服务还可以聚合关于客户端和服务的启发式信息，以便向几乎实时地所有服务器分发更新的策略。然后服务器可以根据基于服务器利用率的规则应用这些策略。
- en: 'Some possibilities for automated response include the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一些自动响应的可能性包括以下内容：
- en: Implementing load-balancing systems that can respond to throttling signals and
    attempt to shift traffic to servers with lower loads
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施能够响应限流信号并尝试将流量转移到负载较低的服务器的负载平衡系统
- en: Providing DoS protections that can assist in response to malicious clients if
    throttling is ineffective or damaging
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供可以在限流无效或有害时协助应对恶意客户端的DoS保护
- en: Using reports of heavy load shedding for critical services to trigger preparation
    for failover to alternative components (a strategy that we discuss later in this
    chapter)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用关于关键服务的大规模停电报告来触发备用组件的故障转移准备（这是我们在本章后面讨论的一种策略）
- en: 'You can also use automation for self-reliant failure detection: a server that
    determines that it can’t serve some or all classes of requests can degrade itself
    to a full load-shedding mode. Self-contained or self-hosted detection is desirable
    because you don’t want to rely on external signals (possibly simulated by an attacker)
    to force an entire fleet of servers into an outage.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用自动化进行自主故障检测：确定无法为某些或所有类别的请求提供服务的服务器可以将自身降级为完全的负载分担模式。自包含或自托管的检测是可取的，因为您不希望依赖外部信号（可能是攻击者模拟的）来迫使整个服务器群陷入故障。
- en: As you implement graceful degradation, it’s important to determine and record
    levels of system degradation, regardless of what triggered the problem. This information
    is useful for diagnosing and debugging. Reporting the actual load shedding or
    throttling (whether self-imposed or directed) can help you evaluate global health
    and capacity and detect bugs or attacks. You also need this information in order
    to evaluate the current remaining system capacity and user impact. In other words,
    you want to know how degraded the individual components and the entire system
    are, and what manual actions you might need to take. After the event, you’ll want
    to evaluate the effectiveness of your degradation mechanisms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施优雅降级时，重要的是确定和记录系统降级的级别，无论是什么触发了问题。这些信息对诊断和调试很有用。报告实际的负载分担或限制（无论是自我施加的还是指导的）可以帮助您评估全局健康和容量，并检测错误或攻击。您还需要这些信息来评估当前剩余的系统容量和用户影响。换句话说，您想知道各个组件和整个系统的降级程度，以及您可能需要采取的手动操作。事件发生后，您将希望评估您的降级机制的有效性。
- en: Automate Responsibly
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任地自动化
- en: Exercise caution when creating automated response mechanisms so that they do
    not degrade system security and reliability to an unintended degree.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建自动响应机制时要谨慎，以防止它们使系统的安全性和可靠性降低到意外程度。
- en: Failing safe versus failing secure
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全失败与安全失败
- en: When designing a system to handle failure, you must balance between optimizing
    for reliability by failing open (safe) and optimizing for security by failing
    closed (secure):^([6](ch08.html#ch08fn6))
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计处理故障的系统时，您必须在优化可靠性的失败开放（安全）和优化安全性的失败关闭（安全）之间取得平衡：^（[6](ch08.html#ch08fn6)）
- en: To maximize *reliability*, a system should resist failures and serve as much
    as possible in the face of uncertainty. Even if the system’s integrity is not
    intact, as long as its configuration is viable, a system optimized for availability
    will serve what it can. If ACLs failed to load, the assumed default ACL is “allow
    all.”
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最大程度地提高*可靠性*，系统应该抵抗故障，并在面对不确定性时尽可能提供服务。即使系统的完整性没有得到保证，只要其配置是可行的，一个针对可用性进行优化的系统将提供其所能提供的服务。如果ACL加载失败，则假定默认ACL为“允许所有”。
- en: To maximize *security*, a system should lock down fully in the face of uncertainty.
    If the system cannot verify its integrity—regardless of whether a failed disk
    took away a part of its configs or an attacker changed the configs for an exploit—it
    can’t be trusted to operate and should protect itself as much as possible. If
    ACLs failed to load, the assumed default ACL is “deny all.”
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最大程度地提高*安全性*，系统在面对不确定性时应完全封锁。如果系统无法验证其完整性——无论是由于故障的磁盘带走了其配置的一部分，还是攻击者更改了配置以进行利用——它就不能被信任运行，应尽可能保护自己。如果ACL加载失败，则假定默认ACL为“拒绝所有”。
- en: These principles of reliability and security are clearly at odds. To resolve
    this tension, each organization must first determine its minimal nonnegotiable
    security posture, and then find ways to provide the required reliability of critical
    features of security services. For example, a network configured to drop low-QoS
    (quality of service) packets might require that security-oriented RPC traffic
    be tagged for special QoS to prevent packet drops. Security-oriented RPC servers
    might need special tagging to avoid CPU starvation by workload schedulers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可靠性和安全性原则显然是相互矛盾的。为了解决这种紧张局势，每个组织必须首先确定其最低不可妥协的安全姿态，然后找到提供所需安全服务关键特性的可靠性的方法。例如，配置为丢弃低QoS（服务质量）数据包的网络可能需要为安全导向的RPC流量标记特殊的QoS，以防止数据包丢失。安全导向的RPC服务器可能需要特殊标记，以避免被工作负载调度程序耗尽CPU。
- en: A foothold for humans
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类的立足点
- en: Sometimes humans must get involved in service degradation decisions. For example,
    the ability of rule-based systems to make a judgment call is inherently limited
    by predefined rules. Automation doesn’t act when faced with unforeseen circumstances
    that don’t map to any of the system’s predefined responses. An automated response
    might also produce unforeseen circumstances due to a programming error. Allowing
    appropriate human intervention to deal with these and similar situations requires
    some forethought in system design.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有时人类必须参与服务降级决策。例如，基于规则的系统进行判断的能力受到预定义规则的固有限制。当自动化面临不符合系统任何预定义响应的未预见情况时，自动化不会起作用。由于编程错误，自动化响应也可能产生未预见的情况。允许适当的人类干预来处理这些和类似情况需要在系统设计中进行一些事先考虑。
- en: First, you should prevent automation from disabling the services that employees
    use to recover your infrastructure (see [“Emergency Access”](ch09.html#emergency_access)).
    It’s important to design protections for these systems so that even DoS attacks
    cannot completely prevent access. For example, a SYN attack must not stop a responder
    from opening a TCP connection for an SSH session. Be sure to implement low-dependency
    alternatives, and continuously validate the capabilities of those alternatives.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您应该防止自动化禁用员工用于恢复基础设施的服务（请参阅[“紧急访问”](ch09.html#emergency_access)）。重要的是为这些系统设计保护，以便即使是DoS攻击也不能完全阻止访问。例如，SYN攻击不应该阻止响应者为SSH会话打开TCP连接。确保实施低依赖性的替代方案，并持续验证这些替代方案的能力。
- en: In addition, don’t allow automation to make unsupervised policy changes of either
    large magnitude (for example, a single server shedding *all* RPCs) or substantial
    scope (*all* servers shedding some RPC). Consider implementing a change budget
    instead. When automation exhausts that budget, no automatic refresh occurs. Instead,
    a human must increase the budget or make a different judgment call. Note that
    despite this human intervention, automation is still in place.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不要允许自动化进行大规模（例如，单个服务器放弃*所有*RPC）或大范围（*所有*服务器放弃一些RPC）的无监督策略更改。考虑实施变更预算。当自动化耗尽该预算时，不会发生自动刷新。相反，必须由人类增加预算或做出不同的判断。请注意，尽管有人类干预，自动化仍然存在。
- en: Controlling the Blast Radius
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制爆炸半径
- en: You can add another layer to your defense-in-depth strategy by limiting the
    scope of each part of your system. For example, consider network segmentation.
    In the past, it was common for an organization to have a single network that contained
    all of its resources (machines, printers, storage, databases, and so on). These
    resources were visible to any user or service on that network, and access was
    controlled by the resource itself.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过限制系统的每个部分的范围，您可以为您的深度防御策略增加另一层。例如，考虑网络分段。过去，组织通常拥有一个包含所有资源（机器、打印机、存储、数据库等）的单一网络。这些资源对网络上的任何用户或服务都是可见的，并且访问由资源本身控制。
- en: Today, a common way to improve security is to *segment* your network and grant
    access to each segment to specific classes of users and services. You can do this
    by using virtual LANs (VLANs) with network ACLs, which is an easy-to-configure,
    industry-standard solution. You can control traffic into each segment, and control
    which segments are allowed to communicate. You can also limit each segment’s access
    to “need to know” information.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，改善安全性的常见方法是对网络进行分段，并为特定类别的用户和服务授予对每个段的访问权限。您可以通过使用具有网络ACL的虚拟局域网（VLAN）来实现这一点，这是一种易于配置的行业标准解决方案。您可以控制进入每个段的流量，并控制允许通信的段。您还可以限制每个段对“需要知道”的信息的访问。
- en: Network segmentation is a good example of the general idea of compartmentalization,
    which we discussed in [Chapter 6](ch06.html#design_for_understandability). *Compartmentalization*
    involves deliberately creating small individual operational units (compartments)
    and limiting access to and from each one. It’s a good idea to compartmentalize
    most aspects of your systems—servers, applications, storage, and so on. When you
    use a single-network setup, an attacker who compromises a user’s credentials can
    potentially access every device on the network. When you use compartmentalization,
    however, a security breach or traffic overload in one compartment does not jeopardize
    all of the compartments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分段是隔离的一般概念的一个很好的例子，我们在第6章中讨论过。隔离涉及有意地创建小的个体操作单元（隔间），并限制对每个隔间的访问和来自每个隔间的访问。对系统的大多数方面进行隔间化是一个好主意，包括服务器、应用程序、存储等等。当您使用单一网络设置时，入侵者如果窃取了用户的凭据，可能会访问网络上的每个设备。然而，当您使用隔间化时，一个隔间中的安全漏洞或流量过载并不会危及所有隔间。
- en: Controlling the blast radius means compartmentalizing the impact of an event,
    similar to the way compartments on a ship grant resilience against the whole ship
    sinking. Designing for resilience, you should create compartmental barriers that
    constrain both attackers *and* accidental failures. These barriers allow you to
    better tailor and automate your responses. You can also use these boundaries to
    create failure domains that deliver component redundancy and failure isolation,
    as discussed in [“Failure Domains and Redundancies”](#failure_domains_and_redundancies).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 控制爆炸半径意味着将事件的影响分隔开，类似于船舶上的隔舱可以使整艘船免于沉没。在设计弹性时，您应该创建约束攻击者和意外故障的隔舱壁垒。这些隔离壁垒可以让您更好地定制和自动化您的响应。您还可以使用这些边界来创建故障域，提供组件冗余和故障隔离，如在“故障域和冗余”中所讨论的那样。
- en: Compartments also aid in quarantine efforts, reducing the need for responders
    to actively balance defending and preserving evidence. Some compartments can be
    isolated and frozen for analysis while other compartments are recovered. Additionally,
    compartments create natural boundaries for replacement and repair during incident
    response—a compartment may be jettisoned to save the remainder of the system.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 隔间还有助于隔离努力，减少了响应者需要积极平衡防御和保留证据的需求。一些隔间可以被隔离和冻结以进行分析，而其他隔间则可以被恢复。此外，隔间在事件响应期间为更换和修复创建了自然边界，一个隔间可能被舍弃以拯救系统的其余部分。
- en: To control the blast radius of an incursion, you must have a way to establish
    boundaries and to be sure those boundaries are secure. Consider a job running
    in production as one compartment.^([7](ch08.html#ch08fn7)) This job must permit
    some access (you want the compartment to be useful), but not unrestricted access
    (you want to protect the compartment). Restricting who can access the job relies
    on your ability to recognize endpoints in production and confirm their identity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制入侵的爆炸半径，您必须有一种建立边界并确保这些边界安全的方法。将生产中运行的作业视为一个隔间。这个作业必须允许一些访问（您希望这个隔间有用），但不能是无限制的访问（您希望保护这个隔间）。限制谁可以访问作业取决于您识别生产中的端点并确认其身份的能力。
- en: You can do this by using authenticated remote procedure calls, which identify
    both parties within one connection. To protect the parties’ identities from spoofing
    and to conceal their contents from the network, these RPCs use mutually authenticated
    connections, which can certify the identities of both parties connected to the
    service. To permit endpoints to make more informed decisions about other compartments,
    you may add additional information that endpoints publish along with their identity.
    For example, you can add location information to the certificate so that you can
    reject nonlocal requests.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用经过身份验证的远程过程调用来实现这一点，该调用可以在一个连接中识别双方。为了保护各方的身份免受欺骗，并将其内容隐藏在网络中，这些RPC使用相互认证的连接，可以证明连接到服务的双方的身份。为了让端点对其他隔间做出更明智的决定，您可以添加端点与其身份一起发布的附加信息。例如，您可以向证书添加位置信息，以便拒绝非本地请求。
- en: 'Once mechanisms to establish compartments are in place, you face a difficult
    tradeoff: you need to constrain your operations with enough separation to deliver
    useful-sized compartments, but without creating *too much* separation. For example,
    one balanced approach to compartmentalization would be to consider every RPC method
    as a separate compartment. This aligns compartments along logical application
    boundaries, and the count of compartments is linear to the number of system features.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立隔间的机制到位，您将面临一个艰难的抉择：您需要通过足够的分离来限制您的操作，以提供有用大小的隔间，但又不会创建*太多*的分离。例如，隔间化的一个平衡方法是将每个RPC方法视为单独的隔间。这样可以沿着逻辑应用边界对齐隔间，而隔间的数量与系统功能的数量成正比。
- en: Compartment separation that controls the acceptable parameter values of RPC
    methods would warrant more careful consideration. While this would create tighter
    security controls, the number of possible violations per RPC method is proportional
    to the number of RPC clients. This complexity would compound across all of the
    system’s features, and require coordination of changes in client code and server
    policy. On the other hand, compartments that wrap an entire server (regardless
    of its RPC services or their methods) are much easier to manage, but provide comparatively
    much less value. When balancing this tradeoff, it’s necessary to consult with
    the incident management and operations teams to consider your choices of compartment
    types and to validate the utility of your choices.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 控制RPC方法可接受参数值的隔间分离需要更加谨慎的考虑。虽然这会创建更严格的安全控制，但每个RPC方法可能的违规次数与RPC客户端的数量成正比。这种复杂性会在系统的所有功能中累积，并需要协调客户端代码和服务器策略的更改。另一方面，无论RPC服务或其方法如何，包装整个服务器的隔间要容易得多，但提供的价值相对较少。在权衡这种权衡时，有必要与事件管理和运营团队协商，以考虑隔间类型的选择，并验证您的选择的效用。
- en: Imperfect compartments that don’t perfectly cover all edge cases can also provide
    value. For example, the process of finding the edge cases may cause an attacker
    to make a mistake that alerts you to their presence. Any time that it takes such
    an adversary to escape a compartment is additional time that your incident response
    team has to react.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不完美的隔间化并不能完全覆盖所有边缘情况，但也可以提供价值。例如，寻找边缘情况的过程可能会导致攻击者犯错，从而提醒您他们的存在。攻击者逃离隔间所需的任何时间都是您的事件响应团队有机会做出反应的额外时间。
- en: Incident management teams must plan and practice tactics for sealing compartments
    to contain an incursion or a bad actor. Turning off part of your production environment
    is a dramatic step. Well-designed compartments give incident management teams
    the option to perform actions that are proportional to the incidents, so they
    don’t necessarily have to take an entire system offline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 事件管理团队必须计划和实践封锁隔间以遏制入侵或不良行为的策略。关闭生产环境的一部分是一个戏剧性的举措。设计良好的隔间给事件管理团队提供了执行与事件成比例的操作的选项，因此他们不一定要将整个系统下线。
- en: When you implement compartmentalization, you face a tradeoff between having
    all customers share a single instance of a given service,^([8](ch08.html#ch08fn8))
    or running separate service instances that support individual customers or subsets
    of customers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当您实施隔间化时，您面临着一个抉择：让所有客户共享给定服务的单个实例，或者运行支持单个客户或客户子集的单独服务实例。
- en: 'For example, running two virtual machines (VMs)—each controlled by different
    mutually distrustful entities—on the same hardware comes with a certain risk:
    exposure to zero-day vulnerabilities in the virtualization layer perhaps, or subtle
    cross-VM information leaks. Some customers may choose to eliminate these risks
    by compartmentalizing their deployments based on physical hardware. To facilitate
    this approach, many cloud providers offer deployment on per-customer dedicated
    hardware.^([9](ch08.html#ch08fn9)) In this case, the cost of reduced resource
    utilization is reflected in a pricing premium.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在同一硬件上运行由不同互不信任的实体控制的两个虚拟机（VM）存在一定风险：可能会暴露在虚拟化层的零日漏洞，或者存在微妙的跨VM信息泄漏。一些客户可能会选择通过基于物理硬件对其部署进行隔间化来消除这些风险。为了促进这种方法，许多云提供商提供基于每个客户专用硬件的部署。在这种情况下，减少资源利用的成本反映在定价溢价中。
- en: Compartment separation adds resilience to a system as long as the system has
    mechanisms to maintain the separation. The difficult task is tracking those mechanisms
    and ensuring they remain in place. To prevent regressions, it’s valuable to validate
    that operations prohibited across separation boundaries indeed fail (see [“Continuous
    Validation”](#continuous_validation)). Conveniently, because operational redundancy
    relies on compartmentalization (covered in [“Failure Domains and Redundancies”](#failure_domains_and_redundancies)),
    your validation mechanisms can cover both prohibited and expected operations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 隔间分离为系统增加了韧性，只要系统有机制来维持这种分离。困难的任务是跟踪这些机制并确保它们保持在位。为了防止退化，验证跨隔间边界禁止的操作失败是有价值的。方便的是，因为运营冗余依赖于隔间化（在“故障域和冗余”中介绍），您的验证机制可以涵盖被禁止和预期的操作。
- en: Google compartmentalizes by role, location, and time. When an attacker tries
    to compromise a compartmentalized system, the potential scope of any single attack
    is greatly reduced. If the system is compromised, the incident management teams
    have options to disable only parts of it to purge the effects of the compromise
    while leaving other parts operational. The following sections explore the different
    types of compartmentalization in detail.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌通过角色、位置和时间进行隔间化。当攻击者试图破坏隔间化系统时，任何单次攻击的潜在范围都大大减少。如果系统受到攻击，事件管理团队可以选择仅禁用部分系统以清除受攻击的影响，同时保持其他部分运行。接下来的部分将详细探讨不同类型的隔间化。
- en: Role Separation
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 角色分离
- en: Most modern microservices architecture systems allow users to run jobs as particular
    *roles*, sometimes referred to as *service accounts*. The jobs are then provided
    with credentials that allow them to authenticate to other microservices on the
    network in their specific roles. If an adversary compromises a single job, they
    will be able to impersonate the job’s corresponding role across the network. Because
    this allows the adversary to access all data that the other jobs running as that
    role could access, this effectively means that adversary has compromised the other
    jobs as well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代微服务架构系统允许用户以特定的*角色*或*服务账户*运行作业。然后，这些作业将获得凭据，允许它们以特定角色在网络上进行身份验证。如果对手损害了一个作业，他们将能够在网络上冒充该作业对应的角色。因为这允许对手访问其他作业作为该角色可以访问的所有数据，这实际上意味着对手也损害了其他作业。
- en: To limit the blast radius of such a compromise, different jobs should typically
    be run as different roles. For example, if you have two microservices that need
    access to two different classes of data (say, photos and text chats), running
    these two microservices as different roles can increase the resilience of your
    system even if the two microservices are developed and run by the same team.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制这种损害的影响范围，不同的作业通常应该以不同的角色运行。例如，如果您有两个需要访问两种不同类型数据（比如照片和文本聊天）的微服务，将这两个微服务作为不同的角色运行可以增加系统的弹性，即使这两个微服务由同一个团队开发和运行。
- en: Location Separation
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置分离
- en: '*Location separation* helps to limit an attacker’s impact along an additional
    dimension: the location where the microservice is running. For example, you might
    want to prevent an adversary who has physically compromised a single datacenter
    from being able to read data in all your other datacenters. Similarly, you might
    want your most powerful administrative users to have their access permissions
    limited to only specific regions to mitigate insider risk.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*位置分离*有助于限制攻击者的影响的另一个维度：微服务运行的位置。例如，您可能希望防止已经物理损害了一个数据中心的对手能够读取所有其他数据中心的数据。同样，您可能希望您最有权力的管理用户的访问权限仅限于特定地区，以减轻内部风险。'
- en: The most obvious way to achieve location separation is to run the same microservices
    as different roles in different locations (like datacenters or cloud regions,
    which also typically correspond to different physical locations). You can then
    use your normal access control mechanisms to protect instances of the same service
    in different locations from each other, just as you would protect different services
    running as different roles from each other.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实现位置分离的最明显方法是在不同位置（如数据中心或云区域，通常对应不同的物理位置）运行相同的微服务作为不同的角色。然后，您可以使用正常的访问控制机制来保护不同位置的相同服务实例，就像您会保护不同角色的不同服务一样。
- en: Location separation helps you resist an attack that moves from one location
    to another. Location-based cryptographic compartments let you limit access to
    applications and their stored data to specific locations, containing the blast
    radius of local attacks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 位置分离有助于抵抗从一个位置转移到另一个位置的攻击。基于位置的加密隔间可以让您限制对特定位置的应用程序和其存储的数据的访问，从而限制本地攻击的影响范围。
- en: Physical location is a natural compartmentalization border, since many adverse
    events are connected to physical locations. For example, natural disasters are
    confined to a region, as are other localized mishaps such as fiber cuts, power
    outages, or fires. Malicious attacks that require the physical presence of the
    attacker are also confined to locations the attacker can actually get to, and
    all but the most capable (state-level attackers, for example) likely don’t have
    the capability to send attackers to many locations all at once.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 物理位置是自然的隔间边界，因为许多不利事件与物理位置有关。例如，自然灾害局限于一个地区，其他局部事件如光纤切断、停电或火灾也是如此。需要攻击者在物理上出现的恶意攻击也局限于攻击者实际可以到达的位置，而除了最有能力的（例如国家级攻击者）可能没有能力同时派遣攻击者到多个位置。
- en: Similarly, the degree of risk exposure can depend on the nature of the physical
    location. For example, the risk of specific kinds of natural disasters varies
    with geographical region. Also, the risk of an attacker tailgating into a building
    and finding an open network port to plug into is higher in an office location
    with heavy employee and visitor traffic, as opposed to a datacenter with tightly
    controlled physical access.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，风险暴露的程度可能取决于物理位置的性质。例如，特定类型的自然灾害风险随地理区域而异。此外，入侵者尾随进入建筑并找到一个开放的网络端口插入的风险在员工和访客流量大的办公地点要高于对物理访问严格受控的数据中心。
- en: 'With this in mind, you’ll want to take location into account when designing
    your systems, to ensure that localized impacts stay confined to systems in that
    region, while letting your multiregional infrastructure continue to operate. For
    example, it’s important to ensure that a service provided by servers in several
    regions does not have a critical dependency on a backend that is single-homed
    in one datacenter. Similarly, you’ll want to ensure that physical compromise of
    one location does not allow an attacker to easily compromise other locations:
    tailgating into an office and plugging into an open port in a conference room
    should not give an intruder network access to production servers in your datacenter.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，您在设计系统时需要考虑位置，以确保局部影响保持在该地区的系统中，同时让您的多区域基础设施继续运行。例如，重要的是要确保由几个地区的服务器提供的服务不会对单一数据中心中的后端产生关键依赖。同样，您需要确保一个位置的物理损害不会让攻击者轻易损害其他位置：尾随进入办公室并插入会议室的开放端口不应该让入侵者轻易获得对数据中心生产服务器的网络访问。
- en: Aligning physical and logical architecture
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整物理和逻辑架构
- en: When compartmentalizing an architecture into logical failure and security domains,
    it’s valuable to align relevant physical boundaries with logical boundaries. For
    example, it’s useful to segment your network on both network-level risks (such
    as networks exposed to malicious internet traffic versus trusted internal networks)
    and risks of physical attacks. Ideally, you’d have network segregation between
    corporate and production environments housed in physically separate buildings.
    Beyond that, you might further subdivide your corporate network to segregate areas
    with high visitor traffic, such as conference and meeting areas.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当将架构分隔为逻辑故障和安全域时，将相关的物理边界与逻辑边界对齐是有价值的。例如，将网络分割为网络级风险（如受恶意互联网流量影响的网络与受信任的内部网络）和物理攻击风险是有用的。理想情况下，您应该在不同的建筑物中为公司和生产环境之间进行网络隔离。此外，您可能会进一步将公司网络细分，以隔离访客流量较大的区域，如会议和会议区域。
- en: In many cases, a physical attack, such as stealing or backdooring a server,
    can give an attacker access to important secrets, encryption keys, or credentials
    that then might permit them to further penetrate your systems. With this in mind,
    it’s a good idea to logically compartmentalize distribution of secrets, keys,
    and credentials to physical servers to minimize the risk of a physical compromise.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，物理攻击，比如窃取或在服务器上植入后门，可能会使攻击者获得重要的机密、加密密钥或凭证，从而可能允许他们进一步渗透您的系统。考虑到这一点，将机密、密钥和凭证的分发在逻辑上隔离到物理服务器上是一个好主意，以最小化物理妥协的风险。
- en: 'For example, if you operate web servers in several physical datacenter locations,
    it can be advantageous to deploy a separate certificate to each server, or share
    a certificate only across servers in one location, instead of sharing a single
    certificate across *all* your servers. This can make your response to the physical
    compromise of one datacenter more agile: you can drain its traffic, revoke just
    the cert(s) deployed to that datacenter, and take the datacenter offline for incident
    response and recovery, all the while serving traffic from your remaining datacenters.
    If you had a single certificate deployed to all servers, you’d instead have to
    very quickly replace the cert on all of them—even the ones that were not actually
    compromised.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您在几个物理数据中心位置运行Web服务器，将为每台服务器部署单独的证书或仅在一个位置的服务器之间共享证书，而不是在*所有*服务器上共享单个证书，这可能是有利的。这可以使您对一个数据中心的物理妥协做出更灵活的响应：您可以将其流量排空，仅撤销部署到该数据中心的证书，并将数据中心下线进行事件响应和恢复，同时从其余数据中心提供流量。如果您在所有服务器上部署了单个证书，您将不得不非常快速地替换所有证书——甚至那些实际上没有受到威胁的证书。
- en: Isolation of trust
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信任隔离
- en: While services may need to communicate across location boundaries to operate
    properly, a service might also want to reject requests from locations it doesn’t
    expect to communicate with. To do this, you can restrict communications by default,
    and allow only the expected communications across location boundaries. It’s also
    unlikely that all APIs on any service will use the same set of location restrictions.
    User-facing APIs are typically open globally, while control plane APIs are usually
    constrained. This makes fine-grained (per API call) control of permitted locations
    necessary. Creating tools that make it easy for any given service to measure,
    define, and enforce location limits on individual APIs enables teams to use their
    per-service knowledge to implement location isolation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然服务可能需要跨位置边界进行通信以正常运行，但服务也可能希望拒绝来自其不希望通信的位置的请求。为了做到这一点，您可以默认限制通信，并只允许跨位置边界的预期通信。任何服务上的所有API都不太可能使用相同的位置限制集。用户面向的API通常是全球开放的，而控制平面API通常是受限的。这使得对允许位置的细粒度（每个API调用）控制成为必要。创建工具，使得任何给定服务能够测量、定义和强制执行对个别API的位置限制，使团队能够利用其对每个服务的知识来实施位置隔离。
- en: To restrict communications based on location, each identity needs to include
    location metadata. Google’s job control system certifies and runs jobs in production.
    When the system certifies a job to run in a given compartment, it annotates the
    job’s certificate with that compartment’s location metadata. Each location has
    its own copy of the job control system that certifies jobs to run in that location,
    and machines in that location only accept jobs from that system. This is designed
    to prevent an attacker from piercing the compartment boundary and affecting other
    locations. Contrast this approach to a single centralized authority—if there were
    only one job control system for all of Google, its location would be quite valuable
    to an attacker.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 限制基于位置的通信，每个身份都需要包含位置元数据。谷歌的作业控制系统对生产中的作业进行认证和运行。当系统认证一个作业在特定隔间运行时，它会用该隔间的位置元数据注释作业的证书。每个位置都有自己的作业控制系统，用于认证在该位置运行的作业，并且该位置的机器只接受该系统的作业。这旨在防止攻击者突破隔间边界并影响其他位置。与单一的集中管理机构相比，这种方法有所不同——如果谷歌只有一个作业控制系统，那么它的位置对攻击者来说将是非常有价值的。
- en: Once trust isolation is in place, we can extend ACLs on stored data to include
    location restrictions. This way, we can separate locations for storage (where
    we put the data) from locations for access (who can retrieve or modify the data).
    This also opens up the possibility of trusting physical security versus trusting
    access by API—sometimes the additional requirement of a physical operator is worthwhile,
    as it removes the possibility of remote attacks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦信任隔离就位，我们可以扩展存储数据的ACL，以包括位置限制。这样，我们可以将存储位置（放置数据的地方）与访问位置（谁可以检索或修改数据）分开。这也打开了信任物理安全与信任API访问的可能性——有时，物理操作员的额外要求是值得的，因为它消除了远程攻击的可能性。
- en: To help control compartment violations, Google has a root of trust in each location
    and distributes the list of trusted roots and the locations they represent to
    all machines in the fleet. This way, each machine can detect spoofing across locations.
    We can also revoke a location’s identity by distributing an updated list to all
    machines declaring the location untrustworthy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助控制隔间违规行为，谷歌在每个位置都设有一个信任根，并将受信任的根和它们代表的位置的列表分发给机群中的所有机器。这样，每台机器都可以检测跨位置的欺骗行为。我们还可以通过向所有机器分发更新后的列表来撤销某个位置的身份，宣布该位置不可信任。
- en: Limitations of location-based trust
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于位置的信任的局限性
- en: At Google, we have chosen to design our corporate network infrastructure so
    that location does not imply any trust. Instead, under the the zero trust networking
    paradigm of our BeyondCorp infrastructure (see [Chapter 5](ch05.html#design_for_least_privilege)),
    a workstation is trusted based on a certificate issued to the individual machine,
    and assertions about its configuration (such as up-to-date software). Plugging
    an untrusted machine into an office-floor network port will assign it to an untrusted
    guest VLAN. Only authorized workstations (authenticated via the 802.1x protocol)
    are assigned to the appropriate workstation VLAN.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌，我们选择设计我们的企业网络基础设施，使位置不意味着任何信任。相反，在我们的BeyondCorp基础设施的零信任网络范式下（参见[第5章](ch05.html#design_for_least_privilege)），工作站是基于颁发给个体机器的证书以及关于其配置的断言（如最新软件）而受信任的。将一个不受信任的机器插入办公楼网络端口将其分配到一个不受信任的访客VLAN。只有经过授权的工作站（通过802.1x协议进行身份验证）才会被分配到适当的工作站VLAN。
- en: We have also chosen to not even rely on physical location to establish trust
    for servers in datacenters. One motivating experience came out of a Red Team assessment
    of a datacenter environment. In this exercise, the Red Team placed a wireless
    device on top of a rack and quickly plugged it into an open port, to allow further
    penetration of the datacenter’s internal network from outside the building. When
    they returned to clean up after the exercise, they found that an attentive datacenter
    technician had neatly zip-tied the access point’s cabling—apparently offended
    by the untidy install job and on the assumption that the device must be legitimate.
    This story illustrates the difficulty of ascribing trust based on physical location,
    even within a physically secure area.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还选择不依赖物理位置来建立数据中心服务器的信任。一个激励性的经验来自于对数据中心环境的红队评估。在这次演习中，红队在机架顶部放置了一个无线设备，并迅速将其插入一个开放端口，以便从建筑外部进一步渗透数据中心的内部网络。当他们回来清理演习后，他们发现一个细心的数据中心技术人员已经整齐地用拉链扎紧了接入点的电缆——显然对不整洁的安装工作感到不满，并假设该设备一定是合法的。这个故事说明了基于物理位置归因信任的困难，即使在一个物理安全的区域内也是如此。
- en: In Google’s production environment, similarly to the BeyondCorp design, authentication
    between production services is rooted in machine-to-machine trust based on per-machine
    credentials. A malicious implant on an unauthorized device would not be trusted
    by Google’s production environment.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌的生产环境中，类似于BeyondCorp设计，生产服务之间的身份验证是基于每台机器凭据的机器间信任。未经授权设备上的恶意植入物将不会受到谷歌生产环境的信任。
- en: Isolation of confidentiality
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保密隔离
- en: Once we have a system to isolate trust, we need to isolate our encryption keys
    to ensure that data secured through a root of encryption in one location is not
    compromised by exfiltration of encryption keys in another location. For example,
    if one branch of a company is compromised, attackers should not be able to read
    data from the company’s other branches.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一个隔离信任的系统，我们需要隔离我们的加密密钥，以确保通过一个位置的加密根保护的数据不会因为在另一个位置泄露加密密钥而受损。例如，如果公司的一个分支遭到攻击，攻击者不应该能够从公司的其他分支读取数据。
- en: Google has base encryption keys that protect key trees. These keys eventually
    protect data at rest through key wrapping and key derivation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌拥有保护密钥树的基本加密密钥。这些密钥最终通过密钥封装和密钥派生来保护静态数据。
- en: To isolate encryption and key wrapping to a location, we need to ensure that
    the root keys for a location are only available to the correct location. This
    requires a distribution system that only places root keys in the correct locations.
    A key access system should leverage trust isolation to ensure that these keys
    cannot be accessed by entities that aren’t in the appropriate location.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将加密和密钥封装隔离到一个位置，我们需要确保该位置的根密钥只能被正确的位置访问。这需要一个分发系统，只将根密钥放置在正确的位置。密钥访问系统应该利用信任隔离，确保这些密钥不能被不在适当位置的实体访问。
- en: Using these principles, a given location allows the use of ACLs on local keys
    to prevent remote attackers from decrypting data. Decryption is prevented even
    if attackers have access to the encrypted data (through internal compromise or
    exfiltration).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些原则，给定位置允许在本地密钥上使用ACLs，以防止远程攻击者解密数据。即使攻击者可以访问加密数据（通过内部妥协或外泄），也可以防止解密。
- en: Transitioning from a global key tree to a local key tree should be gradual.
    While any part of the tree may move from global to local independently, isolation
    isn’t complete for a given leaf or branch of the tree until all keys above it
    have transitioned to local keys.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从全局密钥树过渡到本地密钥树应该是逐渐的。虽然树的任何部分可以独立地从全局转移到本地，但在所有上面的密钥都转移到本地密钥之前，对于给定的叶子或分支，隔离并不完整。
- en: Time Separation
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间分离
- en: Finally, it’s useful to limit the abilities of an adversary over time. The most
    common scenario to consider here is an adversary who has compromised a system
    and stolen a key or credential. If you rotate your keys and credentials over time
    and expire the old ones, the adversary must maintain their presence to reacquire
    the new secrets, which gives you more opportunities to detect the theft. Even
    if you never do detect the theft, rotation is still critical because you might
    close the avenue the adversary used to gain access to the key or credential during
    normal security hygiene work (e.g., by patching the vulnerability).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，限制对手随时间的能力是有用的。在这里要考虑的最常见情况是，对手已经破坏了系统并窃取了密钥或凭证。如果您随时间旋转您的密钥和凭证，并使旧的失效，对手必须保持他们的存在以重新获取新的秘密，这给了您更多的机会来检测窃取。即使您从未检测到窃取，旋转仍然至关重要，因为您可能会在正常的安全卫生工作中关闭对手用来获取密钥或凭证的途径（例如，通过修补漏洞）。
- en: As we discuss in [Chapter 9](ch09.html#design_for_recovery), doing key and credential
    rotation and expiration reliably requires careful tradeoffs. For example, using
    wall clock–based expiration for credentials can be problematic if there’s a failure
    that prevents rotation to new credentials before the time the old credentials
    expire. Providing useful time separation requires balancing the frequency of rotation
    against the risk of downtime or loss of data if the rotation mechanism fails.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第9章](ch09.html#design_for_recovery)中讨论的那样，可靠地进行密钥和凭证旋转和失效需要仔细权衡。例如，如果存在阻止在旧凭证失效之前将其旋转到新凭证的故障，那么基于壁钟的凭证失效可能会有问题。提供有用的时间分隔需要在旋转频率与旋转机制失败时的停机风险或数据丢失风险之间进行平衡。
- en: 'DEEP DIVE: Failure Domains and Redundancies'
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨：故障域和冗余
- en: So far we’ve covered how to design systems that adjust their behavior in response
    to attacks and contain attack fallout by using compartmentalization. To address
    complete failures of system components, system design must incorporate redundancies
    and distinct failure domains. These tactics can hopefully limit the impact of
    failures and avert complete collapse. It’s particularly important to mitigate
    failures of critical components, since any system that depends on failed critical
    components is also at risk of complete failure.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何设计系统以响应攻击并通过隔离来包含攻击后果。为了解决系统组件的完全故障，系统设计必须包含冗余和不同的故障域。这些策略可以希望限制故障的影响并避免完全崩溃。减轻关键组件的故障尤为重要，因为任何依赖于失败的关键组件的系统也面临完全失败的风险。
- en: 'Rather than aiming to prevent all failures at all times, you can create a balanced
    solution for your organization by combining the following approaches:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与其旨在始终防止所有故障，不如通过结合以下方法为您的组织创建一个平衡的解决方案：
- en: Break up systems into independent failure domains.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将系统分解为独立的故障域。
- en: Aim to reduce the probability of a single root cause affecting elements in multiple
    failure domains.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旨在降低单一根本原因影响多个故障域中元素的概率。
- en: Create redundant resources, components, or procedures that can replace the failed
    ones.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建可以替代失败部分的冗余资源、组件或程序。
- en: Failure Domains
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障域
- en: A *failure domain* is a type of blast radius control. Instead of structurally
    separating by role, location, or time, failure domains achieve functional isolation
    by partitioning a system into multiple equivalent but completely independent copies.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*故障域*是一种爆炸半径控制。与按角色、位置或时间进行结构分离不同，故障域通过将系统分区为多个等效但完全独立的副本来实现功能隔离。'
- en: Functional isolation
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功能隔离
- en: A failure domain looks like a single system to its clients. If necessary, any
    of the individual partitions can take over for the entire system during an outage.
    Because a partition has only a fraction of the system’s resources, it can support
    only a fraction of the system’s capacity. Unlike managing role, location, and
    time separations, operating failure domains and maintaining their isolation requires
    ongoing effort. In exchange, failure domains increase system resilience in ways
    other blast radius controls can’t.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对其客户端来说，故障域看起来像一个单一的系统。必要时，任何单个分区都可以在停机期间接管整个系统。因为分区只有系统资源的一部分，它只能支持系统容量的一部分。与管理角色、位置和时间分离不同，操作故障域并保持其隔离需要持续的努力。作为交换，故障域增加了系统的弹性，其他爆炸半径控制无法做到。
- en: Failure domains help protect systems from global impact because a single event
    doesn’t typically affect all failure domains at once. However, in extreme cases,
    a significant event can disrupt multiple, or even all, failure domains. For example,
    you can think of a storage array’s underlying devices (HDDs or SSDs) as failure
    domains. Although any one device may fail, the entire storage system remains functional
    because it creates a new data replica elsewhere. If a large number of storage
    devices fail and there aren’t sufficient spare devices to maintain data replicas,
    further failures might result in data loss in the storage system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 故障域有助于保护系统免受全局影响，因为单个事件通常不会同时影响所有故障域。然而，在极端情况下，重大事件可能会破坏多个，甚至所有故障域。例如，您可以将存储阵列的基础设备（HDD或SSD）视为故障域。尽管任何一个设备可能会失败，但整个存储系统仍然正常运行，因为它在其他地方创建了一个新的数据副本。如果大量存储设备失败，并且没有足够的备用设备来维护数据副本，进一步的故障可能导致存储系统中的数据丢失。
- en: Data isolation
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据隔离
- en: You need to prepare for the possibility of having bad data at the data source
    or within individual failure domains. Therefore, each failure domain instance
    needs its own data copy in order to be functionally independent of the other failure
    domains. We recommend a twofold approach to achieve data isolation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要为数据源或单个故障域内可能存在错误数据的可能性做好准备。因此，每个故障域实例都需要其自己的数据副本，以便在功能上独立于其他故障域。我们建议采取双重方法来实现数据隔离。
- en: First, you can restrict how data updates can enter a failure domain. A system
    accepts new data only after it passes all validation checks for typical and safe
    changes. Some exceptions are escalated for justification, and a breakglass mechanism^([10](ch08.html#ch08fn10))
    may allow new data to enter the failure domain. As a result, you are more likely
    to prevent attackers or software bugs from making disruptive changes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您可以限制数据更新进入故障域的方式。系统只有在通过了所有典型和安全更改的验证检查后才接受新数据。一些异常情况需要进行理由说明，并且可以通过紧急机制^([10](ch08.html#ch08fn10))允许新数据进入故障域。因此，您更有可能防止攻击者或软件错误造成破坏性的更改。
- en: For example, consider ACL changes. A human mistake or a bug in ACL-generating
    software could produce an empty ACL, which might result in denying access to everyone.^([11](ch08.html#ch08fn11))
    Such an ACL change could cause system malfunction. Similarly, an attacker might
    try to expand their reach by adding a “permit all” clause to an ACL.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑ACL更改。人为错误或ACL生成软件中的错误可能会产生一个空的ACL，这可能导致拒绝所有人的访问。^([11](ch08.html#ch08fn11))这样的ACL更改可能导致系统故障。同样，攻击者可能会尝试通过向ACL添加“允许所有”条款来扩大其影响范围。
- en: At Google, individual services generally have an RPC endpoint for intake of
    new data and for signaling. Programming frameworks, such as those presented in
    [Chapter 12](ch12.html#writing_code), include APIs for versioning data snapshots
    and evaluating their validity. Client applications can take advantage of the programming
    framework’s logic for qualifying new data as safe. Centralized data push services
    implement quality controls for data updates. The data push services check where
    to get the data from, how to package it, and when to push the packaged data. To
    prevent automation from causing a widespread outage, Google rate limits global
    changes using per-application quotas. We prohibit actions that change multiple
    applications at once or that change the application capacity too quickly within
    a time period.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌，个别服务通常具有用于接收新数据和信令的RPC端点。编程框架，如[第12章](ch12.html#writing_code)中介绍的那些，包括用于对数据快照进行版本控制和评估其有效性的API。客户端应用程序可以利用编程框架的逻辑来确定新数据是否安全。集中式数据推送服务实施数据更新的质量控制。数据推送服务检查数据的获取位置、打包方式以及何时推送打包的数据。为了防止自动化导致广泛的故障，谷歌使用每个应用程序的配额来限制全局更改的速度。我们禁止同时更改多个应用程序或在一段时间内过快地更改应用程序容量的操作。
- en: 'Second, enabling systems to write the last known good configuration to disk
    makes the systems resilient to losing access to configuration APIs: they can use
    the saved config. Many of Google’s systems preserve old data for a limited duration
    of time in case the most recent data becomes corrupted for any reason. This is
    another example of defense in depth, helping provide long-term resilience.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，使系统能够将最后已知的良好配置写入磁盘，使系统能够抵御失去对配置API的访问：它们可以使用保存的配置。谷歌的许多系统在有限的时间内保留旧数据，以防最新数据因任何原因而损坏。这是深度防御的另一个例子，有助于提供长期的弹性。
- en: Practical aspects
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际方面
- en: 'Even splitting a system into only two failure domains brings substantial benefits:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 即使将系统分割成只有两个故障域也会带来实质性的好处：
- en: Having two failure domains provides A/B regression capabilities and limits the
    blast radius of system changes to a single failure domain. To achieve this functionality,
    use one failure domain as a canary, and have a policy that doesn’t allow updates
    to both failure domains at the same time.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有两个故障域提供了A/B回归的能力，并将系统更改的影响范围限制在单个故障域内。为了实现这种功能，使用一个故障域作为金丝雀，并制定一个政策，不允许同时更新两个故障域。
- en: Geographically separated failure domains can provide isolation for natural disasters.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理上分离的故障域可以为自然灾害提供隔离。
- en: You can use different software versions in different failure domains, thereby
    reducing the chances of a single bug breaking all servers or corrupting all data.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在不同的故障域中使用不同的软件版本，从而减少单个错误破坏所有服务器或损坏所有数据的机会。
- en: Combining data and functional isolation enhances overall resilience and incident
    management. This approach limits the risk of data changes that are accepted without
    justification. When issues do arise, isolation delays their propagation to the
    individual functional units. This gives other defense mechanisms more time to
    detect and react, which is especially beneficial during hectic and time-sensitive
    incident response. By pushing multiple candidate fixes to distinct failure domains
    in parallel, you can independently evaluate which fixes have the intended effect.
    That way, you can avoid accidentally pushing a rushed update with a mistaken “fix”
    globally, further degrading your entire system.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据和功能隔离相结合可以增强整体的弹性和事件管理。这种方法限制了未经理由接受的数据更改的风险。当问题出现时，隔离会延迟其传播到个别的功能单元。这给其他防御机制更多的时间来检测和反应，这在繁忙和时间紧迫的事件响应过程中尤为有益。通过并行地将多个候选修复方案推送到不同的故障域中，您可以独立地评估哪些修复方案产生了预期的效果。这样，您就可以避免意外地全局推送一个匆忙的更新，进一步降低整个系统的稳定性。
- en: 'Failure domains incur operational costs. Even a simple service with a few failure
    domains requires you to maintain multiple copies of service configurations, keyed
    by failure domain identifiers. Doing so requires the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 故障域会产生运营成本。即使是一个简单的服务，也需要您维护由故障域标识符键入的多个服务配置的副本。这需要以下工作：
- en: Ensuring configuration consistency
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保配置的一致性
- en: Protecting all configurations from simultaneous corruption
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护所有配置免受同时损坏
- en: Hiding the separation into failure domains from client systems to prevent accidental
    coupling to a particular failure domain
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏故障域的分离，以防止客户系统意外地耦合到特定的故障域
- en: Potentially partitioning all dependencies, because one shared dependency change
    might accidentally propagate to all failure domains
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在地分区所有的依赖关系，因为一个共享的依赖关系变化可能会意外地传播到所有的故障域
- en: It’s worth noting that a failure domain may suffer complete failure if even
    one of its critical components fails. After all, you partitioned the original
    system into failure domains in the first place so that the system can stay up
    even when a failure domain’s copies fail completely. However, failure domains
    simply shift the problem one level down. The following section discusses how you
    can use alternative components to mitigate the risk of complete failure of all
    failure domains.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，如果故障域的任何一个关键组件发生故障，故障域可能会遭受完全故障。毕竟，您最初将原始系统划分为故障域，以便在故障域的副本完全失败时系统仍然可以保持运行。然而，故障域只是将问题向下移动了一个级别。接下来的部分将讨论如何使用替代组件来减轻所有故障域完全失败的风险。
- en: Component Types
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组件类型
- en: The resilient quality of a failure domain is expressed as the combined reliability
    of both its components and their dependencies. Resilience of the entire system
    increases with the number of failure domains. However, this increased resilience
    is offset by the operational overhead of maintaining more and more failure domains.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 故障域的韧性质量表现为其组件和它们的依赖关系的综合可靠性。整个系统的韧性随着故障域的数量增加而增加。然而，这种增加的韧性被维护更多故障域的操作开销所抵消。
- en: You can achieve further improvements in resilience by slowing down or stopping
    new feature development, gaining more stability in exchange. If you avoid adding
    a new dependency, you also avoid its potential failure modes. If you stop updating
    code, the rate of new bugs decreases. However, even if you halt all new feature
    development, you still need to react to occasional changes in state, like security
    vulnerabilities and increases in user demand.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减缓或停止新功能开发，您可以进一步提高韧性，以换取更稳定性。如果您避免添加新的依赖关系，您也将避免其潜在的故障模式。如果停止更新代码，新错误的速率会减少。然而，即使停止所有新功能的开发，您仍然需要对状态的偶发变化做出反应，比如安全漏洞和用户需求的增加。
- en: 'Obviously, halting all new feature development isn’t a viable strategy for
    most organizations. In the following sections, we present a hierarchy of alternative
    approaches to balancing reliability and value. In general, there are three broad
    classes of reliability for services: high capacity, high availability, and low
    dependency.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，停止所有新功能的开发对大多数组织来说并不是一种可行的策略。在接下来的章节中，我们将介绍一系列替代方法来平衡可靠性和价值。一般来说，服务的可靠性有三种广泛的类别：高容量、高可用和低依赖。
- en: High-capacity components
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高容量组件
- en: The components that you build and run in the normal course of business make
    up your *high-capacity* service. That’s because these components make up the main
    fleet serving your users. This is where your service absorbs spikes in user requests
    or resource consumption due to new features. High-capacity components also absorb
    DoS traffic, until DoS mitigation takes effect or graceful degradation kicks in.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 您在日常业务中构建和运行的组件构成了您的*高容量*服务。这是因为这些组件构成了为用户提供服务的主要车队。这是您的服务吸收用户请求或由于新功能而导致的资源消耗激增的地方。高容量组件还吸收DoS流量，直到DoS缓解生效或优雅降级生效。
- en: Because these components are the most critically important to your service,
    you should focus your efforts here first—for example, by following best practices
    for capacity planning, software and configuration rollouts, and more, as covered
    in Part III of the [SRE book](https://landing.google.com/sre/sre-book/toc/index.html)
    and Part II of the [SRE workbook](https://landing.google.com/sre/workbook/toc/).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些组件对于您的服务非常重要，您应该首先在这里集中精力，例如，遵循容量规划、软件和配置部署的最佳实践，如[SRE书](https://landing.google.com/sre/sre-book/toc/index.html)的第三部分和[SRE工作手册](https://landing.google.com/sre/workbook/toc/)的第二部分所涵盖的内容。
- en: High-availability components
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高可用组件
- en: If your system has components whose failures impact all users, or otherwise
    have significant wide-reaching consequences—the high-capacity components discussed
    in the preceding section—you may mitigate these risks by deploying copies of those
    components. These copies of components are *high availability* if they offer a
    provably lower probability of outages.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的系统有组件的故障会影响所有用户，或者具有重大广泛影响的其他故障后果——在前一节中讨论的高容量组件——您可以通过部署这些组件的副本来减轻这些风险。如果这些组件的副本提供了更低的故障概率，那么这些组件的副本就是*高可用*的。
- en: 'To achieve lower probability of outages, the copies should be configured with
    fewer dependencies and a limited rate of changes. This approach reduces the chances
    of infrastructure failures or operational errors breaking the components. For
    example, you might do the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低故障概率，这些副本应该配置更少的依赖关系和有限的变更速率。这种方法减少了基础设施故障或操作错误破坏组件的机会。例如，您可以做以下事情：
- en: Use data cached on local storage to avoid depending on a remote database.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本地存储中缓存的数据，避免依赖远程数据库。
- en: Use older code and configs to avoid recent bugs in newer versions.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用旧代码和配置，避免新版本中的最近错误。
- en: Running high-availability components has little operational overhead, but it
    requires additional resources whose costs scale proportionally to the size of
    the fleet. Determining whether the high-availability components should sustain
    your entire user base or only a portion of that base is a cost/benefit decision.
    Configure graceful degradation capabilities the same way between each high-capacity
    and high-availability component. This allows you to trade fewer resources for
    more aggressive degradation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 运行高可用组件几乎没有操作开销，但需要额外的资源，其成本与车队规模成比例。确定高可用组件是否应该支撑整个用户群或仅支撑部分用户群是一个成本/效益决策。在每个高容量和高可用组件之间以相同的方式配置优雅降级功能。这使您可以用更积极的降级来交换更少的资源。
- en: Low-dependency components
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低依赖组件
- en: If failures in the high-availability components are unacceptable, a *low-dependency*
    service is the next level of resilience. Low dependency requires an alternative
    implementation with minimal dependencies. Those minimal dependencies are also
    low dependency. The total set of services, processes, or jobs that may fail is
    as small as the business needs and costs can bear. High-capacity and high-availability
    services can serve large user bases and offer rich features because of layers
    of cooperating platforms (virtualization, containerization, scheduling, application
    frameworks). While these layers help scaling by permitting services to add or
    move nodes rapidly, they also incur higher rates of outages as error budgets across
    the cooperating platforms add up.^([12](ch08.html#ch08fn12)) In contrast, low-dependency
    services have to simplify their serving stack until they can accept the stack’s
    aggregate error budget. In turn, simplifying the serving stack may lead to having
    to remove features.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果高可用性组件的故障是不可接受的，*低依赖性*服务是下一个弹性级别。低依赖性需要具有最小依赖关系的替代实现。这些最小依赖关系也是低依赖性的。可能失败的服务、进程或作业的总集合与业务需求和成本相匹配。高容量和高可用性的服务可以为大型用户群提供服务并提供丰富的功能，因为它们具有多层合作平台（虚拟化、容器化、调度、应用框架）。虽然这些层通过允许服务快速添加或移动节点来帮助扩展，但它们也会导致合作平台的错误预算累积率更高。相比之下，低依赖性服务必须简化其服务堆栈，直到能够接受堆栈的总体错误预算。反过来，简化服务堆栈可能导致必须删除功能。
- en: Low-dependency components require you to determine if it’s possible to build
    an alternative for a critical component, where the critical and alternative components
    do not share any failure domains. After all, the success of redundancy is inversely
    proportional to the probability of the same root cause affecting both components.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 低依赖性组件要求您确定是否可能为关键组件构建替代方案，其中关键组件和替代组件不共享任何故障域。毕竟，冗余的成功与相同根本原因影响两个组件的概率成反比。
- en: Consider storage space as a fundamental building block of a distributed system—you
    might want to store local data copies as a fallback when the RPC backends for
    data storage are unavailable. However, a general approach of storing local data
    copies isn’t always practical. Operational costs increase to support the redundant
    components, while the benefit the extra components provide is typically zero.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 将存储空间视为分布式系统的基本构建块——当数据存储的RPC后端不可用时，您可能希望存储本地数据副本作为备用。然而，通常存储本地数据副本的一般方法并不总是切实可行。支持冗余组件的运营成本增加，而额外组件提供的好处通常为零。
- en: In practice, you end up with a small set of low-dependency components with limited
    users, features, and costs, but that are confidently available for temporary loads
    or recovery. While most useful features usually rely on multiple dependencies,
    a severely degraded service is better than an unavailable one.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您最终会得到一组低依赖性组件，用户、功能和成本有限，但可以自信地用于临时负载或恢复。虽然大多数有用的功能通常依赖于多个依赖项，但严重受损的服务总比不可用的服务好。
- en: As a small-scale example, imagine a device for which write-only or read-only
    operations are presumed to be available over the network. In a home security system,
    such operations include recording event logs (write-only) and looking up emergency
    phone numbers (read-only). An intruder’s break-in plan includes disabling the
    home’s internet connectivity, thus disrupting the security system. To counter
    this type of failure, you configure the security system to also use a local server
    that implements the same APIs as the remote service. The local server writes event
    logs to local storage, updates the remote service, and retries failed attempts.
    The local server also responds to emergency phone number lookup requests. The
    phone number list is periodically refreshed from the remote service. From the
    home security console’s perspective, the system is working as expected, writing
    logs and accessing emergency numbers. Additionally, a low-dependency, hidden landline
    may provide dialing capabilities as backup to a disabled wireless connection.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 举个小例子，想象一台设备，假定可以通过网络进行只写或只读操作。在家庭安全系统中，这些操作包括记录事件日志（只写）和查找紧急电话号码（只读）。入侵者的入侵计划包括禁用家庭的互联网连接，从而破坏安全系统。为了对抗这种类型的故障，您配置安全系统还使用一个实现与远程服务相同的API的本地服务器。本地服务器将事件日志写入本地存储，更新远程服务，并重试失败的尝试。本地服务器还会响应紧急电话号码查找请求。电话号码列表会定期从远程服务刷新。从家庭安全控制台的角度来看，系统正在按预期工作，记录日志并访问紧急号码。此外，一个低依赖性的隐藏座机可能作为备用提供拨号功能，以应对禁用的无线连接。
- en: As a business-scale example, a global network failure is one of the scariest
    types of outages, because it impacts both service functionality and the ability
    of responders to fix the outage. Large networks are managed dynamically and are
    more at risk for global outages. Building an alternative network that fully avoids
    reusing the same network elements as in the main network—links, switches, routers,
    routing domains, or [SDN](https://oreil.ly/Row8c) software—requires careful design.
    This design must target a specific and narrow subset of use cases and operating
    parameters, allowing you to focus on simplicity and understandability. Aiming
    for minimal capital expenditures for this infrequently used network also naturally
    leads to limiting the available features and bandwidth. Despite the limitations,
    the results are sufficient. The goal is to support only the most critical features,
    and only for a fraction of the usual bandwidth.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 作为业务规模的例子，全球网络故障是最可怕的故障类型之一，因为它影响了服务功能和响应者修复故障的能力。大型网络是动态管理的，更容易发生全球性故障。构建一个完全避免重复使用主网络中相同网络元素的备用网络-链接、交换机、路由器、路由域或[SDN](https://oreil.ly/Row8c)软件-需要仔细设计。这种设计必须针对特定和狭窄的用例和操作参数，使您能够专注于简单和可理解性。为这种很少使用的网络追求最小的资本支出也自然地导致限制可用功能和带宽。尽管存在限制，结果是足够的。目标是仅支持最关键的功能，并且仅为通常带宽的一小部分。
- en: Controlling Redundancies
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制冗余
- en: Redundant systems are configured to have more than a single option for each
    of their dependencies. Managing the choice between these options is not always
    straightforward, and attackers can potentially exploit the differences between
    the redundant systems—for example, by pushing the system toward the less secure
    option. Remember that a resilient design achieves security *and* reliability without
    sacrificing one for the other. If anything, when low-dependency alternatives have
    stronger security, this can serve as a disincentive to attackers who are considering
    wearing down your system.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余系统配置为每个依赖项都有多个选项。管理这些选项之间的选择并不总是直截了当的，攻击者可能会潜在地利用冗余系统之间的差异-例如，通过将系统推向较不安全的选项。请记住，弹性设计实现了安全性*和*可靠性，而不是牺牲其中一个。如果有的话，当低依赖性的替代方案具有更强的安全性时，这可以作为攻击者考虑削弱您的系统的一种不利因素。
- en: Failover strategies
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障转移策略
- en: Supplying a set of backends, usually through load-balancing technologies, adds
    resilience in the face of a backend failure. For example, it is impractical to
    rely on a single RPC backend. Whenever that backend needs to restart, the system
    will hang. For simplicity, the system usually treats redundant backends as *interchangeable*,
    as long as all backends provide the same feature behaviors.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过负载平衡技术通常提供一组后端，可以在后端故障时增加弹性。例如，依赖单个RPC后端是不切实际的。每当该后端需要重新启动时，系统都会挂起。为简单起见，系统通常将冗余后端视为*可互换*，只要所有后端提供相同的功能行为。
- en: A system that needs different *reliability* behaviors (for the same set of feature
    behaviors) should rely on a distinct set of interchangeable backends that provide
    the desired reliability behaviors. The system itself must implement logic to determine
    which set of behaviors to use and when to use them—for example, through flags.
    This gives you full control over the system’s reliability, especially during recovery.
    Contrast this approach to requesting low-dependency behavior from the same high-availability
    backend. Using an RPC parameter, you might prevent the backend from attempting
    to contact its unavailable runtime dependency. If the runtime dependency is also
    a startup dependency, your system is still one process restart from disaster.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 需要不同*可靠性*行为的系统（对于相同的功能行为集）应依赖于提供所需可靠性行为的一组可互换的后端。系统本身必须实现逻辑来确定使用哪组行为以及何时使用它们-例如，通过标志。这使您完全控制系统的可靠性，特别是在恢复期间。将此方法与从相同的高可用性后端请求低依赖性行为进行对比。使用RPC参数，您可以阻止后端尝试联系其不可用的运行时依赖项。如果运行时依赖项也是启动依赖项，则您的系统仍然离灾难只有一次进程重启。
- en: When to fail over to a component with better stability is situation-specific.
    If automatic failover is a goal, you should address the differences in available
    capacity by using the means covered in [“Controlling Degradation”](#controlling_degradation).
    After failover, such a system switches to using throttling and load-shedding policies
    tuned for the alternative component. If you want the system to fail back after
    the failed component recovers, provide a way to disable that failback—you may
    need to stabilize fluctuations or precisely control failover in some cases.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 何时转移到具有更好稳定性的组件取决于特定情况。如果自动故障转移是目标，您应该通过使用[“控制退化”](#controlling_degradation)中涵盖的方法来解决可用容量的差异。故障转移后，这样的系统会切换到使用针对备用组件调整的限流和负载放弃策略。如果您希望系统在失败的组件恢复后进行故障返回，请提供一种禁用故障返回的方法-在某些情况下，您可能需要稳定波动或精确控制故障转移。
- en: Common pitfalls
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见陷阱
- en: We’ve observed some common pitfalls with operating alternative components, regardless
    of whether they’re high availability or low dependency.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到一些常见的操作备用组件的常见陷阱，无论它们是高可用性还是低依赖性。
- en: For instance, over time you can grow to rely on alternative components for normal
    operation. Any dependent system that begins to treat the alternative systems as
    backup likely overloads them during an outage, making the alternative system an
    unexpected cause for denial of service. The opposite problem occurs when the alternative
    components are not routinely used, resulting in rot and surprise failures whenever
    they are needed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，随着时间的推移，您可能会开始依赖备用组件进行正常操作。任何开始将备用系统视为备份的依赖系统在故障期间可能会过载它们，使备用系统成为拒绝服务的意外原因。相反的问题是备用组件通常不会被常规使用，导致腐烂和意外故障每当它们需要时。
- en: Another pitfall is unchecked growth of dependence on other services or amounts
    of required compute resources. Systems tend to evolve as user demands change and
    developers add features. Over time, dependencies and dependents grow, and systems
    may use resources less efficiently. High-availability copies may fall behind high-capacity
    fleets, or low-dependency services may lose consistency and reproducibility when
    their intended operating constraints are not continuously monitored and validated.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个陷阱是对其他服务或所需计算资源的依赖未经检查地增长。随着用户需求的变化和开发人员添加功能，系统往往会发展。随着时间的推移，依赖关系和依赖方增加，系统可能使用资源的效率降低。高可用副本可能落后于高容量舰队，或者低依赖服务可能在其预期的操作约束条件未经持续监控和验证时失去一致性和可重现性。
- en: 'It is crucial that failover to alternative components does not compromise the
    system’s integrity or security. Consider the following scenarios in which the
    right choice depends on your organization’s circumstances:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，备用组件的故障转移不会损害系统的完整性或安全性。考虑以下情景，正确的选择取决于您组织的情况：
- en: 'You have a high-availability service that runs six-week-old code for security
    reasons (to defend against recent bugs). However, this same service requires an
    urgent security fix. Which risk would you choose: not applying the fix, or potentially
    breaking the code with the fix?'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于安全原因（防御最近的漏洞），您的高可用服务运行了六周前的代码。然而，同一服务需要紧急安全修复。您会选择哪种风险：不应用修复，还是可能通过修复破坏代码？
- en: A remote key service’s startup dependency for fetching private keys that decrypt
    data may be made low dependency by storing private keys on local storage. Does
    this approach create an unacceptable risk to those keys, or can an increase in
    key rotation frequency sufficiently counteract that risk?
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个远程密钥服务的启动依赖于获取解密数据的私钥，可以通过将私钥存储在本地存储中来降低依赖性。这种方法是否会给这些密钥带来无法接受的风险，或者增加密钥轮换频率是否足以抵消这种风险？
- en: You determine that you can free up resources by reducing the frequency of updates
    to data that changes infrequently (for example, ACLs, certificate revocation lists,
    or user metadata). Is it worthwhile to free up these resources, even if doing
    so potentially gives an attacker more time to make changes to that data or enables
    those changes to persist undetected for longer?
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您确定可以通过减少不经常更改的数据（例如ACL、证书吊销列表或用户元数据）的更新频率来释放资源。即使这样做可能会给攻击者更多时间进行更改或使更改未被检测到更长时间，这样做是否值得？
- en: Finally, you need to make certain to prevent your system from autorecovering
    at the wrong time. If resilience measures automatically throttled the system’s
    performance, it’s OK for those same measures to automatically unthrottle it. However,
    if you applied a manual failover, don’t permit automation to override the failover—the
    drained system might be quarantined because of a security vulnerability, or your
    team might be mitigating a cascading failure.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要确保系统不会在错误的时间自动恢复。如果弹性措施自动限制了系统的性能，那么这些措施自动解除限制是可以的。然而，如果您应用了手动故障转移，请不要允许自动化覆盖故障转移——因为受到安全漏洞的影响，被排除的系统可能被隔离，或者您的团队可能正在减轻级联故障。
- en: 'DEEP DIVE: Continuous Validation'
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨：持续验证
- en: From both a reliability and a security perspective, we want to be sure that
    our systems behave as anticipated under both normal and unexpected circumstances.
    We also want to be sure that new features or bug fixes don’t gradually erode a
    system’s layered resilience mechanisms. There is no substitute for actually exercising
    the system and validating that it works as intended.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从可靠性和安全性的角度来看，我们希望确保我们的系统在正常和意外情况下都能如预期般运行。我们还希望确保新功能或错误修复不会逐渐削弱系统的分层弹性机制。实际运行系统并验证其按预期工作是无法替代的。
- en: Validation focuses on observing the system under *realistic* but *controlled*
    circumstances, targeting workflows within a single system or across multiple systems.^([13](ch08.html#ch08fn13))
    Unlike [chaos engineering](https://oreil.ly/Fvx4L), which is exploratory in nature,
    validation confirms specific system properties and behaviors covered in this chapter
    and Chapters [5](ch05.html#design_for_least_privilege), [6](ch06.html#design_for_understandability),
    and [9](ch09.html#design_for_recovery). When you validate regularly, you ensure
    that the outcomes remain as expected and that the validation practices themselves
    remain functional.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 验证侧重于观察系统在*真实*但*受控*的情况下，针对单个系统或跨多个系统的工作流程。与[混沌工程](https://oreil.ly/Fvx4L)不同，后者是探索性质的，验证确认了本章和第[5](ch05.html#design_for_least_privilege)、[6](ch06.html#design_for_understandability)和[9](ch09.html#design_for_recovery)章中涵盖的特定系统属性和行为。定期进行验证可以确保结果仍然符合预期，并且验证实践本身保持正常运行。
- en: There’s some art to making validation *meaningful*. To start with, you can use
    some of the concepts and practices covered in [Chapter 15](ch15.html#onefive_investigating_systems)—for
    example, how to choose what to validate, and how to measure effective system attributes.
    Then you can gradually evolve your validation coverage by creating, updating,
    or removing checks. You can also extract useful details from actual incidents—these
    details are the ultimate truths of your system’s behaviors, and often highlight
    needed design changes or gaps in your validation coverage. Finally, it’s important
    to remember that as business factors change, individual services tend to evolve
    and change as well, potentially resulting in incompatible APIs or unanticipated
    dependencies.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在使验证*有意义*方面有一些技巧。首先，您可以使用[第15章](ch15.html#onefive_investigating_systems)中涵盖的一些概念和实践，例如如何选择要验证的内容，以及如何衡量有效的系统属性。然后，您可以逐渐扩展验证范围，创建、更新或删除检查。您还可以从实际事件中提取有用的细节——这些细节是系统行为的最终真相，并经常突出了需要的设计更改或验证范围的空白。最后，重要的是要记住，随着业务因素的变化，个别服务往往也会发展和变化，可能导致不兼容的API或意想不到的依赖关系。
- en: 'A general validation maintenance strategy includes the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的验证维护策略包括以下内容：
- en: Discovering new failures
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发现新的故障
- en: Implementing validators for each failure
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个故障实施验证器
- en: Executing all validators repeatedly
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反复执行所有验证器
- en: Phasing out validators when the relevant features or behaviors no longer exist
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当相关功能或行为不再存在时淘汰验证器
- en: 'To discover relevant failures, rely on the following sources:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要发现相关故障，请依赖以下来源：
- en: Regular bug reports from users and employees
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和员工的定期错误报告
- en: Fuzzing and fuzzing-like approaches (described in [Chapter 13](ch13.html#onethree_testing_code))
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊测试和类似模糊测试的方法（在[第13章](ch13.html#onethree_testing_code)中描述）
- en: Failure-injection approaches (akin to the [Chaos Monkey tool](https://oreil.ly/fvSKQ))
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障注入方法（类似于[混沌猴工具](https://oreil.ly/fvSKQ)）
- en: Analytical judgment of subject matter experts who operate your systems
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作您系统的主题专家的分析判断。
- en: Building an automation framework can help you schedule incompatible checks to
    run at different times so that they don’t conflict with each other. You should
    also monitor and establish periodic audits of automation to catch broken or compromised
    behaviors.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自动化框架可以帮助您安排不兼容的检查在不同时间运行，以避免它们之间的冲突。您还应该监视并定期审计自动化，以捕捉破损或受损的行为。
- en: Validation Focus Areas
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证重点领域
- en: 'It’s beneficial to validate whole systems and the end-to-end cooperation among
    their services. But because validating the failure response of whole systems that
    serve real users is expensive and risky, you have to compromise. Validating smaller
    system replicas is usually more affordable, and still provides insights that are
    impossible to obtain by validating individual system components in isolation.
    For example, you can do the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 验证整个系统以及其服务之间的端到端协作是有益的。但是，由于验证为真实用户提供服务的整个系统的故障响应是昂贵且风险的，因此您必须做出妥协。验证较小的系统副本通常更为经济实惠，并且仍然提供了通过验证单独的系统组件无法获得的见解。例如，您可以执行以下操作：
- en: Tell how callers react to an RPC backend that is responding slowly or becoming
    unreachable.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告诉调用者如何应对响应缓慢或变得无法访问的RPC后端。
- en: See what happens when resource shortages occur, and whether it’s feasible to
    obtain an emergency resource quota when resource consumption spikes.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看资源短缺时会发生什么，以及在资源消耗激增时是否可获取紧急资源配额。
- en: Another practical solution is to rely upon logs to analyze interactions between
    systems and/or their components. If your system implements compartmentalization,
    operations that attempt to cross boundaries of role, location, or time separation
    should fail. If your logs record unexpected successes instead, these successes
    should be flagged. Log analysis should be active at all times, letting you observe
    actual system behaviors during validation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个实用的解决方案是依靠日志来分析系统和/或其组件之间的交互。如果您的系统实施了分隔，那么试图跨越角色、位置或时间分隔的操作应该失败。如果您的日志记录了意外的成功，那么这些成功应该被标记。日志分析应始终处于活动状态，让您在验证过程中观察实际的系统行为。
- en: 'You should validate the attributes of your security design principles: least
    privilege, understandability, adaptability, and recovery. Validating recovery
    is especially critical, because recovery efforts necessarily involve human actions.
    Humans are unpredictable, and unit tests cannot check human skills and habits.
    When validating recovery design, you should review both the readability of recovery
    instructions and the efficacy and interoperability of different recovery workflows.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该验证安全设计原则的属性：最小特权、可理解性、适应性和恢复能力。验证恢复尤为关键，因为恢复工作必然涉及人类行为。人类是不可预测的，单元测试无法检查人类的技能和习惯。在验证恢复设计时，您应该审查恢复说明的可读性以及不同恢复工作流程的效力和互操作性。
- en: Validating security attributes means going beyond ensuring correct system responses.
    You should also check that the code or configuration doesn’t have any known vulnerabilities.
    Active penetration testing of a deployed system gives a black-box view of the
    system’s resilience, often highlighting attack vectors the developers did not
    consider.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 验证安全属性意味着不仅要确保系统正确响应，还要检查代码或配置是否存在已知的漏洞。对部署系统进行主动渗透测试可以黑盒视角地查看系统的弹性，通常会突出开发人员未考虑的攻击向量。
- en: Interactions with low-dependency components deserve special attention. By definition,
    these components are deployed in the most critical circumstances. There is no
    fallback beyond these components. Fortunately, a well-designed system should have
    a limited number of low-dependency components, which makes the goal of defining
    validators for all critical functions and interactions feasible. You realize the
    return on investment in these low-dependency components *only* if they work when
    needed. Your recovery plans should often rely on the low-dependency components,
    and you should validate their use by humans for the possible situation where the
    system degrades to that level.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 与低依赖组件的交互值得特别关注。根据定义，这些组件部署在最关键的情况下。除了这些组件之外没有后备。幸运的是，设计良好的系统应该具有有限数量的低依赖组件，这使得为所有关键功能和交互定义验证器的目标成为可能。只有在需要时这些低依赖组件才能发挥作用。您的恢复计划通常应依赖于低依赖组件，并且您应该验证人类在系统降级到该级别时的使用情况。
- en: Validation in Practice
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的验证
- en: This section presents a few validation scenarios that have been used at Google
    to demonstrate the wide spectrum of approaches to continuous validation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了谷歌用于展示持续验证方法的几种验证场景。
- en: Inject anticipated changes of behavior
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注入预期的行为变化
- en: You can validate system response to load shedding and throttling by injecting
    a change of behavior into the server, and then observing whether all affected
    clients and backends respond appropriately.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过向服务器注入行为变化来验证系统对负载分担和限流的响应，然后观察所有受影响的客户端和后端是否适当地响应。
- en: For example, Google implements server libraries and control APIs that permit
    us to add arbitrary delays or failures to any RPC server. We use this functionality
    in periodic disaster readiness exercises, and teams may easily run experiments
    at any time. Using this approach, we study isolated RPC methods, whole components,
    or larger systems, and specifically look for signs of cascading failures. Starting
    with a small increase in latency, we build a step function toward simulating a
    full outage. Monitoring graphs clearly reflect changes in response latencies just
    as they would for real problems, at the step points. Correlating these timelines
    with monitoring signals from clients and backend servers, we can observe the propagation
    of effect. If error rates spike disproportionately to the patterns we observed
    at the earlier steps, we know to step back, pause, and investigate whether the
    behavior is unexpected.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Google实施了服务器库和控制API，允许我们向任何RPC服务器添加任意延迟或故障。我们在定期的灾难准备演习中使用这些功能，团队可以随时轻松地进行实验。使用这种方法，我们研究隔离的RPC方法、整个组件或更大的系统，并专门寻找级联故障的迹象。从延迟的小幅增加开始，我们构建一个阶跃函数，以模拟完全的故障。监控图表清楚地反映了响应延迟的变化，就像在真正的问题中一样，在阶跃点。将这些时间轴与客户端和后端服务器的监控信号相关联，我们可以观察效果的传播。如果错误率与我们在早期步骤中观察到的模式不成比例地飙升，我们就知道要退后一步，暂停，并调查行为是否出乎意料。
- en: It’s important to have a reliable mechanism for canceling the injected behaviors
    quickly and safely. If there’s a failure, even if the cause doesn’t seem related
    to the validation, the right decision is to abort experiments first, and then
    evaluate when it is safe to try again.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 快速而安全地取消注入行为的可靠机制非常重要。如果发生故障，即使原因似乎与验证无关，正确的决定是首先中止实验，然后评估何时可以安全地再次尝试。
- en: Exercise emergency components as part of normal workflows
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将紧急组件作为正常工作流程的一部分
- en: We can be certain that a low-dependency or high-availability system is functioning
    correctly and ready to roll out to production when we observe the system performing
    its intended functions. To test readiness, we push either a small fraction of
    real traffic or a small fraction of real users to the system we are validating.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察系统执行其预期功能时，我们可以确信低依赖或高可用系统正在正确运行并准备投入生产。为了测试准备就绪，我们将真实流量的一小部分或真实用户的一小部分推送到我们正在验证的系统。
- en: 'High-availability systems (and sometimes low-dependency systems) are validated
    by mirroring requests: clients send two identical requests, one to the high-capacity
    component and one to the high-availability component. By modifying the client
    code or injecting a server that can duplicate one input traffic stream into two
    equivalent output streams,^([14](ch08.html#ch08fn14)) you can compare the responses
    and report the differences. Monitoring services send alerts when the response
    discrepancies exceed anticipated levels. Some discrepancies are expected; for
    example, if the fallback system has older data or features. For that reason, a
    client should use the response from the high-capacity system, unless an error
    occurs or the client was explicitly configured to ignore that system—both of which
    might happen in emergencies. Mirroring the requests requires not only code changes
    at the client, but also the ability to customize the mirroring behavior. Because
    of that, this strategy is easier to deploy on frontend or backend servers rather
    than on end-user devices.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用系统（有时是低依赖系统）通过镜像请求进行验证：客户端发送两个相同的请求，一个发送到高容量组件，另一个发送到高可用组件。通过修改客户端代码或注入一个可以将一个输入流量复制成两个等效输出流的服务器，您可以比较响应并报告差异。当响应差异超出预期水平时，监控服务会发送警报。一些差异是可以预期的；例如，如果备用系统具有较旧的数据或功能。因此，客户端应该使用高容量系统的响应，除非发生错误或客户端明确配置为忽略该系统——这两种情况都可能在紧急情况下发生。镜像请求不仅需要在客户端进行代码更改，还需要能够自定义镜像行为。因此，这种策略更容易部署在前端或后端服务器上，而不是在最终用户设备上。
- en: 'Low-dependency systems (and occasionally high-availability systems) are better
    suited for validation by real users than by request mirroring. This is because
    low-dependency systems differ substantially from their higher-unreliability counterparts
    in terms of features, protocols, and system capacity. At Google, on-call engineers
    use low-dependency systems as an integral part of their on-call duties. We use
    this strategy for a few reasons:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 低依赖系统（有时是高可用系统）更适合通过真实用户而不是请求镜像进行验证。这是因为低依赖系统在功能、协议和系统容量方面与其更不可靠的对应系统有很大不同。在Google，值班工程师将低依赖系统作为值班职责的一个重要部分。我们之所以使用这种策略，有几个原因：
- en: Many engineers participate in on-call rotations, but only a small fraction of
    engineers are on call at once. This naturally restricts the set of people involved
    in validating.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多工程师参与值班轮班，但一次只有少部分工程师值班。这自然限制了参与验证的人员范围。
- en: When engineers are on call, they might need to rely on emergency paths. Well-practiced
    use of low-dependency systems reduces the time it takes an on-call engineer to
    switch to using these systems in a true emergency, and avoids the risk of unexpected
    misconfiguration.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当工程师值班时，他们可能需要依赖紧急路径。熟练使用低依赖系统可以减少值班工程师在真正紧急情况下切换到使用这些系统所需的时间，并避免意外的配置错误风险。
- en: Transitioning on-call engineers to using only low-dependency systems can be
    implemented gradually and by different means, depending on the business criticality
    of each system.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步将值班工程师转换为仅使用低依赖系统可以逐步实施，并且可以通过不同的方式实施，具体取决于每个系统的业务关键性。
- en: Split when you cannot mirror traffic
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当无法镜像流量时进行分割
- en: As an alternative to request mirroring, you can split requests between disjoint
    sets of servers. This is appropriate if request mirroring is not feasible—for
    example, when you have no control over client code, but load balancing at the
    level of request routing is feasible. Consequently, splitting requests works only
    when alternative components use the same protocols, as is often the case with
    high-capacity and high-availability versions of a component.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 作为请求镜像的替代方案，您可以将请求分割到不相交的服务器集之间。如果请求镜像不可行，例如当您无法控制客户端代码，但在请求路由级别进行负载均衡是可行的。因此，只有在替代组件使用相同的协议时，请求分割才有效，这在高容量和高可用性版本的组件中经常发生。
- en: Another application of this strategy is to distribute traffic across a set of
    failure domains. If your load balancing targets a single failure domain, you can
    run focused experiments against that domain. Because a failure domain has lower
    capacity, attacking it and eliciting resilient responses requires less load. You
    can quantify the impact of your experiment by comparing the monitoring signals
    from other failure domains. By adding load shedding and throttling, you further
    increase the quality of output from the experiment.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的另一个应用是将流量分布到一组故障域中。如果您的负载均衡针对单个故障域，您可以针对该域运行集中的实验。由于故障域的容量较低，攻击它并引发弹性响应需要更少的负载。您可以通过比较其他故障域的监控信号来量化实验的影响。通过增加负载放置和限流，您可以进一步提高实验的输出质量。
- en: Oversubscribe but prevent complacency
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超额预订但防止自满
- en: Quota assigned to customers but not consumed is a waste of resources. Therefore,
    in the interest of maximizing resource utilization, many services oversubscribe
    resources by some sane margin. A margin call on resources may happen at any time.
    A resilient system tracks priorities so it can release lower-priority resources
    to fulfill demand for higher-priority resources. However, you should validate
    whether the system can actually release those resources reliably, and in an acceptable
    amount of time.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给客户但未使用的配额是资源的浪费。因此，为了最大限度地利用资源，许多服务通过一定合理的边际进行资源超额预订。资源的边际调用可能随时发生。弹性系统跟踪优先级，以便释放较低优先级的资源以满足对较高优先级资源的需求。但是，您应该验证系统是否能够可靠地释放这些资源，并且在可接受的时间内释放这些资源。
- en: 'Google once had a service that needed a lot of disk space for batch processing.
    User services take priority over batch processing and allocate significant disk
    reserves for usage spikes. We permitted the batch processing service to utilize
    disks unused by the user services, under a specific condition: any disks in a
    particular cluster must be fully released within *X* hours. The validation strategy
    we developed consisted of periodically moving the batch processing service out
    of a cluster, measuring how long the move took, and fixing any new issues uncovered
    at each attempt. This was not a simulation. Our validation ensured that the engineers
    who promised the SLO of *X* hours had both real evidence and real experience.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌曾经有一个需要大量磁盘空间进行批处理的服务。用户服务优先于批处理，并为使用高峰分配了大量磁盘保留空间。我们允许批处理服务利用用户服务未使用的磁盘，但有一个特定条件：特定集群中的任何磁盘必须在*X*小时内完全释放。我们开发的验证策略包括定期将批处理服务从集群中移出，测量移动所需的时间，并在每次尝试中修复发现的任何新问题。这不是模拟。我们的验证确保承诺*X*小时SLO的工程师既有真实的证据又有真实的经验。
- en: These validations are expensive, but most of the costs are absorbed by automation.
    Load balancing limits the costs to managing the resource provisioning at the source
    and target locations. If resource provisioning is mostly automated—for example,
    as is the case with cloud services—it becomes a matter of running scripts or playbooks
    to execute a series of requests to the automation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些验证成本高，但大部分成本都由自动化吸收。负载均衡将成本限制在管理资源在源和目标位置的配置上。如果资源配置大部分是自动化的，例如云服务的情况，那么只需要运行脚本或playbook来执行一系列请求即可。
- en: For smaller services or companies, the strategy of periodically executing a
    rebalancing applies similarly. The resulting confidence in responding predictably
    to shifts in application load is part of the foundation for software architecture
    that can serve a global user base.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的服务或公司，定期执行再平衡的策略同样适用。对应用负载变化做出可预测响应的信心是可以为全球用户基础提供服务的软件架构的基础之一。
- en: Measure key rotation cycles
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量关键旋转周期
- en: 'Key rotation is simple in theory, but in practice it may bring unpleasant surprises,
    including full service outages. When validating that key rotation works, you should
    look for at least two distinct outcomes:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 关键旋转在理论上很简单，但在实践中可能会带来令人不快的惊喜，包括完全的服务中断。在验证关键旋转是否有效时，您应该寻找至少两个不同的结果：
- en: Key rotation latency
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 关键旋转延迟
- en: The time it takes to complete a single rotation cycle
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 完成单个旋转周期所需的时间
- en: Verified loss of access
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 验证失去访问权限
- en: Certainty that the old key is fully useless after rotation
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 确保旧密钥在旋转后完全无用
- en: We recommend periodically rotating keys so they remain ready for nonnegotiable
    emergency key rotations prompted by a security compromise. This means rotating
    keys even if you don’t have to. If the rotation process is expensive, look for
    ways to lower its costs.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议定期旋转密钥，以便它们保持随时准备好应对由安全威胁引发的不可协商的紧急密钥旋转。这意味着即使不必旋转密钥，也要进行旋转。如果旋转过程成本高，可以寻找降低成本的方法。
- en: 'At Google, we’ve experienced that measuring key rotation latency helps with
    multiple objectives:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌，我们已经经历了测量关键旋转延迟对多个目标有所帮助：
- en: You learn whether every service that uses the key is actually able to update
    its configuration. Perhaps a service was not designed for key rotation, or was
    designed for it but was never tested, or a change broke what previously worked.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以了解使用该密钥的每个服务是否能够更新其配置。也许某个服务并不是为密钥旋转而设计的，或者虽然设计了但从未经过测试，或者更改破坏了以前的工作。
- en: You learn how long each service takes to rotate your key. Key rotation might
    be as trivial as a file change and server restart, or as involved as a gradual
    rollout across all world regions.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以了解每项服务旋转密钥所需的时间。密钥旋转可能只是一个文件更改和服务器重启，也可能是逐步在所有世界地区推出。
- en: You discover how other system dynamics delay the key rotation process.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您会发现其他系统动态如何延迟密钥旋转过程。
- en: Measuring key rotation latency has helped us form a realistic expectation of
    the entire cycle, both in normal and emergency circumstances. Account for possible
    rollbacks (caused by key rotation or other events), change freezes for services
    out of error budget, and serialized rollouts due to failure domains.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 测量密钥旋转延迟有助于我们形成对整个周期的现实预期，无论是在正常情况下还是在紧急情况下。考虑可能由密钥旋转或其他事件引起的回滚，服务超出错误预算的更改冻结，以及由于故障域而产生的序列化推出。
- en: How to verify loss of access via an old key is likely case-specific. It’s not
    easy to prove that all instances of the old key were destroyed, so ideally you
    can demonstrate that attempting to use the old key fails, after which point you
    can destroy the old key. When this approach isn’t practical, you can rely on key
    deny-list mechanisms (for example, CRLs). If you have a central certificate authority
    and good monitoring, you may be able to create alerts if any ACLs list the old
    key’s fingerprint or serial number.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如何验证通过旧密钥丧失访问的可能情况特定。很难证明旧密钥的所有实例都被销毁，因此理想情况下，您可以证明尝试使用旧密钥失败，之后您可以销毁旧密钥。当这种方法不切实际时，您可以依赖密钥拒绝列表机制（例如CRL）。如果您有一个中央证书颁发机构和良好的监控，您可能能够在任何ACL列出旧密钥的指纹或序列号时创建警报。
- en: 'Practical Advice: Where to Begin'
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实用建议：从何处开始
- en: Designing resilient systems isn’t a trivial task. It takes time and energy,
    and diverts efforts from other valuable work. You need to consider the tradeoffs
    carefully, according to the degree of resilience you want, and then pick from
    the wide spectrum of options we describe—the few or many solutions that fit your
    needs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 设计弹性系统并不是一项微不足道的任务。这需要时间和精力，并且会转移其他有价值的工作。您需要根据您想要的弹性程度仔细考虑权衡，然后从我们描述的广泛选项中选择——适合您需求的少数或多数解决方案。
- en: 'In order of costs:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 按成本顺序：
- en: Failure domains and blast radius controls have the lowest costs because of their
    relatively static nature, yet offer significant improvements.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故障域和冲击半径控制具有较低的成本，因为它们的相对静态性质，但提供了显著的改进。
- en: High-availability services are the next most cost-effective solution.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高可用性服务是成本效益的下一个解决方案。
- en: 'Consider these options next:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑接下来的这些选项：
- en: Consider deploying load-shedding and throttling capabilities if your organization’s
    scale or risk aversion justifies investing in active automation for resilience.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的组织规模或风险规避需要投资于弹性的主动自动化，考虑部署负载分担和限流能力。
- en: Evaluate the effectiveness of your defenses against DoS attacks (see [Chapter 10](ch10.html#mitigating_denial_of_service_attacks)).
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估您的防御措施对抗DoS攻击的有效性（参见[第10章](ch10.html#mitigating_denial_of_service_attacks)）。
- en: If you build a low-dependency solution, introduce a process or a mechanism to
    ensure that it stays low dependency over time.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果构建低依赖解决方案，请引入一个过程或机制，以确保它随着时间的推移保持低依赖性。
- en: 'It can be hard to overcome a resistance to invest in resilience improvements,
    because the benefits manifest as an absence of problems. These arguments might
    help:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 克服对投资于弹性改进的抵制可能很困难，因为好处表现为问题的缺失。以下论点可能有所帮助：
- en: Deploying failure domains and blast radius controls will have a lasting effect
    on future systems. The isolation techniques can encourage or enforce good separation
    of operational failure domains. Once in place, they will inevitably make it harder
    to design and deploy unnecessarily coupled or fragile systems.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署故障域和冲击半径控制将对未来系统产生持久影响。隔离技术可以鼓励或强制执行操作故障域的良好分离。一旦建立，它们将不可避免地使设计和部署不必要耦合或脆弱的系统变得更加困难。
- en: Regular key change and rotation techniques and exercises not only ensure preparation
    for security incidents, but also give you general cryptographic agility—for example,
    knowing you can upgrade encryption primitives.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期更改和旋转密钥的技术和练习不仅确保了安全事件的准备，还为您提供了一般的加密灵活性，例如，知道您可以升级加密原语。
- en: The relatively low additional cost of deploying high-availability instances
    of a service provides for a cheap way to examine how much you might be able to
    improve the service’s availability. It’s also cheap to abandon.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署高可用性服务实例的相对较低额外成本提供了一种廉价的方法，可以检查您可能能够提高服务可用性的程度。放弃也很便宜。
- en: Load-shedding and throttling capabilities, along with the other approaches covered
    in [“Controlling Degradation”](#controlling_degradation), reduce the cost of the
    resources the company needs to maintain. The resulting user-visible improvements
    often apply to the most valued product features.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载分担和限流能力以及[“控制退化”](#controlling_degradation)中涵盖的其他方法，减少了公司需要维护的资源成本。由此产生的用户可见改进通常适用于最受重视的产品特性。
- en: Controlling degradation critically contributes to the speed and effectiveness
    of first reaction when defending against DoS attacks.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制退化对于在防御DoS攻击时的第一反应的速度和有效性至关重要。
- en: Low-dependency solutions are relatively expensive yet rarely used in practice.
    To determine whether they are worth their cost, it can help to know how much time
    it would take to bring up all the dependencies of the business-critical services.
    You can then compare the costs and conclude whether it’s better to invest your
    time elsewhere.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低依赖解决方案相对昂贵，但在实践中很少被使用。要确定它们是否值得成本，可以帮助了解启动业务关键服务的所有依赖项需要多长时间。然后可以比较成本，并得出是否最好将时间投资在其他地方的结论。
- en: Whatever resilience solutions you put together, look for affordable ways to
    keep them continuously validated, and avoid cost cutting that risks their effectiveness.
    The benefit from investing in validation is in locking in, for the long term,
    the compounding value of all other resilience investments. If you automate these
    techniques, the engineering and support teams can focus on delivering new value.
    The cost of automating and monitoring will ideally be amortized across other efforts
    and products your company is pursuing.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: You will probably periodically run out of money or time you can invest into
    resilience. The next time you have the opportunity to spend more of these limited
    resources, consider streamlining the costs of your already deployed resilience
    mechanisms first. Once you are confident in their quality and efficiency, venture
    into more resilience options.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed various ways to build resilience into the security
    and reliability of a system, starting from the design phase. In order to provide
    resilience, humans need to make choices. We can optimize some choices with automation,
    but for others we still need humans.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Resilience of reliability properties helps preserve a system’s most important
    functionality, so the system doesn’t fully succumb under conditions of excessive
    load or extensive failures. If the system does break, this functionality extends
    the time available for responders to organize, prevent more damage, or, if necessary,
    engage in manual recovery. Resilience helps systems withstand attacks and defends
    against attempts to gain long-term access. If an attacker breaks into the system,
    design features like blast radius controls limit the damage.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Ground your design strategies in defense in depth. Examine a system’s security
    the same way you view uptime and reliability. At its core, defense in depth is
    like *N*+1 redundancy for your defenses. You don’t trust all of your network capacity
    to a single router or switch, so why trust a single firewall or other defense
    measure? In designing for defense in depth, always assume and check for failures
    in different layers of security: failures in outer perimeter security, the compromise
    of an endpoint, an insider attack, and so on. Plan for lateral moves with the
    intent of stopping them.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Even when you design your systems to be resilient, it’s possible that resilience
    will fall short at some point and your system will break. The next chapter discusses
    what happens *after* that happens: how do you recover broken systems, and how
    can you minimize the damage caused by breakages?'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#ch08fn1-marker)) Protected sandboxes provide an isolated environment
    for untrusted code and data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.html#ch08fn2-marker)) Google runs bug bounty [reward programs](https://oreil.ly/ZQGNW).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch08.html#ch08fn3-marker)) See Singh, Soram Ranbir, Ajoy Kumar Khan,
    and Soram Rakesh Singh. 2016\. “Performance Evaluation of RSA and Elliptic Curve
    Cryptography.” *Proceedings of the 2nd International Conference on Contemporary
    Computing and Informatics*: 302–306\. doi:10.1109/IC3I.2016.7917979.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.html#ch08fn4-marker)) This is described in [Chapter 20 of the SRE
    book](https://landing.google.com/sre/sre-book/chapters/load-balancing-datacenter/).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.html#ch08fn5-marker)) See [Chapter 21 of the SRE book](https://landing.google.com/sre/sre-book/chapters/handling-overload/).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.html#ch08fn6-marker)) The concepts of “failing open” and “fail closed”
    refer to the service *remaining operational* (being reliable) or *shutting down*
    (being secure), respectively. The terms “fail open” and “fail closed” are often
    used interchangeably with “fail safe” and “fail secure,” as described in [Chapter 1](ch01.html#the_intersection_of_security_and_reliab).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.html#ch08fn7-marker)) See [Chapter 2 of the SRE book](https://landing.google.com/sre/sre-book/chapters/production-environment/)
    for a description of the production environment at Google.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch08.html#ch08fn7-marker)) 有关Google生产环境的描述，请参阅[SRE书籍第2章](https://landing.google.com/sre/sre-book/chapters/production-environment/)。
- en: ^([8](ch08.html#ch08fn8-marker)) Typically, this instance of the service would
    still be served by many replicas of the underlying server, but it would function
    as a single logical compartment.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch08.html#ch08fn8-marker)) 通常，该服务的实例仍然由基础服务器的许多副本提供，但它将作为单个逻辑隔间运行。
- en: ^([9](ch08.html#ch08fn9-marker)) For example, Google Cloud Platform offers so-called
    [sole-tenant nodes](https://oreil.ly/anLXq).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch08.html#ch08fn9-marker)) 例如，Google Cloud Platform提供所谓的[独占租户节点](https://oreil.ly/anLXq)。
- en: ^([10](ch08.html#ch08fn10-marker)) A breakglass mechanism is one that can bypass
    policies to allow engineers to quickly resolve outages. See [“Breakglass”](ch05.html#breakglass).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch08.html#ch08fn10-marker)) 突发情况处理机制是一种可以绕过策略以允许工程师快速解决故障的机制。请参阅[“突发情况处理”](ch05.html#breakglass)。
- en: ^([11](ch08.html#ch08fn11-marker)) Systems using ACLs must fail closed (secure),
    with access explicitly granted by ACL entries.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch08.html#ch08fn11-marker)) 使用ACL的系统必须失败关闭（安全），只有ACL条目明确授予访问权限。
- en: ^([12](ch08.html#ch08fn12-marker)) See [Chapter 3 in the SRE book](https://landing.google.com/sre/sre-book/chapters/embracing-risk/).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch08.html#ch08fn12-marker)) 请参阅[SRE书籍第3章](https://landing.google.com/sre/sre-book/chapters/embracing-risk/)。
- en: ^([13](ch08.html#ch08fn13-marker)) This differs from unit tests, integration
    tests, and load tests, which are covered in [Chapter 13](ch13.html#onethree_testing_code).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch08.html#ch08fn13-marker)) 这与单元测试、集成测试和负载测试不同，这些内容在[第13章](ch13.html#onethree_testing_code)中有介绍。
- en: ^([14](ch08.html#ch08fn14-marker)) This is similar to what the Unix command
    `tee` does for `stdin`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch08.html#ch08fn14-marker)) 这类似于Unix命令`tee`对`stdin`的操作。

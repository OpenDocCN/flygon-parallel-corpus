- en: Chapter 20\. Understanding Roles and Responsibilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By Heather Adkins, Cyrus Vesuna, Hunter King, Felix Gröbert, and David Challoner
  prefs: []
  type: TYPE_NORMAL
- en: with Susanne Landers, Steven Roddis, Sergey Simakov, Shylaja Nukala, Janet Vong,
    Douglas Colish, Betsy Beyer, and Paul Blankinship
  prefs: []
  type: TYPE_NORMAL
- en: 'As this book emphasizes many times, building systems is a *process*, and the
    processes for improving security and reliability rely on people. This means that
    building secure and reliable systems involves tackling two important questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Who is responsible for security and reliability in the organization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are security and reliability efforts integrated into the organization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer to these questions is highly dependent on your organization’s objectives
    and culture (the topic of the next chapter). The following sections lay out some
    high-level guidance for how to think about these questions, and offer insight
    into how Google has approached them over time.
  prefs: []
  type: TYPE_NORMAL
- en: Who Is Responsible for Security and Reliability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Who works on security and reliability in a given organization? We believe that
    security and reliability should be integrated into the lifecycle of systems; therefore,
    they’re everyone’s responsibility. We’d like to challenge the myth that organizations
    should place the burden for these concerns solely on dedicated experts.
  prefs: []
  type: TYPE_NORMAL
- en: If reliability and security are delegated to an isolated team of people who
    can’t mandate that other teams make security-related changes, the same failures
    will happen repeatedly. Their task may start to feel Sisyphean—repetitive and
    unproductive.
  prefs: []
  type: TYPE_NORMAL
- en: 'We encourage organizations to make reliability and security the responsibility
    of *everyone*: developers, SREs, security engineers, test engineers, tech leads,
    managers, project managers, tech writers, executives, and so on. That way, the
    nonfunctional requirements described in [Chapter 4](ch04.html#design_tradeoffs)
    become a focus for the whole organization throughout a system’s entire lifecycle.'
  prefs: []
  type: TYPE_NORMAL
- en: The Roles of Specialists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If everyone is responsible for security and reliability, then you might wonder:
    what exactly is the role of a security specialist or reliability expert? According
    to one school of thought, the engineers building a given system should primarily
    focus on its core functionality. For example, developers might focus on building
    a set of critical user journeys for a mobile phone–based app. Complementing the
    work of the developer team, a security-focused engineer will look at the app from
    the perspective of an attacker aiming to undermine its safety. A reliability-focused
    engineer can help understand the dependency chains and, based on these, identify
    what metrics should be measured that will lead to happy customers and an SLA-compliant
    system. This division of labor is common in many development environments, but
    it’s important that these types of roles work together rather than in isolation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To expand on this idea further, depending on the complexity of a system, an
    organization may need people with specialized experience to make nuanced judgment
    calls. Since it’s not possible to build an absolutely secure system that’s resilient
    against every attack, or a system that is perfectly reliable, advice from experts
    can help steer development teams. Ideally, this guidance should be integrated
    into the development lifecycle. This integration can take multiple forms, and
    security and reliability specialists should work directly with developers or other
    specialists that consult at each stage of the lifecycle to improve systems.^([1](ch20.html#ch20fn1))
    For example, security consultation can happen in multiple stages:'
  prefs: []
  type: TYPE_NORMAL
- en: A *security design review* at the outset of a project to determine how security
    is integrated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ongoing security audits* to make sure a product is built correctly per security
    specifications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Testing* to see what vulnerabilities an independent person can find'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Security experts should be responsible for implementing security-specific technologies
    that require specialist knowledge. Cryptography is the canonical example: “don’t
    roll your own crypto” is a common industry catchphrase meant to discourage enterprising
    developers from implementing their own solutions. Cryptography implementations,
    whether in libraries or hardware, should be left to experts. If your organization
    needs to provide secure services (such as a web service over HTTPS), use industry-accepted
    and verified solutions instead of attempting to write your own encryption algorithm.
    Specialist security knowledge can also be required to implement other types of
    highly complex security infrastructure, such as custom authentication, authorization,
    and auditing (AAA) systems, or new secure frameworks to prevent common security
    vulnerabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Reliability engineers (such as SREs) are best positioned to develop centralized
    infrastructure and organization-wide automation. [Chapter 7 of the SRE book](https://landing.google.com/sre/sre-book/chapters/automation-at-google/)
    discusses the value and evolution of horizontal solutions, and shows how critical
    software that enables product development and launches can evolve into a platform.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, specialists in security and reliability can devise best practices,
    policies, and training tailored to your organization’s workflows. These tools
    should empower developers to adopt best practices and implement effective security
    and reliability practices. A specialist should aim to build a brain trust of knowledge
    for the organization by constantly educating themselves on developments in the
    industry and generating broader awareness (see [“Culture of Awareness”](ch21.html#culture_of_awareness)).
    In creating awareness, a specialist can help the organization become more secure
    and reliable in an iterative way. For example, Google has SRE- and security-focused
    educational programs that provide a baseline level of knowledge to all new hires
    in these specific roles. In addition to making the course material available company-wide,
    we also offer employees many self-study courses on these topics.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Security Expertise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Anyone who has tried to hire security professionals into their organization
    knows that the task can be challenging. If you’re not a security specialist yourself,
    what should you look for when hiring one? Medical professionals provide a good
    analogy: most have a general understanding of the fundamentals of human health,
    but many specialize at some point. In the medical field, family doctors or general
    practitioners are typically responsible for primary care, but more serious conditions
    may call for a specialist in neurology, cardiology, or some other area. Similarly,
    all security professionals tend to command a general body of knowledge, but they
    also tend to specialize in a few specific areas.'
  prefs: []
  type: TYPE_NORMAL
- en: Before you hire a security specialist, it’s important to know the types of skills
    your organization will need. If your organization is small—for example, if you’re
    a startup or an open source project—a generalist may cover many of your needs.
    As your organization grows and matures, its security challenges may become more
    complex and require increased specialization. [Table 20-1](#security_expertise_needed_at_key_milest)
    presents some key milestones in Google’s early history that needed corresponding
    security expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Table 20-1\. Security expertise needed at key milestones in Google’s history
  prefs: []
  type: TYPE_NORMAL
- en: '| Company milestones | Expertise needed | Security challenges |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Google Search (1998) *Google Search provides users with the ability to find
    publicly available information.* | General | Search query log data protection
    Denial-of-service protection'
  prefs: []
  type: TYPE_NORMAL
- en: Network and system security |
  prefs: []
  type: TYPE_NORMAL
- en: '| Google AdWords (2000) *Google AdWords (Google Ads) enables advertisers to
    show ads on Google Search and other products.* | General Data security'
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs: []
  type: TYPE_NORMAL
- en: Systems security
  prefs: []
  type: TYPE_NORMAL
- en: Application security
  prefs: []
  type: TYPE_NORMAL
- en: Compliance and audit
  prefs: []
  type: TYPE_NORMAL
- en: Anti-fraud
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs: []
  type: TYPE_NORMAL
- en: Denial of service
  prefs: []
  type: TYPE_NORMAL
- en: Insider risk | Financial data protection Regulatory compliance
  prefs: []
  type: TYPE_NORMAL
- en: Complex web applications
  prefs: []
  type: TYPE_NORMAL
- en: Identity
  prefs: []
  type: TYPE_NORMAL
- en: Account abuse
  prefs: []
  type: TYPE_NORMAL
- en: Fraud and insider abuse |
  prefs: []
  type: TYPE_NORMAL
- en: '| Blogger (2003) *Blogger is a platform that allows users to host their own
    web pages.* | General Data security'
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs: []
  type: TYPE_NORMAL
- en: Systems security
  prefs: []
  type: TYPE_NORMAL
- en: Application security
  prefs: []
  type: TYPE_NORMAL
- en: Content abuse
  prefs: []
  type: TYPE_NORMAL
- en: Denial of service | Denial of service Platform abuse
  prefs: []
  type: TYPE_NORMAL
- en: Complex web applications |
  prefs: []
  type: TYPE_NORMAL
- en: '| Google Mail (Gmail) (2004) *Gmail is Google’s free webmail system, with advanced
    features available via a paid GSuite account.* | General Privacy'
  prefs: []
  type: TYPE_NORMAL
- en: Data security
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs: []
  type: TYPE_NORMAL
- en: Systems security
  prefs: []
  type: TYPE_NORMAL
- en: Application security
  prefs: []
  type: TYPE_NORMAL
- en: Cryptography
  prefs: []
  type: TYPE_NORMAL
- en: Anti-spam
  prefs: []
  type: TYPE_NORMAL
- en: Anti-abuse
  prefs: []
  type: TYPE_NORMAL
- en: Incident response
  prefs: []
  type: TYPE_NORMAL
- en: Insider risk
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise security | Protecting highly sensitive user content at rest and in
    transit Threat models involving highly capable external attackers
  prefs: []
  type: TYPE_NORMAL
- en: Complex web applications
  prefs: []
  type: TYPE_NORMAL
- en: Identity systems
  prefs: []
  type: TYPE_NORMAL
- en: Account abuse
  prefs: []
  type: TYPE_NORMAL
- en: Email spam and abuse
  prefs: []
  type: TYPE_NORMAL
- en: Denial of service
  prefs: []
  type: TYPE_NORMAL
- en: Insider abuse
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise needs |
  prefs: []
  type: TYPE_NORMAL
- en: Certifications and Academia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some security experts seek to earn certifications in their field of interest.
    Security-focused industry certifications are offered by institutions worldwide,
    and can be good indicators of someone’s interest in developing relevant skills
    for their career and their ability to learn key concepts. These certifications
    typically involve a standardized knowledge-based test. Some certifications require
    a minimum amount of classroom, conference, or job experience. Nearly all expire
    after a certain amount of time, or require certificants to refresh minimum requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'These standardized testing mechanisms may not necessarily attest to someone’s
    aptitude for success in a security role at your organization, so we recommend
    taking a balanced approach to assessing security specialists, considering all
    of their qualifications in totality: their practical experience, certifications,
    and personal interest. While certifications may speak to someone’s ability to
    pass exams, we have seen credentialed professionals who’ve had difficulty applying
    their knowledge to solving problems. At the same time, early career candidates,
    or those coming to the field from other specialist roles, may use certifications
    to upgrade their knowledge quickly. With a keen interest in the field, or practical
    experience with open source projects (instead of workplace experience), such early
    career candidates may be able to add value quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Because security experts are increasingly in demand, many industries and universities
    have been developing and evolving security-focused academic programs. Some institutions
    offer general security-focused degrees that cover many security domains. Other
    degree programs concentrate on a specific security domain (which is common for
    doctoral students), and some offer a blended curriculum that focuses on the overlap
    between cybersecurity issues and domains such as public policy, law, and privacy.
    As with certifications, we recommend considering a candidate’s academic achievements
    in the context of their practical experience and your organization’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might want to bring on an experienced professional as your
    first security hire, and then hire early career talent once the team is established
    and can offer mentorship. Alternatively, if your organization is working on a
    niche technical problem (such as securing self-driving cars), a new PhD graduate
    with deep knowledge in that specific research area but little work experience
    might fit the role nicely.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Security into the Organization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Knowing when to start working on security is more of an art than a science.
    Opinions on this topic are plentiful and varied. However, it’s generally safe
    to say that the sooner you start thinking about security, the better off you’ll
    be. In more concrete terms, we’ve observed certain conditions over the years that
    are likely to trigger organizations (including our own) to start building a security
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: When an organization begins to handle data of a personal nature, such as logs
    of sensitive user activity, financial information, health records, or email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an organization needs to build highly secure environments or custom technologies,
    such as custom security features in a web browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When regulations require adherence to a standard (such as Sarbanes-Oxley, PCI
    DSS, or GDPR) or a related audit^([2](ch20.html#ch20fn2))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an organization has contractual requirements with customers, especially
    around breach notification or minimum security standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During or after a compromise or data breach (ideally, before)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a reaction to the compromise of a peer operating in the same industry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, you’ll want to start working on security far before any of these
    conditions are met, and especially before a data breach! It’s far simpler to implement
    security before, rather than after, such an event. For example, if your company
    plans to launch a new product that accepts online payments, you may want to consider
    a specialty vendor for that functionality. Vetting a vendor and ensuring that
    they have good data handling practices will take time.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you launch with a vendor that doesn’t integrate the online payment
    system securely. A data breach could incur regulatory fines, loss of customer
    trust, and a hit to productivity as your engineers reimplement the system correctly.
    Many organizations cease to exist after such incidents.^([3](ch20.html#ch20fn3))
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, imagine that your company is signing a new contract with a partner
    that has additional data handling requirements. Hypothetically, your legal team
    may advise you to implement those requirements before signing the contract. What
    might happen if you delay those extra protections and suffer a breach as a result?
  prefs: []
  type: TYPE_NORMAL
- en: 'Related questions often arise when considering the cost of a security program
    and the resources your company can invest in the program: how expensive is implementing
    security, and can the company afford it? While this chapter can’t cover this very
    complex topic deeply, we’ll emphasize two main takeaways.'
  prefs: []
  type: TYPE_NORMAL
- en: First off, for security to be effective, it must be carefully balanced with
    your organization’s other requirements. To put this guideline in perspective,
    we can make Google nearly 100% safe from malicious actors by turning off our datacenters,
    networks, computing devices, and so on. While doing so would achieve a high level
    of safety, Google would no longer have customers and would disappear into the
    annals of failed companies. Availability is a core tenet of security! In order
    to craft a reasonable security strategy, you need to understand what the business
    requires to operate (and in the case of most companies, what it takes to earn
    a profit). Find the right balance between the requirements of your business and
    adequate security controls.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, security is everyone’s responsibility. You can reduce the cost of
    some security processes by distributing it among the teams affected the most.
    For example, consider a company that has six products, each staffed with a product
    team and protected by 20 firewalls. In this scenario, one common approach is to
    have a central security team maintain the configuration of all 120 firewalls.
    This setup requires the security team to have extensive knowledge of six different
    products—a recipe for eventual reliability issues or delays in system changes,
    all of which can increase the cost of your security program. An alternative approach
    is to assign responsibility to the security team for operating an automated configuration
    system that accepts, validates, approves, and pushes firewall changes proposed
    by the six product teams. This way, each product team can efficiently propose
    minor changes for review and scale the configuration process. These kinds of optimizations
    can save time and even improve system reliability by catching errors early without
    human involvement.
  prefs: []
  type: TYPE_NORMAL
- en: Because security is such an integral part of an organization’s lifecycle, nontechnical
    areas of the organization also need to consider security early on. For example,
    boards of directors often examine the security and privacy practices of the entities
    they oversee. Lawsuits in the aftermath of data breaches, such as the shareholder
    suit against Yahoo! in 2017, are driving this trend.^([4](ch20.html#ch20fn4))
    When preparing your roadmap for security, be sure to consider these types of stakeholders
    in your process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s important to create processes for maintaining a constant understanding
    of the current issues you need to address, along with their priorities. When treated
    as a continuous process, security requires an ongoing assessment of the risks
    the business is facing. In order to iterate defense-in-depth security controls
    over time, you need to incorporate risk assessment into your software development
    lifecycle and security practices. The next section discusses some practical strategies
    for doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding Security Specialists and Security Teams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the years, we’ve seen many companies experiment with where to embed security
    specialists and security teams inside their organizations. The configurations
    have ranged from fully embedded security specialists inside product teams (see
    [Chapter 19](ch19.html#onenine_case_study_chrome_security_team)) to fully centralized
    security teams. Google’s central security team is organizationally configured
    as a hybrid of both options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many companies also have different accountability arrangements for decision
    making. We’ve seen Chief Information Security Officers (CISOs) and other leadership
    roles responsible for security report to just about every C-level executive: the
    CEO, CFO, CIO, or COO, the general counsel for the company, a VP of Engineering,
    and even the CSO (Chief Security Officer, usually responsible also for physical
    security). There is no right or wrong configuration, and the choice your organization
    makes will be highly dependent on what’s most effective for your security efforts.'
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this section offers some details on configuration options that we’ve
    had success with over the years. While we’re a big technology company, many of
    these components also work well in small or medium-sized organizations. However,
    we imagine this configuration may not work well for a financial company or a public
    utility, where accountability for security may have different stakeholders and
    drivers. Your mileage may vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Embedding Security at Google'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At Google, we first built out a central security organization that operates
    as a peer to product engineering. The head of this organization is a senior leader
    within engineering (a VP). This creates a reporting structure in which security
    is seen as an engineering ally, but also allows the security team sufficient independence
    to raise issues and resolve disputes without conflicts of interest other leaders
    may have. This is similar to the way the SRE organization at Google maintains
    separate reporting chains from product development teams.^([5](ch20.html#ch20fn5))
    In this way, we create an open and transparent engagement model that focuses on
    improvements. Otherwise, you risk having a team with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Unable to raise serious issues because launches are overprioritized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seen as a blocking gate that needs to be circumvented organizationally via silent
    launches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slowed down by insufficient documentation or code access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google’s central security team relies on standard processes like ticketing systems
    to interact with the rest of the organization when teams need to request a design
    review, an access control change, and so on. For a sense of how this workflow
    functions, see [“Google’s Smart System for Intake”](#googleapostrophes_smart_system_for_inta).
  prefs: []
  type: TYPE_NORMAL
- en: As Google has grown, it has also become useful to embed a “security champion”
    within individual product engineering peer groups. The security champion becomes
    the gateway to facilitate collaboration between the central security team and
    the product team. When starting out, this role is ideal for senior engineers with
    good standing in the organization and an interest or a background in security.
    These engineers also become the technical leads for product security initiatives.
    As product teams become more complex, this role is assigned to a senior decider,
    such as a director or VP—this person can make tough calls (such as balancing launches
    versus security fixes), acquire resources, and resolve conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: In the security champion model, it’s important to establish and agree upon an
    engagement process and responsibilities. For example, the central team may continue
    to perform design reviews and audits, set organization-wide security policies
    and standards, build safe application frameworks (see [Chapter 13](ch13.html#onethree_testing_code)),
    and devise common infrastructure such as least privilege methods (see [Chapter 5](ch05.html#design_for_least_privilege)).
    Distributed security champions are key stakeholders for these activities, and
    should help decide how these controls will work in their product teams. The security
    champions also drive the implementation of policies, frameworks, infrastructure,
    and methods within their respective product teams. This organizational configuration
    requires a tight communication loop through team charters, cross-functional meetings,
    mailing lists, chat channels, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Because of Google and Alphabet’s large size, in addition to a central security
    team and distributed security champions, we also have special decentralized security
    teams for more complex products. For example, the Android security team sits within
    the Android engineering organization. Chrome has a similar model (see [Chapter 19](ch19.html#onenine_case_study_chrome_security_team)).
    This means the Android and Chrome security teams are responsible for the end-to-end
    security of their respective products, which includes deciding on product-specific
    standards, frameworks, infrastructure, and methods. These specialized security
    teams run the product security review process and have special programs to harden
    the products. For example, the Android security team has worked to [harden the
    media stack](https://oreil.ly/rME44) and has benefited from an integrated security
    and engineering approach.
  prefs: []
  type: TYPE_NORMAL
- en: In all of these models, it’s important for the security team to be open and
    approachable. In addition to a security review process, during which developers
    can receive help from subject matter experts, engineers need timely and consistent
    feedback on security-related issues throughout the project lifecycle. We address
    a number of cultural issues around these interactions in [Chapter 21](ch21.html#twoone_building_a_culture_of_security_a).
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Teams: Blue and Red Teams'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security teams are often tagged using colors to denote their role in securing
    an organization.^([6](ch20.html#ch20fn6)) All of these color-coded teams work
    toward the common goal of improving the security posture of the company.
  prefs: []
  type: TYPE_NORMAL
- en: '*Blue Teams* are primarily responsible for assessing and hardening software
    and infrastructure. They’re also responsible for detection, containment, and recovery
    in the event of a successful attack. Blue Team members can be anyone in an organization
    who works on defending it, including the people who build secure and reliable
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Red Teams* run offensive security exercises: end-to-end attacks that simulate
    realistic adversaries. These exercises reveal weaknesses in an organization’s
    defenses and test its ability to detect and defend against attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, Red Teams focus on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A specific goal
  prefs: []
  type: TYPE_NORMAL
- en: For example, a Red Team might seek to exploit customer account data (or more
    specifically, to find and exfiltrate to a safe destination some customer account
    data that is available in your environment). Such exercises are very similar to
    the way adversaries operate.
  prefs: []
  type: TYPE_NORMAL
- en: Surveillance
  prefs: []
  type: TYPE_NORMAL
- en: The aim is to determine whether your detection methods can detect reconnaissance
    by an adversary. Surveillance can also serve as a map for future goal-based engagements.
  prefs: []
  type: TYPE_NORMAL
- en: Targeted attacks
  prefs: []
  type: TYPE_NORMAL
- en: The aim is to demonstrate the feasibility of exploiting security issues that
    are supposedly theoretical and very unlikely to be exploited. As a result, you
    can determine which issues merit building a defense.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting a Red Team program, be sure to obtain buy-in from parts of the
    organization that might be affected by these exercises, including legal and executives.
    This is also a good time to define boundaries—for example, Red Teams should not
    access customer data or disrupt production services, and they should use approximations
    for data theft and service outages (e.g., by compromising only the data of test
    accounts). These boundaries need to strike a balance between conducting a realistic
    exercise and establishing a timing and scope that your partner teams are comfortable
    with. Of course, your adversaries won’t respect these boundaries, so Red Teams
    should pay extra attention to key areas that are not well protected.
  prefs: []
  type: TYPE_NORMAL
- en: Some Red Teams share their attack plans with the Blue Team, and work very closely
    with them to get fast and comprehensive insight into the detection situation.
    This relationship can even be formalized with a Purple Team that bridges the two.^([7](ch20.html#ch20fn7))
    This can be useful if you are conducting many exercises and want to move fast,
    or if you want to distribute Red Team activity among product engineers. This configuration
    can also inspire the Red Team to look in places it might not otherwise consider.
    The engineers that design, implement, and maintain systems know the system best,
    and usually have an instinct for where the weaknesses are.
  prefs: []
  type: TYPE_NORMAL
- en: Red Teams are not vulnerability scanning or penetration testing teams. *Vulnerability
    scanning teams* look for predictable and known weaknesses in software and configurations
    that can be automatically scanned for. *Penetration testing teams* focus more
    on finding a large set of vulnerabilities and the testers trying to exploit them.
    Their scope is narrower, focused on a particular product, infrastructure component,
    or process. As these teams mostly test prevention aspects and some detection aspects
    of an organization’s security defense, their typical engagement lasts days.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, Red Team engagements are goal-oriented and typically last weeks.
    Their goals are specific targets, such as intellectual property or customer data
    exfiltration. They are broadly scoped and use any means necessary to attain their
    goals (within safety limits) by traversing product, infrastructure, and internal/external
    boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Given time, good Red Teams can attain their goals, often without being detected.
    Rather than viewing a successful Red Team attack as a judgment of a poor or ineffective
    business unit, use this information to better understand some of your more complex
    systems in a blameless way.^([8](ch20.html#ch20fn8)) Use Red Team exercises as
    an opportunity to better learn how these systems are interconnected and how they
    share trust boundaries. Red Teams are designed to help bolster threat models and
    build defenses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because they don’t exactly mirror the behavior of external attackers, Red Team
    attacks aren’t a perfect test of your detection and response capabilities. This
    is especially true if the Red Team is staffed by internal engineers who have existing
    knowledge about the systems they’re attempting to penetrate.
  prefs: []
  type: TYPE_NORMAL
- en: You also can’t feasibly conduct Red Team attacks frequently enough to provide
    a real-time view of your vulnerability to attacks or statistically significant
    metrics for your detection and response teams. Red Teams are meant to find the
    rare edge cases that normal testing cannot. All caveats aside, regularly conducting
    Red Team exercises is a good way to understand your security posture end to end.
  prefs: []
  type: TYPE_NORMAL
- en: You can also leverage Red Teams to teach the people who design, implement, and
    maintain systems about the adversarial mindset. Embedding these people directly
    into the attack team—for example, via a small-scoped project—will give them firsthand
    insight into how attackers scrutinize a system for possible vulnerabilities and
    work around defenses. They can inject this knowledge into their team’s development
    process later on.
  prefs: []
  type: TYPE_NORMAL
- en: Engaging with a Red Team helps you better understand your organization’s security
    posture and develop a roadmap for implementing meaningful risk reduction projects.
    By understanding the implications of your current risk tolerance, you can determine
    whether you need to make adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: External Researchers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to examine and improve your security posture is to work closely
    with outside researchers and enthusiasts who find vulnerabilities in your systems.
    As we mentioned in [Chapter 2](ch02.html#understanding_adversaries), this can
    be a useful way to get feedback about your systems.
  prefs: []
  type: TYPE_NORMAL
- en: Many companies work with outside researchers by establishing *Vulnerability
    Reward Programs (VRPs)*, also colloquially referred to as *bug bounty programs*.
    These programs offer rewards in exchange for responsibly disclosing vulnerabilities
    about your system, which may or may not come in cash form.^([9](ch20.html#ch20fn9))
    Google’s first VRP, started in 2006, offered a T-shirt and a simple thank you
    message on our public-facing web page. Through reward programs, you can expand
    the hunt for security-related bugs outside of your immediate organization and
    engage with a larger number of security researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting a VRP, it’s a good idea to first cover the basics of finding
    and addressing regular security issues that thorough reviews and basic vulnerability
    scanning can find. Otherwise, you end up paying external people to find bugs that
    your own teams could have easily detected. This is not the intended purpose of
    VRPs. It also has the downside that [more than one researcher](https://oreil.ly/6qBkN)
    may report the same issue to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing how to set up a bug bounty program requires a little bit of legwork
    up front. If you choose to run a bug bounty program, you can follow these basic
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine whether your organization is ready for this program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scope the areas of your system to target. For example, you might not be able
    to target corporate systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine payout levels and set aside funds for payouts.^([10](ch20.html#ch20fn10))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider whether you want to run an in-house bug bounty program or hire an organization
    that specializes in these programs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If running your own, set up a process for bug intake, triage, investigation,
    validation, follow-up, and fixes. In our experience, you can estimate this process
    to take approximately 40 hours for each serious issue, excluding fixes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a process for making payments. Remember that reporters may be located
    all over the world, not just in your home country. You will need to work with
    your legal and finance teams to understand any constraints that may exist on your
    organization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch, learn, and iterate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Every bug bounty program faces some likely challenges, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The need to fine-tune the firehose of issues being reported
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your industry reputation, the attack surface, payout amounts, and
    the ease of finding bugs, you may be fielding an overwhelming number of reports.
    Understand up front what level of response will be required from your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Poor report quality
  prefs: []
  type: TYPE_NORMAL
- en: A bug bounty program can become burdensome if most of your engineers are chasing
    down basic issues or nonissues. We have found this is especially true for web
    services, since many users have misconfigured browsers and “find” bugs that aren’t
    actually bugs. Security researchers are less likely to be in this pool, but sometimes
    it’s hard to discern a bug reporter’s qualifications up front.
  prefs: []
  type: TYPE_NORMAL
- en: Language barriers
  prefs: []
  type: TYPE_NORMAL
- en: A vulnerability researcher may not necessarily report a bug to you in your native
    language. Tools for language translation can be helpful here, or your organization
    may have someone who understands the language used by the reporter.
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerability disclosure guidelines
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules for disclosing vulnerabilities are not generally agreed upon. Should
    the researcher go public with what they know, and if so, when? How long should
    the researcher give your organization to fix the bug? What types of findings will
    be rewarded, and what types won’t? There are many differing opinions about the
    “right” methods to use here. Here are some suggestions for further reading:'
  prefs: []
  type: TYPE_NORMAL
- en: The Google security team has written a [blog post](https://oreil.ly/t_FP9) on
    responsible disclosure.^([11](ch20.html#ch20fn11))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Zero, an internal vulnerability research team at Google, has also written
    a [blog post](https://oreil.ly/oDjT-) on data-driven updates to disclosure policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Oulu University Secure Programming Group provides a useful collection of
    [vulnerability disclosure publications](https://oreil.ly/qILyu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The International Standards Organization (ISO) provides [recommendations](https://oreil.ly/kRc5Z)
    for vulnerability disclosure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be prepared to address issues researchers report to you in a timely manner.
    Also be aware that they may find issues that have been actively exploited by a
    malicious actor for some period of time, in which case you may also have a security
    breach to address.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security and reliability are created by the quality or absence of processes
    and practices. People are the most important drivers of these processes and practices.
    Effective employees are able to collaborate across roles, departments, and cultural
    boundaries. We live in a world where the future is unknowable and our adversaries
    are unpredictable. At the end of the day, ensuring that everyone in your organization
    is responsible for security and reliability is the best defense!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch20.html#ch20fn1-marker)) The development arc described in [Chapter 18
    of the SRE workbook](https://landing.google.com/sre/workbook/chapters/engagement-model/)
    demonstrates the value that an experienced SRE offers throughout the entire product
    lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch20.html#ch20fn2-marker)) The Sarbanes-Oxley Act of 2002 (the Public
    Company Accounting Reform and Investor Protection Act) sets standards for public
    US companies regarding their accounting practices, and includes information security
    topics. The Payment Card Industry Data Security Standard sets minimum guidelines
    around protecting credit card information; compliance is required for anyone doing
    payment processing of this kind. The General Data Protection Regulation is an
    EU regulation concerned with the handling of personal data.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch20.html#ch20fn3-marker)) A few notable cases of organizations that went
    out of business or filed for bankruptcy after breaches are [Code Spaces](https://oreil.ly/Oj9ng)
    and the [American Medical Collection Agency](https://oreil.ly/BcR9C).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch20.html#ch20fn4-marker)) In the US, executives and boards of directors
    are increasingly being held accountable for security in their organizations. The
    Concord Law School at Purdue University has written a good [article](https://oreil.ly/w17Yn)
    on this trend.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch20.html#ch20fn5-marker)) See [Chapter 31 in the SRE book](https://landing.google.com/sre/sre-book/chapters/communication-and-collaboration/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch20.html#ch20fn6-marker)) This color scheme is derived from [the US military](https://oreil.ly/nuNjO).
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch20.html#ch20fn7-marker)) For more on Purple Teams, see Brotherston,
    Lee, and Amanda Berlin. 2017\. [*Defensive Security Handbook: Best Practices for
    Securing Infrastructure*](http://shop.oreilly.com/product/0636920051671.do). Sebastopol,
    CA: O’Reilly Media.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch20.html#ch20fn8-marker)) Do so by building on a culture of blameless
    postmortems, as described in [Chapter 15 of the SRE book](https://landing.google.com/sre/sre-book/chapters/postmortem-culture/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch20.html#ch20fn9-marker)) See Google researcher sirdarckcat’s [blog post
    on rewards](https://oreil.ly/Lgtv3) for a more philosophical outlook.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch20.html#ch20fn10-marker)) For further reading, see sirdarckcat’s post
    about [vulnerability pricing](https://oreil.ly/11Q2P).
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch20.html#ch20fn11-marker)) sirdarckcat has also written a post about
    [vulnerability disclosure](https://oreil.ly/iQn3z).
  prefs: []
  type: TYPE_NORMAL

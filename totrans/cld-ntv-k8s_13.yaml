- en: '*Chapter 10*: Troubleshooting Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：排除故障的Kubernetes'
- en: This chapter reviews the best-practice methods for effectively troubleshooting
    Kubernetes clusters and the applications that run on them. This includes a discussion
    of common Kubernetes issues, as well as how to debug the masters and workers separately.
    The common Kubernetes issues will be discussed and taught in a case study format,
    split into cluster issues and application issues.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将审查有效排除Kubernetes集群和运行在其中的应用程序的最佳实践方法。这包括讨论常见的Kubernetes问题，以及如何分别调试主节点和工作节点。常见的Kubernetes问题将以案例研究的形式进行讨论和教学，分为集群问题和应用程序问题。
- en: We will start with a discussion of some common Kubernetes failure modes, before
    moving on to how to best troubleshoot clusters and applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论一些常见的Kubernetes故障模式，然后再讨论如何最好地排除集群和应用程序的故障。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding failure modes for distributed applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布式应用的故障模式
- en: Troubleshooting Kubernetes clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除故障的Kubernetes集群
- en: Troubleshooting applications on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上排除故障
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持`kubectl`命令行工具的计算机，以及一个正常运行的Kubernetes集群。请参阅[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解快速启动和运行Kubernetes的几种方法，以及如何安装`kubectl`工具的说明。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在书籍的GitHub存储库中找到，网址为[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10)。
- en: Understanding failure modes for distributed applications
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布式应用的故障模式
- en: Kubernetes components (and applications running on Kubernetes) are distributed
    by default if they run more than one replica. This can result in some interesting
    failure modes, which can be hard to debug.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes组件（以及在Kubernetes上运行的应用程序）是分布式的，如果它们运行多个副本。这可能导致一些有趣的故障模式，这些故障模式可能很难调试。
- en: For this reason, applications on Kubernetes are less prone to failure if they
    are stateless – in which case, the state is offloaded to a cache or database running
    outside of Kubernetes. Kubernetes primitives such as StatefulSets and PersistentVolumes
    can make it much easier to run stateful applications on Kubernetes – and with
    every release, the experience of running stateful applications on Kubernetes improves.
    Still, deciding to run fully stateful applications on Kubernetes introduces complexity
    and therefore the potential for failure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果应用程序是无状态的，它们在Kubernetes上就不太容易失败-在这种情况下，状态被卸载到在Kubernetes之外运行的缓存或数据库中。Kubernetes的原语，如StatefulSets和PersistentVolumes，可以使在Kubernetes上运行有状态的应用程序变得更加容易-并且随着每个版本的发布，在Kubernetes上运行有状态的应用程序的体验也在不断改善。然而，决定在Kubernetes上运行完全有状态的应用程序会引入复杂性，因此也会增加失败的可能性。
- en: Failure in distributed applications can be introduced by many different factors.
    Things as simple as network reliability and bandwidth constraints can cause major
    issues. These are so varied that *Peter Deutsch* at *Sun Microsystems* helped
    pen the *Fallacies of distributed computing* (along with *James Gosling*, who
    added the 8th point), which are commonly agreed-upon factors for failures in distributed
    applications. In the paper *Fallacies of distributed computing explained*, *Arnon
    Rotem-Gal-Oz* discusses the source of these fallacies ([https://www.rgoarchitects.com/Files/fallacies.pdf](https://www.rgoarchitects.com/Files/fallacies.pdf)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式应用程序的故障可能由许多不同的因素引起。诸如网络可靠性和带宽限制等简单事物可能会导致重大问题。这些问题如此多样化，以至于*Sun Microsystems*的*Peter
    Deutsch*帮助撰写了*分布式计算的谬论*（连同*James Gosling*一起添加了第八点），这些谬论是关于分布式应用程序失败的共识因素。在论文*解释分布式计算的谬论*中，*Arnon
    Rotem-Gal-Oz*讨论了这些谬论的来源。
- en: 'The fallacies are as follows, in numerical order:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些谬论按照数字顺序如下：
- en: The network is reliable.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络是可靠的。
- en: Latency is zero.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 延迟为零。
- en: Bandwidth is infinite.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带宽是无限的。
- en: The network is secure.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络是安全的。
- en: The topology doesn't change.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拓扑结构不会改变。
- en: There is one administrator.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有一个管理员。
- en: Transport cost is zero.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传输成本为零。
- en: The network is homogeneous.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络是同质的。
- en: Kubernetes has been engineered and developed with these fallacies in mind and
    is therefore more tolerant. It also helps address these issues for applications
    running on Kubernetes – but not perfectly. It is therefore very possible that
    your applications, when containerized and running on Kubernetes, will exhibit
    problems when faced with any of these issues. Each fallacy, when assumed to be
    untrue and taken to its logical conclusion, can introduce failure modes in distributed
    applications. Let's go through each of the fallacies as applied to Kubernetes
    and applications running on Kubernetes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在设计和开发时考虑了这些谬论，因此更具有容忍性。它还有助于解决在Kubernetes上运行的应用程序的这些问题-但并非完美。因此，当您的应用程序在Kubernetes上进行容器化并运行时，很可能会在面对这些问题时出现问题。每个谬论，当假设为不真实并推导到其逻辑结论时，都可能在分布式应用程序中引入故障模式。让我们逐个讨论Kubernetes和在Kubernetes上运行的应用程序的每个谬论。
- en: The network is reliable
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络是可靠的。
- en: Applications running on multiple logical machines must communicate over the
    internet – so any reliability problems in the network can introduce issues. On
    Kubernetes specifically, the control plane itself can be distributed in a highly
    available setup (which means a setup with multiple master Nodes – see [*Chapter
    1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating with Kubernetes*),
    which means that failure modes can be introduced at the controller level. If the
    network is unreliable, then kubelets may not be able to communicate with the control
    plane, leading to Pod placement issues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个逻辑机器上运行的应用程序必须通过互联网进行通信-因此，网络中的任何可靠性问题都可能引入问题。特别是在Kubernetes上，控制平面本身可以在高度可用的设置中进行分布（这意味着具有多个主节点的设置-请参见[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*），这意味着故障模式可能会在控制器级别引入。如果网络不可靠，那么kubelet可能无法与控制平面进行通信，从而导致Pod放置问题。
- en: Similarly, the Nodes of the control plane may not be able to communicate with
    each other – though `etcd` is of course built with a consensus protocol that can
    tolerate communication failures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，控制平面的节点可能无法彼此通信-尽管`etcd`当然是使用一致性协议构建的，可以容忍通信故障。
- en: Finally, the worker Nodes may not be able to communicate with each other – which,
    in a microservices scenario, could cause problems depending on Pod placement.
    In some cases, the workers may all be able to communicate with the control plane
    while still not being able to communicate with each other, which can cause issues
    with the Kubernetes overlay network.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，工作节点可能无法彼此通信-在微服务场景中，这可能会根据Pod的放置而引起问题。在某些情况下，工作节点可能都能够与控制平面通信，但仍然无法彼此通信，这可能会导致Kubernetes叠加网络出现问题。
- en: As with general unreliability, latency can also cause many of the same problems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与一般的不可靠性一样，延迟也可能引起许多相同的问题。
- en: Latency is zero
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟是零
- en: If network latency is significant, many of the same failures as with network
    unreliability will also apply. For instance, calls between kubelets and the control
    plane may fail, leading to periods of inaccuracy in `etcd` because the control
    plane may not be able to contact the kubelets – or properly update `etcd`. Similarly,
    requests could be lost between applications running on worker Nodes that would
    otherwise work perfectly if the applications were collocated on the same Node.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络延迟显着，许多与网络不可靠性相同的故障也会适用。例如，kubelet和控制平面之间的调用可能会失败，导致`etcd`中出现不准确的时期，因为控制平面可能无法联系kubelet-或者正确更新`etcd`。同样，如果运行在工作节点上的应用程序之间的请求丢失，否则如果这些应用程序在同一节点上共存，则可以完美运行。
- en: Bandwidth is infinite
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带宽是无限的
- en: Bandwidth limitations can expose similar issues as with the previous two fallacies.
    Kubernetes does not currently have a fully supported method to place Pods based
    on bandwidth subscription. This means that Nodes that are hitting their network
    bandwidth limits can still have new Pods scheduled to them, causing increased
    failure rates and latency issues for requests. There have been requests to add
    this as a core Kubernetes scheduling feature (basically, a way to schedule on
    Node bandwidth consumption as with CPU and memory), but for now, the solutions
    are mostly restricted to **Container Network Interface** (**CNI**) plugins.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽限制可能会暴露与前两个谬论类似的问题。Kubernetes目前没有完全支持的方法来基于带宽订阅来放置Pod。这意味着达到网络带宽限制的节点仍然可以安排新的Pod，导致请求的失败率和延迟问题增加。已经有要求将此作为核心Kubernetes调度功能添加的请求（基本上，一种根据节点带宽消耗进行调度的方法，就像CPU和内存一样），但目前，解决方案大多受限于容器网络接口（CNI）插件。
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For instance, the CNI bandwidth plugin supports traffic shaping at the Pod level
    – see [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，CNI带宽插件支持在Pod级别进行流量整形-请参阅[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)。
- en: Third-party Kubernetes networking implementations may also provide additional
    features around bandwidth – and many are compatible with the CNI bandwidth plugin.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方Kubernetes网络实现也可能提供围绕带宽的附加功能-并且许多与CNI带宽插件兼容。
- en: The network is secure
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络是安全的
- en: Network security has effects that reach far beyond Kubernetes – as any insecure
    network is privy to a whole class of attacks. Attackers may be able to gain SSH
    access to the master or worker Nodes in a Kubernetes cluster, which can cause
    significant breaches. Since so much of Kubernetes' magic happens over the network
    rather than in a single machine, access to the network is doubly problematic in
    an attack situation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全的影响远不止于Kubernetes——因为任何不安全的网络都可能遭受各种攻击。攻击者可能能够获得对Kubernetes集群中的主节点或工作节点的SSH访问权限，这可能会造成重大破坏。由于Kubernetes的许多功能都是通过网络而不是在单台机器上完成的，因此在攻击情况下对网络的访问会变得更加棘手。
- en: The topology doesn't change
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拓扑结构不会改变
- en: This fallacy is extra relevant in the context of Kubernetes, since not only
    can the meta network topology change with new Nodes being added and removed –
    the overlay network topology is also altered directly by the Kubernetes control
    plane and CNI.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种谬误在Kubernetes的背景下尤为重要，因为不仅可以通过添加和移除新节点来改变元网络拓扑结构，覆盖网络拓扑结构也会直接受到Kubernetes控制平面和CNI的影响。
- en: For this reason, an application that is running in one logical location at one
    moment may be running in a completely different spot in the network. For this
    reason, the use of Pod IPs to identify logical applications is a bad idea – this
    is one of the purposes of the Service abstraction (see [*Chapter 5*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127),
    *Service and Ingress* – *Communicating with the outside world*). Any application
    concerns that do not assume an indefinite topology (at least concerning IPs) within
    the cluster may have issues. As an example, routing applications to a specific
    Pod IP only works until something happens to that Pod. If that Pod shuts down,
    the Deployment (for instance) controlling it will start a new Pod to replace it,
    but the IP will be completely different. A cluster DNS (and by extension, Services)
    offers a much better way to make requests between applications in a cluster, unless
    your application has the capability to adjust on the fly to cluster changes such
    as Pod placements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个应用程序在某一时刻在一个逻辑位置运行，可能在网络中的完全不同位置运行。因此，使用Pod IP来识别逻辑应用程序是一个不好的主意——这是服务抽象的一个目的（参见[*第5章*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127)，*服务和入口*——*与外部世界通信*）。任何不考虑集群内部拓扑结构（至少涉及IP）的应用程序可能会出现问题。例如，将应用程序路由到特定的Pod
    IP只能在该Pod发生变化之前起作用。如果该Pod关闭，控制它的部署（例如）将启动一个新的Pod来替代它，但IP将完全不同。集群DNS（以及由此衍生的服务）为在集群中的应用程序之间进行请求提供了更好的方式，除非您的应用程序具有动态调整到集群变化（如Pod位置）的能力。
- en: There is only one administrator
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 只有一个管理员
- en: Multiple administrators and conflicting rules can cause issues in the base network,
    and multiple Kubernetes administrators can cause further issues by changing resource
    configurations such as Pod resource limits, leading to unintended behavior. Use
    of Kubernetes **Role-Based Access Control** (**RBAC**) capabilities can help address
    this by giving Kubernetes users only the permissions they need (read-only, for
    instance).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础网络中，多个管理员和冲突的规则可能会导致问题，多个Kubernetes管理员还可能通过更改资源配置（例如Pod资源限制）而引发进一步的问题，导致意外行为。使用Kubernetes的**基于角色的访问控制**（**RBAC**）功能可以通过为Kubernetes用户提供他们所需的权限（例如只读权限）来解决这个问题。
- en: Transport cost is zero
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运输成本为零
- en: There are two common ways this fallacy is interpreted. Firstly, that the latency
    cost of transport is zero – which is obviously untrue, as the speed of data transfer
    over wires is not infinite, and lower-level networking concerns add latency. This
    is essentially identical to the effects stemming from the *latency is zero* fallacy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种谬误有两种常见的解释方式。首先，传输的延迟成本为零 - 这显然是不真实的，因为数据在电线上传输的速度并不是无限的，而且更低级的网络问题会增加延迟。这与“延迟为零”谬误产生的影响本质上是相同的。
- en: Secondly, this statement can be interpreted to mean that the cost of creating
    and operating a network for the purposes of transport is zero – as in zero dollars
    and zero cents. While also being patently untrue (just look at your cloud provider's
    data transfer fees for proof of this), this does not specifically correspond to
    application troubleshooting on Kubernetes, so we will focus on the first interpretation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这个声明可以被解释为创建和操作网络传输的成本为零 - 就像零美元和零美分一样。虽然这也是显然不真实的（只需看看您的云服务提供商的数据传输费用就可以证明），但这并不特别对应于
    Kubernetes 上的应用程序故障排查，所以我们将专注于第一种解释。
- en: The network is homogeneous
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络是同质的
- en: This final fallacy has less to do with Kubernetes' components, and more to do
    with applications running on Kubernetes. However, the fact is that developers
    operating in today's environment are well aware that application networking may
    have different implementations across applications – from HTTP 1 and 2 to protocols
    such as *gRPC*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的谬误与 Kubernetes 的组件关系不大，而与在 Kubernetes 上运行的应用程序有更多关系。然而，事实是，今天的环境中操作的开发人员都清楚地知道，应用程序网络可能在不同的应用程序中有不同的实现
    - 从 HTTP 1 和 2 到诸如 *gRPC* 的协议。
- en: Now that we've reviewed some major reasons for application failure on Kubernetes,
    we can dive into the actual process of troubleshooting both Kubernetes and applications
    that run on Kubernetes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了一些 Kubernetes 应用失败的主要原因，我们可以深入研究排查 Kubernetes 和在 Kubernetes 上运行的应用程序的实际过程。
- en: Troubleshooting Kubernetes clusters
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排查 Kubernetes 集群
- en: Since Kubernetes is a distributed system that has been designed to tolerate
    failure where applications are run, most (but not all) issues tend to be centered
    on the control plane and API. A worker Node failing, in most scenarios, will just
    result in the Pods being rescheduled to another Node – though compounding factors
    can introduce issues.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kubernetes 是一个分布式系统，旨在容忍应用程序运行的故障，大多数（但不是全部）问题往往集中在控制平面和 API 上。在大多数情况下，工作节点的故障只会导致
    Pod 被重新调度到另一个节点 - 尽管复合因素可能会引入问题。
- en: In order to walk through common Kubernetes cluster issue scenarios, we will
    use a case study methodology. This should give you all the tools you need to investigate
    real-world cluster issues. Our first case study is centered on the failure of
    the API server itself.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示常见的 Kubernetes 集群问题场景，我们将使用案例研究方法。这应该为您提供调查真实世界集群问题所需的所有工具。我们的第一个案例研究集中在
    API 服务器本身的故障上。
- en: Important note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the purposes of this tutorial, we will assume a self-managed cluster. Managed
    Kubernetes services such as EKS, AKS, and GKE generally remove some of the failure
    domains (by autoscaling and managing master Nodes, for instance). A good rule
    is to check your managed service documentation first, as any issues may be specific
    to the implementation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将假设一个自管理的集群。托管的 Kubernetes 服务，如 EKS、AKS 和 GKE 通常会消除一些故障域（例如通过自动缩放和管理主节点）。一个好的规则是首先检查您的托管服务文档，因为任何问题可能是特定于实现的。
- en: Case study – Kubernetes Pod placement failure
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究 - Kubernetes Pod 放置失败
- en: 'Let''s set the scene. Your cluster is up and running, but you are experiencing
    a problem with Pod scheduling. Pods stay stuck in the `Pending` state indefinitely.
    Let''s confirm this with the command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来设定场景。您的集群正在运行，但是您遇到了Pod调度的问题。Pods 一直停留在 `Pending` 状态，无限期地。让我们用以下命令确认一下：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the command is the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As we can see, none of our Pods are running. Furthermore, we''re running three
    replicas of the application and none of them are getting scheduled. A great next
    step would be to check the Node state and see if there are any issues there. Run
    the following command to get the output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们的Pod都没有在运行。此外，我们正在运行应用程序的三个副本，但没有一个被调度。下一个很好的步骤是检查节点状态，看看是否有任何问题。运行以下命令以获取输出：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This output gives us some good information – we only have one worker Node, and
    it isn't available for scheduling. When a `get` command doesn't give us enough
    information to go by, `describe` is usually a good next step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出给了我们一些很好的信息 - 我们只有一个工作节点，并且它无法用于调度。当 `get` 命令没有给我们足够的信息时，`describe` 通常是一个很好的下一步。
- en: 'Let''s run `kubectl describe node node-01` and check the `conditions` key.
    We''ve dropped a column in order to fit everything neatly on the page, but the
    most important columns are there:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行 `kubectl describe node node-01` 并检查 `conditions` 键。我们已经删除了一列，以便将所有内容整齐地显示在页面上，但最重要的列都在那里：
- en: '![Figure 10.1 – Describe Node Conditions output](image/B14790_10_001.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 - 描述节点条件输出](image/B14790_10_001.jpg)'
- en: Figure 10.1 – Describe Node Conditions output
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 - 描述节点条件输出
- en: 'What we have here is an interesting split: both `MemoryPressure` and `DiskPressure`
    are fine, while the `OutOfDisk` and `Ready` conditions are unknown with the message
    `kubelet stopped posting node status`. At a first glance this seems nonsensical
    – how can `MemoryPressure` and `DiskPressure` be fine while the kubelet stopped
    working?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里有一个有趣的分裂：`MemoryPressure` 和 `DiskPressure` 都很好，而 `OutOfDisk` 和 `Ready`
    条件的状态是未知的，消息是 `kubelet stopped posting node status`。乍一看，这似乎是荒谬的 - `MemoryPressure`
    和 `DiskPressure` 怎么可能正常，而 kubelet 却停止工作了呢？
- en: The important part is in the `LastTransitionTime` column. The kubelet's most
    recent memory- and disk-specific communication sent positive statuses. Then, at
    a later time, the kubelet stopped posting its Node status, leading to `Unknown`
    statuses for the `OutOfDisk` and `Ready` conditions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的部分在 `LastTransitionTime` 列中。kubelet 最近的内存和磁盘特定通信发送了积极的状态。然后，在稍后的时间，kubelet
    停止发布其节点状态，导致 `OutOfDisk` 和 `Ready` 条件的状态为 `Unknown`。
- en: At this point, we're certain that our Node is the problem – the kubelet is no
    longer sending the Node status to the control plane. However, we don't know why
    this occurred. It could be a network error, a problem with the machine itself,
    or something more specific. We'll need to dig further to figure it out.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以肯定我们的节点是问题所在 - kubelet不再将节点状态发送到控制平面。然而，我们不知道为什么会发生这种情况。可能是网络错误，机器本身的问题，或者更具体的问题。我们需要进一步挖掘才能弄清楚。
- en: A good next step here is to get closer to our malfunctioning Node, as we can
    reasonably assume that it is encountering some sort of issue. If you have access
    to the `node-01` VM or machine, now is a great time to SSH into it. Once we are
    in the machine, let's start troubleshooting further.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里一个很好的下一步是接近我们的故障节点，因为我们可以合理地假设它遇到了某种问题。如果您可以访问 `node-01` VM 或机器，现在是SSH进入的好时机。一旦我们进入机器，让我们进一步进行故障排除。
- en: 'First, let''s check whether the Node can access the control plane over the
    network. If not, this is an obvious reason why the kubelet wouldn''t be able to
    post statuses. Let''s assume a scenario where our cluster control plane (for instance
    an on-premise load balancer) is available at `10.231.0.1`. In order to check whether
    our Node can access the Kubernetes API server, we can ping the control plane as
    follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查节点是否可以通过网络访问控制平面。如果不能，这显然是kubelet无法发布状态的明显原因。假设我们的集群控制平面（例如，本地负载均衡器）位于`10.231.0.1`，为了检查我们的节点是否可以访问Kubernetes
    API服务器，我们可以像下面这样ping控制平面：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Important note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In order to find the control plane IP or DNS, please check your cluster configuration.
    In a managed Kubernetes service such as AWS Elastic Kubernetes Service or Azure
    AKS, this will likely be available to view in the console. If you bootstrapped
    your own cluster using kubeadm, for instance, this is a value that you provided
    during the setup as part of the installation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到控制平面的IP或DNS，请检查您的集群配置。在AWS Elastic Kubernetes Service或Azure AKS等托管的Kubernetes服务中，这可能可以在控制台中查看。例如，如果您使用kubeadm自己引导了集群，那么这是您在安装过程中提供的值之一。
- en: 'Let''s check the results:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查结果：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That confirms it – our Node can indeed talk to the Kubernetes control plane.
    So, the network isn't the issue. Next, let's check the actual kubelet service.
    The Node itself seems to be operational, and the network is fine, so logically,
    the kubelet is the next thing to check.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了 - 我们的节点确实可以与Kubernetes控制平面通信。因此，网络不是问题。接下来，让我们检查实际的kubelet服务。节点本身似乎是正常运行的，网络也正常，所以逻辑上，kubelet是下一个要检查的东西。
- en: Kubernetes components run as system services on Linux Nodes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes组件在Linux节点上作为系统服务运行。
- en: Important note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: On Windows Nodes, the troubleshooting instructions will be slightly different
    – see the Kubernetes documentation for more information ([https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows节点上，故障排除说明会略有不同 - 请参阅Kubernetes文档以获取更多信息（[https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/)）。
- en: 'In order to find out the status of our `kubelet` service, we can run the following
    command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出我们的`kubelet`服务的状态，我们可以运行以下命令：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives us the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Looks like our kubelet is currently not running – it exited with a failure.
    This explains everything we've seen as far as cluster status and Pod issues.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的kubelet目前没有运行 - 它以失败退出。这解释了我们所看到的集群状态和Pod问题。
- en: 'To actually fix the issue, we can first try to restart the `kubelet` using
    the command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上修复问题，我们可以首先尝试使用以下命令重新启动`kubelet`：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s re-check the status of our `kubelet` with our status command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们的状态命令重新检查`kubelet`的状态：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It looks like the `kubelet` failed again. We're going to need to source some
    additional information about the failure mode in order to find out what happened.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来`kubelet`又失败了。我们需要获取一些关于失败模式的额外信息，以便找出发生了什么。
- en: 'Let''s use the `journalctl` command to find out if there are any relevant logs:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`journalctl`命令查看是否有相关的日志：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output should show us logs of the `kubelet` service where a failure occurred:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示`kubelet`服务的日志，其中发生了故障：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Looks like we've found the cause – Kubernetes does not support running on Linux
    machines with `swap` set to `on` by default. Our only choices here are either
    disabling `swap` or restarting the `kubelet` with the `--fail-swap-on` flag set
    to `false`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们已经找到了原因-Kubernetes默认不支持在Linux机器上运行时将`swap`设置为`on`。我们在这里的唯一选择要么是禁用`swap`，要么是使用设置为`false`的`--fail-swap-on`标志重新启动`kubelet`。
- en: 'In our case, we''ll just change the `swap` setting by using the following command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将使用以下命令更改`swap`设置：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, restart the `kubelet` service:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重新启动`kubelet`服务：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, let''s check to see if our fix worked. Check the Nodes using the following
    command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查一下我们的修复是否奏效。使用以下命令检查节点：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This should show output similar to the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示类似于以下内容的输出：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our Node is finally posting a `Ready` status!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的节点最终发布了“Ready”状态！
- en: 'Let''s check on our Pod with the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令检查我们的Pod：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This should show output like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示如下输出：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Success! Our cluster is healthy, and our Pods are running.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的集群健康，我们的Pod正在运行。
- en: Next, let's look at how to troubleshoot applications on Kubernetes once any
    cluster issues are sorted out.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看在解决了任何集群问题后如何排除Kubernetes上的应用程序故障。
- en: Troubleshooting applications on Kubernetes
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上排除应用程序故障
- en: A perfectly running Kubernetes cluster may still have application issues to
    debug. These could be due to bugs in the application itself, or due to misconfigurations
    in the Kubernetes resources that make up the application. As with troubleshooting
    the cluster, we will dive into these concepts by using a case study.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全运行良好的Kubernetes集群可能仍然存在需要调试的应用程序问题。这可能是由于应用程序本身的错误，也可能是由于组成应用程序的Kubernetes资源的错误配置。与排除集群故障一样，我们将通过使用案例研究来深入了解这些概念。
- en: Case study 1 – Service not responding
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究1-服务无响应
- en: We're going to break this section down into troubleshooting at various levels
    of the Kubernetes stack, starting with higher-level components, then ending with
    a deep dive into Pod and container debugging.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这一部分分解为Kubernetes堆栈各个级别的故障排除，从更高级别的组件开始，然后深入到Pod和容器调试。
- en: Let's assume that we have configured our application `app-1` to respond to requests
    via a `NodePort` Service, on port `32688`. The application listens on port `80`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经配置我们的应用程序`app-1`通过`NodePort`服务响应端口`32688`的请求。该应用程序监听端口`80`。
- en: 'We can try to access our application via a `curl` request on one of our Nodes.
    The command will look as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试通过在我们的节点之一上使用`curl`请求来访问我们的应用程序。命令将如下所示：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the `curl` command if it fails will look like the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果curl命令失败，输出将如下所示：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'At this point, our `NodePort` Service isn''t routing requests to any Pod. Following
    our typical debug path, let''s first see which resources are running in the cluster
    with the following command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的`NodePort`服务没有将请求路由到任何Pod。按照我们典型的调试路径，让我们首先查看使用以下命令在集群中运行的哪些资源：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Add the `-o` wide flag to see additional information. Next, run the following
    command:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 添加“-o”宽标志以查看更多信息。接下来，运行以下命令：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This gives us the following output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们以下输出：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It is clear that our Service exists with a proper Node port – but our requests
    are not being routed to the Pods, as is obvious from the failed `curl` command.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们的服务存在一个正确的节点端口-但是我们的请求没有被路由到Pod，这是从失败的curl命令中显而易见的。
- en: 'To see which routes our Service has set up, let''s use the `get endpoints`
    command. This will list the Pod IPs, if any, for the Service as configured:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看我们的服务设置了哪些路由，让我们使用`get endpoints`命令。这将列出服务配置的Pod IP（如果有的话）。
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s check the resulting output of the command:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查命令的结果输出：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Well, something is definitely wrong here.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这里肯定有问题。
- en: Our Service isn't pointing to any Pods. This likely means that there aren't
    any Pods matching our Service selector available. This could be because there
    are no Pods available at all – or because those Pods don't properly match the
    Service selector.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务没有指向任何Pod。这很可能意味着没有任何与我们的服务选择器匹配的Pod可用。这可能是因为根本没有可用的Pod - 或者因为这些Pod不正确地匹配了服务选择器。
- en: 'To check on our Service selector, let''s take the next step in the debug path
    and use the `describe` command as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查我们的服务选择器，让我们沿着调试路径迈出下一步，并使用以下命令：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This gives us an output like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个类似以下的输出：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, our Service is configured to talk to the correct port on our
    application. However, the selector is looking for Pods that match the label `app
    = app-11`. Since we know our application is named `app-1`, this could be the cause
    of our issue.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们的服务配置为与我们的应用程序上的正确端口进行通信。但是，选择器正在寻找与标签“app = app-11”匹配的Pod。由于我们知道我们的应用程序名称为“app-1”，这可能是我们问题的原因。
- en: 'Let''s edit our Service to look for the correct Pod label, `app-1`, running
    another `describe` command to be sure:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编辑我们的服务，以寻找正确的Pod标签“app-1”，再次运行另一个“describe”命令以确保：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This gives the following output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, you can see in the output that our Service is looking for the proper Pod
    selector, but we still do not have any endpoints. Let''s check to see what is
    going on with our Pods by using the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在输出中看到我们的服务正在寻找正确的Pod选择器，但我们仍然没有任何端点。让我们使用以下命令来查看我们的Pod的情况：
- en: '[PRE29]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This shows the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了以下输出：
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our Pods are still waiting to be scheduled. This explains why, even with the
    proper selector, our Service isn''t functioning. To get some granularity on why
    our Pods aren''t being scheduled, let''s use the `describe` command:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Pod仍在等待调度。这解释了为什么即使有正确的选择器，我们的服务也无法正常运行。为了更细致地了解为什么我们的Pod没有被调度，让我们使用“describe”命令：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the output. Let''s focus on the `Events` section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出。让我们专注于“事件”部分：
- en: '![Figure 10.2 – Describe Pod Events output](image/B14790_10_002.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 - 描述Pod事件输出](image/B14790_10_002.jpg)'
- en: Figure 10.2 – Describe Pod Events output
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 - 描述Pod事件输出
- en: From the `Events` section, it looks like our Pod is failing to be scheduled
    due to container image pull failure. There are many possible reasons for this
    – our cluster may not have the necessary authentication mechanisms to pull from
    a private repository, for instance – but that would present a different error
    message.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从“事件”部分来看，似乎我们的Pod由于容器镜像拉取失败而无法被调度。这可能有很多原因 - 例如，我们的集群可能没有必要的身份验证机制来从私有仓库拉取，但这会出现不同的错误消息。
- en: From the context and the `Events` output, we can probably assume that the issue
    is that our Pod definition is looking for a container named `myappimage:lates`
    instead of `myappimage:latest`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从上下文和“事件”输出来看，我们可能可以假设问题在于我们的Pod定义正在寻找一个名为“myappimage:lates”的容器，而不是“myappimage:latest”。
- en: Let's update our Deployment spec with the proper image name and roll out the
    update.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用正确的镜像名称更新我们的部署规范并进行更新。
- en: 'Use the following command to get confirmation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令来确认：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output looks like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Our Pods are now running – let''s check to see that our Service has registered
    the proper endpoints. Use the following command to do this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Pod现在正在运行 - 让我们检查一下我们的服务是否已注册了正确的端点。使用以下命令来执行此操作：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output should look like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是这样的：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Success! Our Service is properly pointing to our application Pods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的服务正确地指向了我们的应用程序Pod。
- en: In the next case study, we'll dig a bit deeper by troubleshooting a Pod with
    incorrect startup parameters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个案例研究中，我们将通过排除具有不正确启动参数的Pod来深入挖掘一些问题。
- en: Case study 2 – Incorrect Pod startup command
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's assume we have our Service properly configured and our Pods running and
    passing health checks. However, our Pod is not responding to requests as we would
    expect. We are sure that this is less of a Kubernetes problem and more of an application
    or configuration problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application container works as follows: it takes a startup command with
    a flag for `color` and combines it with a variable for `version number` based
    on the container''s `image` tag, and echoes that back to the requester. We are
    expecting our application to return `green 3`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, Kubernetes gives us some good tools to debug applications, which
    we can use to delve into our specific containers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s `curl` the application to see what response we get:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We expected `green 3` but got `red 2`, so it looks like something is wrong with
    the input, and the version number variable. Let's start with the former.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we begin with checking our Pods with the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output should look like the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Everything looks good in this output. It seems that our app is running as part
    of a Deployment (and therefore, a ReplicaSet) – we can make sure by running the
    following command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output should look like the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s look a bit closer at our Deployment to see how our Pods are configured
    using the following command:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output looks like the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Broken-deployment-output.yaml
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Let's see if we can fix our issue, which is really quite simple. We're using
    the wrong version of our application, and our startup command is wrong. In this
    case, let's assume we don't have a file with our Deployment spec – so let's just
    edit it in place.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use `kubectl edit deployment app-1-pod`, and edit the Pod spec to the
    following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: fixed-deployment-output.yaml
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the Deployment is saved, you should start seeing your new Pods come up.
    Let''s double-check by using the following command:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output should look like the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And finally – let''s make a `curl` request to check that everything is working:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output of the command is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Success!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Case study 3 – Pod application malfunction with logs
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After spending the previous chapter, [*Chapter 9*](B14790_9_Final_PG_ePub.xhtml#_idTextAnchor212),
    *Observability on Kubernetes*, implementing observability to our applications,
    let's take a look at a case where those tools can really come in handy. We will
    use manual `kubectl` commands for the purposes of this case study – but know that
    by aggregating logs (for instance, in our EFK stack implementation), we could
    make the process of debugging this application significantly easier.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章[*第9章*]（B14790_9_Final_PG_ePub.xhtml#_idTextAnchor212），*Kubernetes上的可观测性*中，我们为我们的应用程序实现了可观测性，让我们看一个案例，这些工具确实非常有用。我们将使用手动的`kubectl`命令来进行这个案例研究
    - 但要知道，通过聚合日志（例如，在我们的EFK堆栈实现中），我们可以使调试这个应用程序的过程变得更容易。
- en: 'In this case study, we once again have a deployment of Pods – to check it,
    let''s run the following command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们再次部署了Pod - 为了检查它，让我们运行以下命令：
- en: '[PRE48]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output of the command is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE49]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: It looks like, in this case, we are working with a StatefulSet instead of a
    Deployment – a key characteristic here is the incrementing Pod IDs starting from
    0.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，在这种情况下，我们使用的是StatefulSet而不是Deployment - 这里的一个关键特征是从0开始递增的Pod ID。
- en: 'We can confirm this by checking for StatefulSets using the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下命令来确认这一点：
- en: '[PRE50]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output of the command is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Let's take a closer look at our StatefulSet with `kubectl get statefulset -o
    yaml app-2-ss`. By using the `get` command along with `-o yaml` we can get our
    `describe` output in the same format as the typical Kubernetes resource YAML.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`kubectl get statefulset -o yaml app-2-ss`来更仔细地查看我们的StatefulSet。通过使用`get`命令以及`-o
    yaml`，我们可以以与典型的Kubernetes资源YAML相同的格式获得我们的`describe`输出。
- en: 'The output of the preceding command is as follows. We''ve removed the Pod spec
    section to keep it shorter:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下。我们已经删除了Pod规范部分以使其更短：
- en: statefulset-output.yaml
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: statefulset-output.yaml
- en: '[PRE52]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We know that our app is using a service. Let's see which one it is!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的应用程序正在使用一个服务。让我们看看是哪一个！
- en: 'Run `kubectl get services -o wide`. The output should be something like the
    following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `kubectl get services -o wide`。输出应该类似于以下内容：
- en: '[PRE53]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'It''s clear that our service is called `app-2-svc`. Let''s see our exact service
    definition using the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显我们的服务叫做`app-2-svc`。让我们使用以下命令查看我们的确切服务定义：
- en: '[PRE54]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE55]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To see exactly what our application is returning for a given input, we can
    use `curl` on our `NodePort` Service:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要确切地查看我们的应用程序对于给定输入返回的内容，我们可以在我们的`NodePort`服务上使用`curl`：
- en: '[PRE56]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Based on our existing knowledge of the application, we would assume that this
    call should return `2`, not `3`. The application developer on our team has asked
    us to investigate any logging output that would help them figure out what the
    issue is.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对应用程序的现有知识，我们会假设这个调用应该返回`2`而不是`3`。我们团队的应用程序开发人员已经要求我们调查任何日志输出，以帮助他们找出问题所在。
- en: 'We know from previous chapters that you can investigate the logging output
    with `kubectl logs <pod name>`. In our case, we have three replicas of our application,
    so we may not be able to find our logs in a single iteration of this command.
    Let''s pick a Pod at random and see if it was the one that served our request:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道从之前的章节中，你可以使用`kubectl logs <pod name>`来调查日志输出。在我们的情况下，我们有三个应用程序的副本，所以我们可能无法在一次迭代中找到我们的日志。让我们随机选择一个Pod，看看它是否是为我们提供服务的那个：
- en: '[PRE57]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: It looks like this was not the Pod that served our request, as our application
    developer has told us that the application definitely logs to `stdout` when a
    `GET` request is made to the server.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这不是为我们提供服务的Pod，因为我们的应用程序开发人员告诉我们，当向服务器发出`GET`请求时，应用程序肯定会记录到`stdout`。
- en: 'Instead of checking through the other two Pods individually, we can use a joint
    command to get logs from all three Pods. The command will be as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用联合命令从所有三个Pod中获取日志，而不是逐个检查另外两个Pod。命令将如下：
- en: '[PRE58]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'And the output is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE59]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: That did the trick – and what's more, we can see some good insight into our
    issue.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就解决了问题 - 而且更重要的是，我们可以看到一些关于我们问题的很好的见解。
- en: Everything seems as we would expect, other than the log line reading `Second
    Number`. Our request clearly used `1plus1` as the query string, which would make
    both the first number and the second number (split by the operator value) equal
    to one.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了日志行读取`Second Number`之外，一切都如我们所期望的那样。我们的请求明显使用`1plus1`作为查询字符串，这将使第一个数字和第二个数字（由运算符值分隔）都等于一。
- en: This will take some additional digging. We could triage this issue by sending
    additional requests and checking the output in order to guess what is happening,
    but in this case it may be better to just get bash access to the Pod and figure
    out what is going on.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一些额外的挖掘。我们可以通过发送额外的请求并检查输出来对这个问题进行分类，以猜测发生了什么，但在这种情况下，最好只是获取对Pod的bash访问并弄清楚发生了什么。
- en: 'First, let''s check our Pod spec, which was removed from the preceding StatefulSet
    YAML. To see the full StatefulSet spec, check the GitHub repository:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查一下我们的Pod规范，这是从前面的StatefulSet YAML中删除的。要查看完整的StatefulSet规范，请检查GitHub存储库：
- en: Statefulset-output.yaml
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Statefulset-output.yaml
- en: '[PRE60]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: It looks like our Pod is mounting an empty volume as a scratch disk. It also
    has two containers in each Pod – a sidecar used for application tracing, and our
    app itself. We'll need this information to `ssh` into one of the Pods (it doesn't
    matter which one for this exercise) using the `kubectl exec` command.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的Pod正在挂载一个空卷作为临时磁盘。每个Pod中还有两个容器 - 一个用于应用程序跟踪的sidecar，以及我们的应用程序本身。我们需要这些信息来使用`kubectl
    exec`命令`ssh`到其中一个Pod（对于这个练习来说，无论选择哪一个都可以）。
- en: 'We can do it using the following command:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来完成：
- en: '[PRE61]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This command should give you a bash terminal as the output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令应该给你一个bash终端作为输出：
- en: '[PRE62]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, using the terminal we just created, we should be able to investigate our
    application code. For the purposes of this tutorial, we are using a highly simplified
    Node.js application.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用我们刚创建的终端，我们应该能够调查我们的应用程序代码。在本教程中，我们使用了一个非常简化的Node.js应用程序。
- en: 'Let''s check our Pod filesystem to see what we''re working with using the following
    command:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的Pod文件系统，看看我们使用以下命令在处理什么：
- en: '[PRE63]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Looks like we have two JavaScript files, and our previously mentioned `scratch`
    folder. It's probably a good bet to assume that `app.js` contains the logic for
    bootstrapping and serving the application, and `calculate.js` contains our controller
    code for doing the calculations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们有两个JavaScript文件，以及我们之前提到的`scratch`文件夹。可以假设`app.js`包含引导和提供应用程序的逻辑，而`calculate.js`包含我们的控制器代码来进行计算。
- en: 'We can confirm by printing the contents of the `calculate.js` file:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过打印`calculate.js`文件的内容来确认：
- en: Broken-calculate.js
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Broken-calculate.js
- en: '[PRE64]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Even with little to no knowledge of JavaScript, it's pretty obvious what the
    issue is here. The code is incrementing the `second` variable before performing
    the calculation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对JavaScript几乎一无所知，这里的问题也是非常明显的。代码在执行计算之前递增了`second`变量。
- en: 'Since we''re inside of the Pod, and we''re using a non-compiled language, we
    can actually edit this file inline! Let''s use `vi` (or any text editor) to correct
    this file:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在Pod内部，并且正在使用非编译语言，我们实际上可以内联编辑这个文件！让我们使用`vi`（或任何文本编辑器）来纠正这个文件：
- en: '[PRE65]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'And edit the file to read as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 并编辑文件如下所示：
- en: fixed-calculate.js
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: fixed-calculate.js
- en: '[PRE66]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now, our code should run properly. It's important to state that this fix is
    only temporary. As soon as our Pod shuts down or gets replaced by another Pod,
    it will revert to the code that was originally included in the container image.
    However, this pattern does allow us to try out quick fixes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的代码应该正常运行。重要的是要说明，这个修复只是临时的。一旦我们的Pod关闭或被另一个Pod替换，它将恢复到最初包含在容器镜像中的代码。然而，这种模式确实允许我们尝试快速修复。
- en: 'After exiting the `exec` session using the `exit` bash command, let''s try
    our URL again:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`exit` bash命令退出`exec`会话后，让我们再次尝试我们的URL：
- en: '[PRE67]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As you can see, our hotfixed container shows the right result! Now, we can update
    our code and Docker image in a more permanent way with our fix. Using `exec` is
    a great way to troubleshoot and debug running containers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们的热修复容器显示了正确的结果！现在，我们可以使用我们的修复以更加永久的方式更新我们的代码和Docker镜像。使用`exec`是一个很好的方法来排除故障和调试运行中的容器。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about troubleshooting applications on Kubernetes.
    First, we covered some common failure modes of distributed applications. Then,
    we learned how to triage issues with Kubernetes components. Finally, we reviewed
    several scenarios where Kubernetes configuration and application debugging were
    performed. The Kubernetes debugging and troubleshooting techniques you learned
    in this chapter will help you when triaging issues with any Kubernetes clusters
    and applications you may work on.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在Kubernetes上调试应用程序。首先，我们介绍了分布式应用程序的一些常见故障模式。然后，我们学习了如何对Kubernetes组件的问题进行分类。最后，我们回顾了几种Kubernetes配置和应用程序调试的场景。在本章中学到的Kubernetes调试和故障排除技术将帮助你在处理任何Kubernetes集群和应用程序的问题时。
- en: In the next chapter, [*Chapter 11*](B14790_11_Final_PG_ePub.xhtml#_idTextAnchor251),
    *Template Code Generation and CI/CD on Kubernetes*, we will look into some ecosystem
    extensions for templating Kubernetes resource manifests and continuous integration/continuous
    deployment with Kubernetes.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[*第11章*](B14790_11_Final_PG_ePub.xhtml#_idTextAnchor251)，*Kubernetes上的模板代码生成和CI/CD*，我们将探讨一些用于模板化Kubernetes资源清单和与Kubernetes一起进行持续集成/持续部署的生态系统扩展。
- en: Questions
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How does the distributed systems fallacy, "*the topology doesn't change*," apply
    to applications on Kubernetes?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式系统谬误“*拓扑结构不会改变*”如何适用于Kubernetes上的应用程序？
- en: How are the Kubernetes control plane components (and kubelet) implemented at
    the OS level?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes控制平面组件（和kubelet）在操作系统级别是如何实现的？
- en: How would you go about debugging an issue where Pods are stuck in the `Pending`
    status? What would be your first step? And your second?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当Pod被卡在“Pending”状态时，你会如何调试问题？你的第一步会是什么？第二步呢？
- en: Further reading
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The CNI plugin for traffic shaping: [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于流量整形的CNI插件：[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)

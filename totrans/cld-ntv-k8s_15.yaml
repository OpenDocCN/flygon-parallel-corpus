- en: '*Chapter 12*: Kubernetes Security and Compliance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about some of the key pieces of Kubernetes security.
    We'll discuss some recent Kubernetes security issues, and the finding of a recent
    audit that was performed on Kubernetes. Then, we'll look at implementing security
    at each level of our cluster, starting with the security of Kubernetes resources
    and their configurations, moving on to container security, and then finally, runtime
    security with intrusion detection. To start, we will discuss some key security
    concepts as they relate to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding security on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing CVEs and security audits for Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing tools for cluster configuration and container security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling intrusion detection, runtime security, and compliance on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool, along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you will need a machine that supports the Helm CLI tool, which
    typically has the same prerequisites as `kubectl` – for details, check the Helm
    documentation at [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
  prefs: []
  type: TYPE_NORMAL
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding security on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When discussing security on Kubernetes, it is very important to note security
    boundaries and shared responsibility. The *Shared Responsibility Model* is a common
    term used to describe how security is handled in public cloud services. It states
    that the customer is responsible for the security of their applications, and the
    security of their configuration of public cloud components and services. The public
    cloud provider, on the other hand, is responsible for the security of the services
    themselves as well as the infrastructure they run on, all the way to the data
    center and physical layer.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, security on Kubernetes is shared. Though upstream Kubernetes is not
    a commercial product, the thousands of Kubernetes contributors and significant
    organizational heft from large tech companies ensure that the security of Kubernetes
    components is maintained. Additionally, the large ecosystem of individual contributors
    and companies using the technology ensures that it gets better as CVEs are reported
    and handled. Unfortunately, as we will discuss in the next section, the complexity
    of Kubernetes means that there are many possible attack vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the shared responsibility model then, as a developer you are responsible
    for the security of how you configure Kubernetes components, the security of the
    applications that you run on Kubernetes, and access-level security in your cluster
    configuration. While the security of your applications and containers themselves
    are not quite in scope for this book, they are definitely important to Kubernetes
    security. We will spend most of our time discussing configuration-level security,
    access security, and runtime security.
  prefs: []
  type: TYPE_NORMAL
- en: Either Kubernetes itself or the Kubernetes ecosystem provides tooling, libraries,
    and full-blown products to handle security at each of these levels – and we'll
    be reviewing some of these options in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before we discuss these solutions, it's best to start with a base understanding
    of why they may be needed in the first place. Let's move on to the next section,
    where we'll detail some issues that Kubernetes has encountered in the realm of
    security.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing CVEs and security audits for Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has encountered several **Common Vulnerabilities and Exposures**
    (**CVEs**) in its storied history. The MITRE CVE database, at the time of writing,
    lists 73 CVE announcements from 2015 to 2020 when searching for `kubernetes`.
    Each one of these is related either directly to Kubernetes, or to a common open
    source solution that runs on Kubernetes (like the NGINX ingress controller, for
    instance).
  prefs: []
  type: TYPE_NORMAL
- en: Several of these were critical enough to require hotfixes to the Kubernetes
    source, and thus they list the affected versions in the CVE description. A full
    list of all CVEs related to Kubernetes can be found at [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes).
    To give you an idea of some of the issues that have been found, let's review a
    few of these CVEs in chronological order.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CVE-2016-1905 – Improper admission control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This CVE was one of the first major security issues with production Kubernetes.
    The National Vulnerability Database (a NIST website) gives this issue a base score
    of 7.7, putting it in the high-impact category.
  prefs: []
  type: TYPE_NORMAL
- en: With this issue, a Kubernetes admission controller would not ensure that a `kubectl
    patch` command followed admission rules, allowing users to completely work around
    the admission controller – a nightmare in a multitenant scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CVE-2018-1002105 – Connection upgrading to the backend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This CVE was likely the most critical to date in the Kubernetes project. In
    fact, NVD gives it a 9.8 criticality score! In this CVE, it was found that it
    was possible in some versions of Kubernetes to piggyback on an error response
    from the Kubernetes API server and then upgrade the connection. Once the connection
    was upgraded, it was possible to send authenticated requests to any backend server
    in the cluster. This allowed a malicious user to essentially emulate a perfectly
    authenticated TLS request without proper credentials.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these CVEs (and likely partially driven by them), the CNCF sponsored
    a third-party security audit of Kubernetes in 2019\. The results of the audit
    are open source and publicly available and are worth a review.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the 2019 security audit results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned in the previous section, the 2019 Kubernetes security audit
    was performed by a third party, and the results of the audit are completely open
    source. The full audit report with all sections can be found at [https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/](https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, this audit focused on the following pieces of Kubernetes functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud-controller-manager`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubelet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-proxy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Container Runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intent was to focus on the most important and relevant pieces of Kubernetes
    when it came to security. The results of the audit included not just a full security
    report, but also a threat model and a penetration test, as well as a whitepaper.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deep into the audit results is not in the scope of this book, but there
    are some major takeaways that are great windows into the crux of many of the biggest
    Kubernetes security issues.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the audit found that since Kubernetes is a complex, highly networked
    system with many different settings, there are many possible configurations that
    inexperienced engineers may perform and in doing so, open their cluster to outside
    attackers.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of Kubernetes being complex enough that an insecure configuration
    could happen easily is important to note and take to heart.
  prefs: []
  type: TYPE_NORMAL
- en: The entire audit is worth a read – for those with significant knowledge of network
    security and containers, it is an excellent view of some of the security decisions
    that were made as part of the development of Kubernetes as a platform.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed where Kubernetes security issues have been found,
    we can start looking into ways to increase the security posture of your clusters.
    Let's start with some default Kubernetes functionality for security.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing tools for cluster configuration and container security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes gives us many inbuilt options for the security of cluster configurations
    and container permissions. Since we''ve already talked about RBAC, TLS Ingress,
    and encrypted Kubernetes Secrets, let''s discuss a few concepts that we haven''t
    had time to review yet: admission controllers, Pod security policies, and network
    policies.'
  prefs: []
  type: TYPE_NORMAL
- en: Using admission controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Admission controllers are an often overlooked but extremely important Kubernetes
    feature. Many of Kubernetes' advanced features use admission controllers under
    the hood. In addition, you can create new admission controller rules in order
    to add custom functionality to your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two general types of admission controllers:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutating admission controllers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating admission controllers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutating admission controllers take in Kubernetes resource specifications and
    return an updated resource specification. They also perform side-effect calculations
    or make external calls (in the case of custom admission controllers).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, validating admission controllers simply accept or deny Kubernetes
    resource API requests. It is important to know that both types of controllers
    only act on create, update, delete, or proxy requests. These controllers cannot
    mutate or change requests to list resources.
  prefs: []
  type: TYPE_NORMAL
- en: When a request of one of those types comes into the Kubernetes API server, it
    will first run the request through all the relevant mutating admission controllers.
    Then, the output, which may be mutated, will pass through the validating admission
    controllers, before finally being acted upon (or not, if the call is denied by
    an admission controller) in the API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structurally, the Kubernetes-provided admission controllers are functions or
    "plugins," which run as part of the Kubernetes API server. They rely on two webhook
    controllers (which are admission controllers themselves, just special ones): **MutatingAdmissionWebhook**
    and **ValidatingAdmissionWebhook**. All other admission controllers use either
    one of these webhooks under the hood, depending on their type. In addition, any
    custom admission controllers you write can be attached to either one of these
    webhooks.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at the process of creating a custom admission controller, let's
    review a few of the default admission controllers that Kubernetes provides. For
    a full list, check the Kubernetes official documentation at [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding default admission controllers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are quite a few default admission controllers present in a typical Kubernetes
    setup – many of which are required for some fairly important basic functionality.
    Here are some examples of default admission controllers.
  prefs: []
  type: TYPE_NORMAL
- en: The NamespaceExists admission controller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **NamespaceExists** admission controller checks any incoming Kubernetes
    resource (other than namespaces themselves). This is to check whether the namespace
    attached to the resource exists. If not, it denies the resource request at the
    admission controller level.
  prefs: []
  type: TYPE_NORMAL
- en: The PodSecurityPolicy admission controller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **PodSecurityPolicy** admission controller supports Kubernetes Pod security
    policies, which we will learn about momentarily. This controller prevents resources
    that do not follow Pod security policies from being created.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the default admission controllers, we can create custom admission
    controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom admission controllers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating a custom admission controller can be done dynamically using one of
    the two webhook controllers. The way this works is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You must write your own server or script that runs separately to the Kubernetes
    API server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you configure one of the two previously mentioned webhook triggers to
    make a request with resource data to your custom server controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the result, the webhook controller will then tell the API server whether
    or not to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s start with the first step: writing a quick admission server.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing a server for a custom admission controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create our custom admission controller server (which will accept webhooks
    from the Kubernetes control plane), we can use any programming language. As with
    most extensions to Kubernetes, Go has the best support and libraries that make
    the task of writing a custom admission controller easier. For now, we will use
    some pseudocode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The control flow for our server will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Admission-controller-server.pseudo
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a simple server for our custom admission controller, we can
    configure a Kubernetes admission webhook to call it.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Kubernetes to call a custom admission controller server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to tell Kubernetes to call our custom admission server, it needs a
    place to call. We can run our custom admission controller anywhere – it doesn't
    need to be on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being said, it''s easy for the purposes of this chapter to run it on Kubernetes.
    We won''t go through the full manifest, but let''s assume we have a Service and
    a Deployment that it is pointed at, running a container that is our server. The
    Service would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Service-webhook.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It's important to note that our server needs to use HTTPS in order for Kubernetes
    to accept webhook responses. There are many ways to configure this, and we won't
    get into it in this book. The certificate can be self-signed, but the common name
    of the certificate and CA needs to match the one used when setting up the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our server running and accepting HTTPS requests, let's tell
    Kubernetes where to find it. To do this, we use `MutatingWebhookConfiguration`.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of `MutatingWebhookConfiguration` is shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutating-webhook-config-service.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let's pick apart the YAML for our `MutatingWebhookConfiguration`. As you can
    see, we can configure more than one webhook in this configuration – though we've
    only done one in this example.
  prefs: []
  type: TYPE_NORMAL
- en: For each webhook, we set `name`, `rules`, and a `configuration`. The `name`
    is simply the identifier for the webhook. The `rules` allow us to configure exactly
    in which cases Kubernetes should make a request to our admission controller. In
    this case, we have configured our webhook to fire whenever a `CREATE` event for
    resources of the types `pods`, `deployments`, and `configmaps` occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `clientConfig`, where we specify exactly where and how
    Kubernetes should make the webhook request. Since we're running our custom server
    on Kubernetes, we specify the Service name as in the previous YAML, in addition
    to the path on our server to hit (`"/mutate"` is a best practice here), and the
    CA of the cluster to compare to the certificate of the HTTPS termination. If your
    custom admission server is running somewhere else, there are other possible configuration
    fields – check the docs if you need them ([https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)).
  prefs: []
  type: TYPE_NORMAL
- en: Once we create the `MutatingWebhookConfiguration` in Kubernetes, it is easy
    to test the validation. All we need to do is create a Pod, Deployment, or ConfigMap
    as normal, and check whether our requests are denied or patched according to the
    logic in our server.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume for now that our server is set to deny any Pod with a name that
    includes the string `deny-me`. It is also set up to add an error response to the
    `AdmissionReviewResponse`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use a Pod spec as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To-deny-pod.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create our Pod to check the admission controller. We can use the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And that's it! Our custom admission controller has successfully denied a Pod
    that doesn't match the conditions we specified in our server. For resources that
    are patched (not denied, but altered), `kubectl` will not show any special response.
    You will need to fetch the resource in question to see the patch in action.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've explored custom admission controllers, let's look at another
    way to impose cluster security practices – Pod security policies.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Pod security policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basics of Pod security policies are that they allow a cluster administrator
    to create rules that Pods must follow in order to be scheduled onto a node. Technically,
    Pod security policies are just another type of admission controller. However,
    this feature is officially supported by Kubernetes and is worth an in-depth discussion,
    since many options are available.
  prefs: []
  type: TYPE_NORMAL
- en: Pod security policies can be used to prevent Pods from running as root, put
    limits on ports and volumes used, restrict privilege escalation, and much more.
    We will review a subset of Pod security policy capabilities now, but for a full
    list of Pod security policy configuration types, check the official PSP documentation
    at [https://kubernetes.io/docs/concepts/policy/pod-security-policy/](https://kubernetes.io/docs/concepts/policy/pod-security-policy/).
  prefs: []
  type: TYPE_NORMAL
- en: As a final note, Kubernetes also supports low-level primitives for controlling
    container permissions – namely *AppArmor*, *SELinux*, and *Seccomp*. These configurations
    are outside the scope of this book, but they can be useful for highly secure environments.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to create a Pod security policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several steps to implementing Pod security policies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the Pod security policy admission controller must be enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will prevent all Pods from being created in your cluster since it requires
    a matched Pod security policy and role to be able to create a Pod. You will likely
    want to create your Pod security policies and roles before enabling the admission
    controller for this reason.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the admission controller is enabled, the policy itself must be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, a `Role` or `ClusterRole` object must be created with access to the Pod
    security policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, that role can be bound with a **ClusterRoleBinding** or **RoleBinding**
    to a user or service `accountService` account, allowing Pods created with that
    service account to use permissions available to the Pod security policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In some cases, you may not have the Pod security policy admission controller
    enabled by default on your cluster. Let's look at how to enable it.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the Pod security policy admission controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to enable the PSP admission controller, the `kube-apiserver` must be
    started with a flag that specifies admission controllers to start with. On managed
    Kubernetes (EKS, AKS, and others), the PSP admission controller will likely be
    enabled by default, along with a privileged Pod security policy created for use
    by the initial admin user. This prevents the PSP from causing any issues with
    creating Pods in the new cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re self-managing Kubernetes and you haven''t yet enabled the PSP admission
    controller, you can do so by restarting the `kube-apiserver` component with the
    following flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If your Kubernetes API server is run using a `systemd` file (as it would be
    if following *Kubernetes: The Hard Way*), you should update the flags there instead.
    Typically, `systemd` files are placed in the `/etc/systemd/system/` folder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find out which admission plugins are already enabled, you can run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will present a long list of admission plugins that are enabled.
    For instance, you will see the following admission plugins in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we are sure the PSP admission controller is enabled, we can actually
    create a PSP.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the PSP resource
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pod security policies themselves can be created using typical Kubernetes resource
    YAML. Here''s a YAML file for a privileged Pod security policy:'
  prefs: []
  type: TYPE_NORMAL
- en: Privileged-psp.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This Pod security policy allows the user or service account (via a **RoleBinding**
    or **ClusterRoleBinding**) to create Pods that have privileged capabilities. For
    instance, the Pod using this `PodSecurityPolicy` would be able to bind to the
    host network on ports `2000`-`65535`, run as any user, and bind to any volume
    type. In addition, we have an annotation for a `seccomp` restriction on `allowedProfileNames`
    – to give you an idea of how `Seccomp` and `AppArmor` annotations work with `PodSecurityPolicies`.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, just creating the PSP does nothing. For any service
    account or user that will be creating privileged Pods, we need to give them access
    to the Pod security policy via a **Role** and **RoleBinding** (or `ClusterRole`
    and `ClusterRoleBinding`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create a `ClusterRole` that has access to this PSP, we can use
    the following YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: Privileged-clusterrole.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can bind our newly created `ClusterRole` to the user or service account
    with which we intend to create privileged Pods. Let''s do this with a `ClusterRoleBinding`:'
  prefs: []
  type: TYPE_NORMAL
- en: Privileged-clusterrolebinding.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In our case, we want to let every authenticated user on the cluster create privileged
    Pods, so we bind to the `system:authenticated` group.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is likely that we do not want all our users or Pods to be privileged.
    A more realistic Pod security policy places restrictions on what Pods are capable
    of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some example YAML of a PSP that has these restrictions:'
  prefs: []
  type: TYPE_NORMAL
- en: unprivileged-psp.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can tell, this Pod security policy is vastly different in the restrictions
    it imposes on created Pods. No Pods under this policy are allowed to run as root
    or escalate to root. They also have restrictions on the types of volumes they
    can bind to (this section has been highlighted in the preceding code snippet)
    – and they cannot use host networking or bind directly to host ports.
  prefs: []
  type: TYPE_NORMAL
- en: In this YAML, both the `runAsUser` and `supplementalGroups` sections control
    the Linux user ID and group IDs that can run or be added by the container, while
    the `fsGroup` key controls the filesystem groups that can be used by the container.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using rules like `MustRunAsNonRoot`, it is possible to directly
    specify which user ID a container can run with – and any Pods not running specifically
    with that ID in their spec will not be able to schedule onto a Node.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a sample PSP that restricts users to a specific ID, look at the following
    YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: Specific-user-id-psp.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This Pod security policy, when applied, will prevent any Pods from running as
    user ID `0` or `3001`, or higher. In order to create a Pod that satisfies this
    condition, we use the `runAs` option in the `securityContext` in a Pod spec.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example Pod that satisfies this constraint and would be successfully
    scheduled even with this Pod security policy in place:'
  prefs: []
  type: TYPE_NORMAL
- en: Specific-user-pod.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, in this YAML, we give our Pod a specific user to run with, ID
    `1000`. We also disallowed our Pod from escalating to root. This Pod spec would
    successfully schedule even when `specific-user-psp` is in place.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discussed how Pod security policies can secure Kubernetes by
    placing restrictions on how a Pod runs, we can move onto network policies, where
    we can restrict how Pods network.
  prefs: []
  type: TYPE_NORMAL
- en: Using network policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network policies in Kubernetes work similar to firewall rules or route tables.
    They allow users to specify a group of Pods via a selector and then determine
    how and where those Pods can communicate.
  prefs: []
  type: TYPE_NORMAL
- en: For network policies to work, your chosen Kubernetes network plugin (such as,
    *Weave*, *Flannel*, or *Calico*) must support the network policy spec. Network
    policies can be created as all other Kubernetes resources are – through a YAML
    file. Let's start with a very simple network policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a network policy spec that restricts access to Pods with the label
    `app=server`:'
  prefs: []
  type: TYPE_NORMAL
- en: Label-restriction-policy.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's pick apart this network policy YAML since it will help us explain
    some more complicated network policies as we progress.
  prefs: []
  type: TYPE_NORMAL
- en: First, in our spec, we have a `podSelector`, which works similarly to node selectors
    in functionality. Here, we are using `matchLabels` to specify that this network
    policy will only affect Pods with the label `app=server`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we specify a policy type for our network policy. There are two policy
    types: `ingress` and `egress`. A network policy can specify one or both types.
    `ingress` refers to making network rules that come into effect for connections
    to the matched Pods, and `egress` refers to network rules that come into effect
    for connections leaving the matched Pods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific network policy, we are simply dictating a single `ingress`
    rule: the only traffic that will be accepted by Pods with the label `app=server`
    is traffic that originates from Pods with the label `app:frontend`. Additionally,
    the only port that will accept traffic on Pods with the label `app=server` is
    `80`.'
  prefs: []
  type: TYPE_NORMAL
- en: There can be multiple `from` blocks in an `ingress` policy set that correspond
    to multiple traffic rules. Similarly, with `egress`, there can be multiple `to`
    blocks.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that network policies work by namespace. By default,
    if there isn't a single network policy in a namespace, there are no restrictions
    on Pod-to-Pod communication within that namespace. However, as soon as a specific
    Pod is selected by a single network policy, all traffic to and from that Pod must
    explicitly match a network policy rule. If it doesn't match a rule, it will be
    blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, we can easily create policies that enforce broad restrictions
    on Pod networking. Let''s take a look at the following network policy:'
  prefs: []
  type: TYPE_NORMAL
- en: Full-restriction-policy.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this `NetworkPolicy`, we specify that we will be including both an `Ingress`
    and `Egress` policy, but we don't write a block for either of them. This has the
    effect of automatically denying any traffic for both `Egress` and `Ingress` since
    there are no rules for traffic to match against.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, our `{}` Pod selector value corresponds to selecting every Pod
    in the namespace. The end result of this rule is that every Pod in the `development`
    namespace will not be able to accept ingress traffic or send egress traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that network policies are interpreted by combining
    all the separate network policies that affect a Pod and then applying the combination
    of all those rules to Pod traffic.
  prefs: []
  type: TYPE_NORMAL
- en: This means that even though we have restricted all ingress and egress traffic
    in the `development` namespace in our preceding example, we can still enable it
    for specific Pods by adding another network policy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that now our `development` namespace has complete traffic restriction
    for Pods, we want to allow a subset of Pods to receive network traffic on port
    `443` and send traffic on port `6379` to a database Pod. In order to do this,
    we simply need to create a new network policy that, by the additive nature of
    policies, allows this traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the network policy looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: Override-restriction-network-policy.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this network policy, we are allowing our server Pods in the `development`
    namespace to receive traffic from frontend Pods on port `443` and send traffic
    to database Pods on port `6379`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If instead, we wanted to open up all Pod-to-Pod communication without any restrictions,
    while still actually instituting a network policy, we could do so with the following
    YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: All-open-network-policy.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now we have discussed how we can use network policies to set rules on Pod-to-Pod
    traffic. However, it is also possible to use network policies as an external-facing
    firewall of sorts. To do this, we create network policy rules based not on Pods
    as origin or destination, but external IPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example network policy where we are restricting communication
    to and from a Pod, with a specific IP range as the target:'
  prefs: []
  type: TYPE_NORMAL
- en: External-ip-network-policy.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this network policy, we are specifying a single `Ingress` rule and a single
    `Egress` rule. Each of these rules accepts or denies traffic based not on which
    Pod it is coming from but on the source IP of the network request.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have selected a `/16` subnet mask range (with a specified `/24`
    CIDR exception) for both our `Ingress` and `Egress` rules. This has the side effect
    of preventing any traffic from within our cluster from reaching these Pods since
    none of our Pod IPs will match the rules in a default cluster networking setup.
  prefs: []
  type: TYPE_NORMAL
- en: However, traffic from outside the cluster in the specified subnet mask (and
    not in the exception range) will be able to both send traffic to the `worker`
    Pods and also be able to accept traffic from the `worker` Pods.
  prefs: []
  type: TYPE_NORMAL
- en: With the end of our discussion on network policies, we can move onto a completely
    different layer of the security stack – runtime security and intrusion detection.
  prefs: []
  type: TYPE_NORMAL
- en: Handling intrusion detection, runtime security, and compliance on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have set your Pod security policies and network policies – and generally
    ensured that your configuration is as watertight as possible – there are still
    many attack vectors that are possible in Kubernetes. In this section, we will
    focus on attacks from within a Kubernetes cluster. Even with highly specific Pod
    security policies in place (which definitely do help, to be clear), it is possible
    for containers and applications running in your cluster to perform unexpected
    or malicious operations.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this problem, many professionals look to runtime security
    tools, which allow constant monitoring and alerting of application processes.
    For Kubernetes, a popular open source tool that can accomplish this is *Falco*.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Falco
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Falco bills itself as a *behavioral activity monitor* for processes on Kubernetes.
    It can monitor both your containerized applications running on Kubernetes as well
    as the Kubernetes components themselves.
  prefs: []
  type: TYPE_NORMAL
- en: How does Falco work? In real time, Falco parses system calls from the Linux
    kernel. It then filters these system calls through rules – which are sets of configurations
    that can be applied to the Falco engine. Whenever a rule is broken by a system
    call, Falco triggers an alert. It's that simple!
  prefs: []
  type: TYPE_NORMAL
- en: Falco ships with an extensive set of default rules that add significant observability
    at the kernel level. Custom rules are of course supported by Falco – and we will
    show you how to write them.
  prefs: []
  type: TYPE_NORMAL
- en: First, however, we need to install Falco on our cluster! Luckily, Falco can
    be installed using Helm. However, it is very important to note that there are
    a few different ways to install Falco, and they differ significantly in how effective
    they can be in the event of a breach.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to be installing Falco using the Helm chart, which is simple and
    works well for managed Kubernetes clusters, or any scenario where you may not
    have direct access to the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: However, for the best possible security posture, Falco should be installed directly
    onto the Kubernetes nodes at the Linux level. The Helm chart, which uses a DaemonSet
    is great for ease of use but is inherently not as secure as a direct Falco installation.
    To install Falco directly to your nodes, check the installation instructions at
    [https://falco.org/docs/installation/](https://falco.org/docs/installation/).
  prefs: []
  type: TYPE_NORMAL
- en: 'With that caveat out of the way, we can install Falco using Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to add the `falcosecurity` repo to our local Helm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can proceed with actually installing Falco using Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Falco Helm chart has many possible variables that can be changed in the
    values file – for a full review of those, you can check the official Helm chart
    repo at [https://github.com/falcosecurity/charts/tree/master/falco](https://github.com/falcosecurity/charts/tree/master/falco).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Falco, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This command will install Falco using the default values, which you can see
    at [https://github.com/falcosecurity/charts/blob/master/falco/values.yaml](https://github.com/falcosecurity/charts/blob/master/falco/values.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's dive into what Falco offers a security-conscious Kubernetes administrator.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Falco's capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, Falco ships with a set of default rules, but we can
    easily add more rules using new YAML files. Since we're using the Helm version
    of Falco, passing custom rules to Falco is as simple as either creating a new
    values file or editing the default one with custom rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding custom rules looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom-falco.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now is a good time to discuss the structure of a Falco rule. To illustrate,
    let's borrow a few lines of rules from the `Default` Falco ruleset that ships
    with the Falco Helm chart.
  prefs: []
  type: TYPE_NORMAL
- en: When specifying Falco configuration in YAML, we can use three different types
    of keys to help compose our rules. These are macros, lists, and rules themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The specific rule we're looking at in this example is called `Launch Privileged
    Container`. This rule will detect when a privileged container has been started
    and log some information about the container to `STDOUT`. Rules can do all sorts
    of things when it comes to alerts, but logging to `STDOUT` is a good way to increase
    observability when high-risk events happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the rule entry itself. This rule uses a few helper entries,
    several macros, and lists – but we''ll get to those in a second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, a Falco rule has several parts. First, we have the rule name
    and description. Then, we specify the triggering condition for the rule – which
    acts as a filter for Linux system calls. If a system call matches all the logic
    filters in the `condition` block, the rule is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: When a rule is triggered, the output key allows us to set a format for how the
    text of the output appears. The `priority` key lets us assign a priority, which
    can be one of `emergency`, `alert`, `critical`, `error`, `warning`, `notice`,
    `informational`, and `debug`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `tags` key applies tags to the rule in question, making it easier
    to categorize rules. This is especially important when using alerts that aren't
    simply plain text `STDOUT` entries.
  prefs: []
  type: TYPE_NORMAL
- en: The syntax for `condition` is especially important here, and we will focus on
    how this system of filtering works.
  prefs: []
  type: TYPE_NORMAL
- en: First off, since the filters are essentially logical statements, you will see
    some familiar syntax (if you have ever programmed or written pseudocode) – and,
    and not, and so on. This syntax is pretty simple to learn, and a full discussion
    of it – the *Sysdig* filter syntax – can be found at [https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering](https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering).
  prefs: []
  type: TYPE_NORMAL
- en: As a note, the Falco open source project was originally created by *Sysdig*,
    which is why it uses the common *Sysdig* filter syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will see reference to `container_started` and `container`, as well
    as `falco_privileged_containers` and `user_privileged_containers`. These are not
    plain strings but the use of macros – references to other blocks in the YAML that
    specify additional functionality, and generally make it much easier to write rules
    without repeating a lot of configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this rule really works, let''s look at a full reference for all
    the macros that were referenced in the preceding rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You will see in the preceding YAML that each macro is really just a reusable
    block of `Sysdig` filter syntax, often using other macros to accomplish the rule
    functionality. Lists, not pictured here, are like macros except that they do not
    describe filter logic. Instead, they include a list of string values that can
    be used as part of a comparison using the filter syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, `(``trusted_images)` in the `falco_privileged_containers` macro
    references a list called `trusted_images`. Here''s the source for that list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this particular list is empty in the default rules, but a custom
    ruleset could use a list of trusted images in this list, which would then automatically
    be consumed by all the other macros and rules that use the `trusted_image` list
    as part of their filter rules.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, in addition to tracking Linux system calls, Falco can
    also track Kubernetes control plane events as of Falco v0.13.0.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Kubernetes audit event rules in Falco
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Structurally, these Kubernetes audit event rules work the same way as Falco''s
    Linux system call rules. Here''s an example of one of the default Kubernetes rules
    in Falco:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This rule acts on Kubernetes audit events in Falco (essentially, control plane
    events) to alert when a Pod is created that isn't on the list `allowed_k8s_containers`.
    The default `k8s` audit rules contain many similar rules, most of which output
    formatted logs when triggered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we talked about Pod security policies a bit earlier in this chapter –
    and you may be seeing some similarities between PSPs and Falco Kubernetes audit
    event rules. For instance, take this entry from the default Kubernetes Falco rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This rule, which is triggered when a Pod is attempting to start using the host
    network, maps directly to host network PSP settings.
  prefs: []
  type: TYPE_NORMAL
- en: Falco capitalizes on this similarity by letting us use Falco as a way to `trial`
    new Pod security policies without applying them cluster-wide and causing issues
    with running Pods.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, `falcoctl` (the Falco command-line tool) comes with the `convert
    psp` command. This command takes in a Pod security policy definition and turns
    it into a set of Falco rules. These Falco rules will just output logs to `STDOUT`
    when triggered (instead of causing Pod scheduling failures like a PSP mismatch),
    which makes it much easier to test out new Pod security policies in an existing
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to use the `falcoctl` conversion tool, check out the official Falco
    documentation at [https://falco.org/docs/psp-support/](https://falco.org/docs/psp-support/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good grounding on the Falco tool, let's discuss how it can
    be used to implement compliance controls and runtime security.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Falco to compliance and runtime security use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because of its extensibility and ability to audit low-level Linux system calls,
    Falco is a great tool for continuous compliance and runtime security.
  prefs: []
  type: TYPE_NORMAL
- en: On the compliance side, it is possible to leverage Falco rulesets that map specifically
    to the requirements of a compliance standard – for instance, PCI or HIPAA. This
    allows users to quickly detect and act on any processes that do not comply with
    the standard in question. There are open and closed source Falco rulesets for
    several standards.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for runtime security, Falco exposes an alerting/eventing system,
    which means that any runtime events that trigger an alert can also trigger automated
    intervention and remediation processes. This can work for both security and compliance.
    As an example, if a Pod triggers a Falco alert for non-compliance, a process can
    work off that alert and delete the offending Pod immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about security in the context of Kubernetes. First,
    we reviewed the basics of security on Kubernetes – which layers of the security
    stack are relevant to our cluster and some broad strokes of how to manage that
    complexity. Next, we learned about some of the major security issues that Kubernetes
    has encountered, as well as discussing the results of the 2019 security audit.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we implemented security at two different levels of the stack in Kubernetes
    – first, in configuration with Pod security policies and network policies, and
    finally, runtime security with Falco.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to make Kubernetes your own by building
    custom resources. This will allow you to add significant new functionality to
    your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the names of the two webhook controllers that a custom admission controller
    can use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What effect does a blank `NetworkPolicy` for ingress have?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What sort of Kubernetes control plane events would be valuable to track in order
    to prevent attackers from altering Pod functionality?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes CVE Database: [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

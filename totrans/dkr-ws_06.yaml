- en: 6\. Introduction to Docker Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to provide you with a concise overview of how container
    networking works, how it differs from networking at the level of the Docker host,
    and how containers can leverage Docker networking to provide direct network connectivity
    to other containerized services. By the end of this chapter, you will know how
    to deploy containers using networking configurations such as `bridge`, `overlay`,
    `macvlan`, and `host`. You will learn the benefits of different networking drivers
    and under which circumstances you should choose certain network drivers. Finally,
    we will look at containerized networking between hosts deployed in a Docker swarm
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this workshop, we have looked at many aspects of containerization
    and microservices architecture in relation to Docker. We have learned about how
    encapsulating applications into microservices that perform discrete functions
    creates an incredibly flexible architecture that enables rapid deployments and
    powerful horizontal scaling. Perhaps one of the more interesting and complex topics
    as it relates to containerization is networking. After all, in order to develop
    a flexible and agile microservices architecture, proper networking considerations
    need to be made to ensure reliable connectivity between container instances.
  prefs: []
  type: TYPE_NORMAL
- en: When referring to **container networking**, always try to keep in mind the difference
    between networking on the container host (underlay networking) and networking
    between containers on the same host or within different clusters (`overlay` networking).
    Docker supports many different types of network configurations out of the box
    that can be customized to suit the needs of your infrastructure and deployment
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a container may have an IP address, unique to that container instance,
    that exists on a virtual subnet between the container hosts. This type of networking
    is typical of a Docker swarm clustered configuration in which network traffic
    gets encrypted and passed over the host machine's network interfaces, only to
    be decrypted on a different host and then passed to the receiving microservice.
    This type of network configuration usually involves Docker maintaining a mapping
    of container and service names to container IP addresses. This provides powerful
    service discovery mechanisms that allow container networking even when containers
    terminate and restart on different cluster hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, containers may run in a more simplistic host networking mode.
    In this scenario, containers running in a cluster or a standalone host expose
    ports on the host machine's network interfaces to send and receive network traffic.
    The containers themselves may still have their IP addresses, which get mapped
    to physical network interfaces on the hosts by Docker. This type of network configuration
    is useful when your microservices need to communicate primarily with services
    that exist outside your containerized infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Docker operates in a **bridge network mode**. A `bridge` network
    creates a single network interface on the host that acts as a bridge to another
    subnet configured on the host. All incoming (ingress) and outgoing (egress) network
    traffic travel between the container subnet and the host using the `bridge` network
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: After installing Docker Engine in a Linux environment, if you run the `ifconfig`
    command, Docker will create a new virtual bridged network interface called `docker0`.
    This interface bridges a Docker private subnet that gets created by default (usually
    `172.16.0.0/16`) to the host machine's networking stack. If a container is running
    in the default Docker network with an IP address of `172.17.8.1` and you attempt
    to contact that IP address, the internal route tables will direct that traffic
    through the `docker0` `bridge` interface and pass the traffic to the IP address
    of the container on the private subnet. Unless ports are published through Docker,
    this container's IP address cannot be accessed by the outside world. Throughout
    this chapter, we will dive deep into various network drivers and configuration
    options provided by Docker.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will look at creating Docker containers in the default
    Docker `bridge` network and how to expose container ports to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.01: Hands-On with Docker Networking'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, when you run a container in Docker, the container instance you create
    will exist in a Docker network. Docker networks are collections of subnets, rules,
    and metadata that Docker uses to allocate network resources to containers running
    in the immediate Docker server or across servers in a Docker swarm cluster. The
    network will provide the container with access to other containers in the same
    subnet, and even outbound (egress) access to other external networks, including
    the internet. Each Docker network is associated with a network driver that determines
    how the network will function within the context of the system the containers
    are running on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, you will run Docker containers and use basic networking to
    run two simple web servers (Apache2 and NGINX) that will expose ports in a few
    different basic networking scenarios. You will then access the exposed ports of
    the container to learn more about how Docker networking works at the most basic
    level. Launching containers and exposing the service ports to make them available
    is one of the most common networking scenarios when first starting with containerized
    infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List the networks that are currently configured in your Docker environment
    using the `docker network ls` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displayed will show all the configured Docker networks available
    on your system. It should resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When creating a container using Docker without specifying a network or networking
    driver, Docker will create the container using a `bridge` network. This network
    exists behind a `bridge` network interface configured in your host OS. Use `ifconfig`
    in a Linux or macOS Bash shell, or `ipconfig` in Windows PowerShell, to see which
    interface the Docker bridge is configured as. It is generally called `docker0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this command will list all the network interfaces available in
    your environment, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Listing the available network interfaces'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1: Listing the available network interfaces'
  prefs: []
  type: TYPE_NORMAL
- en: It can be observed in the preceding figure that the Docker `bridge` interface
    is called `docker0` and has an IP address of `172.17.0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker run` command to create a simple NGINX web server container,
    using the `latest` image tag. Set the container to start in the background using
    the `-d` flag and give it a human-readable name of `webserver1` using the `--name`
    flag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If the command is successful, no output will be returned in the terminal session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the `docker ps` command to check whether the container is up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `webserver1` container is up and running as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `docker inspect` command to check what networking configuration
    this container has by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker will return the verbose details about the running container in JSON
    format. For this exercise, focus on the `NetworkSettings` block. Pay special attention
    to the `Gateway`, `IPAddress`, `Ports`, and `NetworkID` parameters underneath
    the `networks` sub-block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: Output of the docker inspect command'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2: Output of the docker inspect command'
  prefs: []
  type: TYPE_NORMAL
- en: From this output, it can be concluded that this container lives in the default
    Docker `bridge` network. Looking at the first 12 characters of `NetworkID`, you
    will observe that it is the same identifier used in the output of the `docker
    network ls` command, which was executed in *step 1*. It should also be noted that
    the `Gateway` this container is configured to use is the IP address of the `docker0`
    `bridge` interface. Docker will use this interface as an egress point to access
    networks in other subnets outside itself, as well as forwarding traffic from our
    environment to the containers in the subnet. It can also be observed that this
    container has a unique IP address within the Docker bridge network, `172.17.0.2`
    in this example. Our local machine has the ability to route to this subnet since
    we have the `docker0` `bridge` interface available to forward traffic. Finally,
    it can be observed that the NGINX container is by default exposing TCP port `80`
    for incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a web browser, access the `webserver1` container by IP address over port
    `80`. Enter the IP address of the `webserver1` container in your favorite web
    browser:![Figure 6.3: Accessing an NGINX web server container by IP address through'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the default Docker bridge network
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3: Accessing an NGINX web server container by IP address through the
    default Docker bridge network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, use the `curl` command to see similar output, albeit in text
    format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following HTML response indicates that you have received a response from
    the running NGINX container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Accessing the IP address of a container in the local `bridge` subnet works
    well for testing containers locally. To expose your service on the network to
    other users or servers, use the `-p` flag in the `docker run` command. This will
    allow you to map a port on the host to an exposed port on the container. This
    is similar to port forwarding on a router or other network device. To expose a
    container by the port to the outside world, use the `docker run` command followed
    by the `-d` flag to start the container in the background. The `-p` flag will
    enable you to specify a port on the host, separated by a colon and the port on
    the container that you wish to expose. Also, give this container a unique name,
    `webserver2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Upon successful container startup, your shell will not return anything. However,
    certain versions of Docker may show the full container ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `docker ps` command to check whether you have two NGINX containers
    up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The two running containers, `webserver1` and `webserver2`, will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the `PORTS` column, you will see that Docker is now forwarding port `80`
    on the `webserver` container to port `8080` on the host machine. That is deduced
    from the `0.0.0.0:8080->80/tcp` part of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that the host machine port is always to the left
    of the colon, while the container port is to the right when specifying ports with
    the `-p` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your web browser, navigate to `http://localhost:8080` to see the running
    container instance you just spawned:![Figure 6.4: NGINX default page indicating
    that you have successfully forwarded'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a port to your web server container
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.4: NGINX default page indicating that you have successfully forwarded
    a port to your web server container'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you have two NGINX instances running in the same Docker environment with
    slightly different networking configurations. The `webserver1` instance is running
    solely on the Docker network without any ports exposed. Inspect the configuration
    of the `webserver2` instance using the `docker inspect` command followed by the
    container name or ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `NetworkSettings` section at the bottom of the JSON output will resemble
    the following. Pay close attention to the parameters (`Gateway`, `IPAddress`,
    `Ports`, and `NetworkID`) underneath the `networks` sub-block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: Output from the docker inspect command'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.5: Output from the docker inspect command'
  prefs: []
  type: TYPE_NORMAL
- en: As the `docker inspect` output displays, the `webserver2` container has an IP
    address of `172.17.0.3`, whereas your `webserver1` container has an IP address
    of `172.17.0.1`. The IP addresses in your local environment may be slightly different
    depending on how Docker assigns the IP addresses to the containers. Both the containers
    live on the same Docker network (`bridge`) and have the same default gateway,
    which is the `docker0` `bridge` interface on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since both of these containers live on the same subnet, you can test communication
    between the containers within the Docker `bridge` network. Run the `docker exec`
    command to gain access to a shell on the `webserver1` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The prompt should noticeably change to a root prompt, indicating you are now
    in a Bash shell on the `webserver1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At the root shell prompt, use the `apt` package manager to install the `ping`
    utility in this container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The aptitude package manager will then install the `ping` utility in the `webserver1`
    container. Please note that the `apt` package manager will install `ping` as well
    as other dependencies that are required to run the `ping` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: Installing the ping command inside a Docker container'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.6: Installing the ping command inside a Docker container'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `ping` utility has successfully installed, use it to ping the IP address
    of the other container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should display ICMP response packets, indicating that the containers
    can successfully ping each other through the Docker `bridge` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also access the NGINX default web interface using the `curl` command.
    Install `curl` using the `apt` package manager:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output should display, indicating that the `curl` utility and
    all required dependencies are being installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: Installing the curl utility'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.7: Installing the curl utility'
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing `curl`, use it to curl the IP address of `webserver2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the `Welcome to nginx!` page displayed in HTML format, indicating
    that you were able to successfully contact the IP address of the `webserver2`
    container through the Docker `bridge` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Since you are using `curl` to navigate to the NGINX welcome page, it will render
    on your terminal display in raw HTML format.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have successfully spawned two NGINX web server instances
    in the same Docker environment. We configured one instance to not expose any ports
    outside the default Docker network, while we configured the second NGINX instance
    to run on the same network but to expose port `80` to the host system on port
    `8080`. We saw how these containers could be accessed using a standard internet
    web browser as well as by the `curl` utility in Linux.
  prefs: []
  type: TYPE_NORMAL
- en: During this exercise, we also saw how containers can use Docker networks to
    talk to other containers directly. We used the `webserver1` container to call
    the IP address of the `webserver2` container and display the output of the web
    page the container was hosting.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we were also able to demonstrate network connectivity between
    container instances using the native Docker `bridge` network. However, when we
    deploy containers at scale, there is no easy way to know which IP address in the
    Docker network belongs to which container.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at native Docker DNS and learn how to use
    human-readable DNS names to reliably send network traffic to other container instances.
  prefs: []
  type: TYPE_NORMAL
- en: Native Docker DNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest benefits of running a containerized infrastructure is the
    ability to quickly and effortlessly scale your workloads horizontally. Having
    more than one machine in a cluster with a shared `overlay` network between them
    means that you can have many containers running across fleets of servers.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous exercise, Docker gives us the power to allow containers
    to directly talk to other containers in a cluster through the various network
    drivers that Docker provides, such as `bridge`, `macvlan`, and `overlay` drivers.
    In the previous example, we leveraged Docker `bridge` networking to allow containers
    to talk to each other by their respective IP addresses. However, when your containers
    are deployed on real servers, you can't normally rely on containers having consistent
    IP addresses that they can use to talk to each other. Every time a new container
    instance terminates or respawns, Docker will give that container a new IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a traditional infrastructure scenario, we can leverage DNS within
    container networks to give containers a reliable way to communicate with each
    other. By assigning human-readable names to containers within Docker networks,
    users no longer have to look up the IP address each time they want to initiate
    communication between containers on a Docker network. Docker itself will keep
    track of the IP addresses of the containers as they spawn and respawn.
  prefs: []
  type: TYPE_NORMAL
- en: In older legacy versions of Docker, simple DNS resolution was possible by establishing
    links between containers using the `--link` flag in the `docker run` command.
    Using linking, Docker would create an entry in the linked container's `hosts`
    file, which would enable simple name resolution. However, as you will see in the
    upcoming exercise, using links between containers can be slow, not scalable, and
    prone to errors. Recent versions of Docker support a native DNS service between
    containers running on the same Docker network. This allows containers to look
    up the names of other containers running in the same Docker network. The only
    caveat with this approach is that native Docker DNS doesn't work on the default
    Docker `bridge` network; thus, other networks must first be created to build your
    containers in.
  prefs: []
  type: TYPE_NORMAL
- en: For native Docker DNS to work, we must first create a new network using the
    `docker network create` command. We can then create new containers in that network
    using `docker run` with the `--network-alias` flag. In the following exercise,
    we are going to use these commands to learn how native Docker DNS works to enable
    scalable communication between container instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.02: Working with Docker DNS'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following exercise, you will learn about name resolution between Docker
    containers running on the same network. You will first enable simple name resolution
    using the legacy link method. You will contrast this approach by using the newer
    and more reliable native Docker DNS service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create two Alpine Linux containers on the default Docker `bridge` network
    that will communicate with each other using the `--link` flag. Alpine is a very
    good base image for this exercise because it contains the `ping` utility by default.
    This will enable you to quickly test the connectivity between containers in the
    various scenarios. To get started, create a container called `containerlink1`
    to indicate that you have created this container using the legacy link method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This will start a container in the default Docker network called `containerlink1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start another container in the default Docker bridge network, called `containerlink2`,
    which will create a link to `containerlink1` to enable rudimentary DNS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will start a container in the default Docker network called `containerlink2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `docker exec` command to access a shell inside the `containerlink2`
    container. This will allow you to investigate how the link functionality is working.
    Since this container is running Alpine Linux, you do not have access to the Bash
    shell by default. Instead, access it using an `sh` shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This should drop you into a root `sh` shell in the `containerlink2` container.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the shell of the `containerlink2` container, ping `containerlink1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get a reply to the `ping` request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `cat` utility to have a look at the `/etc/hosts` file of the `containerlink2`
    container. The `hosts` file is a list of routable names to IP addresses that Docker
    can maintain and override:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `hosts` file should display and resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: From the output of the `hosts` file of the `containerlink2` container, observe
    that Docker is adding an entry for the `containerlink1` container name as well
    as its container ID. This enables the `containerlink2` container to know the name,
    and the container ID is mapped to the IP address `172.17.0.2`. Typing the `exit`
    command will terminate the `sh` shell session and bring you back to your environment's
    main terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run `docker exec` to access an `sh` shell inside the `containerlink1` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This should drop you into the shell of the `containerlink1` container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ping the `containerlink2` container using the `ping` utility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: It is not possible to ping the `containerlink2` container since linking containers
    only works unidirectionally. The `containerlink1` container has no idea that the
    `containerlink2` container exists since no `hosts` file entry has been created
    in the `containerlink1` container instance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can only link to running containers using the legacy link method between
    containers. This means that the first container cannot link to containers that
    get started later. This is one of the many reasons why using links between containers
    is no longer a recommended approach. We are covering the concept in this chapter
    to show you how the functionality works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the limitations using the legacy link method, Docker also supports native
    DNS using user-created Docker networks. To leverage this functionality, create
    a Docker network called `dnsnet` and deploy two Alpine containers within that
    network. First, use the `docker network create` command to create a new Docker
    network using a `192.168.56.0/24` subnet and using the IP address `192.168.54.1`
    as the default gateway:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the version of Docker you are using, the successful execution of
    this command may return the ID of the network you have created.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Simply using the `docker network create dnsnet` command will create a network
    with a Docker-allocated subnet and gateway. This exercise demonstrates how to
    specify the subnet and gateway for your Docker network. It should also be noted
    that if your computer is attached to a subnet in the `192.168.54.0/24` subnet
    or a subnet that overlaps that space, it may cause network connectivity issues.
    Please use a different subnet for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker network ls` command to list the Docker networks available in
    this environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of Docker networks should be returned, including the `dnsnet` network
    you just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `docker network inspect` command to view the configuration for this
    network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The details of the `dnsnet` network should be displayed. Pay close attention
    to the `Subnet` and `Gateway` parameters. These are the same parameters that you
    used to create a Docker network in *Step 8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8: Output from the docker network inspect command'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.8: Output from the docker network inspect command'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is a Docker `bridge` network, Docker will also create a corresponding
    bridge network interface for this network. The IP address of the `bridge` network
    interface will be the same IP address as the default gateway address you specified
    when creating this network. Use the `ifconfig` command to view the configured
    network interfaces on Linux or macOS. If you are using Windows, use the `ipconfig`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This should display the output of all available network interfaces, including
    the newly created `bridge` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: Analyzing the bridge network interface for the newly created
    Docker network'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.9: Analyzing the bridge network interface for the newly created Docker
    network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that a new Docker network has been created, use the `docker run` command
    to start a new container (`alpinedns1`) within this network. Use the `docker run`
    command with the `--network` flag to specify the `dnsnet` network that was just
    created, and the `--network-alias` flag to give your container a custom DNS name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Upon successful execution of the command, the full container ID should be displayed
    before returning to a normal terminal prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a second container (`alpinedns2`) using the same `--network` and `--network-alias`
    settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand the difference between the `â€“network-alias` flag
    and the `--name` flag. The `--name` flag is used to give the container a human-readable
    name within the Docker API. This makes it easy to start, stop, restart, and manage
    containers by name. The `--network-alias` flag, however, is used to create a custom
    DNS entry for the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker ps` command to verify that the containers are running as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will display the running container instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker inspect` command to verify that the IP addresses of the container
    instances are from within the subnet (`192.168.54.0/24`) that was specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is truncated to show the relevant details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure: 6.10: Output from the Networks section of the alpinedns1 container
    instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure: 6.10: Output from the Networks section of the alpinedns1 container
    instance'
  prefs: []
  type: TYPE_NORMAL
- en: It can be observed from the output that the `alpinedns1` container was deployed
    with an IP address of `192.168.54.2`, which is a part of the subnet that was defined
    during the creation of the Docker network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the `docker network inspect` command in a similar fashion for the `alpinedns2`
    container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is again truncated to display the relevant networking details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11: Output of the Networks section of the alpinedns2 container instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.11: Output of the Networks section of the alpinedns2 container instance'
  prefs: []
  type: TYPE_NORMAL
- en: It can be observed in the preceding output that the `alpinedns2` container has
    an IP address of `192.168.54.3`, which is a different IP address within the `dnsnet`
    subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `docker exec` command to access a shell in the `alpinedns1` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This should drop you into a root shell inside of the containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once inside the `alpinedns1` container, use the `ping` utility to ping the
    `alpinedns2` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ping` output should display successful network connectivity to the `alpinedns2`
    container instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `exit` command to return to your primary terminal. Use the `docker
    exec` command to gain access to a shell inside the `alpinedns2` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This should drop you to a shell within the `alpinedns2` container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `ping` utility to ping the `alpinedns1` container by name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should display successful responses from the `alpinedns1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Docker DNS, as opposed to the legacy link method, allows bidirectional communication
    between containers in the same Docker network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `cat` utility inside any of the `alpinedns` containers to reveal that
    Docker is using true DNS as opposed to `/etc/hosts` file entries inside the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This will reveal the contents of the `/etc/hosts` file inside the respective
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Use the `exit` command to terminate the shell session inside of the `alpinedns2`
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clean up your environment by stopping all running containers using the `docker
    stop` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker system prune -fa` command to clean the remaining stopped containers
    and networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Successfully executing this command should clean up the `dnsnet` network as
    well as the container instances and images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Each section of the system prune output will identify and remove Docker resources
    that are no longer in use. In this case, it will remove the `dnsnet` network since
    no container instances are currently deployed in this network.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you looked at the benefits of using name resolution to enable
    communication between the containers over Docker networks. Using name resolution
    is efficient since applications don't have to worry about the IP addresses of
    the other running containers. Instead, communication can be initiated by simply
    calling the other containers by name.
  prefs: []
  type: TYPE_NORMAL
- en: We first explored the legacy link method of name resolution, by which running
    containers can establish a relationship, leveraging a unidirectional relationship
    using entries in the container's `hosts` file. The second and more modern way
    to use DNS between containers is by creating user-defined Docker networks that
    allow DNS resolution bidirectionally. This will enable all containers on the network
    to resolve all other containers by name or container ID without any additional
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this section, Docker provides many unique ways to provide
    reliable networking resources to container instances, such as enabling routing
    between containers on the same Docker network and native DNS services between
    containers. This is only scratching the surface of the network options that are
    provided by Docker.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about deploying containers using other types
    of networking drivers to truly provide maximum flexibility when deploying containerized
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Native Docker Network Drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Docker is one of the most broadly supported container platforms in recent
    times, the Docker platform has been vetted across numerous production-level networking
    scenarios. To support various types of applications, Docker provides various network
    drivers that enable flexibility in how containers are created and deployed. These
    network drivers allow containerized applications to run in almost any networking
    configuration that is supported directly on bare metal or virtualized servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, containers can be deployed that share the host server''s networking
    stack, or in a configuration that allows them to be assigned unique IP addresses
    from the underlay network infrastructure. In this section, we are going to learn
    about the basic Docker network drivers and how to leverage them to provide the
    maximum compatibility for various types of network infrastructures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bridge`: A `bridge` is the default network that Docker will run containers
    in. If nothing is defined when launching a container instance, Docker will use
    the subnet behind the `docker0` interface, in which containers will be assigned
    an IP address in the `172.17.0.0/16` subnet. In a `bridge` network, containers
    have network connectivity to other containers in the `bridge` subnet as well as
    outbound connectivity to the internet. So far, all containers we have created
    in this chapter have been in `bridge` networks. Docker `bridge` networks are generally
    used for simple TCP services that only expose simple ports or require communication
    with other containers that exist on the same host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host`: Containers running in the `host` networking mode have direct access
    to the host machine''s network stack. This means that any ports that are exposed
    to the container are also exposed to the same ports on the host machine running
    the containers. The container also has visibility of all physical and virtual
    network interfaces running on the host. `host` networking is generally preferred
    when running container instances that consume lots of bandwidth or leverage multiple
    protocols.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`none`: The `none` network provides no network connectivity to containers deployed
    in this network. Container instances that are deployed in the `none` network only
    have a loopback interface and no access to other network resources at all. No
    driver operates this network. Containers deployed using the `none` networking
    mode are usually applications that operate on storage or disk workloads and don''t
    require network connectivity. Containers that are segregated from network connectivity
    for security purposes may also be deployed using this network driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`macvlan`: `macvlan` networks created in Docker are used in scenarios in which
    your containerized application requires a MAC address and direct network connectivity
    to the underlay network. Using a `macvlan` network, Docker will allocate a MAC
    address to your container instance via a physical interface on the host machine.
    This makes your container appear as a physical host on the deployed network segment.
    It should be noted that many cloud environments, such as AWS, Azure, and many
    virtualization hypervisors, do not allow `macvlan` networking to be configured
    on container instances. `macvlan` networks allow Docker to assign containers IP
    addresses and MAC addresses from the underlay networks based on a physical network
    interface attached to the host machine. Using `macvlan` networking can easily
    lead to IP address exhaustion or IP address conflicts if not configured correctly.
    `macvlan` container networks are generally used in very specific network use cases,
    such as applications that monitor network traffic modes or other network-intensive
    workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No conversation on Docker networking would be complete without a brief overview
    of **Docker overlay networking**. `Overlay` networking is how Docker handles networking
    with a swarm cluster. When a Docker cluster is defined between nodes, Docker will
    use the physical network linking the nodes together to define a logical network
    between containers running on the nodes. This allows containers to talk directly
    to each other between cluster nodes. In *Exercise 6.03, Exploring Docker Networks*,
    we will look at the various types of Docker network drivers that are supported
    in Docker by default, such as `host`, `none`, and `macvlan`. In *Exercise 6.04*,
    *Defining Overlay Networks*, we will then define a simple Docker swarm cluster
    to discover how `overlay` networking works between Docker hosts configured in
    a cluster mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.03: Exploring Docker Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will look into the various types of Docker network drivers
    that are supported in Docker by default, such as `host`, `none`, and `macvlan`.
    We will start with the `bridge` network and then look into the `none`, `host`,
    and `macvlan` networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to get an idea of how networking is set up in your Docker environment.
    From a Bash or PowerShell terminal, use the `ifconfig` or `ipconfig` command on
    Windows. This will display all the network interfaces in your Docker environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display all the network interfaces you have available. You should
    see a `bridge` interface called `docker0`. This is the Docker `bridge` interface
    that serves as the entrance (or ingress point) into the default Docker network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12: Example ifconfig output from your Docker development environment'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.12: Example ifconfig output from your Docker development environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker network ls` command to view the networks available in your
    Docker environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This should list the three basic network types defined previously, displaying
    the network ID, the name of the Docker network, and the driver associated with
    the network type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'View the verbose details of these networks using the `docker network inspect`
    command, followed by the ID or the name of the network you want to inspect. In
    this step, you will view the verbose details of the `bridge` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker will display the verbose output of the `bridge` network in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13: Inspecting the default bridge network'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.13: Inspecting the default bridge network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key parameters to note in this output are the `Scope`, `Subnet`, and `Gateway`
    keywords. Based on this output, it can be observed that the scope of this network
    is only the local host machine (`Scope: Local`). This indicates the network is
    not shared between hosts in a Docker swarm cluster. The `Subnet` value of this
    network under the `Config` section is `172.17.0.0/16`, and the `Gateway` address
    for the subnet is an IP address within the defined subnet (`172.17.0.1`). It is
    critical that the `Gateway` value of a subnet is an IP address within that subnet
    to enable containers deployed in that subnet to access other networks outside
    the scope of that network. Finally, this network is tied to the host interface,
    `docker0`, which will serve as the `bridge` interface for the network. The output
    of the `docker network inspect` command can be very helpful in getting a full
    understanding of how containers deployed in that network are expected to behave.'
  prefs: []
  type: TYPE_NORMAL
- en: 'View the verbose details of the `host` network using the `docker network inspect`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the details of the `host` network in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14: docker network inspect output for the host network'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.14: docker network inspect output for the host network'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is not very much configuration present in the `host` network.
    Since it uses the `host` networking driver, all the container's networking will
    be shared with the host. Hence, this network configuration does not need to define
    specific subnets, interfaces, or other metadata, as we have seen in the default
    `bridge` network from before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Investigate the `none` network next. Use the `docker network inspect` command
    to view the details of the `none` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The details will be displayed in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15: docker network inspect output for the none network'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.15: docker network inspect output for the none network'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the `host` network, the `none` network is mostly empty. Since containers
    deployed in this network will have no network connectivity by leveraging the `null`
    driver, there isn't much need for configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the difference between the `none` and `host` networks lies in
    the driver they use, despite the fact that the configurations are almost identical.
    Containers launched in the `none` network have no network connectivity at all,
    and no network interfaces are assigned to the container instance. However, containers
    launched in the `host` network will share the networking stack with the host system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now create a container in the `none` network to observe its operation. In your
    terminal or PowerShell session, use the `docker run` command to start an Alpine
    Linux container in the `none` network using the `--network` flag. Name this container
    `nonenet` so we know that it is deployed in the `none` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This will pull and start an Alpine Linux Docker container in the `none` network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker ps` command to verify whether the container is up and running
    as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should display the `nonenet` container as up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `docker inspect` command, along with the container name, `nonenet`,
    to get a deeper understanding of how this container is configured:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `docker inspect` will display the full container configuration
    in JSON format. A truncated version highlighting the `NetworkSettings` section
    is provided here. Pay close attention to the `IPAddress` and `Gateway` settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16: docker inspect output for the nonenet container'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.16: docker inspect output for the nonenet container'
  prefs: []
  type: TYPE_NORMAL
- en: The `docker inspect` output will reveal that this container does not have an
    IP address, nor does it have a gateway or any other networking settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker exec` command to access an `sh` shell inside this container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful execution of this command, you will be dropped into a root
    shell in the container instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `ip a` command to view the network interfaces available in the
    container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display all network interfaces configured in this container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The only network interface available to this container is its `LOOPBACK` interface.
    As this container is not configured with an IP address or default gateway, common
    networking commands will not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the lack of network connectivity using the `ping` utility provided by
    default in the Alpine Linux Docker image. Try to ping the Google DNS servers located
    at IP address `8.8.8.8`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `ping` command should reveal that it has no network connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Use the `exit` command to return to your main terminal session.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have taken a closer look at the `none` network, consider the `host`
    networking driver. The `host` networking driver in Docker is unique since it doesn't
    have any intermediate interfaces or create any extra subnets. Instead, the `host`
    networking driver shares the networking stack with the host operating system such
    that any network interfaces that are available to the host are also available
    to containers running in `host` mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with running a container in `host` mode, execute `ifconfig`
    if you are running macOS or Linux, or use `ipconfig` if you are running on Windows,
    to take inventory of the network interfaces that are available on the host machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output a list of network interfaces available on your host machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: List of network interfaces configured on the host machine'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.17: List of network interfaces configured on the host machine'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the primary network interface of your host machine is `enp1s0`
    with an IP address of `192.168.122.185`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some versions of Docker Desktop on macOS or Windows may not properly be able
    to start and run containers in `host` network mode or using `macvlan` network
    drivers, due to the dependencies on the Linux kernel to provide many of these
    functionalities. When running these examples on macOS or Windows, you may see
    the network details of the underlying Linux virtual machine running Docker, as
    opposed to the network interfaces available on your macOS or Windows host machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker run` command to start an Alpine Linux container in the `host`
    network. Name it `hostnet1` to tell it apart from the other containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Docker will start this container in the background using the `host` network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker inspect` command to look at the network configuration of the
    `hostnet1` container you just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This will reveal the verbose configuration of the running container, including
    the networking details, in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18: docker inspect output for the hostnet1 container'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.18: docker inspect output for the hostnet1 container'
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the output of the `NetworkSettings` block will look
    a lot like the containers you deployed in the `none` network. In the `host` networking
    mode, Docker will not assign an IP address or gateway to the container instance
    since it shares all network interfaces with the host machine directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `docker exec` to access an `sh` shell inside this container, providing
    the name `hostnet1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: This should drop you into a root shell inside the `hostnet1` container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `hostnet1` container, execute the `ifconfig` command to list which
    network interfaces are available to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The full list of network interfaces available inside of this container should
    be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19: Displaying the available network interfaces inside the hostnet1
    container'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.19: Displaying the available network interfaces inside the hostnet1
    container'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this list of network interfaces is identical to that which you encountered
    when querying the host machine directly. This is because this container and the
    host machine are sharing the network directly. Anything available to the host
    machine will also be available to containers running in `host` network mode.
  prefs: []
  type: TYPE_NORMAL
- en: Use the `exit` command to end the shell session and return to the terminal of
    the host machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To understand more fully how the shared networking model works in Docker, start
    an NGINX container in `host` network mode. The NGINX container automatically exposes
    port `80`, which we previously had to forward to a port on the host machine. Use
    the `docker run` command to start an NGINX container on the host machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: This command will start an NGINX container in the `host` networking mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to `http://localhost:80` using a web browser on the host machine:![Figure
    6.20: Accessing the NGINX default web page of a container'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: running in host networking mode
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.20: Accessing the NGINX default web page of a container running in
    host networking mode'
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to see the NGINX default web page displayed in your web browser.
    It should be noted that the `docker run` command did not explicitly forward or
    expose any ports to the host machine. Since the container is running in `host`
    networking mode, any ports that containers expose by default will be available
    directly on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker run` command to create another NGINX instance in the `host`
    network mode. Call this container `hostnet3` to differentiate it from the other
    two container instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now use the `docker ps -a` command to list all the containers, both in running
    and stopped status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of running containers will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding output, you can see that the `hostnet3` container exited
    and is currently in a stopped state. To understand more fully why this is the
    case, use the `docker logs` command to view the container logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The log output should be displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21: NGINX errors in the hostnet3 container'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.21: NGINX errors in the hostnet3 container'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this second instance of an NGINX container was unable to start
    properly because it was unable to bind to port `80` on the host machine. The reason
    for this is that the `hostnet2` container is already listening on that port.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that containers running in `host` networking mode need to be deployed with
    care and consideration. Without proper planning and architecture, container sprawl
    can lead to a variety of port conflicts across container instances that are running
    on the same machine.
  prefs: []
  type: TYPE_NORMAL
- en: The next type of native Docker network you will investigate is `macvlan`. In
    a `macvlan` network, Docker will allocate a MAC address to a container instance
    to make it appear as a physical host on a particular network segment. It can run
    either in `bridge` mode, which uses a parent `host` network interface to gain
    physical access to the underlay network, or in `802.1Q trunk` mode, which leverages
    a sub-interface that Docker creates on the fly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To begin, create a new network utilizing the `macvlan` Docker network driver
    by specifying a physical interface on your host machine as the parent interface
    using the `docker network create` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Earlier in the `ifconfig` or `ipconfig` output, you saw that the `enp1s0` interface
    is the primary network interface on the machine. Substitute the name of the primary
    network interface of your machine. Since you are using the primary network interface
    of the host machine as the parent, specify the same subnet (or a smaller subnet
    within that space) for the network connectivity of our containers. Use a `192.168.122.0/24`
    subnet here, since it is the same subnet of the primary network interface. Likewise,
    you want to specify the same default gateway as the parent interface. Use the
    same subnet and gateway of your host machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: This command should create a network called `macvlan-net1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker network ls` command to confirm that the network has been created
    and is using the `macvlan` network driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will output all the currently configured networks that are defined
    in your environment. You should see the `macvlan-net1` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the `macvlan` network has been defined in Docker, create a container
    in this network and investigate the network connectivity from the host''s perspective.
    Use the `docker run` command to create another Alpine Linux container named `macvlan1`
    using the `macvlan` network `macvlan-net1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This should start an Alpine Linux container instance called `macvlan1` in the
    background.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker ps -a` command to check and make sure this container instance
    is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'This should reveal that the container named `macvlan1` is up and running as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker inspect` command to investigate the networking configuration
    of this container instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The verbose output of the container configuration should be displayed. The
    following output has been truncated to show only the network settings section
    in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22: The docker network inspect output of the macvlan1 network'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.22: The docker network inspect output of the macvlan1 network'
  prefs: []
  type: TYPE_NORMAL
- en: From this output, you can see that this container instance (similar to containers
    in other networking modes) has both an IP address and a default gateway. It can
    also be concluded that this container also has an OSI Model Layer 2 MAC address
    within the `192.168.122.0/24` network, based on the `MacAddress` parameter under
    the `Networks` subsection. Other hosts within this network segment would believe
    this machine is another physical node living in this subnet, not a container hosted
    inside a node on the subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `docker run` to create a second container instance named `macvlan2` inside
    the `macvlan-net1` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This should start another container instance within the `macvlan-net1` network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `docker inspect` command to see the MAC address of the `macvlan-net2`
    container instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the verbose configuration of the `macvlan2` container instance
    in JSON format, truncated here to only show the relevant networking settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23: docker inspect output for the macvlan2 container'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.23: docker inspect output for the macvlan2 container'
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen in this output that the `macvlan2` container has both a different
    IP address and MAC address from the `macvlan1` container instance. Docker assigns
    different MAC addresses to ensure that Layer 2 conflicts do not arise when many
    containers are using `macvlan` networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `docker exec` command to access an `sh` shell inside this container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: This should drop you into a root session inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `ifconfig` command inside the container to observe that the MAC address
    you saw in the `docker inspect` output on the `macvlan1` container is present
    as the MAC address of the container''s primary network interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'In the details for the `eth0` interface, look at the `HWaddr` parameter. You
    may also note the IP address listed under the `inet addr` parameter, as well as
    the number of bytes transmitted and received by this network interface â€“ `RX bytes`
    (bytes received) and `TX bytes` (bytes transmitted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `arping` utility using the `apk` package manager available in the
    Alpine Linux container. This is a tool used to send `arp` messages to a MAC address
    to check Layer 2 connectivity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The `arping` utility should install inside the `macvlan1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the Layer 3 IP address of the `macvlan2` container instance as the
    primary argument to `arping`. Now, `arping` will automatically look up the MAC
    address and check the Layer 2 connectivity to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The `arping` utility should report back the correct MAC address for the `macvlan2`
    container instance, indicating successful Layer 2 network connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the status of the containers using the `docker ps -a` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: The output of this command should show all the running and stopped container
    instances in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, stop all running containers using `docker stop`, followed by the container
    name or ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Repeat this step for all running containers in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clean up the container images and unused networks using the `docker system
    prune` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: This command will clean up all unused container images, networks, and volumes
    remaining on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we looked at the four default networking drivers available
    by default in Docker: `bridge`, `host`, `macvlan`, and `none`. For each example,
    we explored how the network functions, how containers deployed using these network
    drivers function with the host machine, and how they function with other containers
    on the network.'
  prefs: []
  type: TYPE_NORMAL
- en: The networking capability that Docker exposes by default can be leveraged to
    deploy containers in very advanced networking configurations, as we have seen
    so far. Docker also offers the ability to manage and coordinate container networking
    between hosts in a clustered swarm configuration.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at creating networks that will create overlay
    networks between Docker hosts to ensure direct connectivity between container
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Overlay Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Overlay` networks are logical networks that are created on top of a physical
    (underlay) network for specific purposes. A **Virtual Private Network** (**VPN**),
    for example, is a common type of `overlay` network that uses the internet to create
    a link to another private network. Docker can create and manage `overlay` networks
    between containers, which can be used for containerized applications to directly
    talk to one another. When containers are deployed into an `overlay` network, it
    does not matter which host in the cluster they are deployed on; they will have
    direct connectivity to other containerized services that exist in the same `overlay`
    network in the same way that they would if they existed on the same physical host.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.04: Defining Overlay Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker `overlay` networking is used to create mesh networks between machines
    in a Docker swarm cluster. In this exercise, you will use two machines to create
    a basic Docker swarm cluster. Ideally, these machines will exist on the same networking
    segment to ensure direct network connectivity and fast network connectivity between
    them. Furthermore, they should be running the same version of Docker in a supported
    distribution of Linux, such as RedHat, CentOS, or Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will define `overlay` networks that will span hosts in a Docker swarm cluster.
    You will then ensure that containers deployed on separate hosts can talk to one
    another via the `overlay` network:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This exercise requires access to a secondary machine with Docker installed on
    it. Usually, cloud-based virtual machines or machines deployed in another hypervisor
    work best. Deploying a Docker swarm cluster on your system using Docker Desktop
    could lead to networking issues or serious performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: On the first machine, `Machine1`, run `docker --version` to find out which version
    of Docker is currently running on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '`The version details of the Docker installation of Machine1 will be displayed:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can do the same for `Machine2:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '`The version details of the Docker installation of Machine2 will be displayed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Verify that the installed version of Docker is the same before moving forward.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The Docker version may vary depending on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'On `Machine1`, run the `docker swarm init` command to initialize a Docker swarm
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This should print the command you can use on other nodes to join the Docker
    swarm cluster, including the IP address and `join` token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'On `Machine2`, run the `docker swarm join` command, which was provided by `Machine1`,
    to join the Docker swarm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '`Machine2` should successfully join the Docker swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `docker info` command on both nodes to ensure they have successfully
    joined the swarm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Machine1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '`Machine2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is a truncation of the `swarm` portion of the `docker
    info` output. From these details, you will see that these Docker nodes are configured
    in a swarm cluster and there are two nodes in the cluster with a single manager
    node (`Machine1`). These parameters should be identical on both nodes, except
    for the `Is Manager` parameter, for which `Machine1` will be the manager. By default,
    Docker will allocate a default subnet of `10.0.0.0/8` for the default Docker swarm
    `overlay` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'From the `Machine1` box, create an `overlay` network using the `docker network
    create` command. Since this is a network that will span more than one node in
    a simple swarm cluster, specify the `overlay` driver as the network driver. Call
    this network `overlaynet1`. Use a subnet and gateway that are not yet in use by
    any networks on your Docker hosts to avoid subnet collisions. Use `172.45.0.0/16`
    and `172.45.0.1` as the gateway:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The `overlay` network will be created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker network ls` command to verify whether the network was created
    successfully and is using the correct `overlay` driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'A list of networks available on your Docker host will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker service create` command to create a service that will span
    multiple nodes in the swarm cluster. Deploying containers as services allow you
    to specify more than one replica of a container instance for horizontal scaling
    or scaling container instances across nodes in a cluster for high availability.
    To keep this example simple, create a single container service of Alpine Linux.
    Name this service `alpine-overlay1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'A text-based progress bar will display the progress of the `alpine-overlay1`
    service deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeat the same `docker service create` command, but now specify `alpine-overlay2`
    as the service name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'A text-based progress bar will again display the progress of the service deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: More details on creating services in Docker swarm can be found in *Chapter 9,
    Docker Swarm*. As the scope of this exercise is networking, we will focus for
    now on the networking component.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `Machine1` node, execute the `docker ps` command to see which service
    is running on this node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'The running containers will be displayed. Docker will intelligently scale containers
    between nodes in a Docker swarm cluster. In this example, the container from the
    `alpine-overlay1` service landed on `Machine1`. Your environment may vary depending
    on how Docker deploys the services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `docker inspect` command to view the verbose details of the running
    container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'The verbose details of the running container instance will be displayed. The
    following output has been truncated to display the `NetworkSettings` portion of
    the `docker inspect` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24: Inspecting the alpine-overlay1 container instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.24: Inspecting the alpine-overlay1 container instance'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the IP address of this container is as expected within the subnet
    you have specified on `Machine1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the `Machine2` instance, execute the `docker network ls` command to view
    the Docker networks available on the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'A list of all available Docker networks will be displayed on the Docker host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `overlaynet1` network defined on `Machine1` is also available on
    `Machine2`. This is because networks created using the `overlay` driver are available
    to all hosts in the Docker swarm cluster. This enables containers to be deployed
    using this network to run across all hosts in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker ps` command to list the running containers on this Docker instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'A list of all running containers will be displayed. In this example, the container
    in the `alpine-overlay2` service landed on the `Machine2` cluster node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Which node the services land on in your example may differ from what is displayed
    here. Docker makes decisions on how to deploy containers based on various criteria,
    such as available CPU bandwidth, memory, and scheduling restrictions placed on
    the deployed containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `docker inspect` to investigate the network configuration of this container
    as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'The verbose container configuration will be displayed. This output has been
    truncated to display the `NetworkSettings` portion of the output in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25: docker inspect output of the alpine-overlay2 container instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.25: docker inspect output of the alpine-overlay2 container instance'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this container also has an IP address within the `overlaynet1` `overlay`
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since both services are deployed within the same `overlay` network but exist
    in two separate hosts, you can see that Docker is using the `underlay` network
    to proxy the traffic for the `overlay` network. Check the network connectivity
    between the services by attempting a ping from one service to the other. It should
    be noted here that, similar to static containers deployed in the same network,
    services deployed on the same network can resolve each other by name using Docker
    DNS. Use the `docker exec` command on the `Machine2` host to access an `sh` shell
    inside the `alpine-overlay2` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'This should drop you into a root shell on the `alpine-overlay2` container instance.
    Use the `ping` command to initiate network communication to the `alpine-overlay1`
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: Notice that even though these containers are deployed across two separate hosts,
    the containers can communicate with each other by name, using the shared `overlay`
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `Machine1` box, you can attempt the same communication to the `alpine-overlay2`
    service container. Use the `docker exec` command to access an `sh` shell on the
    `Machine1` box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'This should drop you into a root shell inside the container. Use the `ping`
    command to initiate communication to the `alpine-overlay2` container instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Notice again that, by using Docker DNS, the IP address of the `alpine-overlay2`
    container can be resolved between hosts using the `overlay` networking driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `docker service rm` command to delete both services from the `Machine1`
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: For each of these commands, the service name will appear briefly indicating
    the command execution was successful. On both nodes, `docker ps` will display
    that no containers are currently running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the `overlaynet1` Docker network by using the `docker rm` command and
    specifying the name `overlaynet1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: The `overlaynet1` network will be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we looked at Docker `overlay` networking between two hosts
    in a Docker swarm cluster. `Overlay` networking is enormously beneficial in a
    Docker container cluster because it allows the horizontal scaling of containers
    between nodes in a cluster. From a network perspective, these containers can directly
    talk to one another by using a service mesh proxied over the physical network
    interfaces of the host machines. This not only reduces latency but simplifies
    deployments by taking advantage of many of Docker's features, such as DNS.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at all the native Docker network types and examples
    of how they function, we can look at another aspect of Docker networking that
    has recently been gaining popularity. Since Docker networking is very modular,
    as we have seen, Docker supports a plugin system that allows users to deploy and
    manage custom network drivers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about how non-native Docker networks work
    by installing a third-party network driver from Docker Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Native Docker Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the final section of this chapter, we will discuss non-native Docker networks.
    Aside from the native Docker network drivers that are available, Docker also supports
    custom networking drivers that can be written by users or downloaded from third
    parties via Docker Hub. Custom third-party network drivers are useful in circumstances
    that require very particular network configurations, or where container networking
    is expected to behave in a certain way. For example, some network drivers provide
    the ability for users to set custom policies regarding access to internet resources,
    or other defining whitelists for communication between containerized applications.
    This can be helpful from a security, policy, and auditing perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will download and install the Weave Net driver
    and create a network on a Docker host. Weave Net is a highly supported third-party
    network driver that provides excellent visibility into container mesh networks,
    allowing users to create complex service mesh infrastructures that can span multi-cloud
    scenarios. We will install the Weave Net driver from Docker Hub and configure
    a basic network in the simple swarm cluster we defined in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.05: Installing and Configuring the Weave Net Docker Network Driver'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will download and install the Weave Net Docker network
    driver and deploy it within the Docker swarm cluster you created in the previous
    exercise. Weave Net is one of the most common and flexible third-party Docker
    network drivers available. Using Weave Net, very complex networking configurations
    can be defined to enable maximum flexibility in your infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Weave Net driver from Docker Hub using the `docker plugin install`
    command on the `Machine1` node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'This will prompt you to grant Weave Net permissions on the machine you are
    installing it on. It is safe to grant the requested permissions as Weave Net requires
    them to set up the network driver on the host operating system properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Answer the prompt by pressing the *y* key. The Weave Net plugin should be installed
    successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the `Machine2` node, run the same `docker plugin install` command. All nodes
    in the Docker swarm cluster should have the plugin installed since all nodes will
    be participating in the swarm mesh networking:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'The permissions prompt will be displayed. Respond with *y* when prompted to
    continue the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a network using the `docker network create` command on the `Machine1`
    node. Specify the Weave Net driver as the primary driver and the network name
    as `weavenet1`. For the subnet and gateway parameters, use a unique subnet that
    has not yet been used in the previous exercises:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: This should create a network called `weavenet1` in the Docker swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'List the available networks in the Docker swarm cluster using the `docker network
    ls` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'The `weavenet1` network should be displayed in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `docker network ls` command on the `Machine2` node to ensure that
    the `weavenet1` network is present on that machine as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'The `weavenet1` network should be listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'On the `Machine1` node, create a service called `alpine-weavenet1` that uses
    the `weavenet1` network using the `docker service create` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'A text-based progress bar will display the deployment status of the service.
    It should complete without any issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker service create` command again to create another service in
    the `weavenet1` network called `alpine-weavenet2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'A text-based progress bar will again display indicating the status of the service
    creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `docker ps` command to validate that an Alpine container is successfully
    running on each node in the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Machine1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '`Machine2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the service containers should be up and running on both machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Machine1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '`Machine2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker exec` command to access an `sh` shell inside the `weavenet1.1`
    container instance. Make sure to run this command on the node in the swarm cluster
    that is running this container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'This should drop you into a root shell inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `ifconfig` command to view the network interfaces present inside this
    container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display a newly named network interface called `ethwe0`. A core part
    of Weave Net''s core networking policy is to create custom-named interfaces within
    the container for easy identification and troubleshooting. It should be noted
    this interface is assigned an IP address from the subnet that we provided as a
    configuration parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'From inside this container, ping the `alpine-weavenet2` service by name, using
    the `ping` utility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see responses coming from the resolved IP address of the `alpine-weavenet2`
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to recent updates in the Docker libnetwork stack in recent versions of
    Docker and Docker Swarm, pinging the service by name: `alpine-weavenet2` may not
    work. To demonstrate the network is working as intended, try pinging the name
    of the container directly instead: `alpine-weavenet2.1.z8jpiup8yetjrqca62ub0yz9k`
    â€“ Keep in mind, the name of this container will be different in your lab environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Try pinging Google DNS servers (`8.8.8.8`) on the open internet from these
    containers as well to ensure that these containers have internet access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see responses returning, indicating these containers have internet
    access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `docker service rm` command to remove both services from the `Machine1`
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: This will delete both the services, stopping and removing the container instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the Weave Net network that was created by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: The Weave Net network should be deleted and removed.
  prefs: []
  type: TYPE_NORMAL
- en: In the robust system of containerized networking concepts, Docker has a vast
    array of networking drivers to cover almost any circumstance that your workloads
    demand. However, for all the use cases that lie outside the default Docker networking
    drivers, Docker supports third-party custom drivers for almost any networking
    conditions that may arise. Third-party network drivers allow Docker to have flexible
    integrations with various platforms and even across multiple cloud providers.
    In this exercise, we looked at installing and configuring the Weave Net networking
    plugin and creating simple services in a Docker swarm cluster to leverage this
    network.
  prefs: []
  type: TYPE_NORMAL
- en: In the following activity, you will apply what you have learned in this chapter,
    using the various Docker network drivers, to deploy a multi-container infrastructure
    solution. These containers will communicate using different Docker networking
    drivers on the same hosts and even across multiple hosts in a Docker swarm configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.01: Leveraging Docker Network Drivers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in the chapter, we looked at the various types of Docker network drivers
    and how they all function in different ways to bring various degrees of networking
    capability to deliver functionality in your container environment. In this activity,
    you are going to deploy an example container from the Panoramic Trekking application
    in a Docker `bridge` network. You will then deploy a secondary container in `host`
    networking mode that will serve as a monitoring server and will be able to use
    `curl` to verify that the application is running as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a custom Docker `bridge` network with a custom subnet and gateway IP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy an NGINX web server called `webserver1` in that `bridge` network, exposing
    forwarding port `80` on the container to port `8080` on the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy an Alpine Linux container in `host` networking mode, which will serve
    as a monitoring container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Alpine Linux container to `curl` the NGINX web server and get a response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you connect to both the forwarded port `8080` and the IP address of the
    `webserver1` container directly on port `80` upon completion of the activity,
    you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26: Accessing the NGINX web server from the IP address of the container
    instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B15021_06_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.26: Accessing the NGINX web server from the IP address of the container
    instance'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found via [this link](B15021_Solution_Final_SMP.xhtml#_idTextAnchor331).
  prefs: []
  type: TYPE_NORMAL
- en: In the next activity, we will look at how Docker `overlay` networking can be
    leveraged to provide horizontal scalability for our Panoramic Trekking application.
    By deploying Panoramic Trekking across multiple hosts, we can ensure reliability
    and durability, and make use of system resources from more than one node in our
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.02: Overlay Networking in Action'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you have seen how powerful `overlay` networking is when deploying
    multiple containers between cluster hosts with direct network connectivity between
    them. In this activity, you will revisit the two-node Docker swarm cluster and
    create services from the Panoramic Trekking application that will connect using
    Docker DNS between two hosts. In this scenario, different microservices will be
    running on different Docker swarm hosts but will still be able to leverage the
    Docker `overlay` network to directly communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete this activity successfully, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A Docker `overlay` network using a custom subnet and gateway
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One application Docker swarm service called `trekking-app` using an Alpine Linux
    container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One database Docker swarm service called `database-app` using a PostgreSQL 12
    container (extra credit to supply default credentials)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that the `trekking-app` service can communicate with the `database-app`
    service using `overlay` networking
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `trekking-app` service should be able to communicate with the `database-app`
    service, which can be verified by ICMP replies such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found via [this link](B15021_Solution_Final_SMP.xhtml#_idTextAnchor333).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the many facets of networking in relation to microservices
    and Docker containers. Docker comes equipped with numerous drivers and configuration
    options that users can use to tune the way their container networking works in
    almost any environment. By deploying the correct networks and the correct drivers,
    powerful service mesh networks can quickly be spun up to enable container-to-container
    access without egressing any physical Docker hosts. Containers can even be created
    that will bind to the host networking fabric to take advantage of the underlying
    network infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Quite arguably the most powerful network feature that can be enabled in Docker
    is the ability to create networks across clusters of Docker hosts. This can allow
    us to quickly create and deploy horizontal scaling applications between hosts
    for high availability and redundancy. By leveraging the underlay network, `overlay`
    networks within swarm clusters allow containers to directly contact containers
    running on other cluster hosts by taking advantage of the powerful Docker DNS
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at the next pillar of a powerful containerized
    infrastructure: storage. By understanding how container storage can be utilized
    for stateful applications, extremely powerful solutions can be architected that
    involve not only containerized stateless applications, but containerized database
    services that can be deployed, scaled, and optimized as easily as other containers
    across your infrastructure.'
  prefs: []
  type: TYPE_NORMAL

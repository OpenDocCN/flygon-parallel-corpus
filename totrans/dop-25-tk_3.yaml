- en: Collecting and Querying Metrics and Sending Alerts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和查询指标并发送警报
- en: Insufficient facts always invite danger.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 不充分的事实总是会引发危险。
- en: '- *Spock*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- *斯波克*'
- en: So far, we explored how to leverage some of Kubernetes core features. We used
    HorizontalPodAutoscaler and Cluster Autoscaler. While the former relies on Metrics
    Server, the latter is not based on metrics, but on Scheduler's inability to place
    Pods within the existing cluster capacity. Even though Metrics Server does provide
    some basic metrics, we are in desperate need for more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了如何利用一些Kubernetes核心功能。我们使用了HorizontalPodAutoscaler和Cluster Autoscaler。前者依赖于度量服务器，而后者不是基于指标，而是基于调度程序无法将Pod放置在现有集群容量内。尽管度量服务器确实提供了一些基本指标，但我们迫切需要更多。
- en: We have to be able to monitor our cluster and Metrics Server is just not enough.
    It contains a limited amount of metrics, it keeps them for a very short period,
    and it does not allow us to execute anything but simplest queries. I can't say
    that we are blind if we rely only on Metrics Server, but that we are severely
    impaired. Without increasing the number of metrics we're collecting, as well as
    their retention, we get only a glimpse into what's going on in our Kubernetes
    clusters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须能够监视我们的集群，而度量服务器并不足够。它包含有限数量的指标，它们保存的时间很短，而且它不允许我们执行除了最简单的查询之外的任何操作。如果我们只依赖度量服务器，我不能说我们是盲目的，但我们受到严重的影响。如果我们不增加收集的指标数量以及它们的保留时间，我们只能对我们的Kubernetes集群中发生的情况有一瞥。
- en: Being able to fetch and store metrics cannot be the goal by itself. We also
    need to be able to query them in search for a cause of an issue. For that, we
    need metrics to be "rich" with information, and we need a powerful query language.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 能够获取和存储指标本身并不是目标。我们还需要能够查询它们以寻找问题的原因。为此，我们需要指标“丰富”的信息，以及强大的查询语言。
- en: Finally, being able to find the cause of a problem is not worth much without
    being able to be notified that there is an issue in the first place. That means
    that we need a system that will allow us to define alerts that, when certain thresholds
    are reached, will send us notifications or, when appropriate, send them to other
    parts of the system that can automatically execute steps that will remedy issues.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，能够找到问题的原因没有多大意义，如果不能首先被通知存在问题。这意味着我们需要一个系统，可以让我们定义警报，当达到一定阈值时，会向我们发送通知，或者在适当时将它们发送到系统的其他部分，可以自动执行解决问题的步骤。
- en: If we accomplish that, we'll be a step closer to having not only a self-healing
    (Kubernetes already does that) but also a self-adaptive system that will react
    to changed conditions. We might go even further and try to predict that "bad things"
    will happen in the future and be proactive in resolving them before they even
    arise.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们做到了这一点，我们将更接近于拥有不仅自我修复（Kubernetes已经做到了），而且还会对变化的条件做出反应的自适应系统。我们甚至可以进一步尝试预测未来会发生“坏事”，并在它们出现之前积极解决它们。
- en: All in all, we need a tool, or a set of tools, that will allow us to fetch and
    store "rich" metrics, that will allow us to query them, and that will notify us
    when an issue happens or, even better, when a problem is about to occur.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，我们需要一个工具，或一组工具，可以让我们获取和存储“丰富”的指标，可以让我们查询它们，并且在出现问题时通知我们，甚至更好的是，在问题即将发生时通知我们。
- en: We might not be able to build a self-adapting system in this chapter, but we
    can try to create a foundation. But, first things first, we need a cluster that
    will allow us to "play" with some new tools and concepts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们可能无法构建一个自适应系统，但我们可以尝试创建一个基础。但首先，我们需要一个集群，让我们可以“玩”一些新的工具和概念。
- en: Creating a cluster
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个集群
- en: We'll continue using definitions from the `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository. To be on the safe side, we'll pull the latest version first.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9))
    Gist.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, we'll need a few things that were not requirements before,
    even though you probably already used them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: We'll start using UIs so we'll need NGINX Ingress Controller that will route
    traffic from outside the cluster. We'll also need environment variable `LB_IP`
    with the IP through which we can access worker nodes. We'll use it to configure
    a few Ingress resources.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The Gists used to test the examples in this chapters are below. Please use them
    as they are, or as inspiration to create your own cluster or to confirm whether
    the one you already have meets the requirements. Due to new requirements (Ingress
    and `LB_IP`), all the cluster setup Gists are new.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: A note to Docker for Desktop users You'll notice `LB_IP=[...]` command at the
    end of the Gist. You'll have to replace `[...]` with the IP of your cluster. Probably
    the easiest way to find it is through the `ifconfig` command. Just remember that
    it cannot be `localhost`, but the IP of your laptop (for example, `192.168.0.152)`.A
    note to minikube and Docker for Desktop users We have to increase memory to 3
    GB. Please have that in mind in case you were planning only to skim through the
    Gist that matches your Kubernetes flavor.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The Gists are as follows.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '`gke-monitor.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4](https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4)).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eks-monitor.sh`: **EKS** with 3 t2.small worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, and cluster IP stored in environment variable
    **LB_IP** ([https://gist.github.com/vfarcic/211f8dbe204131f8109f417605dbddd5](https://gist.github.com/vfarcic/211f8dbe204131f8109f417605dbddd5)).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aks-monitor.sh`: **AKS** with 3 Standard_B2s worker nodes, **nginx Ingress**,
    and **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/5fe5c238047db39cb002cdfdadcfbad2](https://gist.github.com/vfarcic/5fe5c238047db39cb002cdfdadcfbad2)).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aks-monitor.sh`：**AKS**带有3个Standard_B2s工作节点，**nginx Ingress**，**tiller**，并且集群IP存储在环境变量**LB_IP**中([https://gist.github.com/vfarcic/5fe5c238047db39cb002cdfdadcfbad2](https://gist.github.com/vfarcic/5fe5c238047db39cb002cdfdadcfbad2))。'
- en: '`docker-monitor.sh`: **Docker for Desktop** with **2 CPUs**, **3 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, and cluster IP stored in environment
    variable **LB_IP** ([https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13](https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13)).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-monitor.sh`：**Docker for Desktop**，带有**2个CPU**，**3GB RAM**，**nginx
    Ingress**，**tiller**，**Metrics Server**，并且集群IP存储在环境变量**LB_IP**中([https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13](https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13))。'
- en: '`minikube-monitor.sh`: **minikube** with **2 CPUs**, **3 GB RAM**, **ingress**,
    **storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248](https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248)).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minikube-monitor.sh`：**minikube**带有**2个CPU**，**3GB RAM**，**ingress**，**storage-provisioner**，**default-storageclass**，并且启用了**metrics-server**附加组件，**tiller**，并且集群IP存储在环境变量**LB_IP**中([https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248](https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248))。'
- en: Now that we have a cluster, we'll need to choose the tools we'll use to accomplish
    our goals.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个集群，我们需要选择我们将用来实现我们目标的工具。
- en: Choosing the tools for storing and querying metrics and alerting
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择存储和查询指标以及警报的工具
- en: '**HorizontalPodAutoscaler** (**HPA**) and **Cluster Autoscaler** (**CA**) provide
    essential, yet very rudimentary mechanisms to scale our Pods and clusters.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**HorizontalPodAutoscaler** (**HPA**)和**Cluster Autoscaler** (**CA**)提供了必要但非常基本的机制来扩展我们的Pods和集群。'
- en: While they do scaling decently well, they do not solve our need to be alerted
    when there's something wrong, nor do they provide enough information required
    to find the cause of an issue. We'll need to expand our setup with additional
    tools that will allow us to store and query metrics as well as to receive notifications
    when there is an issue.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们可以很好地进行扩展，但它们并不能解决我们在出现问题时需要接收警报的需求，也不能提供足够的信息来找到问题的原因。我们需要通过额外的工具来扩展我们的设置，这些工具将允许我们存储和查询指标，并在出现问题时接收通知。
- en: If we focus on tools that we can install and manage ourselves, there is very
    little doubt about what to use. If we look at the list of *Cloud Native Computing
    Foundation (CNCF)* projects ([https://www.cncf.io/projects/](https://www.cncf.io/projects/)),
    only two graduated so far (October 2018). Those are *Kubernetes* and *Prometheus*
    ([https://prometheus.io/](https://prometheus.io/)). Given that we are looking
    for a tool that will allow us to store and query metrics and that Prometheus fulfills
    that need, the choice is straightforward. That is not to say that there are no
    other similar tools worth considering. There are, but they are all service based.
    We might explore them later but, for now, we're focused on those that we can run
    inside our cluster. So, we'll add Prometheus to the mix and try to answer a simple
    question. What is Prometheus?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们专注于可以安装和管理的工具，那么我们对使用什么工具几乎没有疑问。如果我们看一下*Cloud Native Computing Foundation
    (CNCF)*项目列表([https://www.cncf.io/projects/](https://www.cncf.io/projects/))，到目前为止只有两个项目已经毕业（2018年10月）。它们分别是*Kubernetes*和*Prometheus*([https://prometheus.io/](https://prometheus.io/))。考虑到我们正在寻找一个可以存储和查询指标的工具，而Prometheus满足了这一需求，选择就很明显了。这并不是说没有其他值得考虑的类似工具。有，但它们都是基于服务的。我们以后可能会探索它们，但现在，我们专注于那些可以在我们的集群内运行的工具。因此，我们将把Prometheus加入到混合中，并尝试回答一个简单的问题。Prometheus是什么？
- en: Prometheus is a database (of sorts) designed to fetch (pull) and store highly
    dimensional time series data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus是一个（某种程度上的）数据库，旨在获取（拉取）和存储高维时间序列数据。
- en: Time series are identified by a metric name and a set of key-value pairs. Data
    is stored both in memory and on disk. Former allows fast retrieval of information,
    while the latter exists for fault tolerance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列由指标名称和一组键值对标识。数据既存储在内存中，也存储在磁盘上。前者可以快速检索信息，而后者存在是为了容错。
- en: Prometheus' query language allows us to easily find data that can be used both
    for graphs and, more importantly, for alerting. It does not attempt to provide
    "great" visualization experience. For that, it integrates with *Grafana* ([https://grafana.com/](https://grafana.com/)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus的查询语言使我们能够轻松找到可用于图表和更重要的警报的数据。它并不试图提供“出色”的可视化体验。为此，它与*Grafana*（[https://grafana.com/](https://grafana.com/)）集成。
- en: Unlike most other similar tools, we do not push data to Prometheus. Or, to be
    more precise, that is not the common way of getting metrics. Instead, Prometheus
    is a pull-based system that periodically fetches metrics from exporters. There
    are many third-party exporters we can use. But, in our case, the most crucial
    exporter is baked into Kubernetes. Prometheus can pull data from an exporter that
    transforms information from Kube API. Through it, we can fetch (almost) everything
    we might need. Or, at least, that's where the bulk of the information will be
    coming from.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数其他类似工具不同，我们不会将数据推送到Prometheus。或者更准确地说，这不是获取指标的常见方式。相反，Prometheus是一个基于拉取的系统，定期从导出器中获取指标。我们可以使用许多第三方导出器。但是，在我们的情况下，最关键的导出器已经内置到Kubernetes中。Prometheus可以从一个将信息从Kube
    API转换的导出器中拉取数据。通过它，我们可以获取（几乎）我们可能需要的所有信息。或者至少，这就是大部分信息将来自的地方。
- en: Finally, storing metrics in Prometheus would not be of much use if we are not
    notified when there's something wrong. Even when we do integrate Prometheus with
    Grafana, that will only provide us with dashboards. I assume that you have better
    things to do than to stare at colorful graphs. So, we'll need a way to send alerts
    from Prometheus to, let's say, Slack. Luckily, *Alertmanager* ([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))
    allows us just that. It is a separate application maintained by the same community.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们在出现问题时没有得到通知，将在Prometheus中存储的指标没有太大用处。即使我们将Prometheus与Grafana集成，那也只会为我们提供仪表板。我假设你有更重要的事情要做，而不是盯着五颜六色的图表。因此，我们需要一种方式将来自Prometheus的警报发送到Slack，比如说。幸运的是，*Alertmanager*（[https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/)）允许我们做到这一点。这是一个由同一个社区维护的独立应用程序。
- en: We'll see how all those pieces fit together through hands-on exercises. So,
    let's get going and install Prometheus, Alertmanager, and a few other applications.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过实际操作来看看所有这些部分是如何组合在一起的。所以，让我们开始安装Prometheus、Alertmanager和其他一些应用程序。
- en: A quick introduction to Prometheus and Alertmanager
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对Prometheus和Alertmanager的快速介绍
- en: We'll continue the trend of using Helm as the installation mechanism. Prometheus'
    Helm Chart is maintained as one of the official Charts. You can find more info
    in the project's *README* ([https://github.com/helm/charts/tree/master/stable/prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)).
    If you focus on the variables in the *Configuration section* ([https://github.com/helm/charts/tree/master/stable/prometheus#configuration](https://github.com/helm/charts/tree/master/stable/prometheus#configuration)),
    you'll notice that there are quite a few things we can tweak. We won't go through
    all the variables. You can check the official documentation for that. Instead,
    we'll start with a basic setup, and extend it as our needs increase.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用Helm作为安装机制。Prometheus的Helm Chart是作为官方Chart之一进行维护的。您可以在项目的*README*中找到更多信息（[https://github.com/helm/charts/tree/master/stable/prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)）。如果您关注*配置部分*中的变量（[https://github.com/helm/charts/tree/master/stable/prometheus#configuration](https://github.com/helm/charts/tree/master/stable/prometheus#configuration)），您会注意到有很多东西可以调整。我们不会遍历所有变量。您可以查看官方文档。相反，我们将从基本设置开始，并随着我们的需求增加而扩展。
- en: Let's take a look at the variables we'll use as a start.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们将作为起点使用的变量。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output is as follows.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: All we're doing for now is defining `resources` for all five applications we'll
    install, as well as enabling Ingress with a few annotations that will make sure
    that we are not redirected to HTTPS version since we do not have certificates
    for our ad-hoc domains. We'll dive into the applications that'll be installed
    later. For now, we'll define the addresses for Prometheus and Alertmanager UIs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们所做的一切都是为我们将安装的所有五个应用程序定义`资源`，以及使用一些注释启用Ingress，这些注释将确保我们不会被重定向到HTTPS版本，因为我们没有我们的临时域的证书。我们将在稍后深入研究将要安装的应用程序。目前，我们将定义Prometheus和Alertmanager
    UI的地址。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's install the Chart.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装图表。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The command we just executed should be self-explanatory, so we'll jump into
    the relevant parts of the output.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚执行的命令应该是不言自明的，所以我们将跳转到输出的相关部分。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that the Chart installed one DeamonSet and four Deployments.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，图表安装了一个DeamonSet和四个部署。
- en: The DeamonSet is Node Exporter, and it'll run a Pod on every node of the cluster.
    It provides node-specific metrics that will be pulled by Prometheus. The second
    exporter (Kube State Metrics) runs as a single replica Deployment. It fetches
    data from Kube API and transforms them into the Prometheus-friendly format. The
    two will provide most of the metrics we'll need. Later on, we might choose to
    expand them with additional exporters. For now, those two together with metrics
    fetched directly from Kube API should provide more metrics than we can absorb
    in a single chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DeamonSet是Node Exporter，它将在集群的每个节点上运行一个Pod。它提供特定于节点的指标，这些指标将被Prometheus拉取。第二个导出器（Kube
    State Metrics）作为单个副本部署运行。它从Kube API获取数据，并将其转换为Prometheus友好的格式。这两个将提供我们所需的大部分指标。稍后，我们可能选择使用其他导出器来扩展它们。目前，这两个连同直接从Kube
    API获取的指标应该提供比我们在单个章节中能吸收的更多的指标。
- en: Further on, we got the Server, which is Prometheus itself. Alertmanager will
    forward alerts to their destination. Finally, there is Pushgateway that we might
    explore in one of the following chapters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有服务器，即Prometheus本身。Alertmanager将警报转发到它们的目的地。最后，还有Pushgateway，我们可能会在接下来的章节中探索它。
- en: While waiting for all those apps to become operational, we might explore the
    flow between them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在等待所有这些应用程序变得可操作时，我们可以探索它们之间的流程。
- en: Prometheus Server pulls data from exporters. In our case, those are Node Exporter
    and Kube State Metrics. The job of those exporters is to fetch data from the source
    and transform it into the Prometheus-friendly format. Node Exporter gets the data
    from `/proc` and `/sys` volumes mounted on the nodes, while Kube State Metrics
    gets it from Kube API. Metrics are stored internally in Prometheus.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus服务器从出口商那里获取数据。在我们的情况下，这些是Node Exporter和Kube State Metrics。这些出口商的工作是从源获取数据并将其转换为Prometheus友好的格式。Node
    Exporter从节点上挂载的`/proc`和`/sys`卷获取数据，而Kube State Metrics从Kube API获取数据。指标在Prometheus内部存储。
- en: Apart from being able to query that data, we can define alerts. When an alert
    reaches its threshold, it is forwarded to Alertmanager that acts as a crossroad.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能够查询这些数据，我们还可以定义警报。当警报达到阈值时，它将被转发到充当十字路口的Alertmanager。
- en: Depending on its internal rules, it can forward those alerts further to various
    destinations like Slack, email, and HipChat (only to name a few).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其内部规则，它可以将这些警报进一步转发到各种目的地，如Slack、电子邮件和HipChat（仅举几例）。
- en: '![](assets/701f20e3-39b9-495f-b689-ccf64772ece1.png)Figure 3-1: The flow of
    data to and from Prometheus (arrows indicate the direction)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/701f20e3-39b9-495f-b689-ccf64772ece1.png)图3-1：数据流向和从Prometheus流向的流程（箭头表示方向）'
- en: By now, Prometheus Server probably rolled out. We'll confirm that just in case.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Prometheus服务器可能已经推出。我们会确认一下以防万一。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's take a look at what is inside the Pod created through the `prometheus-server`
    Deployment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看通过`prometheus-server`部署创建的Pod内部有什么。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Besides the container based on the `prom/prometheus` image, we got another one
    created from `jimmidyson/configmap-reload`. The job of the latter is to reload
    Prometheus whenever we change the configuration stored in a ConfigMap.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于`prom/prometheus`镜像的容器外，我们还从`jimmidyson/configmap-reload`创建了另一个容器。后者的工作是在我们更改存储在ConfigMap中的配置时重新加载Prometheus。
- en: Next, we might want to take a look at the `prometheus-server` ConfigMap, since
    it stores all the configuration Prometheus needs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可能想看一下`prometheus-server` ConfigMap，因为它存储了Prometheus所需的所有配置。
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that the `alerts` are still empty. We'll change that soon.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`alerts`仍然是空的。我们很快会改变这一点。
- en: Further down is the `prometheus.yml` config with `scrape_configs` taking most
    of the space. We could spend a whole chapter explaining the current config and
    the ways we could modify it. We will not do that because the config in front of
    you is bordering insanity. It's the prime example of how something can be made
    more complicated than it should be. In most cases, you should keep it as-is. If
    you do want to fiddle with it, please consult the official documentation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更下面是`prometheus.yml`配置，其中`scrape_configs`占据了大部分空间。我们可以花一个章节的时间来解释当前的配置以及我们可以修改它的方式。我们不会这样做，因为你面前的配置接近疯狂。这是如何使事情变得比应该更复杂的最佳例子。在大多数情况下，您应该保持不变。如果您确实想要玩弄它，请咨询官方文档。
- en: Next, we'll take a quick look at Prometheus' screens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将快速查看Prometheus的屏幕。
- en: A note to Windows users Git Bash might not be able to use the `open` command.
    If that's the case, replace `open` with `echo`. As a result, you'll get the full
    address that should be opened directly in your browser of choice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows用户，Git Bash可能无法使用`open`命令。如果是这种情况，请用`echo`替换`open`。结果，您将获得应直接在您选择的浏览器中打开的完整地址。
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The config screen reflects the same information we already saw in the `prometheus-server`
    ConfigMap, so we'll move on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 配置屏幕反映了我们已经在`prometheus-server` ConfigMap中看到的相同信息，所以我们将继续。
- en: Next, let's take a look at the targets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看看这些目标。
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That screen contains seven targets, each providing different metrics. Prometheus
    is periodically pulling data from those targets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该屏幕包含七个目标，每个目标提供不同的指标。Prometheus定期从这些目标中拉取数据。
- en: 'All the outputs and screenshots in this chapter are taken from AKS. You might
    see some differences depending on your Kubernetes flavor.You might notice that
    this chapter contains much more screenshots than any other. Even though it might
    look like there are too many, I wanted to make sure that you can compare your
    results with mine, since there will be inevitable differences that might sometimes
    look confusing if you do not have a reference (my screenshots).![](assets/f207a763-f021-4f45-966b-948bae855230.png)Figure
    3-2: Prometheus'' targets screenA note to AKS users The *kubernetes-apiservers*
    target might be red indicating that Prometheus cannot connect to it. That''s OK
    since we won''t use its metrics.A note to minikube users The *kubernetes-service-endpoints*
    target might have a few sources in red. There''s no reason for alarm. Those are
    not reachable, but that won''t affect our exercises.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有输出和截图都来自AKS。根据您的Kubernetes版本，可能会看到一些差异。您可能会注意到，本章包含的截图比其他章节多得多。尽管看起来可能有点多，但我想确保您可以将您的结果与我的进行比较，因为不可避免地会有一些差异，有时可能会让人感到困惑，如果没有参考（我的截图）的话。![](assets/f207a763-f021-4f45-966b-948bae855230.png)图3-2：Prometheus的目标屏幕AKS用户注意*kubernetes-apiservers*目标可能是红色的，表示Prometheus无法连接到它。这没关系，因为我们不会使用它的指标。minikube用户注意*kubernetes-service-endpoints*目标可能有一些红色的来源。没有理由担心。这些是不可访问的，但这不会影响我们的练习。
- en: We cannot find out what each of those targets provides from that screen. We'll
    try to query the exporters in the same way as Prometheus pulls them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法从屏幕上找出每个目标提供什么。我们将尝试以与Prometheus拉取它们相同的方式查询导出器。
- en: To do that, we'll need to find out the Services through which we can access
    the exporters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要找出可以访问导出器的服务。
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output, from AKS, is as follows.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 来自AKS的输出如下。
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are interested in `prometheus-kube-state-metrics` and `prometheus-node-exporter`
    since they provide access to data from the exporters we'll use in this chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对“prometheus-kube-state-metrics”和“prometheus-node-exporter”感兴趣，因为它们提供了访问本章中将使用的导出器的数据。
- en: Next, we'll create a temporary Pod through which we'll access the data available
    through the exporters behind those Services.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个临时Pod，通过它我们将访问那些服务后面的导出器提供的数据。
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We created a new Pod based on `appropriate/curl`. That image serves a single
    purpose of providing `curl`. We specified `prometheus-node-exporter:9100/metrics`
    as the command, which is equivalent to running `curl` with that address. As a
    result, a lot of metrics were output. They are all in the same `key/value` format
    with optional labels surrounded by curly braces (`{` and `}`). On top of each
    metric, there is a `HELP` entry that explains its function as well as `TYPE` (for
    example, `gauge`). One of the metrics is as follows.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于“appropriate/curl”创建了一个新的Pod。该镜像只提供“curl”的单一目的。我们指定“prometheus-node-exporter:9100/metrics”作为命令，这相当于使用该地址运行“curl”。结果，输出了大量指标。它们都以相同的“键/值”格式呈现，可选标签用大括号（`{`和`}`）括起来。在每个指标的顶部，都有一个“HELP”条目，解释了其功能以及“TYPE”（例如，“gauge”）。其中一个指标如下。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see that it provides `Memory information field MemTotal_bytes` and that
    the type is `gauge`. Below the `TYPE` is the actual metric with the key (`node_memory_MemTotal_bytes`)
    and value `3.878477824e+09`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它提供了“内存信息字段MemTotal_bytes”，类型为“gauge”。在“TYPE”下面是实际的指标，带有键（`node_memory_MemTotal_bytes`）和值`3.878477824e+09`。
- en: Most of Node Exporter metrics are without labels. So, we'll have to look for
    an example in the `prometheus-kube-state-metrics` exporter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Node Exporter指标都没有标签。因此，我们将不得不在“prometheus-kube-state-metrics”导出器中寻找一个示例。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, the Kube State metrics follow the same pattern as those from
    the Node Exporter. The major difference is that most of them do have labels. An
    example is as follows.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Kube状态指标遵循与节点导出器相同的模式。主要区别在于大多数指标都有标签。一个例子如下。
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: That metric represents the time the Deployment `prometheus-server` was created
    inside the `metrics` Namespace.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标表示在“metrics”命名空间内创建“prometheus-server”部署的时间。
- en: I'll leave it to you to explore those metrics in more detail. We'll use quite
    a few of them soon.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我会让你更详细地探索这些指标。我们很快将使用其中的许多。
- en: For now, just remember that with the combination of the metrics coming from
    the Node Exporter, Kube State Metrics, and those coming from Kubernetes itself,
    we can cover most of our needs. Or, to be more precise, those provide data required
    for most of the basic and common use cases.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需记住，通过来自节点导出器、Kube状态指标以及来自Kubernetes本身的指标的组合，我们可以满足大部分需求。或者更准确地说，这些数据提供了大部分基本和常见用例所需的数据。
- en: Next, we'll take a look at the alerts screen.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看警报屏幕。
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The screen is empty. Do not despair. We'll get back to that screen quite a few
    times. The alerts we'll be increasing as we progress. For now, just remember that's
    where you can find your alerts.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕是空的。不要绝望。我们将会多次返回到那个屏幕。随着我们的进展，警报将会增加。现在，只需记住那里是您可以找到警报的地方。
- en: Finally, we'll open the graph screen.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将打开图形屏幕。
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: That is where you'll spend your time debugging issues you'll discover through
    alerts.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那里是您将花费时间调试通过警报发现的问题的地方。
- en: As our first task, we'll try to retrieve information about our nodes. We'll
    use `kube_node_info` so let's take a look at its description (help) and its type.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的第一个任务，我们将尝试检索有关我们节点的信息。我们将使用“kube_node_info”，所以让我们看一下它的描述（帮助）和类型。
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output, limited to the `HELP` and `TYPE` entries, is as follows.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于“HELP”和“TYPE”条目，如下所示。
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You are likely to see variations between your results and mine. That's normal
    since our clusters probably have different amounts of resources, my bandwidth
    might be different, and so on. In some cases, my alerts will fire, and yours won't,
    or the other way around. I'll do my best to explain my experience and provide
    screenshots that accompany them. You'll have to compare that with what you see
    on your screen.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会看到您的结果与我的结果之间的差异。这是正常的，因为我们的集群可能具有不同数量的资源，我的带宽可能不同，等等。在某些情况下，我的警报会触发，而您的不会，或者反之。我会尽力解释我的经验并提供伴随它们的截图。您将不得不将其与您在屏幕上看到的内容进行比较。
- en: Now, let's try using that metric in Prometheus.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试在Prometheus中使用该指标。
- en: Please type the following query in the expression field.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请在表达式字段中输入以下查询。
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Click the Execute button to retrieve the values of the `kube_node_info` metric.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“执行”按钮以检索“kube_node_info”指标的值。
- en: Unlike previous chapters, the Gist from this one (`03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9)))
    contains not only the commands but also Prometheus expressions. They are all commented
    (with `#`). If you're planning to copy and paste the expressions from the Gist,
    please exclude the comments. Each expression has `# Prometheus expression` comment
    on top to help you identify it. As an example, the one you just executed is written
    in the Gist as follows. `# Prometheus expression` `# kube_node_info`
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往章节不同，这个章节的Gist（`03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9)）不仅包含命令，还包含Prometheus表达式。它们都被注释了（使用`#`）。如果您打算从Gist中复制并粘贴表达式，请排除注释。每个表达式顶部都有`#
    Prometheus expression`的注释，以帮助您识别它。例如，您刚刚执行的表达式在Gist中的写法如下。`# Prometheus expression`
    `# kube_node_info`
- en: If you check the `HELP` entry of the `kube_node_info`, you'll see that it provides
    `information about a cluster node` and that it is a `gauge`. A **gauge** ([https://prometheus.io/docs/concepts/metric_types/#gauge](https://prometheus.io/docs/concepts/metric_types/#gauge))
    is a metric that represents a single numerical value that can arbitrarily go up
    and down.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查`kube_node_info`的`HELP`条目，您会看到它提供了`有关集群节点的信息`，并且它是一个`仪表`。**仪表**([https://prometheus.io/docs/concepts/metric_types/#gauge](https://prometheus.io/docs/concepts/metric_types/#gauge))是表示单个数值的度量，可以任意上升或下降。
- en: That makes sense for information about nodes since their number can increase
    or decrease over time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 关于节点的信息是有道理的，因为它们的数量可能随时间增加或减少。
- en: A Prometheus gauge is a metric that represents a single numerical value that
    can arbitrarily go up and down.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus的仪表是表示单个数值的度量，可以任意上升或下降。
- en: If we focus on the output, you'll notice that there are as many entries as there
    are worker nodes in the cluster. The value (`1`) is useless in this context. Labels,
    on the other hand, can provide some useful information. For example, in my case,
    operating system (`os_image`) is `Ubuntu 16.04.5 LTS`. Through that example, we
    can see that we can use the metrics not only to calculate values (for example,
    available memory) but also to get a glimpse into the specifics of our system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关注输出，您会注意到条目的数量与集群中的工作节点数量相同。在这种情况下，值（`1`）在这种情况下是无用的。另一方面，标签可以提供一些有用的信息。例如，在我的情况下，操作系统（`os_image`）是`Ubuntu
    16.04.5 LTS`。通过这个例子，我们可以看到我们不仅可以使用度量来计算值（例如，可用内存），还可以一窥系统的具体情况。
- en: '![](assets/f2eea74f-64b6-4499-887d-fa8f42a957ef.png)Figure 3-3: Prometheus''
    console output of the kube_node_info metric'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/f2eea74f-64b6-4499-887d-fa8f42a957ef.png)图3-3：Prometheus的控制台输出kube_node_info度量'
- en: Let's see if we can get a more meaningful query by combining that metric with
    one of the Prometheus' functions. We'll `count` the number of worker nodes in
    our cluster. The `count` is one of Prometheus' *aggregation operators* ([https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators](https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以通过将该度量与Prometheus的一个函数结合来获得更有意义的查询。我们将`count`集群中工作节点的数量。`count`是Prometheus的*聚合运算符*之一([https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators](https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators))。
- en: Please execute the expression that follows.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行接下来的表达式。
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output should show the total number of worker nodes in your cluster. In
    my case (AKS) there are `3`. On the first look, that might not be very helpful.
    You might think that you should know without Prometheus how many nodes you have
    in your cluster. But that might not be true. One of the nodes might have failed,
    and it did not recuperate. That is especially true if you're running your cluster
    on-prem without scaling groups. Or maybe Cluster Autoscaler increased or decreased
    the number of nodes. Everything changes over time, either due to failures, through
    human actions, or through a system that adapts itself. No matter the reasons for
    volatility, we might want to be notified when something reaches a threshold. We'll
    use nodes as the first example.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示集群中工作节点的总数。在我的情况下（AKS），有`3`个。乍一看，这可能并不是非常有用。您可能认为，即使没有Prometheus，您也应该知道集群中有多少个节点。但这可能并不正确。其中一个节点可能已经失败，并且没有恢复。如果您在本地运行集群而没有扩展组，这一点尤其正确。或者Cluster
    Autoscaler增加或减少了节点的数量。一切都会随时间而改变，无论是由于故障，人为行为，还是通过自适应的系统。无论波动的原因是什么，当某些情况达到阈值时，我们可能希望得到通知。我们将以节点作为第一个例子。
- en: Our mission is to define an alert that will notify us if there are more than
    three or less than one nodes in the cluster. We'll imagine that those are our
    limits and that we want to know if the lower or the upper thresholds are reached
    due to failures or Cluster Autoscaling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是定义一个警报，如果集群中的节点超过三个或少于一个，将通知我们。我们假设这些是我们的限制，并且我们想知道是由于故障还是集群自动缩放而达到了下限或上限。
- en: We'll take a look at a new definition of the Prometheus Chart's values. Since
    the definition is big and it will grow with time, from now on, we'll only look
    at the differences.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一下Prometheus Chart值的新定义。由于定义很大，并且会随着时间增长，所以从现在开始，我们只会关注其中的差异。
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is as follows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We added a new entry `serverFiles.alerts`. If you check Prometheus' Helm documentation,
    you'll see that it allows us to define alerts (hence the name). Inside it, we're
    using the "standard" Prometheus syntax for defining alerts.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个新条目`serverFiles.alerts`。如果您查看Prometheus的Helm文档，您会发现它允许我们定义警报（因此得名）。在其中，我们使用了“标准”Prometheus语法来定义警报。
- en: Please consult *Alerting Rules documentation* ([https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/))
    for more info about the syntax.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*Alerting Rules documentation* ([https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/))
    以获取有关语法的更多信息。
- en: We defined only one group of rules called `nodes`. Inside it are two `rules`.
    The first one (`TooManyNodes`) will notify us if there are more than `3` nodes
    `for` more than `15` minutes. The other (`TooFewNodes`) will do the opposite.
    It'll tell us if there are no nodes (`<1`) for `15` minutes. Both `rules` have
    `labels` and `annotations` that, for now, serve only informational purposes. Later
    on, we'll see their real usage.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只定义了一个名为`nodes`的规则组。里面有两个`rules`。第一个规则（`TooManyNodes`）会在超过`15`分钟内有超过`3`个节点时通知我们。另一个规则（`TooFewNodes`）则相反，会在`15`分钟内没有节点（`<1`）时通知我们。这两个规则都有`labels`和`annotations`，目前仅用于信息目的。稍后我们会看到它们的真正用途。
- en: Let's upgrade our Prometheus' Chart and see the effect of the new alerts.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们升级我们的Prometheus Chart并查看新警报的效果。
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It'll take a few moments until the new configuration is "discovered" and Prometheus
    is reloaded. After a while, we can open the Prometheus alerts screen and check
    whether we got our first entries.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 新配置被“发现”并重新加载Prometheus需要一些时间。过一会儿，我们可以打开Prometheus警报屏幕，检查是否有我们的第一个条目。
- en: From now on, I won't comment (much) on the need to wait for a while until next
    config is propagated. If what you see on the screen does not coincide with what
    you're expecting, please wait for a while and refresh it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我不会（太多）评论需要等待一段时间直到下一个配置传播的需要。如果您在屏幕上看到的与您期望的不一致，请稍等片刻并刷新一下。
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You should see two alerts.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到两个警报。
- en: Both alerts are green since none evaluates to `true`. Depending on the Kuberentes
    flavor you choose, you either have only one node (for example, Docker for Desktop
    and minikube) or you have three nodes (for example, GKE, EKS, AKS). Since our
    alerts are checking whether we have less than one, or more than three nodes, neither
    of the conditions are met, no matter which Kubernetes flavor you're using.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有一个评估为`true`，所以这两个警报都是绿色的。根据您选择的Kubernetes版本，您可能只有一个节点（例如，Docker for Desktop和minikube），或者有三个节点（例如，GKE，EKS，AKS）。由于我们的警报检查了我们是否有少于一个或多于三个节点，无论您使用哪种Kubernetes版本，都不满足任何条件。
- en: 'If your cluster was not created through one of the Gists provided at the beginning
    of this chapter, then you might have more than three nodes in your cluster, and
    the alert will fire. If that''s the case, I suggest you modify the `mon/prom-values-nodes.yml`
    file to adjust the threshold of the alert.![](assets/c8da3d50-5320-4ddc-b3ed-5175750546d8.png)Figure
    3-4: Prometheus'' alerts screen'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群不是通过本章开头提供的Gists之一创建的，那么您的集群可能有超过三个节点，并且警报将触发。如果是这种情况，我建议您修改`mon/prom-values-nodes.yml`文件以调整警报的阈值。![](assets/c8da3d50-5320-4ddc-b3ed-5175750546d8.png)图3-4：Prometheus的警报屏幕
- en: Seeing inactive alerts is boring, so I want to show you one that fires (becomes
    red). To do that, we can add more nodes to the cluster (unless you're using a
    single node cluster like Docker for Desktop and minikube). However, it would be
    easier to modify the expression of one of the alerts, so that's what we'll do
    next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 看到无效的警报很无聊，所以我想向您展示一个触发的警报（变为红色）。为了做到这一点，我们可以向集群添加更多节点（除非您正在使用像Docker for Desktop和minikube这样的单节点集群）。但是，修改一个警报的表达式会更容易，所以下面我们将这样做。
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is as follows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The new definition changed the condition of the `TooManyNodes` alert to fire
    if there are more than zero nodes. We also changed the `for` statement so that
    we do not need to wait for `15` minutes before the alert fires.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 新的定义将`TooManyNodes`警报的条件更改为如果节点数大于零则触发。我们还修改了`for`语句，这样在警报触发之前我们不需要等待`15`分钟。
- en: Let's upgrade the Chart one more time.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次升级Chart。
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '... and we''ll go back to the alerts screen.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '...然后我们将返回到警报屏幕。'
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A few moments later (don't forget to refresh the screen), the alert will switch
    to the pending state, and the color will change to yellow. That means that the
    conditions for the alert are met (we do have more than zero nodes) but the `for`
    period did not yet expire.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后（不要忘记刷新屏幕），警报将转为挂起状态，颜色将变为黄色。这意味着警报的条件已经满足（我们确实有超过零个节点），但`for`时间段尚未到期。
- en: Wait for a minute (duration of the `for` period) and refresh the screen. The
    alert's state switched to firing and the color changed to red. Prometheus sent
    our first alert.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 等待一分钟（`for`时间段的持续时间）并刷新屏幕。警报状态已切换为触发，并且颜色变为红色。Prometheus发送了我们的第一个警报。
- en: '![](assets/cb890924-63a6-49f3-953b-783f7495ba3b.png)Figure 3-5: Prometheus''
    alerts screen with one of the alerts firing'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/cb890924-63a6-49f3-953b-783f7495ba3b.png)图3-5：Prometheus的警报屏幕，其中一个警报触发'
- en: Where was the alert sent? Prometheus Helm Chart deployed Alertmanager and pre-configured
    Prometheus to send its alerts there. Let's take a look at it's UI.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 警报发送到了哪里？Prometheus Helm Chart部署了Alertmanager，并预先配置了Prometheus将其警报发送到那里。让我们来看看它的UI。
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can see that one alert reached Alertmanager. If we click the + info button
    next to the `TooManyNodes` alert, we'll see the annotations (summary and description)
    as well as the labels (severity).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一个警报已经到达了Alertmanager。如果我们点击`TooManyNodes`警报旁边的+信息按钮，我们将看到注释（摘要和描述）以及标签（严重程度）。
- en: '![](assets/2de2ea86-984f-4529-ad8a-f54a3680104a.png)Figure 3-6: Alertmanager
    UI with one of the alerts expanded'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2de2ea86-984f-4529-ad8a-f54a3680104a.png)图3-6：Alertmanager UI，其中一个警报已展开'
- en: We are likely not going to sit in front of the Alertmanager waiting for issues
    to appear. If that would be our goal, we could just as well wait for the alerts
    in Prometheus.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能不会坐在Alertmanager前等待问题出现。如果这是我们的目标，我们也可以在Prometheus中等待警报。
- en: Displaying alerts is indeed not the reason why we have Alertmanager. It is supposed
    to receive alerts and dispatch them further. It is not doing anything of that
    sort simply because we did not yet define the rules it should use to forward alerts.
    That's our next task.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 显示警报确实不是我们拥有Alertmanager的原因。它应该接收警报并进一步分发它们。它之所以没有做任何这样的事情，只是因为我们还没有定义它应该用来转发警报的规则。这是我们的下一个任务。
- en: We'll take a look at yet another update of the Prometheus Chart values.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一下Prometheus Chart值的另一个更新。
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output is as follows.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: When we apply that definition, we'll add `alertmanager.yml` file to Alertmanager.
    If contains the rules it should use to dispatch alerts. The `route` section contains
    general rules that will be applied to all alerts that do not match one of the
    `routes`. The `group_wait` value makes Alertmanager wait for `10` seconds in case
    additional alerts from the same group arrive. That way, we'll avoid receiving
    multiple alerts of the same type.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用该定义时，我们将向Alertmanager添加`alertmanager.yml`文件。如果包含了它应该用来分发警报的规则。`route`部分包含了将应用于所有不匹配任何一个`routes`的警报的一般规则。`group_wait`值使Alertmanager在同一组的其他警报到达时等待`10`秒。这样，我们将避免接收到相同类型的多个警报。
- en: When the first alert of a group is dispatched, it'll use the value of the `group_interval`
    field (`5m`) before sending the next batch of the new alerts from the same group.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当一组中的第一个警报被发送时，它将在发送同一组的新警报的下一批之前使用`group_interval`字段（`5m`）的值。
- en: The `receiver` field in the `route` section defines the default destination
    of the alerts. Those destinations are defined in the `receivers` section below.
    In our case, we're sending the alerts to the `slack` receiver by default.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`route`部分中的`receiver`字段定义了警报的默认目的地。这些目的地在下面的`receivers`部分中定义。在我们的情况下，我们默认将警报发送到`slack`接收器。'
- en: The `repeat_interval` (set to `3h`) defines the period after which alerts will
    be resent if Alertmanager continues receiving them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`repeat_interval`（设置为`3h`）定义了如果Alertmanager继续接收警报，警报将在之后的时间段内重新发送。'
- en: The `routes` section defines specific rules. Only if none of them match, those
    in the `route` section above will be used. The `routes` section inherits properties
    from above so only those that we define in this section will change. We'll keep
    sending matching `routes` to `slack`, and the only change is the increase of the
    `repeat_interval` from `3h` to `5d`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`routes`部分定义了具体的规则。只有当它们都不匹配时，才会使用上面`route`部分中的规则。`routes`部分继承自上面的属性，因此只有我们在这个部分中定义的规则会改变。我们将继续将匹配的`routes`发送到`slack`，唯一的变化是将`repeat_interval`从`3h`增加到`5d`。'
- en: 'The critical part of the `routes` is the `match` section. It defines filters
    that are used to decide whether an alert is a match or not. In our case, only
    those with the labels `severity: notify` and `frequency: low` will be considered
    a match.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`routes`的关键部分是`match`部分。它定义了用于决定警报是否匹配的过滤器。在我们的情况下，只有那些带有标签`severity: notify`和`frequency:
    low`的警报才会被视为匹配。'
- en: All in all, the alerts with `severity` label set to `notify` and `frequency`
    set to `low` will be resent every five days. All the other alerts will have a
    frequency of three hours.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，带有`severity`标签设置为`notify`和`frequency`设置为`low`的警报将每五天重新发送一次。所有其他警报的频率为三小时。
- en: The last section of our Alertmanager config is `receivers`. We have only one
    receiver named `slack`. Below the `name` is `slack_config`. It contains Slack-specific
    configuration. We could have used `hipchat_config`, `pagerduty_config`, or any
    other of the supported ones. Even if our destination is not one of those, we could
    always fall back to `webhook_config` and send a custom request to the API of our
    tool of choice.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们Alertmanager配置的最后一部分是`receivers`。我们只有一个名为`slack`的接收器。在`name`下面是`slack_config`。它包含特定于Slack的配置。我们可以使用`hipchat_config`，`pagerduty_config`或任何其他受支持的配置。即使我们的目的地不是其中之一，我们也可以始终退回到`webhook_config`并向我们选择的工具的API发送自定义请求。
- en: For the list of all the supported `receivers`, please consult *Alertmanager
    Configuration* page ([https://prometheus.io/docs/alerting/configuration/](https://prometheus.io/docs/alerting/configuration/)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有关所有受支持的`receivers`列表，请参阅*Alertmanager配置*页面 ([https://prometheus.io/docs/alerting/configuration/](https://prometheus.io/docs/alerting/configuration/))。
- en: Inside `slack_configs` section, we have the `api_url` that contains the Slack
    address with the token from one of the rooms in the *devops20* channel.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在`slack_configs`部分中，我们有包含来自*devops20*频道中一个房间令牌的Slack地址的`api_url`。
- en: For information how to general an incoming webhook address for your Slack channel,
    please visit the *Incoming Webhooks* page ([https://api.slack.com/incoming-webhooks](https://api.slack.com/incoming-webhooks)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何为您的Slack频道生成传入Webhook地址的信息，请访问*传入Webhooks*页面 ([https://api.slack.com/incoming-webhooks](https://api.slack.com/incoming-webhooks))。
- en: Next is the `send_resolved` flag. When set to `true`, Alertmanager will send
    notifications not only when an alert is fired, but also when the issue that caused
    it is resolved.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`send_resolved`标志。当设置为`true`时，Alertmanager将在警报触发时发送通知，也会在导致问题解决时发送通知。
- en: We're using `summary` annotation as the `title` of the message, and the `description`
    annotation for the `text`. Both are using *Go Templates* ([https://golang.org/pkg/text/template/](https://golang.org/pkg/text/template/)).
    Those are the same annotations we defined in the Prometheus' alerts.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`summary`注释作为消息的`title`，并使用`description`注释作为`text`。两者都使用*Go模板* ([https://golang.org/pkg/text/template/](https://golang.org/pkg/text/template/))。这些是我们在Prometheus警报中定义的相同注释。
- en: Finally, the `title_link` is set to `http://my-prometheus.com/alerts`. That
    is indeed not the address of your Prometheus UI but, since I could not know in
    advance what will be your domain, I put a non-existing one. Feel free to change
    `my-prometheus.com` to the value of the environment variable `$PROM_ADDR`. Or
    just leave it as-is knowing that if you click the link, it will not take you to
    your Prometheus UI.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`title_link`设置为`http://my-prometheus.com/alerts`。这确实不是您的Prometheus UI的地址，但由于我事先无法知道您的域名是什么，我放了一个不存在的域名。请随意将`my-prometheus.com`更改为环境变量`$PROM_ADDR`的值。或者保持不变，知道如果您单击该链接，它将不会将您带到您的Prometheus
    UI。
- en: Now that we explored Alertmanager configuration, we can proceed and upgrade
    the Chart.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了Alertmanager配置，我们可以继续并升级图表。
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: A few moments later, Alertmanager will be reconfigured, and the next time it
    receives the alert from Prometheus, it'll dispatch it to Slack. We can confirm
    that by visiting the `devops20.slack.com` workspace. If you did not register already,
    please go to [slack.devops20toolkit.com](http://slack.devops20toolkit.com). Once
    you are a member, we can visit the `devops25-tests` channel.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，Alertmanager将被重新配置，下次它从Prometheus接收到警报时，它将将其发送到Slack。我们可以通过访问`devops20.slack.com`工作区来确认。如果您尚未注册，请访问[slack.devops20toolkit.com](http://slack.devops20toolkit.com)。一旦您成为会员，我们可以访问`devops25-tests`频道。
- en: '[PRE37]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You should see the `Cluster increased` notification. Don't get confused if you
    see other messages. You are likely not the only one running the exercises from
    this book.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到`集群增加`通知。如果你看到其他消息，不要感到困惑。你可能不是唯一一个在运行本书练习的人。
- en: '![](assets/d1936f65-8d48-452a-91f0-03da31c126ca.png)Figure 3-7: Slack with
    an alert message received from AlertmanagerSometimes, for reasons I could not
    figure out, Slack receives empty notifications from Alertmanager. For now, I''m
    ignoring the issue out of laziness.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d1936f65-8d48-452a-91f0-03da31c126ca.png)图3-7：从Alertmanager接收到的Slack警报消息有时，由于我无法弄清楚的原因，Slack会收到来自Alertmanager的空通知。目前，我因懒惰而忽略了这个问题。'
- en: Now that we went through the basic usage of Prometheus and Alertmanager, we'll
    take a break from hands-on exercises and discuss the types of metrics we might
    want to use.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Prometheus和Alertmanager的基本用法，我们将暂停一下手动操作，并讨论我们可能想要使用的指标类型。
- en: Which metric types should we use?
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们应该使用哪种指标类型？
- en: If this is the first time you're using Prometheus hooked into metrics from Kube
    API, the sheer amount might be overwhelming. On top of that, consider that the
    configuration excluded many of the metrics offered by Kube API and that we could
    extend the scope even further with additional exporters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你第一次使用连接到Kube API的Prometheus指标，可能会感到压力很大。除此之外，还要考虑到配置排除了Kube API提供的许多指标，并且我们可以通过额外的导出器进一步扩展范围。
- en: While every situation is different and you are likely to need some metrics specific
    to your organization and architecture, there are some guidelines that we should
    follow. In this section, we'll discuss the key metrics. Once you understand them
    through a few examples, you should be able to extend their use to your specific
    use-cases.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每种情况都是不同的，你可能需要一些特定于你的组织和架构的指标，但是有一些指导方针我们应该遵循。在本节中，我们将讨论关键指标。一旦你通过几个例子理解了它们，你应该能够将它们扩展到你特定的用例中。
- en: The four key metrics everyone should utilize are latency, traffic, errors, and
    saturation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人都应该利用的四个关键指标是延迟、流量、错误和饱和度。
- en: Those four metrics been championed by Google **Site Reliability Engineers**
    (**SREs**) as the most fundamental metrics for tracking performance and health
    of a system.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个指标被谷歌的**网站可靠性工程师**（**SREs**）推崇为跟踪系统性能和健康状况的最基本指标。
- en: '**Latency** represents the time it takes to service to respond to a request.
    The focus should not be only on duration but also on distinguishing between the
    latency of successful requests and the latency of failed requests.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟**代表服务响应请求所需的时间。重点不仅应该放在持续时间上，还应该区分成功请求的延迟和失败请求的延迟。'
- en: '**Traffic** is a measure of demand that is being placed on services. An example
    would be the number of HTTP requests per second.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**流量**是对服务所承受的需求的衡量。一个例子是每秒的HTTP请求次数。'
- en: '**Errors** are measured by the rate of requests that fail. Most of the time
    those failures are explicit (for example, HTTP 500 errors) but they can be implicit
    as well (for example, an HTTP 200 response with the body describing that the query
    did not return any results).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误**是由请求失败的速率来衡量的。大多数情况下，这些失败是显式的（例如，HTTP 500错误），但它们也可以是隐式的（例如，一个HTTP 200响应，其中的内容描述了查询没有返回任何结果）。'
- en: '**Saturation** can be described by "fullness" of a service or a system. A typical
    example would be lack of CPU that results in throttling and, consequently, degrades
    the performance of the applications.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**饱和度**可以用来描述服务或系统的“充实程度”。一个典型的例子是缺乏CPU导致节流，从而降低了应用程序的性能。'
- en: Over time, different monitoring methods were developed. We got, for example,
    the **USE** method that states that for every resource, we should check **utilization**,
    **saturation**, and **errors**. Another one is the **RED** method that defines
    **rate**, **errors**, and **duration** as the key metrics. Those and many others
    are similar in their essence and do not differ significantly from SREs demand
    to measure latency, traffic, errors, and saturation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，不同的监控方法被开发出来。例如，我们得到了**USE**方法，该方法规定对于每个资源，我们应该检查**利用率**、**饱和度**和**错误**。另一个是**RED**方法，它将**速率**、**错误**和**持续时间**定义为关键指标。这些和许多其他方法在本质上是相似的，并且与SRE对于测量延迟、流量、错误和饱和度的需求没有明显的区别。
- en: We'll go through each of the four types of measurements described by SREs and
    provide a few examples. We might even extend them with metrics that do not necessarily
    fit into any of the four categories. The first in line is latency.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个讨论SRE描述的四种测量类型，并提供一些示例。我们甚至可能会扩展它们，加入一些不一定适合任何四类的指标。首先是延迟。
- en: Alerting on latency-related issues
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟相关问题的警报
- en: We'll use the `go-demo-5` application to measure latency, so our first step
    is to install it.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`go-demo-5`应用程序来测量延迟，所以我们的第一步是安装它。
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We generated an address that we'll use as Ingress entry-point, and we deployed
    the application using Helm. Now we should wait until it rolls out.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了一个地址，我们将用作Ingress入口点，并使用Helm部署了应用程序。现在我们应该等待直到它完全部署。
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Before we proceed, we'll check whether the application is indeed working correctly
    by sending an HTTP request.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将检查应用程序是否确实通过发送HTTP请求正确工作。
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The output should be the familiar `hello, world!` message.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是熟悉的“hello, world!”消息。
- en: Now, let's see whether we can, for example, get the duration of requests entering
    the system through Ingress.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看是否可以，例如，通过Ingress进入系统的请求的持续时间。
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: If you click on the - insert metrics at cursor - drop-down list, you'll be able
    to browse through all the available metrics. The one we're looking for is `nginx_ingress_controller_request_duration_seconds_bucket`.
    As its name implies, the metric comes from NGINX Ingress Controller, and provide
    request durations in seconds and grouped in buckets.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您点击“在光标处插入指标”下拉列表，您将能够浏览所有可用的指标。我们正在寻找的是`nginx_ingress_controller_request_duration_seconds_bucket`。正如其名称所示，该指标来自NGINX
    Ingress Controller，并提供以秒为单位分组的请求持续时间。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后单击“执行”按钮。
- en: '[PRE42]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this case, seeing the raw values might not be very useful, so please click
    the Graph tab.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，查看原始值可能并不是非常有用，所以请点击“图表”选项卡。
- en: You should see graphs, one for each Ingress. Each is increasing because the
    metric in question is a counter ([https://prometheus.io/docs/concepts/metric_types/#counter](https://prometheus.io/docs/concepts/metric_types/#counter)).
    Its value is growing with each request.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到图表，每个Ingress都有一个。每个图表都在增加，因为所讨论的指标是一个计数器 ([https://prometheus.io/docs/concepts/metric_types/#counter](https://prometheus.io/docs/concepts/metric_types/#counter))。它的值随着每个请求而增加。
- en: A Prometheus counter is a cumulative metric whose value can only increase, or
    be reset to zero on restart.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus计数器是一个累积指标，其值只能增加，或者在重新启动时重置为零。
- en: What we need is to calculate the rate of requests over a period of time. We'll
    accomplish that by combining `sum` and `rate` ([https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()](https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()))
    functions. The former should be self-explanatory.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要计算一段时间内的请求速率。我们将通过结合`sum`和`rate` ([https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()](https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()))
    函数来实现这一点。前者应该是不言自明的。
- en: Prometheus' rate function calculates the per-second average rate of increase
    of the time series in the range vector.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus的速率函数计算了范围向量中时间序列的每秒平均增长率。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击“执行”按钮。
- en: '[PRE43]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The resulting graph shows us the per-second rate of all the requests entering
    the system through Ingress. The rate is calculated based on five minutes intervals.
    If you hover one of the lines, you'll see the additional information like the
    value and the Ingress. The `by` statement allows us to group the results by `ingress`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表向我们显示了通过Ingress进入系统的所有请求的每秒速率。速率是基于五分钟的间隔计算的。如果您将鼠标悬停在其中一条线上，您将看到额外的信息，如值和Ingress。`by`语句允许我们按`ingress`对结果进行分组。
- en: Still, the result by itself is not very useful, so let's redefine our requirement.
    We should be able to find out how many of the requests are slower than 0.25 seconds.
    We cannot do that directly. Instead, we can retrieve all those that are 0.25 second
    or faster.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，单独的结果并不是非常有用，因此让我们重新定义我们的需求。我们应该能够找出有多少请求比0.25秒慢。我们无法直接做到这一点。相反，我们可以检索所有那些0.25秒或更快的请求。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击“执行”按钮。
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: What we really want is to find the percentage of requests that fall into 0.25
    seconds bucket. To accomplish that, we'll get the rate of the requests faster
    than or equal to 0.25 seconds, and divide the result with the rate of all the
    requests.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想要的是找出落入0.25秒区间的请求的百分比。为了实现这一点，我们将获取快于或等于0.25秒的请求的速率，并将结果除以所有请求的速率。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击“执行”按钮。
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You probably won't see much in the graph since we did not yet generate much
    traffic, beyond occasional interaction with Prometheus and Alertmanager and a
    single request we sent to `go-demo-5`. Nevertheless, the few lines you can see
    display the percentage of the requests that responded within 0.25 seconds.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们尚未生成太多流量，您可能在图表中看不到太多内容，除了偶尔与Prometheus和Alertmanager的交互以及我们发送到`go-demo-5`的单个请求。尽管如此，您可以看到的几行显示了响应时间在0.25秒内的请求的百分比。
- en: For now, we are interested only in `go-demo-5` requests, so we'll refine the
    expression further, to limit the results only to `go-demo-5` Ingress.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只对`go-demo-5`的请求感兴趣，因此我们将进一步完善表达式，将结果限制为仅限于`go-demo-5`的Ingress。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击“执行”按钮。
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The graph should be almost empty since we sent only one request. Or, maybe you
    received the `no datapoints found` message. It's time to generate some traffic.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只发送了一个请求，图表应该几乎是空的。或者，您可能收到了“未找到数据点”的消息。现在是时候生成一些流量了。
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We sent thirty requests to `go-demo-5`. The application has a "hidden" feature
    to delay response to a request. Given that we want to generate traffic with random
    response time, we used `DELAY` variable with a random value up to thousand milliseconds.
    Now we can re-run the same query and see whether we can get some more meaningful
    data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`go-demo-5`发送了30个请求。该应用程序具有延迟响应请求的“隐藏”功能。鉴于我们希望生成具有随机响应时间的流量，我们使用了`DELAY`变量，其随机值最多为1000毫秒。现在我们可以重新运行相同的查询，看看是否可以获得一些更有意义的数据。
- en: Please wait for a while until data from new requests is gathered, then type
    the expression that follows (in Prometheus), and press the Execute button.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请等一会儿，直到收集到新请求的数据，然后在Prometheus中输入以下表达式，然后点击“执行”按钮。
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This time, we can see the emergence of a new line. In my case (screenshot following),
    around twenty-five percent of requests have durations that are within 0.25 seconds.
    Or, to put it into different words, around a quarter of the requests are slower
    than expected.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们可以看到新行的出现。在我的情况下（随后的屏幕截图），大约百分之二十五的请求持续时间在0.25秒内。换句话说，大约四分之一的请求比预期慢。
- en: '![](assets/399a7670-e5f9-4753-8c2e-a02ef8ef2241.png)Figure 3-8: Prometheus''
    graph screen with the percentage of requests with 0.25 seconds duration'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/399a7670-e5f9-4753-8c2e-a02ef8ef2241.png)图3-8：带有0.25秒持续时间请求百分比的Prometheus图表屏幕'
- en: Filtering metrics for a specific application (Ingress) is useful when we do
    know that there is a problem and we want to dig further into it. However, we still
    need an alert that will tell us that there is an issue. For that, we'll execute
    a similar query, but this time without limiting the results to a specific application
    (Ingress). We'll also have to define a condition that will fire the alert, so
    we'll set the threshold to ninety-five percent (0.95). Without such a threshold,
    we'd get a notification every time a single request is slow. As a result, we'd
    get swarmed with alarms and would probably start ignoring them soon afterward.
    After all, no system is in danger if a single request is slow, but only if a considerable
    number of them is. In our case, that is five percent of slow requests or, to be
    more precise, less than ninety-five percent of fast requests.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤特定应用程序（Ingress）的指标在我们知道存在问题并希望进一步挖掘时非常有用。但是，我们仍然需要一个警报，告诉我们存在问题。因此，我们将执行类似的查询，但这次不限制结果为特定应用程序（Ingress）。我们还必须定义一个条件来触发警报，因此我们将将阈值设置为百分之九十五（0.95）。如果没有这样的阈值，每次单个请求变慢时我们都会收到通知。结果，我们会被警报淹没，并很快开始忽视它们。毕竟，如果单个请求变慢，系统并不会有危险，只有当有相当数量的请求变慢时才会有危险。在我们的情况下，这是百分之五的慢请求，或者更准确地说，少于百分之九十五的快速请求。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We can see occasional cases when less than ninety-five percent of requests are
    within 0.25 second. In my case (screenshot following), we can see that Prometheus,
    Alertmanager, and `go-demo-5` are occasionally slow.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以偶尔看到少于百分之九十五的请求在0.25秒内。在我的情况下（随后的屏幕截图），我们可以看到Prometheus、Alertmanager和`go-demo-5`偶尔变慢。
- en: '![](assets/35c99ae7-7fd0-453c-bc6f-0d4b25288425.png)Figure 3-9: Prometheus''
    graph screen with the percentage of requests within 0.25 seconds duration and
    limited only to results higher than ninety-five percent'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/35c99ae7-7fd0-453c-bc6f-0d4b25288425.png)图3-9：带有0.25秒持续时间请求百分比的Prometheus图表屏幕，仅限于高于百分之九十五的结果'
- en: The only thing missing is to define an alert based on the previous expression.
    As a result, we should get a notification whenever less than ninety-five percent
    of requests have a duration less than 0.25 seconds.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一缺少的是基于先前表达式定义警报。因此，每当少于百分之九十五的请求持续时间少于0.25秒时，我们应该收到通知。
- en: I prepared an updated set of Prometheus' Chart values, so let's take a look
    at the differences when compared with the one we're using currently.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我准备了一组更新后的Prometheus图表数值，让我们看看与我们当前使用的图表的差异。
- en: '[PRE50]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The output is as follows.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We added a new alert `AppTooSlow`. It'll trigger if the percentage of requests
    with the duration of 0.25 seconds or less is smaller than ninety-five percent
    (`0.95`).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个新的警报`AppTooSlow`。如果持续时间为0.25秒或更短的请求的百分比小于百分之九十五（`0.95`），它将触发。
- en: We also we reverted the threshold of the `TooManyNodes` back to its original
    value of `3`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将`TooManyNodes`的阈值恢复为其原始值`3`。
- en: Next, we'll update the `prometheus` Chart with the new values and open the alerts
    screen to confirm whether the new alert was indeed added.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用新值更新`prometheus`图表，并打开警报屏幕以确认是否确实添加了新警报。
- en: '[PRE52]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: If the `AppTooSlow` alert is still not available, please wait a few moments
    and refresh the screen.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`AppTooSlow`警报仍然不可用，请稍等片刻并刷新屏幕。
- en: '![](assets/68c98ad5-8654-4de8-b3d3-f34e05f3ce6e.png)Figure 3-10: Prometheus''
    alerts screen'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/68c98ad5-8654-4de8-b3d3-f34e05f3ce6e.png)图3-10：Prometheus警报屏幕'
- en: The newly added alert is (probably) green (not triggering). We need to generate
    a few slow requests to see it in action.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 新增的警报可能是绿色的（不会触发）。我们需要生成一些慢请求来看它的作用。
- en: Please execute the command that follows to send thirty requests with a random
    response time of up to ten thousand milliseconds (ten seconds).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行以下命令，发送30个具有随机响应时间的请求，最长为10000毫秒（10秒）。
- en: '[PRE53]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: It'll take a few moments until Prometheus scrapes new metrics and for the alert
    to detect that the threshold is reached. After a while, we can open the alerts
    screen again and check whether the alert is indeed firing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 直到Prometheus抓取新的指标并且警报检测到阈值已达到，需要一些时间。过一会儿，我们可以再次打开警报屏幕，检查警报是否确实触发。
- en: '[PRE54]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can see that the state of the alert is firing. If that's not your case, please
    wait a while longer and refresh the screen. In my case (screenshot following),
    the value is 0.125, meaning that only 12.5 percent of requests have a duration
    of 0.25 seconds or less.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到警报的状态是触发。如果这不是您的情况，请再等一会儿并刷新屏幕。在我的情况下（随后的截图），该值为0.125，意味着只有12.5％的请求持续时间为0.25秒或更短。
- en: 'There might be two or more active alerts inside `AppTooSlow` if `prometheus-server`,
    `prometheus-alertmanager`, or some other application is responding slow.![](assets/17e9cd6e-41dc-486b-9ee1-cfaf8eedb886.png)Figure
    3-11: Prometheus'' alerts screen with one alert firing'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`prometheus-server`，`prometheus-alertmanager`或其他一些应用程序响应缓慢，`AppTooSlow`内可能会有两个或更多活动警报。![](assets/17e9cd6e-41dc-486b-9ee1-cfaf8eedb886.png)图3-11：带有一个触发警报的Prometheus警报屏幕
- en: The alert is red meaning that Prometheus sent it to Alertmanager which, in turn,
    forwarded it to Slack. Let's confirm that.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 警报是红色的，意味着Prometheus将其发送到Alertmanager，后者又将其转发到Slack。让我们确认一下。
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As you can see (screenshot following), we received two notification. Since we
    reverted the threshold of the `TooManyNodes` alert back to greater than three
    nodes, and our cluster has less, Prometheus sent a notification to Alertmanager
    that the problem is resolved. As a result, we got a new notification in Slack.
    This time, the color of the message is green.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的（随后的截图），我们收到了两个通知。由于我们将`TooManyNodes`警报的阈值恢复为大于三个节点，并且我们的集群节点较少，因此Prometheus向Alertmanager发送了问题已解决的通知。结果，我们在Slack中收到了新的通知。这次，消息的颜色是绿色的。
- en: Further on, a new red message appeared indicating that an `Application is too
    slow`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，出现了一个新的红色消息，指示“应用程序太慢”。
- en: '![](assets/f8549f2d-8a88-4179-933a-57b9f13b7e5a.png)Figure 3-12: Slack with
    alerts firing (red) and resolved (green) messages'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/f8549f2d-8a88-4179-933a-57b9f13b7e5a.png)图3-12：Slack显示触发（红色）和解决（绿色）消息'
- en: We often cannot rely on a single rule that will fit all the applications. Prometheus
    and, for example, Jenkins would be a good candidate of internal applications which
    we cannot expect to have less than five percent of response times above 0.25 seconds.
    So, we might want to filter further the alerts. We can use any number of labels
    for that. To keep it simple, we'll continue leveraging `ingress` label but, this
    time, we'll use regular expressions to exclude some applications (Ingresses) from
    the alert.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常不能依赖单一规则来适用于所有应用程序。例如，Prometheus和Jenkins可能是内部应用程序的良好候选者，我们不能期望其响应时间低于0.25秒的百分之五。因此，我们可能需要进一步过滤警报。我们可以使用任意数量的标签来实现这一点。为了简单起见，我们将继续利用`ingress`标签，但这次我们将使用正则表达式来排除一些应用程序（Ingress）的警报。
- en: Let's open the graph screen one more time.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次打开图表屏幕。
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，点击“执行”按钮，然后切换到*图表*选项卡。
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The addition to the previous query is the `ingress!~"prometheus-server|jenkins"`
    filter. The `!~` is used to select metrics with labels that do NOT regex match
    the `prometheus-server|jenkins` string. Since `|` is equivalent to the `or` statement,
    we can translate that filter as "everything that is NOT `prometheus-server` or
    is NOT `jenkins`." We do not have Jenkins in our cluster. I just wanted to show
    you a way to exclude multiple values.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的查询相比，新增的是`ingress!~"prometheus-server|jenkins"`过滤器。`!~`用于选择具有不与`prometheus-server|jenkins`字符串匹配的标签的指标。由于`|`等同于`or`语句，我们可以将该过滤器翻译为“所有不是`prometheus-server`或不是`jenkins`的内容”。我们的集群中没有Jenkins。我只是想向您展示一种排除多个值的方法。
- en: '![](assets/0e6442a6-12b5-48d9-ab0c-b7da6c30abdc.png)Figure 3-13: Prometheus
    graph screen with the percentage of requests with 0.25 seconds duration and the
    results excluding prometheus-server and jenkins'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-13：Prometheus图表屏幕，显示了持续时间为0.25秒的请求百分比，结果不包括prometheus-server和jenkins
- en: We could have complicated it a bit more and specified `ingress!~"prometheus.+|jenkins.+`
    as the filter. In that case, it would exclude all Ingresses with the name that
    starts with `prometheus` and `jenkins`. The key is in the `.+` addition that,
    in RegEx, matches one or more entries (`+`) of any character (`.`).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再复杂一点，指定`ingress!~"prometheus.+|jenkins.+`作为过滤器。在这种情况下，它将排除所有名称以`prometheus`和`jenkins`开头的Ingress。关键在于`.+`的添加，在正则表达式中，它匹配一个或多个任何字符的条目（`+`）。
- en: We won't go into an explanation of RegEx syntax. I expect you to be already
    familiar with it. If you're not, you might want to Google it or to visit *Regular
    expression Wiki* page ([https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细解释正则表达式的语法。我希望您已经熟悉它。如果您不熟悉，您可能需要搜索一下或访问*正则表达式维基*页面（[https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)）。
- en: The previous expression retrieves only the results that are NOT `prometheus-server`
    and `jenkins`. We would probably need to create another one that includes only
    those two.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的表达式只检索不是`prometheus-server`和`jenkins`的结果。我们可能需要创建另一个表达式，只包括这两个。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击“执行”按钮。
- en: '[PRE58]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The only difference, when compared with the previous expression, is that this
    time we used the `=~` operator. It selects labels that regex-match the provided
    string. Also, the bucket (`le`) is now set to `0.5` seconds, given that both applications
    might need more time to respond and we are OK with that.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的表达式相比，唯一的区别是这次我们使用了`=~`运算符。它选择与提供的字符串匹配的标签。此外，桶（`le`）现在设置为`0.5`秒，因为这两个应用程序可能需要更多时间来响应，我们可以接受这一点。
- en: In my case, the graph shows `prometheus-server` as having one hundred percent
    requests with durations within 0.5 seconds (in your case that might not be true).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，图表显示`prometheus-server`的请求百分比为百分之百，持续时间在0.5秒内（在您的情况下可能不是真的）。
- en: '![](assets/1a0043b8-213c-4c76-9209-2fa3a1ca851a.png)Figure 3-14: Prometheus
    graph screen with the percentage of requests with 0.5 seconds duration and the
    results including only prometheus-server and jenkins'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/1a0043b8-213c-4c76-9209-2fa3a1ca851a.png)图3-14：Prometheus图表屏幕，显示了持续0.5秒的请求百分比，以及仅包括prometheus-server和jenkins的结果'
- en: The few latency examples should be enough to get you going with that type of
    metrics, so we'll move to traffic.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 少数延迟示例应该足以让您了解这种类型的指标，所以我们将转向流量。
- en: Alerting on traffic-related issues
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流量相关问题的警报
- en: So far, we measured the latency of our applications, and we created alerts that
    fire when certain thresholds based on request duration are reached. Those alerts
    are not based on the number of requests coming in (traffic), but on the percentage
    of slow requests. The `AppTooSlow` would fire even if only one single request
    enters an application, as long as the duration is above the threshold. For completeness,
    we need to start measuring traffic or, to be more precise, the number of requests
    sent to each application and the system as a whole. Through that, we can know
    if our system is under a lot of stress and make a decision on whether to scale
    our applications, add more workers, or apply some other solution to mitigate the
    problem. We might even choose to block part of the incoming traffic if the number
    of requests reaches abnormal numbers providing a clear indication that we are
    under **Denial of Service** (**DoS**) attack ([https://en.wikipedia.org/wiki/Denial-of-service_attack](https://en.wikipedia.org/wiki/Denial-of-service_attack)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们测量了应用程序的延迟，并创建了在达到基于请求持续时间的特定阈值时触发的警报。这些警报不是基于进入的请求数量（流量），而是基于慢请求的百分比。即使只有一个请求进入应用程序，只要持续时间超过阈值，`AppTooSlow`也会触发。为了完整起见，我们需要开始测量流量，或者更准确地说，是发送到每个应用程序和整个系统的请求数。通过这样做，我们可以知道我们的系统是否承受了很大的压力，并决定是否要扩展我们的应用程序，增加更多的工作人员，或者采取其他解决方案来缓解问题。如果请求的数量达到异常数字，清楚地表明我们正在遭受**拒绝服务**（**DoS**）攻击（[https://en.wikipedia.org/wiki/Denial-of-service_attack](https://en.wikipedia.org/wiki/Denial-of-service_attack)），我们甚至可以选择阻止部分传入流量。
- en: We'll start by creating a bit of traffic that we can use to visualize requests.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始创建一些流量，以便我们可以用来可视化请求。
- en: '[PRE59]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We sent a hundred requests to the `go-demo-5` application and opened Prometheus'
    graph screen.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`go-demo-5`应用程序发送了一百个请求，并打开了Prometheus的图表屏幕。
- en: We can retrieve the number of requests coming into the Ingress controller through
    the `nginx_ingress_controller_requests`. Since it is a counter, we can continue
    using `rate` function combined with `sum`. Finally, we probably want to know the
    rate of requests grouped by the `ingress` label.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`nginx_ingress_controller_requests`获取进入Ingress控制器的请求数。由于它是一个计数器，我们可以继续使用`rate`函数结合`sum`。最后，我们可能想要按`ingress`标签对请求的速率进行分组。
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入下面的表达式，按“执行”按钮，然后切换到*图表*选项卡。
- en: '[PRE60]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can see a spike on the right side of the graph. It shows the requests that
    went to the `go-demo-5` applications through the Ingress with the same name.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的右侧显示了一个峰值。它显示了通过具有相同名称的Ingress发送到`go-demo-5`应用程序的请求。
- en: In my case (screenshot following), the peak is close to one request per second
    (yours will be different).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下（随后的屏幕截图），峰值接近每秒一个请求（您的情况将不同）。
- en: '![](assets/713e0fd7-3d7e-414a-b4df-02526e34bb83.png)Figure 3-15: Prometheus''
    graph screen with the rate of the number of requests'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-15：Prometheus的图形屏幕显示请求数量的速率
- en: We are probably more interested in the number of requests per second per replica
    of an application, so our next task is to find a way to retrieve that data. Since
    `go-demo-5` is a Deployment, we can use `kube_deployment_status_replicas`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能更感兴趣的是每个应用程序每秒每个副本的请求数，因此我们的下一个任务是找到一种检索该数据的方法。由于`go-demo-5`是一个部署，我们可以使用`kube_deployment_status_replicas`。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE61]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We can see the number of replicas of each Deployment in the system. The `go-demo-5`
    application, in my case painted in red (screenshot following), has three replicas.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到系统中每个部署的副本数量。在我的情况下，`go-demo-5` 应用程序以红色（后续截图）显示，有三个副本。
- en: '![](assets/f1c0b08a-afb3-4988-87d2-d90ec102adb9.png)Figure 3-16: Prometheus''
    graph screen with the number of replicas of Deployments'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-16：Prometheus的图形屏幕显示部署的副本数量
- en: Next, we should combine the two expressions, to get the number of requests per
    second per replica. However, we are facing a problem. For two metrics to join,
    they need to have matching labels. Both the Deployment and the Ingress of `go-demo-5`
    have the same name so we can use that to our benefit, given that we can rename
    one of the labels. We'll do that with the help of the `label_join` ([https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join()](https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join()))
    function.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该组合这两个表达式，以获得每个副本每秒的请求数。然而，我们面临一个问题。要使两个指标结合，它们需要具有匹配的标签。`go-demo-5`的部署和入口都具有相同的名称，因此我们可以利用这一点，假设我们可以重命名其中一个标签。我们将借助`label_join`（[https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join()](https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join())）函数来实现这一点。
- en: For each timeseries in v, `label_join(v instant-vector, dst_label string, separator
    string, src_label_1 string, src_label_2 string, ...)` joins all the values of
    all the `src_labels` using the separator and returns the timeseries with the label
    `dst_label` containing the joined value.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于v中的每个时间序列，`label_join(v instant-vector, dst_label string, separator string,
    src_label_1 string, src_label_2 string, ...)`使用分隔符连接所有`src_labels`的值，并返回包含连接值的标签`dst_label`的时间序列。
- en: If the previous explanation of the `label_join` function was confusing, you're
    not alone. Instead, let's go through the example that will transform `kube_deployment_status_replicas`
    by adding `ingress` label that will contain values from the `deployment` label.
    If we are successful, we'll be able to combine the result with `nginx_ingress_controller_requests`
    since both will have the same matching labels (`ingress`).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果之前对`label_join`函数的解释让你感到困惑，你并不孤单。相反，让我们通过一个示例来了解，该示例将通过添加`ingress`标签来转换`kube_deployment_status_replicas`，该标签将包含来自`deployment`标签的值。如果我们成功了，我们将能够将结果与`nginx_ingress_controller_requests`组合，因为两者都具有相同的匹配标签（`ingress`）。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Since we are, this time, interested mostly in values of the labels, please switch
    to the Console view by clicking the tab.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这次我们主要关注标签的值，请通过点击选项卡切换到控制台视图。
- en: As you can see from the output, each metric now contains an additional label
    `ingress` with the same value as `deployment`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，每个指标现在都包含一个额外的标签`ingress`，其值与`deployment`相同。
- en: '![](assets/d8145093-7933-49c7-9fdf-65a9fcf30124.png)Figure 3-17: Prometheus''
    console view of Deployment replicas status and a new label ingress created from
    the deployment label'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-17：Prometheus的控制台视图显示部署副本状态和从部署标签创建的新标签ingress
- en: Now we can combine the two metrics.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以结合这两个指标。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE63]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Switch back to the *Graph* view.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 切换回*图表*视图。
- en: We calculated the rate of the number of requests per application (`ingress`)
    and divided it with the total number of replicas per application (`ingress`).
    The end result is the rate of the number of requests per application (`ingress`)
    per replica.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了每个应用程序（`ingress`）的请求数量的速率，并将其除以每个应用程序（`ingress`）的副本总数。最终结果是每个应用程序（`ingress`）每个副本的请求数量的速率。
- en: It might be worth noting that we can not retrieve the number of requests for
    each specific replica, but rather the average number of requests per replica.
    This method should work, given that Kubernetes networking in most cases performs
    round robin that results in more or less the same amount of requests being sent
    to each replica.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们无法检索每个特定副本的请求数量，而是每个副本的平均请求数量。在大多数情况下，这种方法应该有效，因为Kubernetes网络通常执行轮询，导致向每个副本发送的请求数量多少相同。
- en: All in all, now we know how many requests our replicas are receiving per second.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，现在我们知道我们的副本每秒收到多少请求。
- en: '![](assets/d4f2cc20-dd55-4a80-af2a-177918f74576.png)Figure 3-18: Prometheus''
    graph screen with the rate of requests divided by the number of Deployment replicas'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d4f2cc20-dd55-4a80-af2a-177918f74576.png)图3-18：普罗米修斯图表屏幕，显示请求数量除以部署副本数量的速率'
- en: Now that we learned how to write an expression to retrieve the rate of the number
    of requests per second per replica, we should convert it into an alert.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何编写一个表达式来检索每秒每个副本的请求数量的速率，我们应该将其转换为警报。
- en: So, let's take a look at the difference between the old and the new definition
    of Prometheus' Chart values.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们来看看普罗米修斯图表值的旧定义和新定义之间的区别。
- en: '[PRE64]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The output is as follows.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE65]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We can see that the expression is almost the same as the one we used in Prometheus'
    graph screen. The only difference is the threshold which we set to `0.1`. As a
    result, that alert should notify us whenever a replica receives more than a rate
    of `0.1` requests per second, calculated over the period of five minutes (`[5m]`).
    As you might have guessed, `0.1` requests per second is too low of a figure to
    use it in production. However, it'll allow us to trigger the alert easily and
    see it in action.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到表达式几乎与我们在普罗米修斯图表屏幕中使用的表达式相同。唯一的区别是我们将阈值设置为`0.1`。因此，该警报应在副本每秒收到的请求数超过五分钟内计算的速率`0.1`时通知我们。你可能已经猜到，每秒`0.1`个请求是一个太低的数字，不能在生产中使用。然而，它将使我们能够轻松触发警报并看到它的作用。
- en: Now, let's upgrade our Chart, and open Prometheus' alerts screen.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们升级我们的图表，并打开普罗米修斯的警报屏幕。
- en: '[PRE66]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Please refresh the screen until the `TooManyRequests` alert appears.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 请刷新屏幕，直到“TooManyRequests”警报出现。
- en: '![](assets/84c0a44a-b5bd-4f17-a428-6f1513dec3d7.png)Figure 3-19: Prometheus''
    alerts screen'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/84c0a44a-b5bd-4f17-a428-6f1513dec3d7.png)图3-19：普罗米修斯警报屏幕'
- en: Next, we'll generate some traffic so that we can see the alert is generated
    and sent through Alertmanager to Slack.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将生成一些流量，以便我们可以看到警报是如何生成并通过Alertmanager发送到Slack的。
- en: '[PRE67]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We sent two hundred requests and reopened the Prometheus' alerts screen. Now
    we should refresh the screen until the `TooManyRequests` alert becomes red.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发送了两百个请求，并重新打开了普罗米修斯的警报屏幕。现在我们应该刷新屏幕，直到“TooManyRequests”警报变为红色。
- en: Once Prometheus fired the alert, it was sent to Alertmanager and, from there,
    forwarded to Slack. Let's confirm that.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 普罗米修斯一旦触发了警报，就会被发送到Alertmanager，然后转发到Slack。让我们确认一下。
- en: '[PRE68]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We can see the `Too many requests` notification, thus proving that the flow
    of this alert works.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到“请求过多”的通知，从而证明了这个警报的流程是有效的。
- en: '![](assets/499f9841-7cb1-4865-a6d1-b02dff0a4761.png)Figure 3-20: Slack with
    alert messages'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/499f9841-7cb1-4865-a6d1-b02dff0a4761.png)图3-20：Slack与警报消息'
- en: Next, we'll jump into errors-related metrics.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向与错误相关的指标。
- en: Alerting on error-related issues
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于错误相关问题的警报
- en: We should always be aware of whether our applications or the system is producing
    errors. However, we cannot start panicking on the first occurrence of an error
    since that would generate too many notifications that we'd likely end up ignoring.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该时刻注意我们的应用程序或系统是否产生错误。然而，我们不能在第一次出现错误时就开始惊慌，因为那样会产生太多通知，我们很可能会忽略它们。
- en: Errors happen often, and many are caused by issues that are fixed automatically
    or are due to circumstances that are out of our control. If we are to perform
    an action on every error, we'd need an army of people working 24/7 only on fixing
    issues that often do not need to be fixed. As an example, entering into a "panic"
    mode because there is a single response with code in 500 range would almost certainly
    produce a permanent crisis. Instead, we should monitor the rate of errors compared
    to the total number of requests and react only if it passes a certain threshold.
    After all, if an error persists, that rate will undoubtedly increase. On the other
    hand, if it continues being low, it means that the issue was fixed automatically
    by the system (for example, Kubernetes rescheduled the Pods from the failed node)
    or that it was an isolated case that does not repeat.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 错误经常发生，许多错误是由自动修复的问题或由我们无法控制的情况引起的。如果我们要对每个错误执行操作，我们就需要一支全天候工作的人员团队，专门解决通常不需要解决的问题。举个例子，因为有一个单独的响应代码在500范围内而进入“恐慌”模式几乎肯定会产生永久性危机。相反，我们应该监视错误的比率与总请求数量的比较，并且只有在超过一定阈值时才做出反应。毕竟，如果一个错误持续存在，那么错误率肯定会增加。另一方面，如果错误率持续很低，这意味着问题已经被系统自动修复（例如，Kubernetes重新安排了从失败节点中的Pod）或者这是一个不重复的孤立案例。
- en: Our next mission is to retrieve requests and distinguish separate them by their
    statuses. If we can do that, we should be able to calculate the rate of errors.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个任务是检索请求并根据它们的状态进行区分。如果我们能做到这一点，我们应该能够计算出错误的比率。
- en: We'll start by generating a bit of traffic.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从生成一些流量开始。
- en: '[PRE69]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We sent a hundred requests and opened Prometheus' graph screen.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发送了一百个请求并打开了Prometheus的图形屏幕。
- en: Let's see whether the `nginx_ingress_controller_requests` metric we used previously
    provides the statuses of the requests.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们之前使用的`nginx_ingress_controller_requests`指标是否提供了请求的状态。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击执行按钮。
- en: '[PRE70]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We can see all the data recently scraped by Prometheus. If we pay closer attention
    to the labels, we can see that, among others, there is `status`. We can use it
    to calculate the percentage of those with errors (for example, 500 range) based
    on the total number of requests.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到Prometheus最近抓取的所有数据。如果我们更加关注标签，我们会发现，其中包括`status`。我们可以使用它来根据请求的总数计算出错误的百分比（例如，500范围内的错误）。
- en: We already saw that we can use the `ingress` label to separate calculations
    per application, assuming that we are interested only in those that are public-facing.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到我们可以使用`ingress`标签来按应用程序分别计算，假设我们只对那些面向公众的应用程序感兴趣。
- en: '![](assets/adbe5b3c-eff0-4170-b518-20e9cb90f82d.png)Figure 3-21: Prometheus''
    console view with requests entering through Ingress'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/adbe5b3c-eff0-4170-b518-20e9cb90f82d.png)图3-21：通过Ingress进入的Prometheus控制台视图的请求'
- en: The `go-demo-5` app has a special endpoint `/demo/random-error` that will generate
    random error responses. Approximately, one out of ten requests to that address
    will produce an error. We can use that to test our expressions.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo-5`应用程序有一个特殊的端点`/demo/random-error`，它将生成随机的错误响应。大约每十个对该地址的请求中就会产生一个错误。我们可以用这个来测试我们的表达式。'
- en: '[PRE71]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We sent a hundred requests to the `/demo/random-error` endpoint and approximately
    ten percent of those responded with errors (HTTP status code `500`).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`/demo/random-error`端点发送了一百个请求，大约有10%的请求产生了错误（HTTP状态码`500`）。
- en: Next, we'll have to wait for a few moments for Prometheus to scrape the new
    batch of metrics. After that, we can open the Graph screen and try to write an
    expression that will retrieve the error rate of our applications.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将不得不等待一段时间，让Prometheus抓取新的一批指标。之后，我们可以打开图表屏幕，尝试编写一个表达式，以检索我们应用程序的错误率。
- en: '[PRE72]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按执行按钮。
- en: '[PRE73]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We used `5..` RegEx to calculate the rate of the requests with errors grouped
    by `ingress`, and we divided the result with all the rate of all the requests.
    The result is grouped by `ingress`. In my case (screenshot following), the result
    is approximately 4 percent (`0.04`). Prometheus did not yet scrape all the metrics,
    and I expect that number to get closer to ten percent in the next scraping iteration.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`5..`正则表达式来计算按`ingress`分组的带有错误的请求的比率，并将结果除以所有请求的比率。结果按`ingress`分组。在我的情况下（随后的截图），结果大约为4%（`0.04`）。Prometheus尚未抓取所有指标，我预计在下一次抓取迭代中这个数字会接近10%。
- en: '![](assets/080bb7c6-42d1-486b-9b17-4b938b5c1624.png)Figure 3-22: Prometheus''
    graph screen with the percentage with the requests with error responses'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/080bb7c6-42d1-486b-9b17-4b938b5c1624.png)图3-22：带有错误响应请求百分比的Prometheus图表屏幕'
- en: Let's compare the updated version of the Chart's values file with the one we
    used previously.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较图表值文件的更新版本与我们之前使用的版本。
- en: '[PRE74]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The output is as follows.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE75]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The alert will fire if the rate of errors is over 2.5% of the total rate of
    requests.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果错误率超过总请求率的2.5%，警报将触发。
- en: Now we can upgrade our Prometheus' Chart.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以升级我们的Prometheus图表。
- en: '[PRE76]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: There's probably no need to confirm that the alerting works. We already saw
    that Prometheus sends all the alerts to Alertmanager and that they are forwarded
    from there to Slack.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能不需要确认警报是否有效。我们已经看到Prometheus将所有警报发送到Alertmanager，然后从那里转发到Slack。
- en: Next, we'll move to saturation metrics and alerts.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转移到饱和度指标和警报。
- en: Alerting on saturation-related issues
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 饱和度相关问题的警报
- en: Saturation measures fullness of our services and the system. We should be aware
    if replicas of our services are processing too many requests and being forced
    to queue some of them. We should also monitor whether usage of our CPUs, memory,
    disks, and other resources reaches critical limits.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 饱和度衡量了我们的服务和系统的充实程度。如果我们的服务副本处理了太多的请求并被迫排队处理其中一些，我们应该意识到这一点。我们还应该监视我们的CPU、内存、磁盘和其他资源的使用是否达到了临界限制。
- en: For now, we'll focus on CPU usage. We'll start by opening the Prometheus' graph
    screen.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将专注于CPU使用率。我们将首先打开Prometheus的图表屏幕。
- en: '[PRE77]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Let's see if we can get the rate of used CPU by node (`instance`). We can use
    `node_cpu_seconds_total` metric for that. However, it is split into different
    modes, and we'll have to exclude a few of them to get the "real" CPU usage. Those
    will be `idle`, `iowait`, and any type of `guest` cycles.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以获得节点（`instance`）的CPU使用率。我们可以使用`node_cpu_seconds_total`指标来实现。但是，它被分成不同的模式，我们将不得不排除其中的一些模式，以获得“真实”的CPU使用率。这些将是`idle`，`iowait`，和任何类型的`guest`周期。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按执行按钮。
- en: '[PRE78]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Switch to the *Graph* view.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到*图表*视图。
- en: The output represents the actual usage of CPU in the system. In my case (screenshot
    following), excluding a temporary spike, all nodes are using less than a hundred
    CPU milliseconds.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 输出代表了系统中CPU的实际使用情况。在我的情况下（以下是屏幕截图），除了临时的峰值，所有节点的CPU使用量都低于一百毫秒。
- en: The system is far from being under stress.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 系统远未处于压力之下。
- en: '![](assets/52dcc7f8-4b9f-4e7f-b176-b73855232337.png)Figure 3-23: Prometheus''
    graph screen with the rate of used CPU grouped by node instances'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/52dcc7f8-4b9f-4e7f-b176-b73855232337.png)图3-23：Prometheus的图表屏幕，显示了按节点实例分组的CPU使用率'
- en: As you already noticed, absolute numbers are rarely useful. We should try to
    discover the percentage of used CPU. We'll need to find out how much CPU our nodes
    have. We can do that by counting the number of metrics. Each CPU gets its own
    data entry, one for each mode. If we limit the result to a single mode (for example,
    `system`), we should be able to get the total number of CPUs.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您已经注意到的，绝对数字很少有用。我们应该尝试发现使用的CPU百分比。我们需要找出我们的节点有多少CPU。我们可以通过计算指标的数量来做到这一点。每个CPU都有自己的数据条目，每种模式都有一个。如果我们将结果限制在单个模式（例如`system`）上，我们应该能够获得CPU的总数。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击执行按钮。
- en: '[PRE79]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In my case (screenshot following), there are six cores in total. Yours is likely
    to be six as well if you're using GKE, EKS, or AKS from the Gists. If, on the
    other hand, you're running the cluster in Docker for Desktop or minikube, the
    result should be one node.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下（以下是屏幕截图），总共有六个核心。如果您使用的是GKE、EKS或来自Gists的AKS，您的情况可能也是六个。另一方面，如果您在Docker
    for Desktop或minikube中运行集群，结果应该是一个节点。
- en: Now we can combine the two queries to get the percentage of used CPU
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以结合这两个查询来获取使用的CPU百分比
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击执行按钮。
- en: '[PRE80]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: We summarized the rate of used CPUs and divided it by the total number of CPUs.
    In my case (screenshot following), only three to four percent of CPU is currently
    used.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了使用的CPU速率，并将其除以CPU的总数。在我的情况下（以下是屏幕截图），当前仅使用了三到四个百分比的CPU。
- en: That is not a surprise since most of the system is at rest. Not much is going
    on in our cluster right now.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不奇怪，因为大部分系统都处于休眠状态。我们的集群现在并没有太多活动。
- en: '![](assets/2d2bf1f8-7ac6-4d21-a067-c6c523f0e45e.png)Figure 3-24: Prometheus''
    graph screen with the percentage of available CPU'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2d2bf1f8-7ac6-4d21-a067-c6c523f0e45e.png)图3-24：Prometheus的图表屏幕，显示了可用CPU的百分比'
- en: Now that we know how to fetch the percentage of used CPU of the whole cluster,
    we'll move our attention to applications.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何获取整个集群使用的CPU百分比，我们将把注意力转向应用程序。
- en: We'll try to discover how many allocatable cores we have. From application's
    perspective, at least when they're running in Kubernetes, allocatable CPUs show
    how much can be requested for Pods. Allocatable CPU is always lower than total
    CPU.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试发现我们有多少可分配的核心。从应用程序的角度来看，至少当它们在Kubernetes中运行时，可分配的CPU显示了可以为Pods请求多少。可分配的CPU始终低于总CPU。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入以下表达式，然后点击执行按钮。
- en: '[PRE81]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The output should be lower than the number of cores used by our VMs. The allocatable
    cores show the amount of CPU that can be assigned containers. To be more precise,
    allocatable cores are the number of CPUs assigned to nodes minus those reserved
    by system-level processes. In my case (screenshot following), there are almost
    two full allocatable CPUs.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该低于我们的虚拟机使用的核心数。可分配的核心显示了可以分配给容器的CPU数量。更准确地说，可分配的核心是分配给节点的CPU数量减去系统级进程保留的数量。在我的情况下（以下是屏幕截图），几乎有两个完整的可分配CPU。
- en: '![](assets/7283ecb4-bc9a-4432-ad59-3d1469a62c5f.png)Figure 3-25: Prometheus''
    graph screen with the allocatable CPU for each of the nodes in the cluster'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-25：Prometheus的图表屏幕，显示集群中每个节点的可分配CPU
- en: However, in this context, we are interested in the total amount of allocatable
    CPUs since we are trying to discover how much is used by our Pods inside the whole
    cluster. So, we'll sum the allocatable cores.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们对可分配的CPU总量感兴趣，因为我们试图发现我们的Pods在整个集群中使用了多少。因此，我们将对可分配的核心求和。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE82]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: In my case, the total allocatable CPUs is somewhere around 5.8 cores. For the
    exact number, please hover on the graph line.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，可分配的CPU总数大约为5.8个核心。要获取确切的数字，请将鼠标悬停在图表线上。
- en: Now that we know how many allocatable CPUs we have, we should try to discover
    how much was requested by Pods.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了有多少可分配的CPU，我们应该尝试发现Pods请求了多少。
- en: Please note that requested resources are not the same as used resources. We'll
    get to that use-case later. For now, we want to know how much we requested from
    the system.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，请求的资源与已使用的资源不同。我们稍后会讨论这种情况。现在，我们想知道我们从系统中请求了多少。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE83]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We can see that requested CPU is relatively low. In my case, all the containers
    with requested CPU have their values below 0.15 (hundred and fifty milliseconds).
    Your result might differ.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到请求的CPU相对较低。在我的情况下，所有请求CPU的容器值都低于0.15（一百五十毫秒）。您的结果可能有所不同。
- en: Just as with allocatable CPU, we are interested in the sum of requested CPU.
    Later on, we'll be able to combine the two results and deduce how much is left
    unreserved in the cluster.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 与可分配的CPU一样，我们对请求的CPU总和感兴趣。稍后，我们将能够结合这两个结果，并推断集群中还有多少未保留的资源。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE84]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We summed all the CPU resource requests. As a result, in my case (screenshot
    following), all the requested CPUs are slightly below 1.5.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对所有CPU资源请求求和。结果是，在我的情况下（随后的屏幕截图），所有请求的CPU略低于1.5。
- en: '![](assets/0a3f5d81-4d68-4f87-8a3a-6c8c8bcfc4dd.png)Figure 3-26: Prometheus''
    graph screen with the sum of the requested CPU'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-26：Prometheus的图表屏幕，显示请求的CPU总和
- en: Now, let's combine the two expressions and see the percentage of requested CPU.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们结合这两个表达式，看看请求的CPU百分比。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE85]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: In my case, the output shows that around a quarter (0.25) of all allocatable
    CPU is reserved. That means that we could have four times as many CPU requests
    before we reach the need to expand the cluster. Of course, you already know that,
    if present, Cluster Autoscaler will add nodes before that happens. Still, knowing
    that we are close to reaching CPU limits is important. Cluster Autoscaler might
    not be working correctly, or it might not even be active. The latter case is true
    for most, if not all on-prem clusters.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，输出显示大约四分之一（0.25）的可分配CPU被保留。这意味着在我们达到扩展集群的需要之前，我们可以有四倍的CPU请求。当然，您已经知道，如果存在的话，集群自动扩展器会在此之前添加节点。但是，知道我们接近达到CPU限制是很重要的。集群自动扩展器可能无法正常工作，或者甚至可能根本没有激活。如果有的话，后一种情况对于大多数本地集群来说是真实的。
- en: Let's see whether we can convert the expressions we explored into alerts.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是否可以将我们探索的表达式转换为警报。
- en: We'll explore yet another difference between a new set of Chart values and those
    we used before.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨一组新的图表值与之前使用的值之间的另一个差异。
- en: '[PRE86]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The output is as follows.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE87]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: We can see from the differences that we restored the original threshold of `TooManyRequests`
    back to `1` and that we added two new alerts called `NotEnoughCPU` and `TooMuchCPURequested`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 从差异中我们可以看到，我们将“TooManyRequests”的原始阈值恢复为“1”，并添加了两个名为“NotEnoughCPU”和“TooMuchCPURequested”的新警报。
- en: The `NotEnoughCPU` alert will fire if more than ninety percent of CPU across
    the whole cluster is used for over thirty minutes. That way we'll avoid settings
    alarms if there is a temporary spike in CPU usage.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 如果整个集群的CPU使用率超过百分之九十并持续超过三十分钟，“NotEnoughCPU”警报将会触发。这样我们就可以避免在CPU使用率暂时飙升时设置警报。
- en: The `TooMuchCPURequested` also has the threshold of ninety percent and will
    be triggered if it persists for over thirty minutes. The expression computes the
    total amount of requested divided with the total among allocatable CPU.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: “TooMuchCPURequested”也有百分之九十的阈值，如果持续超过三十分钟，将会触发警报。该表达式计算请求的总量与可分配CPU的总量之间的比率。
- en: Both alerts are reflections of the Prometheus expressions we executed short
    while ago, so you should already be familiar with their purpose.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个警报都是我们之前执行的Prometheus表达式的反映，所以您应该已经熟悉它们的用途。
- en: Let's upgrade Prometheus' Chart with the new values and open the alerts screen.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用新的值升级Prometheus的图表并打开警报屏幕。
- en: '[PRE89]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: All that's left is to wait until the two new alerts appear. If they are not
    already there, please refresh your screen.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是等待两个新的警报出现。如果它们还没有出现，请刷新您的屏幕。
- en: There's probably no need to see the new alerts in action. By now, you should
    trust the flow, and there's no reason to believe that they would not trigger.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能没有必要看新的警报如何起作用。到现在为止，您应该相信这个流程，没有理由认为它们不会触发。
- en: '![](assets/73c6acaf-55dd-42ec-a35c-ccfadef135cd.png)Figure 3-27: Prometheus''
    alerts screen'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/73c6acaf-55dd-42ec-a35c-ccfadef135cd.png)图3-27：Prometheus的警报屏幕'
- en: In the "real world" scenario, receiving one of the two alerts might provoke
    different reactions depending on the Kubernetes flavor we're using.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在“现实世界”场景中，接收到这两个警报中的一个可能会引发不同的反应，这取决于我们使用的Kubernetes版本。
- en: If we do have Cluster Autoscaler (CA), we might not need `NotEnoughCPU` and
    `TooMuchCPURequested` alerts. The fact that ninety percent of node CPUs is in
    use does not prevent the cluster from operating correctly, just as long as our
    CPU requests as set correctly. Similarly, having ninety percent of allocatable
    CPU reserved is also not an issue. If Kubernetes cannot schedule a new Pod due
    to all CPU being reserved, it will scale up the cluster. As a matter of fact,
    reaching almost full CPU usage or having nearly all allocatable CPU reserved is
    a good thing. That means that we are having as much CPU as we need and that we
    are not paying for unused resources. Still, that logic works mostly with Cloud
    providers and not even all of them. Today (October 2018), Cluster Autoscaler works
    only in AWS, GCE, and Azure.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有集群自动缩放器（CA），我们可能不需要“NotEnoughCPU”和“TooMuchCPURequested”警报。节点CPU使用率达到百分之九十并不会影响集群的正常运行，只要我们的CPU请求设置正确。同样，百分之九十的可分配CPU被保留也不是问题。如果Kubernetes无法安排新的Pod，因为所有CPU都被保留，它将扩展集群。实际上，接近满负荷的CPU使用率或几乎所有可分配的CPU被保留是件好事。这意味着我们拥有我们所需的CPU，并且我们不必为未使用的资源付费。然而，这种逻辑主要适用于云服务提供商，甚至并非所有云服务提供商都适用。今天（2018年10月），集群自动缩放器只在AWS、GCE和Azure中运行。
- en: All that does not mean that we should rely only on Cluster Autoscaler. It can
    malfunction, like anything else. However, since CA is based on watching for unschedulable
    Pods, if it does fail to work, we should detect that through observing Pod statuses,
    not CPU usage. Still, it might not be a bad idea to receive alerts when CPU usage
    is too high, but in that case, we might want to increase the threshold to a value
    closer to a hundred percent.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些并不意味着我们应该只依赖于集群自动缩放器。它也可能出现故障，就像其他任何东西一样。然而，由于集群自动缩放器是基于观察无法调度的Pods，如果它无法工作，我们应该通过观察Pod状态而不是CPU使用率来检测到这一点。不过，当CPU使用率过高时接收警报也许并不是一个坏主意，但在这种情况下，我们可能希望将阈值增加到接近百分之百的值。
- en: If our cluster is on-prem or, to be more precise, if it does not have Cluster
    Autoscaler, the alerts we explored are essential if our process for scaling up
    the cluster is not automated or if it's slow. The logic is simple. If we need
    more than a couple of minutes to add new nodes to the cluster, we cannot wait
    until Pods are unschedulable. That would be too late. Instead, we need to know
    that we are out of available capacity before the cluster becomes full (saturated)
    so that we have enough time to react by adding new nodes to the cluster.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的集群是本地的，更准确地说，如果它没有集群自动缩放器，那么我们探讨的警报对于我们的集群扩展流程是否没有自动化或者速度较慢是至关重要的。逻辑很简单。如果我们需要超过几分钟来向集群添加新节点，我们就不能等到Pods无法调度。那将为时已晚。相反，我们需要知道在集群变满（饱和）之前我们已经没有可用容量，这样我们就有足够的时间通过向集群添加新节点来做出反应。
- en: Still, having a cluster that does not auto-scale because Cluster Autoscaler
    does not work is not an excuse good enough. There are plenty of other tools that
    we can use to automate our infrastructure. When we do manage to get to such a
    place that we can automatically add new nodes to the cluster, the destination
    of the alert should change. Instead of receiving notifications to Slack, we might
    want to send a request to a service that will execute the script that will result
    in a new node being added to the cluster. If our cluster is running on VMs, we
    can always add more through a script (or some tool).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有一个因为集群自动缩放器不工作而不自动缩放的集群并不是一个足够好的借口。我们有很多其他工具可以用来自动化我们的基础设施。当我们设法到达这样一个地步，我们可以自动向集群添加新节点时，警报的目的地应该改变。我们可能不再希望收到Slack通知，而是希望向一个服务发送请求，该服务将执行脚本，从而向集群添加新节点。如果我们的集群是在虚拟机上运行的，我们总是可以通过脚本（或某个工具）添加更多节点。
- en: The only real excuse to receive those notifications to Slack is if our cluster
    is running on bare-metal. In such a case, we cannot expect scripts to create new
    servers magically. For everyone else, Slack notifications when too much CPU is
    used or all allocated CPU is reserved should be only a temporary solution until
    proper automation is in place.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 接收Slack通知的唯一真正理由是如果我们的集群是运行在裸机上的。在这种情况下，我们不能指望脚本会神奇地创建新的服务器。对于其他人来说，当CPU使用过高或者所有分配的CPU都被保留时接收Slack通知应该只是一个临时解决方案，直到适当的自动化到位为止。
- en: Now, let's try to accomplish similar goals but, this time, by measuring memory
    usage and reservations.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试通过测量内存使用和保留来实现类似的目标。
- en: Measuring memory consumption is similar to CPU, and yet there are a few differences
    that we should take into account. But, before we get there, let's go back to the
    Prometheus' graph screen and explore our first memory-related metric.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 测量内存消耗与CPU类似，但也有一些我们应该考虑的不同之处。但在我们到达那里之前，让我们回到Prometheus的图表界面，探索我们的第一个与内存相关的指标。
- en: '[PRE90]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Just as with CPU, first we need to find out how much memory each of our nodes
    has.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 就像CPU一样，首先我们需要找出每个节点有多少内存。
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，点击“执行”按钮，然后切换到*图表*选项卡。
- en: '[PRE91]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Your result is likely to be different than mine. In my case, each node has around
    4 GB of RAM.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果可能与我的不同。在我的情况下，每个节点大约有4GB的RAM。
- en: Knowing how much RAM each node has is of no use without knowing how much RAM
    is currently available. We can get that info through the `node_memory_MemAvailable_bytes`
    metric.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 知道每个节点有多少RAM是没有用的，如果不知道当前有多少RAM可用。我们可以通过`node_memory_MemAvailable_bytes`指标获得这些信息。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 请在下面输入表达式，然后按“执行”按钮。
- en: '[PRE92]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: We can see the available memory on each of the nodes of the cluster. In my case
    (screenshot following), each has around 3 GB of available RAM.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到集群中每个节点的可用内存。在我的情况下（如下截图所示），每个节点大约有3GB的可用RAM。
- en: '![](assets/67a644d2-43b6-4eb3-902f-56c07fffd073.png)Figure 3-28: Prometheus''
    graph screen with available memory in each of the nodes of the cluster'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/67a644d2-43b6-4eb3-902f-56c07fffd073.png)图3-28：Prometheus的图表屏幕，显示集群中每个节点的可用内存'
- en: Now that we know how to get total and available memory from each of the nodes,
    we should combine the queries to get the percentage of the used memory of the
    whole cluster.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从每个节点获取总内存和可用内存，我们应该结合查询来获得整个集群已使用内存的百分比。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 请在下面输入表达式，然后按“执行”按钮。
- en: '[PRE93]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Since we are searching for the percentage of used memory, and we have the metric
    with available memory, we started the expression with `1 -` that will invert the
    result. The rest of the expression is a simple division of available and total
    memory. In my case (screenshot following), less than thirty percent of memory
    is used on each of the nodes.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在寻找已使用内存的百分比，并且我们有可用内存的指标，我们从`1 -`开始表达式，这将颠倒结果。表达式的其余部分是可用和总内存的简单除法。在我的情况下（如下截图所示），每个节点上使用的内存不到30%。
- en: '![](assets/9144a5c6-047c-4b6d-b6cf-d3e1e63235a4.png)Figure 3-29: Prometheus''
    graph screen with the percentage of available memory'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/9144a5c6-047c-4b6d-b6cf-d3e1e63235a4.png)图3-29：Prometheus的图表屏幕，显示可用内存的百分比'
- en: Just as with CPU, available and total memory does not paint the whole picture.
    While that is useful information and a base for a potential alert, we also need
    to know how much memory is allocatable and how much of it is in use by Pods. We
    can get the first figure through the `kube_node_status_allocatable_memory_bytes`
    metric.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 就像CPU一样，可用和总内存并不能完全反映实际情况。虽然这是有用的信息，也是潜在警报的基础，但我们还需要知道有多少内存可分配，以及有多少内存被Pod使用。我们可以通过`kube_node_status_allocatable_memory_bytes`指标获得第一个数字。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 请在下面输入表达式，然后按“执行”按钮。
- en: '[PRE94]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Depending on the Kubernetes flavor and the hosting provider you're using, there
    might be a very small, or a big discrepancy between the total and allocatable
    memory. I am running the cluster in AKS, and allocatable memory is a whole GB
    less than total memory. While the former is around 3 GB RAM, the latter was approximately
    4 GB RAM. That's a big difference. I do not have full 4 GB for my Pods, but around
    one quarter less than that. The rest, around 1 GB RAM, is spent on system-level
    services. To make things worse, that's 1 GB RAM spent on each node which, in my
    case, results in 3 GB less in total since my cluster has three nodes. Given such
    a huge difference between the total and the allocatable amount of RAM, there is
    a clear benefit for having less number of bigger nodes. Still, not everyone needs
    big nodes and reducing their number to less than three might not be a good idea
    if we'd like to have our nodes spread in all the zones.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Kubernetes的版本和您使用的托管提供商，总内存和可分配内存之间可能存在很小或很大的差异。我在AKS中运行集群，可分配内存比总内存少了整整1GB。前者大约为3GB
    RAM，而后者大约为4GB RAM。这是一个很大的差异。我的Pod并没有完整的4GB内存，而是少了大约四分之一。其余的大约1GB RAM花费在系统级服务上。更糟糕的是，这1GB
    RAM花费在每个节点上，而在我的情况下，这导致总共少了3GB，因为我的集群有三个节点。鉴于总内存和可分配内存之间的巨大差异，拥有更少但更大的节点是有明显好处的。然而，并不是每个人都需要大节点，如果我们希望节点分布在所有区域，将节点数量减少到少于三个可能不是一个好主意。
- en: Now that we know how to retrieve the amount of allocatable memory, let's see
    how to get the amount of requested memory for each of the applications.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何检索可分配内存量，让我们看看如何获取每个应用程序所请求的内存量。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE95]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: We can see that Prometheus (server) has the most requested memory (500 MB),
    with all the others being way below. Bear in mind that we are seeing only the
    Pods that do have reservations. Those without are not present in the results of
    that query. As you already know, it is OK not to define reservations and limits
    only in exceptional cases like, for example, for short-lived Pods used in CI/CD
    processes.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到Prometheus（服务器）具有最多的请求内存（500MB），而其他所有的请求内存都远低于这个数值。请记住，我们只看到了具有预留的Pod，那些没有预留的Pod不会出现在该查询的结果中。正如您已经知道的那样，在特殊情况下，例如用于CI/CD流程中的短暂Pod，不定义预留和限制是可以的。
- en: '![](assets/6968bff2-2962-4746-946f-61dbd8f5dd69.png)Figure 3-30: Prometheus''
    graph screen with requested memory for each of the Pods'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/6968bff2-2962-4746-946f-61dbd8f5dd69.png)图3-30：Prometheus的图形屏幕显示了每个Pod所请求的内存'
- en: The previous expression returned the amount of memory used by each Pod. However,
    our mission is to discover how much requested memory we have in the system as
    a whole.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表达式返回了每个Pod使用的内存量。然而，我们的任务是发现系统中总共有多少请求的内存。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE96]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: In my case, the total amount of requested memory is around 1.6 GB RAM.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，所请求的内存总量大约为1.6GB RAM。
- en: All that's left is to divide the total requested memory with the amount of all
    the allocatable memory in the cluster.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是将总请求的内存量除以集群中所有可分配的内存量。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE97]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: In my case (screenshot following), the total of the requested memory is around
    twenty percent (`0.2`) of the cluster's allocatable RAM. I am far from being in
    any type of danger, nor there is a need to scale up the cluster. If anything,
    I have too much unused memory and might want to scale down. However, we are at
    the moment only concerned with scaling up. Later we'll explore alerts that might
    result in scaling down.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下（以下是屏幕截图），请求内存的总量约为集群可分配RAM的百分之二十（`0.2`）。我远非处于任何危险之中，也没有必要扩展集群。如果有什么，我有太多未使用的内存，可能想要缩减规模。然而，目前我们只关注扩展规模。稍后我们将探讨可能导致缩减规模的警报。
- en: '![](assets/f2bbaa51-31e9-4115-80a1-7994bc281168.png)Figure 3-31: Prometheus''
    graph screen with the percentage of the requested memory of the total allocatable
    memory in the cluster'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/f2bbaa51-31e9-4115-80a1-7994bc281168.png)图3-31：Prometheus的图表屏幕，显示集群中请求内存占总可分配内存的百分比'
- en: Let's take a look at the differences between the old Chart's values and those
    we are about to use.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看旧图表数值和我们即将使用的数值之间的差异。
- en: '[PRE98]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: The output is as follows.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE99]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: We added two new alerts (`NotEnoughMemory` and `TooMuchMemoryRequested`). The
    definitions themselves should be straightforward since we already created quite
    a few alerts. The expressions are the same as the ones we used in Prometheus graph
    screen, with the addition of the greater than ninety percent (`> 0.9`) threshold.
    So, we'll skip further explanation.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了两个新的警报（“内存不足”和“请求内存过多”）。定义本身应该很简单，因为我们已经创建了相当多的警报。表达式与我们在Prometheus图表屏幕中使用的表达式相同，只是增加了大于百分之九十（`>
    0.9`）的阈值。因此，我们将跳过进一步的解释。
- en: We'll upgrade our Prometheus Chart with the new values, and open the alerts
    screen to confirm that they
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用新值升级我们的Prometheus图表，并打开警报屏幕以确认它们。
- en: '[PRE100]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: If the alerts `NotEnoughMemory` and `TooMuchMemoryRequested` are not yet available,
    please wait a few moments, and refresh the screen.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果警报“内存不足”和“请求内存过多”尚不可用，请稍等片刻，然后刷新屏幕。
- en: '![](assets/c87a9f02-ac71-489f-907e-bd38b2233064.png)Figure 3-32: Prometheus''
    alerts screen'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c87a9f02-ac71-489f-907e-bd38b2233064.png)图3-32：Prometheus的警报屏幕'
- en: The actions based on the memory-based alerts we created so far should be similar
    to those we discussed with CPU. We can use them to decide whether and when to
    scale up our cluster, either through manual actions or through automated scripts.
    Just as before, if we do have our cluster hosted with one of the vendors supported
    by the Cluster Autoscaler (CA), those alerts should be purely informative, while
    on-prem or with unsupported Cloud providers, they are much more than simple notifications.
    They are an indication that we are about to run out of capacity, at least when
    memory is concerned.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，基于内存的警报所采取的操作应该与我们讨论CPU时类似。我们可以使用它们来决定是否以及何时扩展我们的集群，无论是通过手动操作还是通过自动化脚本。就像以前一样，如果我们的集群托管在Cluster
    Autoscaler（CA）支持的供应商之一，这些警报应该纯粹是信息性的，而在本地或不受支持的云提供商那里，它们不仅仅是简单的通知。它们表明我们即将耗尽容量，至少在涉及内存时是这样。
- en: The CPU and memory examples are all focused on the need to know when is the
    right time to scale our cluster. We might create similar alerts that would notify
    us when the usage of CPU or memory is too low. That would give us a clear indication
    that we have too many nodes in the cluster and that we might want to remove some.
    That, again, assumes that we do not have Cluster Autoscaler up-and-running. Still,
    taking only CPU or only memory into account for scaling-down is too risky and
    can lead to unexpected results.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和内存的示例都集中在需要知道何时扩展我们的集群。我们可能会创建类似的警报，通知我们CPU或内存的使用率过低。这将清楚地告诉我们，我们的集群中有太多节点，我们可能需要移除一些。同样，这假设我们没有集群自动缩放器正在运行。但是，仅考虑CPU或内存中的一个来缩减规模太冒险，可能会导致意想不到的结果。
- en: Let's imagine that only twelve percent of allocatable CPU is reserved and that
    we have three worker nodes in the cluster. Such a low CPU usage surely does not
    warrant that many nodes since on average, each has a relatively small amount of
    reserved CPU. As a result, we can choose to scale down, and we remove one of the
    nodes thus allowing other clusters to reuse it. Was that a good thing to do? Well,
    it depends on other resources. If the percentage of memory reservations was low
    as well, removing a node was a good idea. On the other hand, if the reserved memory
    were over sixty-six percent, removal of a node would result in insufficient resources.
    When we removed one of the three nodes, over sixty-six percent of reserved memory
    across three nodes becomes over one hundred percent on two nodes.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 假设只有百分之十二的可分配CPU被保留，并且我们的集群中有三个工作节点。由于平均每个节点的保留CPU量相对较小，这么低的CPU使用率当然不需要那么多节点。因此，我们可以选择缩减规模，并移除一个节点，从而允许其他集群重用它。这样做是一个好主意吗？嗯，这取决于其他资源。如果内存预留的百分比也很低，移除一个节点是一个好主意。另一方面，如果保留的内存超过百分之六十六，移除一个节点将导致资源不足。当我们移除三个节点中的一个时，三个节点中超过百分之六十六的内存预留变成了两个节点中超过百分之一百。
- en: All in all, if we are to receive notifications that our cluster is in need to
    scale down (and we do NOT have Cluster Autoscaler), we need to combine memory
    and CPU, and probably a few other metrics as alert thresholds. Fortunately, the
    expressions are very similar to those we used before. We just need to combine
    them into a single alert and change the thresholds.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，如果我们要收到通知，我们的集群需要缩减规模（而且我们没有集群自动缩放器），我们需要将内存和CPU以及可能还有其他一些指标作为警报阈值的组合。幸运的是，这些表达式与我们之前使用的非常相似。我们只需要将它们组合成一个单一的警报并更改阈值。
- en: As a reminder, the expressions we used before are as follows (there's no need
    to re-run them).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们之前使用的表达式如下（无需重新运行）。
- en: '[PRE101]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Now, let's compare yet another update of the Chart's values with those we're
    using right now.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将图表的值的另一个更新与我们现在使用的进行比较。
- en: '[PRE102]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: The output is as follows.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE103]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We're adding a new alert called `TooMuchCPUAndMemory`. It is a combination of
    the previous two alerts. It will fire only if both CPU and memory usage are below
    fifty percent. That way we'll avoid sending false positives and we will not be
    tempted to de-scale the cluster only because one of the resource reservations
    (CPU or memory) is too low, while the other might be high.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在添加一个名为“TooMuchCPUAndMemory”的新警报。它是以前两个警报的组合。只有当CPU和内存使用率都低于百分之五十时，它才会触发。这样我们就可以避免发送错误的警报，并且不会因为资源预留（CPU或内存）之一太低而诱发缩减集群的决定，而另一个可能很高。
- en: All that's left, before we move into the next subject (or metric type), is to
    upgrade Prometheus' Chart and confirm that the new alert is indeed operational.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一个主题（或指标类型）之前，剩下的就是升级Prometheus的图表并确认新的警报确实是可操作的。
- en: '[PRE104]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Please refresh the alerts screen if the alert is still not present. In my case
    (screenshot following), the total of reserved memory and CPU is below fifty percent,
    and the alert is in the pending state. In your case, that might not be true, and
    the alert might not have reached its thresholds. Nevertheless, I'll continue explaining
    my case, where both CPU and memory usage is less than fifty percent of total available.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 如果警报仍然不存在，请刷新警报屏幕。在我的情况下（以下是屏幕截图），保留内存和CPU的总量低于百分之五十，并且警报处于挂起状态。在您的情况下，这可能并不是真的，警报可能还没有达到其阈值。尽管如此，我将继续解释我的情况，在这种情况下，CPU和内存使用量都低于总可用量的百分之五十。
- en: 'Thirty minutes later (`for: 30m`), the alert fired. It waited for a while (`30m`)
    to confirm that the drop in memory and CPU usage is not temporary. Given that
    I''m running my cluster in AKS, Cluster Autoscaler would remove one of the nodes
    long before thirty minutes. But, since it is configured to operate with a minimum
    of three nodes, CA will not perform that action. As a result, I might need to
    reconsider whether paying for three nodes is a worthwhile investment. If, on the
    other hand, my cluster would be without Cluster Autoscaler, and assuming that
    I do not want to waste resources while other clusters might need more, I would
    need to remove one of the nodes (manually or automatically). If that removal were
    automatic, the destination would not be Slack, but the API of the tool in charge
    of removing nodes.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '三十分钟后（`for: 30m`），警报触发了。它等了一会儿（`30m`）以确认内存和CPU使用率的下降不是暂时的。鉴于我在AKS中运行我的集群，集群自动缩放器会在三十分钟之前删除一个节点。但是，由于它配置为最少三个节点，CA将不执行该操作。因此，我可能需要重新考虑是否支付三个节点是值得的投资。另一方面，如果我的集群没有集群自动缩放器，并且假设我不想在其他集群可能需要更多资源的情况下浪费资源，我需要删除一个节点（手动或自动）。如果那个移除是自动的，目的地将不是Slack，而是负责删除节点的工具的API。'
- en: '![](assets/88738e53-69bf-4683-9ca4-c8fdf35a59b8.png)Figure 3-33: Prometheus''
    alerts screen with one alert in the pending state'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/88738e53-69bf-4683-9ca4-c8fdf35a59b8.png)图3-33：Prometheus的警报屏幕，其中一个警报处于挂起状态'
- en: Now that we have a few examples of saturation, we covered each of the metrics
    championed by Google Site Reliability Engineers and almost any other monitoring
    method. Still, we're not done. There are a few other metrics and alerts I'd like
    to explore. They might not fall into any of the discussed categories, yet they
    might prove to be very useful.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一些饱和的例子，我们涵盖了谷歌网站可靠性工程师和几乎任何其他监控方法所推崇的每个指标。但我们还没有完成。还有一些其他指标和警报我想探索。它们可能不属于讨论的任何类别，但它们可能证明非常有用。
- en: Alerting on unschedulable or failed pods
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对无法调度或失败的Pod进行警报
- en: Knowing whether our applications are having trouble to respond fast to requests,
    whether they are being bombed with more requests than they could handle, whether
    they produce too many errors, and whether they are saturated, is of no use if
    they are not even running. Even if our alerts detect that something is wrong by
    notifying us that there are too many errors or that response times are slow due
    to an insufficient number of replicas, we should still be informed if, for example,
    one, or even all replicas failed to run. In the best case scenario, such a notification
    would provide additional info about the cause of an issue. In the much worse situation,
    we might find out that one of the replicas of the DB is not running. That would
    not necessarily slow it down nor it would produce any errors but would put us
    in a situation where data could not be replicated (additional replicas are not
    running) and we might face a total loss of its state if the last standing replica
    fails as well.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 知道我们的应用程序是否在快速响应请求方面出现问题，是否受到了比其处理能力更多的请求，是否产生了太多错误，以及是否饱和，如果它们甚至没有运行，这就没有用了。即使我们的警报通过通知我们出现了太多错误或由于副本数量不足而导致响应时间变慢，我们仍然应该被告知，例如，一个或甚至所有副本无法运行。在最好的情况下，这样的通知将提供有关问题原因的额外信息。在更糟糕的情况下，我们可能会发现数据库的一个副本没有运行。这不一定会减慢速度，也不会产生任何错误，但会使我们处于数据无法复制的状态（额外的副本没有运行），如果最后一个剩下的副本也失败，我们可能会面临其状态的完全丢失。
- en: There are many reasons why an application would fail to run. There might not
    be enough unreserved resources in the cluster. Cluster Autoscaler will deal with
    that problem if we have it. But, there are many other potential issues. Maybe
    the image of the new release is not available in the registry. Or, perhaps the
    Pods are requesting PersistentVolumes that cannot be claimed. As you might have
    guessed, the list of the things that might cause our Pods to fail, be unschedulable,
    or in an unknown state, is almost infinite.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序无法运行的原因有很多。集群中可能没有足够的未保留资源。如果出现这种情况，集群自动缩放器将处理该问题。但是，还有许多其他潜在问题。也许新版本的镜像在注册表中不可用。或者，Pod可能正在请求无法索赔的持久卷。正如你可能已经猜到的那样，可能导致我们的Pod失败、无法调度或处于未知状态的原因几乎是无限的。
- en: We cannot address all of the causes of problems with Pods individually. However,
    we can be notified if the phase of one or more Pods is `Failed`, `Unknown`, or
    `Pending`. Over time, we might extend our self-healing scripts to address some
    of the specific causes of those statuses. For now, our best first step is to be
    notified if a Pod is in one of those phases for a prolonged period of time (for
    example, fifteen minutes). Alerting as soon as the status of a Pod indicates a
    problem would be silly because that would generate too many false positives. We
    should get an alert and choose how to act only after waiting for a while, thus
    giving Kubernetes time to fix an issue. We should perform some reactive actions
    only if Kubernetes fails to remedy the situation.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法单独解决Pod问题的所有原因。但是，如果一个或多个Pod的阶段是“失败”、“未知”或“挂起”，我们可以收到通知。随着时间的推移，我们可能会扩展我们的自愈脚本，以解决其中一些状态的特定原因。目前，我们最好的第一步是在Pod长时间处于这些阶段之一时收到通知（例如，十五分钟）。如果在Pod的状态指示出现问题后立即发出警报，那将是愚蠢的，因为那样会产生太多的误报。只有在等待一段时间后，我们才应该收到警报并选择如何行动，从而给Kubernetes时间来解决问题。只有在Kubernetes未能解决问题时，我们才应该执行一些反应性操作。
- en: Over time, we'll notice some patterns in the alerts we're receiving. When we
    do, alerts should be converted into automated responses that will remedy selected
    issues without our involvement. We already explored some of the low hanging fruits
    through HorizontalPodAutoscaler and Cluster Autoscaler. For now, we'll focus on
    receiving alerts for all other cases, and failed and unschedulable Pods are a
    few of those. Later on, we might explore how to automate responses. But, that
    moment is not now, so we'll move forward with yet another alert that will result
    in a notification to Slack.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们会注意到我们收到的警报中存在一些模式。当我们发现时，警报应该被转换为自动响应，可以在不需要我们参与的情况下解决选定的问题。我们已经通过HorizontalPodAutoscaler和Cluster
    Autoscaler探索了一些低 hanging fruits。目前，我们将专注于接收所有其他情况的警报，而失败和无法调度的Pod是其中的一部分。稍后，我们可能会探索如何自动化响应。但是，现在还不是时候，所以我们将继续进行另一个警报，这将导致Slack收到通知。
- en: Let's open Prometheus' graph screen.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开Prometheus的图形屏幕。
- en: '[PRE105]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Please type the expression that follows and click the Execute button.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后单击“执行”按钮。
- en: '[PRE106]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: The output shows us each of the Pods in the cluster. If you take a closer look,
    you'll notice that there are five results for each Pod, one for each of the five
    possible phases. If you focus on the `phase` field, you'll see that there is an
    entry for `Failed`, `Pending`, `Running`, `Succeeded`, and `Unknown`. So, each
    Pod has five results, but only one has the value `1`, while the values of the
    other four are all set to `0`.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了集群中每个Pod的情况。如果你仔细观察，你会注意到每个Pod都有五个结果，分别对应五种可能的阶段。如果你关注`phase`字段，你会发现有一个条目是`Failed`、`Pending`、`Running`、`Succeeded`和`Unknown`。因此，每个Pod有五个结果，但只有一个值为`1`，而其他四个的值都设置为`0`。
- en: '![](assets/5f93fbd0-1d80-4b28-9ee1-fd26930ac478.png)Figure 3-34: Prometheus''
    console view with the phases of the Pods'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-34：Prometheus的控制台视图，显示了Pod的阶段
- en: For now, our interest lies primarily with alerts, and they should, in most cases,
    be generic and not related to a specific node, an application, a replica, or some
    other type of resources. Only when we are alerted that there is an issue, should
    we start digging deeper and look for more granular data. With that in mind, we'll
    rewrite our expression to retrieve the number of Pods in each of the phases.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们主要关注警报，它们在大多数情况下应该是通用的，与特定节点、应用程序、副本或其他类型的资源无关。只有当我们收到有问题的警报时，我们才应该开始深入挖掘并寻找更详细的数据。考虑到这一点，我们将重新编写我们的表达式，以检索每个阶段的Pod数量。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后单击“执行”按钮。
- en: '[PRE107]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: The output should show that all the Pods are in the `Running` phase. In my case,
    there are twenty-seven running Pods and none in any of the other phases.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示所有的Pod都处于`Running`阶段。在我的情况下，有二十七个正在运行的Pod，没有一个处于其他任何阶段。
- en: Now, we should not really care about healthy Pods. They are running, and there's
    nothing we should do about that. Instead, we should focus on those that are problematic.
    So, we might just as well rewrite the previous expression to retrieve the sum
    only of those that are in the `Failed`, `Unknown`, or `Pending` phase.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实际上不应该关心健康的Pod。它们正在运行，我们不需要做任何事情。相反，我们应该关注那些有问题的Pod。因此，我们可能会重新编写先前的表达式，只检索那些处于`Failed`、`Unknown`或`Pending`阶段的总和。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后单击“执行”按钮。
- en: '[PRE108]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: As expected, unless you messed up something, the values of the output are all
    set to `0`.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，除非您搞砸了什么，输出的值都设置为`0`。
- en: '![](assets/d0db1323-5bea-4eb6-bdf0-4a1717233f5e.png)Figure 3-35: Prometheus''
    console view with the sum of the Pods in Failed, Unknown, or Pending phases'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-35：Prometheus的控制台视图，显示了处于失败、未知或挂起阶段的Pod的总数
- en: So far, there are no Pods we should worry about. We'll change that by creating
    one that will intentionally fail, by using an image that apparently does not exist.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，没有我们需要担心的Pod。我们将通过创建一个故意失败的Pod来改变这种情况，使用一个显然不存在的镜像。
- en: '[PRE109]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: As we can see from the output, the `pod/problem` was `created`. If we created
    it through a script (for example, CI/CD pipeline), we'd think that everything
    is OK. Even if we'd follow it with `kubectl rollout status`, we would only ensure
    that it started working, not that it continues working.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，`pod/problem`已经被`created`。如果我们通过脚本（例如，CI/CD流水线）创建它，我们可能会认为一切都很好。即使我们跟着使用`kubectl
    rollout status`，我们只能确保它开始工作，而不能确保它继续工作。
- en: But, since we did not create that Pod through a CI/CD pipeline, but manually,
    we can just as well list all the Pods in the `default` Namespace.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于我们没有通过CI/CD流水线创建该Pod，而是手动创建的，我们可以列出`default`命名空间中的所有Pod。
- en: '[PRE110]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: The output is as follows.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE111]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: We'll imagine that we have only short-term memory and that we already forgot
    that the `image` is set to `i-do-not-exist`. What can be the issue? Well, the
    first step would be to describe the Pod.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设我们只有短期记忆，并且已经忘记了`image`设置为`i-do-not-exist`。问题可能是什么？嗯，第一步是描述Pod。
- en: '[PRE112]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: The output, limited to the messages of the `Events` section, is as follows.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于`Events`部分的消息，如下。
- en: '[PRE113]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: The problem is clearly manifested through the `Back-off pulling image "i-do-not-exist"`
    message. Further down, we can see the message from the container server stating
    that `it failed to pull image "i-do-not-exist"`.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 问题显然通过`Back-off pulling image "i-do-not-exist"`消息表现出来。在更下面，我们可以看到来自容器服务器的消息，说明`它未能拉取图像"i-do-not-exist"`。
- en: Of course, we knew in advance that will be the result, but something similar
    could happen without us noticing that there is an issue. The cause could be a
    failure to pull the image, or one of the myriads of others. Nevertheless, we are
    not supposed to sit in front of a terminal, listing and describing Pods and other
    types of resources. Instead, we should receive an alert that Kubernetes failed
    to run a Pod, and only after that, we should start digging for the cause of the
    issue. So, let's create one more alert that will notify us when Pods fail and
    do not recuperate.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们事先知道这将是结果，但类似的事情可能发生而我们没有注意到存在问题。原因可能是未能拉取镜像，或者其他无数的原因之一。然而，我们不应该坐在终端前，列出和描述Pod和其他类型的资源。相反，我们应该收到一个警报，告诉我们Kubernetes未能运行一个Pod，只有在那之后，我们才应该开始查找问题的原因。所以，让我们创建一个新的警报，当Pod失败且无法恢复时通知我们。
- en: Like many times before, we'll take a look at the differences between the old
    and the new definition of Prometheus' Chart values.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 像以前许多次一样，我们将查看Prometheus的图表值的旧定义和新定义之间的差异。
- en: '[PRE114]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: The output is as follows.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE115]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: We defined a new group of alerts called `pod`. Inside it, we have an `alert`
    named `ProblematicPods` that will fire if there are one or more Pods with the
    `Failed`, `Unknown`, or `Pending` phase for more than one minute (`1m`). I intentionally
    set it to very low `for` duration so that we can test it easily. Later on, we'll
    switch to fifteen minutes interval that will be more than enough to allow Kubernetes
    to remedy the issue before we get a notification that will send us into the panic
    mode.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为`pod`的新警报组。在其中，我们有一个名为`ProblematicPods`的`alert`，如果有一个或多个Pod的`Failed`、`Unknown`或`Pending`阶段持续超过一分钟（`1m`）将触发警报。我故意将它设置为非常短的`for`持续时间，以便我们可以轻松测试它。后来，我们将切换到十五分钟的间隔，这将足够让Kubernetes在我们收到通知之前解决问题，而不会让我们陷入恐慌模式。
- en: Let's update Prometheus' Chart with the updated values.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用更新后的值更新Prometheus的图表。
- en: '[PRE116]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Since we did not yet fix the issue with the `problem` Pod, we should see a new
    notification in Slack soon. Let's confirm that.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们尚未解决`problem` Pod的问题，我们很快应该在Slack上收到新的通知。让我们确认一下。
- en: '[PRE117]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: If your notification did not yet arrive, please wait for a few moments.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有收到通知，请稍等片刻。
- en: We got the message stating that `at least one Pod could not run`.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了一条消息，说明“至少有一个Pod无法运行”。
- en: '![](assets/a4035775-47d5-4dbe-a3bd-8142a286be0b.png)Figure 3-36: Slack with
    an alert message'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a4035775-47d5-4dbe-a3bd-8142a286be0b.png)图3-36：Slack上的警报消息'
- en: Now that we received a notification that there is a problem with one of the
    Pods, we should go to Prometheus, dig through data until we find the cause of
    the issue, and fix it. But, since we already know what the problem is (we created
    it intentionally), we'll skip all that, and remove the faulty Pod, before we move
    onto the next subject.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们收到了一个通知，说其中一个Pod出了问题，我们应该去Prometheus，挖掘数据，直到找到问题的原因，并解决它。但是，由于我们已经知道问题是什么（我们是故意创建的），我们将跳过所有这些，然后移除有问题的Pod，然后继续下一个主题。
- en: '[PRE118]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Upgrading old Pods
  id: totrans-526
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 升级旧的Pod
- en: Our primary goal should be to prevent issues from happening by being proactive.
    In cases when we cannot predict that a problem is about to materialize, we must,
    at least, be quick with our reactive actions that mitigate the issues after they
    occur. Still, there is a third category that can only loosely be characterized
    as being proactive. We should keep our system clean and up-to-date.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标应该是通过积极主动的方式防止问题发生。在我们无法预测问题即将出现的情况下，我们必须至少在问题发生后迅速采取反应措施来减轻问题。然而，还有第三种情况，可以宽泛地归类为积极主动。我们应该保持系统清洁和及时更新。
- en: Among many things we could do to keep the system up-to-date, is to make sure
    that our software is relatively recent (patched, updated, and so on). A reasonable
    rule could be to try to renew software after ninety days, if not earlier. That
    does not mean that everything we run in our cluster should be newer than ninety
    days, but that it might be a good starting point. Further on, we might create
    finer policies that would allow some kinds of applications (usually third-party)
    to live up to, let's say, half a year without being upgraded. Others, especially
    software we're actively developing, will probably be upgraded much more frequently.
    Nevertheless, our starting point is to detect all the applications that were not
    upgraded in ninety days or more.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多可以保持系统最新的事项中，是确保我们的软件相对较新（已打补丁、已更新等）。一个合理的规则可能是在九十天后尝试更新软件，如果不是更早。这并不意味着我们在集群中运行的所有东西都应该比九十天更新，但这可能是一个很好的起点。此外，我们可能会创建更精细的策略，允许某些类型的应用程序（通常是第三方）在不升级的情况下存活，比如说半年。其他的，特别是我们正在积极开发的软件，可能会更频繁地升级。尽管如此，我们的起点是检测所有在九十天或更长时间内没有升级的应用程序。
- en: Just as in almost all other exercises in this chapter, we'll start by opening
    Prometheus' graph screen, and explore the metrics that might help us reach our
    goal.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本章中几乎所有其他练习一样，我们将从打开Prometheus的图形屏幕开始，探索可能帮助我们实现目标的指标。
- en: '[PRE119]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: If we inspect the available metrics, we'll see that there is `kube_pod_start_time`.
    Its name provides a clear indication of its purpose. It provides the Unix timestamp
    that represents the start time of each Pod in the form of a Gauge. Let's see it
    in action.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查可用的指标，我们会看到有`kube_pod_start_time`。它的名称清楚地表明了它的目的。它以一个仪表的形式提供了每个Pod的启动时间的Unix时间戳。让我们看看它的作用。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后单击“执行”按钮。
- en: '[PRE120]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Those values alone are of no use, and there's no point in teaching you how to
    calculate the human date from those values. What matters, is the difference between
    now and those timestamps.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值本身没有用，教你如何从这些值计算出人类日期也没有意义。重要的是现在和那些时间戳之间的差异。
- en: '![](assets/d9204c74-681c-4097-8ce8-b76c8b78b6ab.png)Figure 3-37: Prometheus''
    console view with the start time of the Pods'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d9204c74-681c-4097-8ce8-b76c8b78b6ab.png)图3-37：Prometheus控制台视图，显示了Pod的启动时间'
- en: We can use Prometheus' `time()` function to return the number of seconds since
    January 1, 1970 UTC (or Unix timestamp).
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Prometheus的`time()`函数来返回自1970年1月1日UTC（或Unix时间戳）以来的秒数。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击执行按钮。
- en: '[PRE121]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Just as with the `kube_pod_start_time`, we got a long number that represents
    seconds since 1970\. The only noticeable difference, besides the value, is that
    there is only one entry, while with `kube_pod_start_time` we got a result for
    each Pod in the cluster.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`kube_pod_start_time`一样，我们得到了一个代表自1970年以来的秒数的长数字。除了值之外，唯一显着的区别是只有一个条目，而对于`kube_pod_start_time`，我们得到了集群中每个Pod的结果。
- en: Now, let's combine the two metrics in an attempt to retrieve the age of each
    of the Pods.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试结合这两个指标，以尝试检索每个Pod的年龄。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击执行按钮。
- en: '[PRE122]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: The results are this time much smaller numbers representing the seconds between
    now and creation of each of the Pods. In my case (screenshot following), the first
    Pod (one of the `go-demo-5` replicas), is slightly over six thousand seconds old.
    That would be around a hundred minutes (6096 / 60), or less than two hours (100
    min / 60 min = 1.666 h).
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这次的结果是表示现在与每个Pod创建之间的秒数的更小的数字。在我的情况下（以下是屏幕截图），第一个Pod（`go-demo-5`的一个副本）已经超过六千秒。那将是大约一百分钟（6096/60），或不到两个小时（100分钟/60分钟=1.666小时）。
- en: '![](assets/d38868b1-8abb-4ad3-aab7-915d496b32bc.png)Figure 3-38: Prometheus''
    console view with the time passed since the creation of the Pods'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d38868b1-8abb-4ad3-aab7-915d496b32bc.png)图3-38：Prometheus控制台视图，显示了Pod创建以来经过的时间'
- en: Since there are probably no Pods older than our target of ninety days, we'll
    lower it temporarily to a minute (sixty seconds).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可能没有Pod比我们的目标九十天更老，我们将临时将其降低到一分钟（六十秒）。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击执行按钮。
- en: '[PRE123]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: In my case, all the Pods are older than a minute (as are probably yours as well).
    We confirmed that it works so we can increase the threshold to ninety days. To
    get to ninety days, we should multiply the threshold with sixty to get minutes,
    with another sixty to get hours, with twenty-four to get days, and, finally, with
    ninety. The formula would be `60 * 60 * 24 * 90`. We could use the final value
    of `7776000`, but that would make the query harder to decipher. I prefer using
    the formula instead.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，所有的Pod都比一分钟大（你的情况可能也是如此）。我们确认它可以工作，所以我们可以将阈值增加到九十天。要达到九十天，我们应该将阈值乘以六十得到分钟，再乘以六十得到小时，再乘以二十四得到天，最后再乘以九十。公式将是`60
    * 60 * 24 * 90`。我们可以使用最终值`7776000`，但那会使查询更难解读。我更喜欢使用公式。
- en: Please type the expression that follows and click the Execute button.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击执行按钮。
- en: '[PRE124]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: It should come as no surprise that there are (probably) no results. If you created
    a new cluster for this chapter, you'd need to be the slowest reader on earth if
    it took you ninety days to get here. This might be the longest chapter I've written
    so far, but it's still not worth ninety days of reading.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，可能没有结果。如果您为本章创建了一个新的集群，如果您花了九十天才到这里，那您可能是地球上最慢的读者。这可能是我迄今为止写过的最长的一章，但仍不值得花九十天的时间来阅读。
- en: Now that we know which expression to use, we can add one more alert to our setup.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道要使用哪个表达式，我们可以在我们的设置中添加一个警报。
- en: '[PRE125]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: The output is as follows.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE126]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: We can see that the difference between the old and the new values is in the
    `OldPods` alert. It contains the same expression we used a few moments ago.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到旧值和新值之间的差异在`OldPods`警报中。它包含了我们几分钟前使用的相同表达式。
- en: We kept the low threshold of `60` seconds so that we can see the alert in action.
    Later on, we'll increase that value to ninety days.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保持了`60`秒的低阈值，以便我们可以看到警报的作用。以后，我们将把该值增加到90天。
- en: There was no need to specify `for` duration. The alert should fire the moment
    the age of one of the Pods reaches three months (give or take).
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要指定`for`持续时间。一旦其中一个Pod的年龄达到三个月（加减），警报就会触发。
- en: Let's upgrade our Prometheus' Chart with the updated values and open the Slack
    channel where we should see the new message.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用更新后的值升级我们的Prometheus图表，并打开Slack频道，我们应该能看到新消息。
- en: '[PRE127]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: All that's left is to wait for a few moments until the new message arrives.
    It should contain the title *Old Pods* and the text stating that *At least one
    Pod has not been updated to more than 90 days*.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只需等待片刻，直到新消息到达。它应该包含标题*旧的Pod*和文本说明*至少有一个Pod未更新超过90天*。
- en: '![](assets/83874a98-4079-4787-9dce-83f28f3c7db4.png)Figure 3-39: Slack with
    multiple fired and resolved alert messages'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/83874a98-4079-4787-9dce-83f28f3c7db4.png)图3-39：Slack显示多个触发和解决的警报消息'
- en: Such a generic alert might not work for all your use-cases. But, I'm sure that
    you'll be able to split it into multiple alerts based on Namespaces, names, or
    something similar.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个通用的警报可能不适用于您所有的用例。但是，我相信您可以根据命名空间、名称或类似的内容将其拆分为多个警报。
- en: Now that we have a mechanism to receive notifications when our Pods are too
    old and might require upgrades, we'll jump into the next topic and explore how
    to retrieve memory and CPU used by our containers.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个机制，可以在我们的Pod过旧并且可能需要升级时接收通知，我们将进入下一个主题，探讨如何检索容器使用的内存和CPU。
- en: Measuring containers memory and CPU usage
  id: totrans-565
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量容器的内存和CPU使用
- en: If you are familiar with Kubernetes, you understand the importance of defining
    resource requests and limits. Since we already explored `kubectl top pods` command,
    you might have set the requested resources to match the current usage, and you
    might have defined the limits as being above the requests. That approach might
    work on the first day. But, with time, those numbers will change and we will not
    be able to get the full picture through `kubectl top pods`. We need to know how
    much memory and CPU containers use when on their peak loads, and how much when
    they are under less stress. We should observe those metrics over time, and adjust
    periodically.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉Kubernetes，您就会理解定义资源请求和限制的重要性。由于我们已经探讨了`kubectl top pods`命令，您可能已经设置了请求的资源以匹配当前的使用情况，并且可能已经定义了限制高于请求。这种方法可能在第一天有效。但是，随着时间的推移，这些数字将发生变化，我们将无法通过`kubectl
    top pods`获得全面的图片。我们需要知道容器在峰值负载时使用多少内存和CPU，以及在压力较小时使用多少。我们应该随时间观察这些指标，并定期进行调整。
- en: Even if we do somehow manage to guess how much memory and CPU a container needs,
    those numbers might change from one release to another. Maybe we introduced a
    feature that requires more memory or CPU?
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们设法猜出容器需要多少内存和CPU，这些数字也可能会从一个版本到另一个版本发生变化。也许我们引入了一个需要更多内存或CPU的功能？
- en: What we need is to observe resource usage over time and to make sure that it
    does not change with new releases or with increased (or decreased) number of users.
    For now, we'll focus on the former case and explore how to see how much memory
    and CPU our containers used over time.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要观察资源使用情况随时间的变化，并确保它不会随着新版本的发布或用户数量的增加（或减少）而改变。现在，我们将专注于前一种情况，并探讨如何查看容器随时间使用了多少内存和CPU。
- en: As usual, we'll start by opening the Prometheus' graph screen.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将首先打开Prometheus的图表屏幕。
- en: '[PRE128]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: We can retrieve container memory usage through the `container_memory_usage_bytes`.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`container_memory_usage_bytes`来检索容器的内存使用情况。
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* screen.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入下面的表达式，点击执行按钮，然后切换到*图表*屏幕。
- en: '[PRE129]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: If you take a closer look at the top usage, you'll probably end up confused.
    It seems that some containers are using way more than the expected amount of memory.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察顶部的使用情况，你可能会感到困惑。似乎有些容器使用的内存远远超出预期的数量。
- en: The truth is that some of the `container_memory_usage_bytes` records contain
    cumulative values, and we should exclude them so that only memory usage of individual
    containers is retrieved. We can do that by retrieving only the records that have
    a value in the `container_name` field.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一些`container_memory_usage_bytes`记录包含累积值，我们应该排除它们，以便只检索单个容器的内存使用情况。我们可以通过仅检索在`container_name`字段中具有值的记录来实现这一点。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入下面的表达式，然后点击执行按钮。
- en: '[PRE130]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Now the result makes much more sense. It reflects memory usage of the containers
    running inside our cluster.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 现在结果更有意义了。它反映了我们集群内运行的容器的内存使用情况。
- en: We'll get to alerts based on container resources a bit later. For now, we'll
    imagine that we'd like to check memory usage of a specific container (for example,
    `prometheus-server`). Since we already know that one of the available labels is
    `container_name`, retrieving the data we need should be straightforward.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将基于容器资源设置警报。现在，我们假设我们想要检查特定容器的内存使用情况（例如`prometheus-server`）。由于我们已经知道可用标签之一是`container_name`，检索我们需要的数据应该是直截了当的。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入下面的表达式，然后点击执行按钮。
- en: '[PRE131]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: We can see the oscillations in memory usage of the container over the last hour.
    Normally, we'd be interested in a longer period like a day or a week. We can accomplish
    that by clicking - and + buttons above the graph, or by typing the value directly
    in the field between them (for example, `1w`). However, changing the duration
    might not help much since we haven't been running the cluster for too long. We
    might not be able to squeeze more data than a few hours unless you are a slow
    reader.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到容器在过去一小时内内存使用情况的波动。通常，我们会对一天或一周这样更长的时间段感兴趣。我们可以通过点击图表上方的-和+按钮，或者直接在它们之间的字段中输入值（例如`1w`）来实现这一点。然而，改变持续时间可能不会有太大帮助，因为我们运行集群的时间不长。除非你阅读速度很慢，否则我们可能无法获得比几个小时更多的数据。
- en: '![](assets/a6c281c9-3cdc-451b-88db-a3e4409d6e53.png)Figure 3-40: Prometheus''
    graph screen with container memory usage limited to prometheus-server'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a6c281c9-3cdc-451b-88db-a3e4409d6e53.png)图3-40：限制为prometheus-server的Prometheus图表屏幕上的容器内存使用情况'
- en: Similarly, we should be able to retrieve CPU usage of a container as well. In
    that case, the metric we're looking for could be `container_cpu_usage_seconds_total`.
    However, unlike `container_memory_usage_bytes` that is a gauge, `container_cpu_usage_seconds_total`
    is a counter, and we'll have to combine `sum` and `rate` to get the changes in
    values over time.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们应该能够检索容器的CPU使用情况。在这种情况下，我们正在寻找的指标可能是`container_cpu_usage_seconds_total`。然而，与`container_memory_usage_bytes`不同，它是一个计数器，我们将不得不结合`sum`和`rate`来获得随时间变化的值。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE132]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: The query shows summed CPU seconds rate over five minutes intervals. We added
    `by (pod_name)` to the mix so that we can distinguish different Pods and see when
    one was created, and the other was destroyed.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 查询显示了五分钟间隔内的CPU秒速总和。我们添加了`by (pod_name)`到混合中，以便我们可以区分不同的Pod，并查看一个是何时创建的，另一个是何时销毁的。
- en: '![](assets/3c443319-d0bf-42c8-bb32-fd7c0f423ed4.png)Figure 3-41: Prometheus''
    graph screen with the rate of container CPU usage limited to prometheus-server'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-41：Prometheus的图形屏幕，限制为prometheus-server的容器CPU使用率
- en: If that were a "real world" situation, our next step would be to compare actual
    resource usage with what we defined as Prometheus `resources`. If what we defined
    differs considerably compared to what it actually is, we should probably update
    our Pod definition (the `resources` section).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个“现实世界”的情况，我们下一步将是将实际资源使用与我们在Prometheus`资源`中定义的进行比较。如果我们定义的与实际情况相比差异很大，我们可能应该更新我们的Pod定义（`资源`部分）。
- en: The problem is that using "real" resource usage to define Kubernetes `resources`
    better will provide valid values only temporarily. Over time, our resource usage
    will change. The load might increase, new features might be more resource-hungry,
    and so on. No matter the reasons, the critical thing to note is that everything
    is dynamic and that there is no reason to think otherwise for resources. In that
    spirit, our next challenge is to figure out how to get a notification when the
    actual resource usage differs too much from what we defined in container `resources`.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于使用“真实”资源使用情况来定义Kubernetes的`资源`将只会暂时提供有效值。随着时间的推移，我们的资源使用情况将会发生变化。负载可能会增加，新功能可能会更加耗费资源，等等。无论原因是什么，需要注意的关键一点是一切都是动态的，对于资源来说没有理由认为会有其他情况。在这种精神下，我们下一个挑战是找出当实际资源使用与我们在容器`资源`中定义的差异太大时如何获得通知。
- en: Comparing actual resource usage with defined requests
  id: totrans-591
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将实际资源使用与定义的请求进行比较
- en: If we define container `resources` inside a Pod and without relying on actual
    usage, we are just guessing how much memory and CPU we expect a container to use.
    I'm sure that you already know why guessing, in the software industry, is a terrible
    idea, so I'll focus on Kubernetes aspects only.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在Pod中定义容器的`资源`而不依赖于实际使用情况，我们只是在猜测容器将使用多少内存和CPU。我相信你已经知道为什么在软件行业中猜测是一个糟糕的主意，所以我将只关注Kubernetes方面。
- en: Kubernetes treats Pods with containers that do not have specified resources
    as the **BestEffort Quality of Service** (**QoS**). As a result, if it ever runs
    out of memory or CPU to serve all the Pods, those are the first to be forcefully
    removed to leave space for others. If such Pods are short lived as, for example,
    those used as one-shot agents for continuous delivery processes, BestEffort QoS
    is not a bad idea. But, when our applications are long-lived, BestEffort QoS should
    be unacceptable. That means that in most cases, we do have to define container
    `resources`.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes将没有指定资源的容器的Pod视为**BestEffort Quality of Service**（**QoS**）。因此，如果它的内存或CPU不足以为所有的Pod提供服务，那么这些Pod将被强制删除，为其他Pod腾出空间。如果这样的Pod是短暂的，例如用作持续交付流程的一次性代理，BestEffort
    QoS并不是一个坏主意。但是，当我们的应用是长期运行时，BestEffort QoS应该是不可接受的。这意味着在大多数情况下，我们必须定义容器的`resources`。
- en: If container `resources` are (almost always) a must, we need to know which values
    to put. I often see teams that merely guess. "It's a database; therefore it needs
    a lot of RAM" and "it's only an API, it shouldn't need much" are only a few of
    the sentences I hear a lot. Those guesstimates are often the result of not being
    able to measure actual usage. When something would blow up, those teams would
    just double the allocated memory and CPU. Problem solved!
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器的`resources`（几乎总是）是必须的，我们需要知道要设置哪些值。我经常看到团队仅仅是猜测。“这是一个数据库，因此它需要大量的RAM”，“它只是一个API，不应该需要太多”是我经常听到的一些句子。这些猜测往往是由于无法测量实际使用情况而产生的。当出现问题时，这些团队会简单地将分配的内存和CPU加倍。问题解决了！
- en: I never understood why would anyone invent how much memory and CPU an application
    needs. Even without any "fancy" tools, we always had `top` command in Linux. We
    could know how much our application uses. Over time, better tools were developed,
    and all we had to do is Google "how to measure memory and CPU of my applications."
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 我从来不明白为什么会有人发明应用程序需要多少内存和CPU。即使没有任何“花哨”的工具，我们在Linux中总是有`top`命令。我们可以知道我们的应用程序使用了多少。随着时间的推移，出现了更好的工具，我们所要做的就是谷歌“如何测量我的应用程序的内存和CPU”。
- en: You already saw `kubectl top pods` in action when you need current data, and
    you are becoming familiar with the power of Prometheus to give you much more.
    You do not have an excuse to guesstimate.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要当前数据时，你已经看到了`kubectl top pods`的操作，并且你已经开始熟悉Prometheus的强大功能。你没有理由去猜测。
- en: But, why do we care about resource usage compared with requested resources?
    Besides the fact that might reveal a potential problem (for example, memory leak),
    inaccurate resource requests and limits prevent Kubernetes from doing its job
    efficiently. If, for example, we define memory request to 1 GB RAM, that's how
    much Kubernetes will remove from allocatable memory. If a node has 2 GB of allocatable
    RAM, only two such containers could run there, even if each uses only 50 MB RAM.
    Our nodes would use only a fraction of allocatable memory and, if we have Cluster
    Autoscaler, new nodes would be added even if the old ones still have plenty of
    unused memory.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么我们关心资源使用情况与请求的资源相比呢？除了可能揭示潜在问题（例如内存泄漏）之外，不准确的资源请求和限制会阻止Kubernetes有效地完成其工作。例如，如果我们将内存请求定义为1GB
    RAM，那么Kubernetes将从可分配的内存中删除这么多。如果一个节点有2GB的可分配RAM，即使每个节点只使用50MB RAM，也只能运行两个这样的容器。我们的节点将只使用可分配内存的一小部分，如果我们有集群自动缩放器，即使旧节点仍有大量未使用的内存，新节点也会被添加。
- en: Even though now we know how to get actual memory usage, it would be a waste
    of time to start every day by comparing YAML files with the results in Prometheus.
    Instead, we'll create yet another alert that will send us a notification whenever
    the requested memory and CPU differs too much from the actual usage. That's our
    next mission.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 即使现在我们知道如何获取实际内存使用情况，每天都通过比较YAML文件和Prometheus中的结果来开始工作将是浪费时间。相反，我们将创建另一个警报，当请求的内存和CPU与实际使用情况相差太大时，它将发送通知给我们。这是我们的下一个任务。
- en: First, we'll reopen Prometheus' graph screen.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将重新打开Prometheus的图表屏幕。
- en: '[PRE133]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: We already know how to get memory usage through `container_memory_usage_bytes`,
    so we'll jump straight into retrieving requested memory. If we can combine the
    two, we'll get the discrepancy between the requested and the actual memory usage.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何通过`container_memory_usage_bytes`获取内存使用情况，所以我们将直接开始检索请求的内存。如果我们可以将这两者结合起来，我们将得到请求的内存和实际内存使用之间的差异。
- en: The metric we're looking for is `kube_pod_container_resource_requests_memory_bytes`,
    so let's take it for a spin with, let's say, `prometheus-server` Pod.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找的指标是`kube_pod_container_resource_requests_memory_bytes`，所以让我们以`prometheus-server`
    Pod为例来试一试。
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击“执行”按钮，切换到*图表*选项卡。
- en: '[PRE134]'
  id: totrans-604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: We can see from the result that we requested 500 MB RAM for the `prometheus-server`
    container.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以看到，我们为`prometheus-server`容器请求了500MB的RAM。
- en: '![](assets/c2002dc2-0574-48b0-93a5-22d701f67efa.png)Figure 3-42: Prometheus''
    graph screen with container requested memory limited to prometheus-server'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c2002dc2-0574-48b0-93a5-22d701f67efa.png)图3-42：Prometheus的图表屏幕，容器请求的内存限制为prometheus-server'
- en: The problem is that the `kube_pod_container_resource_requests_memory_bytes`
    metric has, among others, `pod` label while, on the other hand, `container_memory_usage_bytes`
    uses `pod_name`. If we are to combine the two, we need to transform the label
    `pod` into `pod_name`. Fortunately, this is not the first time we're facing that
    problem, and we already know that the solution is to use the `label_join` function
    that will create a new label based on one or more of the existing labels.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于`kube_pod_container_resource_requests_memory_bytes`指标中，除了`pod`标签外，还有`container_memory_usage_bytes`使用`pod_name`。如果我们要将两者结合起来，我们需要将标签`pod`转换为`pod_name`。幸运的是，这不是我们第一次面临这个问题，我们已经知道解决方案是使用`label_join`函数，它将基于一个或多个现有标签创建一个新标签。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击“执行”按钮。
- en: '[PRE135]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: This time, not only that we added a new label to the metric, but we also grouped
    the results by that very same label (`by (pod)`).
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们不仅为指标添加了一个新标签，而且还通过这个新标签（`by (pod)`）对结果进行了分组。
- en: '![](assets/ca12f3cb-0be1-487f-93c4-626630e0b2b0.png)Figure 3-43: Prometheus''
    graph screen with container memory usage limited to prometheus-server and grouped
    by the pod label extracted from pod_name'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ca12f3cb-0be1-487f-93c4-626630e0b2b0.png)图3-43：Prometheus的图表屏幕，容器内存使用限制为prometheus-server，并按从pod_name提取的pod标签进行分组。'
- en: Now we can combine the two metrics and find out the discrepancy between the
    requested and the actual memory usage.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这两个指标结合起来，找出请求的内存和实际内存使用之间的差异。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后点击“执行”按钮。
- en: '[PRE136]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: In my case (screenshot following), the discrepancy was becoming gradually smaller.
    It started somewhere around sixty percent, and now it's approximately seventy-five
    percent. Such a difference is not big enough for us to take any corrective action.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下（以下是屏幕截图），差异逐渐变小。它开始时大约为百分之六十，现在大约为百分之七十五。这样的差异对我们来说不足以采取任何纠正措施。
- en: '![](assets/2f253a04-5003-490d-8be4-ed23bc20e24f.png)Figure 3-44: Prometheus''
    graph screen with the percentage of container memory usage based on requested
    memory'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-44：Prometheus的图形屏幕，显示基于请求内存的容器内存使用百分比
- en: Now that we saw how to get the difference between reserved and actual memory
    usage for a single container, we should probably make the expression more general
    and get all the containers in the cluster. However, all might be a bit too much.
    We probably do not want to mess with the Pods running in the `kube-system` Namespace.
    They are likely pre-installed in the cluster, and we might want to leave them
    as they are, at least for now. So, we'll exclude them from the query.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何获取单个容器的保留和实际内存使用之间的差异，我们可能应该使表达更加普遍，并获取集群中的所有容器。但是，获取所有可能有点太多了。我们可能不想干扰运行在`kube-system`命名空间中的Pod。它们可能是预先安装在集群中的，至少目前我们可能希望将它们保持原样。因此，我们将在查询中排除它们。
- en: Please type the expression that follows, and press the Execute button.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，然后按“执行”按钮。
- en: '[PRE137]'
  id: totrans-619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: The result should be the list of percentages of a difference between requested
    and actual memory, with the Pods in the `kube-system` excluded.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是请求和实际内存之间差异的百分比列表，排除了`kube-system`中的Pod。
- en: In my case, there are quite a few containers that use quite a lot more memory
    than what we requested. The main culprit is `prometheus-alertmanager` which uses
    more than three times more memory than what we requested. That can be due to several
    reasons. Maybe we requested too little memory, or perhaps it contains containers
    that do not have `requests` specified. In either case, we should probably redefine
    requests not only for the Alertmanager but also for all the other Pods that use
    more than, let's say 50% more memory than requested.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，有相当多的容器使用的内存比我们请求的要多得多。主要问题是`prometheus-alertmanager`，它使用的内存比我们请求的要多三倍以上。这可能由于几个原因。也许我们请求的内存太少，或者它包含的容器没有指定`requests`。无论哪种情况，我们可能应该重新定义请求，不仅针对Alertmanager，还针对所有使用的内存比请求的多50%以上的其他Pod。
- en: '![](assets/9f028f63-5a85-46be-8f15-2727d4f2c585.png)Figure 3-45: Prometheus''
    graph screen with the percentage of container memory usage based on requested
    memory and with those from the kube-system Namespace excluded'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-45：Prometheus的图形屏幕，显示基于请求内存的容器内存使用百分比，排除了来自kube-system命名空间的容器
- en: We are about to define a new alert that will deal with cases when requested
    memory is much more or much less than the actual usage. But, before we do that,
    we should discuss the conditions we should use. One alert could fire when actual
    memory usage is over 150% of the requested memory for over an hour. That would
    remove false positives caused by a temporary spike in memory usage (that's why
    we have `limits` as well). The other alert could deal with the situation when
    memory usage is more than 50% below the requested amount. But, in case of that
    alert, we might add another condition.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将定义一个新的警报，用于处理请求的内存远高于或远低于实际使用情况的情况。但在这样做之前，我们应该讨论应该使用的条件。一个警报可以在实际内存使用超过请求内存的150%以上并持续一个小时以上时触发。这将消除由内存使用暂时激增引起的误报（这就是为什么我们也有`limits`）。另一个警报可以处理内存使用量低于请求量50%以上的情况。但是，在这种警报情况下，我们可能会添加另一个条件。
- en: Some applications are too small, and we might never be able to fine-tune their
    requests. We can exclude those cases by adding another condition that will ignore
    the Pods with only 5 MB reserved RAM, or less.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 有些应用程序太小，我们可能永远无法调整它们的请求。我们可以通过添加另一个条件来排除这些情况，该条件将忽略仅保留了5MB或更少内存的Pod。
- en: Finally, this alert might not need to fire as frequently as the previous one.
    We should know relatively quickly if our application uses more memory than we
    intended to give since that can be a sign of a memory leak, significantly increased
    traffic, or some other, potentially dangerous situation. But, if memory uses much
    less than intended, the issue is not as critical. We should correct it, but there
    is no need to act urgently. Therefore, we'll set the duration of the latter alert
    to six hours.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个警报可能不需要像之前的那样频繁地触发。我们应该相对快速地知道我们的应用程序是否使用了比我们打算给予的更多内存，因为这可能是内存泄漏、显著增加的流量或其他潜在危险情况的迹象。但是，如果内存使用远低于预期，问题就不那么紧急了。我们应该纠正它，但没有必要紧急采取行动。因此，我们将后一个警报的持续时间设置为六小时。
- en: Now that we set a few rules we should follow, we can take a look at yet another
    differences between the old and the new set of Chart's values.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经制定了一些规则，我们可以看一下旧值和新值之间的另一个差异。
- en: '[PRE138]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: The output is as follows.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE139]'
  id: totrans-629
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: First, we set the threshold of the `OldPods` alert back to its intended value
    of ninety days (`60 * 60 * 24 * 90`). That way we'll stop it from firing alerts
    only for test purposes.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将`OldPods`警报的阈值重新设置为其预期值九十天（`60 * 60 * 24 * 90`）。这样我们就可以阻止它仅用于测试目的触发警报。
- en: Next, we defined a new alert called `ReservedMemTooLow`. It will fire if used
    memory is more than `1.5` times bigger than the requested memory. The duration
    for the pending state of the alert is set to `1m`, only so that we can see the
    outcome without waiting for the full hour. Later on, we'll restore it back to
    `1h`.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一个名为`ReservedMemTooLow`的新警报。如果使用的内存比请求的内存大`1.5`倍，它将触发。警报的挂起状态持续时间设置为`1m`，只是为了我们可以在不等待整个小时的情况下看到结果。稍后，我们将把它恢复为`1h`。
- en: The `ReservedMemTooHigh` alert is (partly) similar to the previous one, except
    that it has the condition that will cause the alert to fire if the difference
    between the actual and the requested memory is less than `0.5` and if that continues
    being the case for over `6m` (we'll change it later to `6h`). The second part
    of the expression is new. It requires that all the containers in a Pod have more
    than 5 MB of the requested memory (`5.25e+06`). Through that second statement
    (separated with `and`), we're saving ourselves from dealing with too small applications.
    If it requires less than 5 MB RAM, we should ignore it and, probably, congratulate
    the team behind it for making it that efficient.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReservedMemTooHigh`警报与之前的部分类似，不同之处在于，如果实际内存和请求内存之间的差异小于`0.5`，并且如果这种情况持续超过`6m`（我们稍后将其更改为`6h`），则会触发警报。表达式的第二部分是新的。它要求Pod中的所有容器都具有超过5MB的请求内存（`5.25e+06`）。通过第二个语句（用`and`分隔），我们可以避免处理太小的应用程序。如果需要的内存小于5MB，我们应该忽略它，并且可能要祝贺背后的团队使其如此高效。'
- en: Now, let's upgrade our Prometheus' Chart with the updated values and open the
    graph screen.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用更新后的值升级我们的Prometheus图表，并打开图表屏幕。
- en: '[PRE140]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: We won't wait until the alerts start firing. Instead, we'll try to accomplish
    similar objectives, but with CPU.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会等到警报开始触发。相反，我们将尝试实现类似的目标，但使用CPU。
- en: There's probably no need to go through the process of explaining the expressions
    we'll use. We'll jump straight into the CPU-based alerts by exploring the difference
    between the old and the new set of Chart's values.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 可能没有必要解释我们将使用的表达式的过程。我们将直接跳入基于CPU的警报，探索旧值和新值之间的差异。
- en: '[PRE141]'
  id: totrans-637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: The output is as follows.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE142]'
  id: totrans-639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: The first two sets of differences are defining more sensible thresholds for
    `ReservedMemTooLow` and `ReservedMemTooHigh` alerts we explored previously. Further
    down, we can see the two new alerts.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 前两组差异是为我们之前探讨的`ReservedMemTooLow`和`ReservedMemTooHigh`警报定义更明智的阈值。在更下面，我们可以看到两个新的警报。
- en: The `ReservedCPUTooLow` alert will fire if CPU usage is more than 1.5 times
    bigger than requested. Similarly, the `ReservedCPUTooHigh` alert will fire only
    if CPU usage is less than half of the requested and if we requested more than
    5 CPU milliseconds. Getting notifications because 5 MB RAM is too much would be
    a waste of time.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 如果CPU使用量超过请求量的1.5倍，将触发`ReservedCPUTooLow`警报。同样，只有当CPU使用量少于请求量的一半，并且我们请求的CPU毫秒数超过5时，才会触发`ReservedCPUTooHigh`警报。因为5MB
    RAM太多而收到通知将是浪费时间。
- en: Both alerts are set to fire if the issues persist for a short period (`1m` and
    `6m`) so that we can see them in action without having to wait for too long.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个警报都设置为在短时间内持续存在（`1m`和`6m`），这样我们就可以看到它们的作用，而不必等待太长时间。
- en: Now, let's upgrade our Prometheus' Chart with the updated values.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用更新后的值升级我们的Prometheus图表。
- en: '[PRE143]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: I'll leave it to you to check whether any of the alerts fire and whether they
    are forwarded from Alertmanager to Slack. You should know how to do that by now.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 我会让你去检查是否有任何警报触发，以及它们是否从Alertmanager转发到Slack。你现在应该知道如何做了。
- en: Next, we'll move to the last alert in this chapter.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转移到本章的最后一个警报。
- en: Comparing actual resource usage with defined limits
  id: totrans-647
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较实际资源使用与定义的限制
- en: Knowing when a container uses too much or too few resources compared to requests
    helps us be more precise with resource definitions and, ultimately, help Kubernetes
    make better decisions where to schedule our Pods. In most cases, having too big
    of a discrepancy between requested and actual resource usage will not result in
    malfunctioning. Instead, it is more likely to result in an unbalanced distribution
    of Pods or in having more nodes than we need. Limits, on the other hand, are a
    different story.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 了解容器使用资源与请求相比使用过多或过少的情况有助于我们更精确地定义资源，并最终帮助Kubernetes更好地决定在哪里调度我们的Pods。在大多数情况下，请求和实际资源使用之间存在较大的差异通常不会导致故障。相反，更有可能导致Pods的分布不平衡或节点过多。另一方面，限制则是另一回事。
- en: If resource usage of our containers enveloped as Pods reaches the specified
    `limits`, Kubernetes might kill those containers if there's not enough memory
    for all. It does that as a way to protect the integrity of the rest of the system.
    Killed Pods are not a permanent problem since Kubernetes will almost immediately
    reschedule them if there is enough capacity.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们容器作为Pods的资源使用达到指定的`limits`，Kubernetes可能会杀死这些容器，如果没有足够的内存。它这样做是为了保护系统的完整性。被杀死的Pod并不是一个永久性的问题，因为Kubernetes几乎会立即重新调度它们，如果有足够的容量。
- en: If we do use Cluster Autoscaling, even if there isn't enough capacity, new nodes
    will be added as soon as it detects that some Pods are in the pending state (unschedulable).
    So, the world is not likely to end if resource usage goes over the limits.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用集群自动缩放，即使容量不足，一旦检测到一些Pod处于挂起状态（无法调度），新节点将被添加。因此，如果资源使用超过限制，世界不太可能会结束。
- en: Nevertheless, killing and rescheduling Pods can result in downtime. There are,
    apparently, worse scenarios that might happen. But we won't go into them. Instead,
    we'll assume that we should be aware that a Pod is about to reach its limits and
    that we might want to investigate what's going on and we that might need to take
    some corrective measures. Maybe the latest release introduced a memory leak? Or
    perhaps the load increased beyond what we expected and tested and that results
    in increased memory usage. The cause of using memory that is close to the limit
    is not the focus right now. Detecting that we are reaching the limit is.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，杀死和重新安排Pod可能会导致停机时间。显然，可能会发生更糟糕的情况。但我们不会深入讨论。相反，我们将假设我们应该意识到一个Pod即将达到其极限，我们可能需要调查发生了什么，并且可能需要采取一些纠正措施。也许最新的发布引入了内存泄漏？或者负载增加超出了我们预期和测试的范围，导致内存使用增加。目前不关注接近极限的内存使用的原因，而是关注是否达到了极限。
- en: First, we'll go back to Prometheus' graph screen.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将返回Prometheus的图表屏幕。
- en: '[PRE144]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: We already know that we can get actual memory usage through the `container_memory_usage_bytes`
    metric. Since we already explored how to get requested memory, we can guess that
    limits are similar. They indeed are, and they can be retrieved through the `kube_pod_container_resource_limits_memory_bytes`.
    Since one of the metrics is the same as before, and the other is very similar,
    we'll jump straight into executing the full query.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道可以通过`container_memory_usage_bytes`指标获取实际内存使用情况。由于我们已经探讨了如何获取请求的内存，我们可以猜测极限是类似的。它们确实是，可以通过`kube_pod_container_resource_limits_memory_bytes`获取。由于其中一个指标与以前相同，另一个非常相似，我们将直接执行完整查询。
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 请键入以下表达式，按“执行”按钮，然后切换到*图表*选项卡。
- en: '[PRE145]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: In my case (screenshot following), we can see that quite a few Pods use more
    memory than what is defined as their limits.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下（以下是屏幕截图），我们可以看到相当多的Pod使用的内存超过了定义的极限。
- en: Fortunately, I do have spare capacity in my cluster, and there is no imminent
    need for Kubernetes to kill any of the Pods. Moreover, the issue might not be
    in Pods using more than what is set as their limits, but that not all containers
    in those Pods have the limits set. In either case, I should probably update the
    definition of those Pods/containers and make sure that their limits are above
    their average usage over a few days or even weeks.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我的集群中有多余的容量，Kubernetes没有迫切需要杀死任何Pod。此外，问题可能不在于Pod使用的超出其极限的情况，而是这些Pod中并非所有容器都设置了极限。无论哪种情况，我可能应该更新这些Pod/容器的定义，并确保它们的极限高于几天甚至几周的平均使用量。
- en: '![](assets/c57b7c9d-4904-4167-b320-ac2a31877b8a.png)Figure 3-46: Prometheus''
    graph screen with the percentage of container memory usage based on memory limits
    and with those from the kube-system Namespace excluded'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c57b7c9d-4904-4167-b320-ac2a31877b8a.png)图3-46：基于内存限制的容器内存使用百分比的Prometheus图表屏幕，排除了kube-system命名空间中的内存使用情况'
- en: Next, we'll go through the drill of exploring the difference between the old
    and the new version of the values.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细探讨旧值和新值之间的差异。
- en: '[PRE146]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: The output is as follows.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE147]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Apart from restoring sensible thresholds for the alerts we used before, we defined
    a new alert called `MemoryAtTheLimit`. It will fire if the actual usage is over
    eighty percent (`0.8`) of the limit for more than one hour (`1h`).
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 除了恢复以前使用的警报的合理阈值之外，我们定义了一个名为`MemoryAtTheLimit`的新警报。如果实际使用超过极限的百分之八十（`0.8`）超过一小时（`1h`），它将触发。
- en: Next is the upgrade of our Prometheus Chart.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是升级我们的Prometheus图表。
- en: '[PRE148]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Finally, we can open the Prometheus' alerts screen and confirm that the new
    alert was indeed added to the mix.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打开Prometheus的警报屏幕，并确认新的警报确实被添加到了其中。
- en: '[PRE149]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: We won't go through the drill of creating a similar alert for CPU. You should
    know how to do that yourself.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会重复为 CPU 创建类似的警报的步骤。你应该知道如何自己做。
- en: What now?
  id: totrans-670
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在呢？
- en: We explored quite a few Prometheus metrics, expressions, and alerts. We saw
    how to connect Prometheus alerts with Alertmanager and, from there, to forward
    them to one application to another.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了相当多的Prometheus指标、表达式和警报。我们看到了如何将Prometheus警报与Alertmanager连接，并从那里将它们转发到一个应用程序到另一个应用程序。
- en: What we did so far is only the tip of the iceberg. If would take too much time
    (and space) to explore all the metrics and expressions we might use. Nevertheless,
    I believe that now you know some of the more useful ones and that you'll be able
    to extend them with those specific to you.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的只是冰山一角。要探索所有我们可能使用的指标和表达式将需要太多的时间（和空间）。尽管如此，我相信现在你知道了一些更有用的指标，而且你将能够用你自己特定的指标来扩展它们。
- en: I urge you to send me expressions and alerts you find useful. You know where
    to find me (*DevOps20* ([http://slack.devops20toolkit.com/](http://slack.devops20toolkit.com/))
    Slack, `viktor@farcic` email, `@vfarcic` on Twitter, and so on).
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 我敦促你发送给我你发现有用的表达式和警报。你知道在哪里找到我（*DevOps20* ([http://slack.devops20toolkit.com/](http://slack.devops20toolkit.com/))
    Slack，`viktor@farcic` 邮件，`@vfarcic` 推特，等等）。
- en: For now, I'll leave you to decide whether to move straight into the next chapter,
    to destroy the entire cluster, or only to remove the resources we installed. If
    you choose the latter, please use the commands that follow.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我会让你决定是直接进入下一章，销毁整个集群，还是只移除我们安装的资源。如果你选择后者，请使用接下来的命令。
- en: '[PRE150]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: Before you leave, you might want to go over the main points of this chapter.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在你离开之前，你可能想回顾一下本章的要点。
- en: Prometheus is a database (of sorts) designed to fetch (pull) and store highly
    dimensional time series data.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus是一个设计用于获取（拉取）和存储高维时间序列数据的数据库（某种程度上）。
- en: The four key metrics everyone should utilize are latency, traffic, errors, and
    saturation.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个人都应该利用的四个关键指标是延迟、流量、错误和饱和度。

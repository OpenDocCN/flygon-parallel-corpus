- en: Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: 'Monitoring and logging are a crucial part of a site''s reliability. We''ve
    learned how to leverage various controllers to take care of our application, and
    about utilizing service together with Ingress to serve our web applications. Next,
    in this chapter, we''ll learn how to keep track of our application by means of
    the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和日志记录是站点可靠性的重要组成部分。我们已经学会了如何利用各种控制器来管理我们的应用程序，以及如何利用服务和Ingress一起为我们的Web应用程序提供服务。接下来，在本章中，我们将学习如何通过以下主题跟踪我们的应用程序：
- en: Getting status snapshot of a container
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取容器的状态快照
- en: Monitoring in Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的监控
- en: Converging metrics from Kubernetes by Prometheus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Prometheus汇总Kubernetes的指标
- en: Concepts of logging in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中日志记录的概念
- en: Logging with Fluentd and Elasticsearch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Fluentd和Elasticsearch进行日志记录
- en: Inspecting a container
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查一个容器
- en: Whenever our application behaves abnormally, we will definitely want to know
    what happened, using all means, such as checking logs, resource usage, processes
    watchdog, or even getting into the running host directly to dig problems out.
    In Kubernetes, we have `kubectl get` and `kubectl describe` that can query deployment
    states, which will help us determine if an application has crashed or works as
    desired.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们的应用程序表现异常时，我们肯定会想知道发生了什么，通过各种手段，比如检查日志、资源使用情况、进程监视器，甚至直接进入运行的主机来排查问题。在Kubernetes中，我们有`kubectl
    get`和`kubectl describe`可以查询部署状态，这将帮助我们确定应用程序是否已崩溃或按预期工作。
- en: 'Further, if we want to know what is going on from the outputs of an application,
    we also have `kubectl logs` that redirects a container''s `stdout` to our Terminal.
    For CPU and memory usage stats, there''s also a top-like command we can employ,
    `kubectl top`. `kubectl top node`, which gives an overview of the resource usages
    of nodes, and `kubectl top pod <POD_NAME>` which displays per-pod usage:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们想要了解应用程序的输出发生了什么，我们还有`kubectl logs`，它将容器的`stdout`重定向到我们的终端。对于CPU和内存使用统计，我们还可以使用类似top的命令`kubectl
    top`。`kubectl top node`提供了节点资源使用情况的概览，`kubectl top pod <POD_NAME>`显示了每个pod的使用情况：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To use `kubectl top`, you'll need Heapster deployed in your cluster. We'll discuss
    this later in the chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`kubectl top`，您需要在集群中部署Heapster。我们将在本章后面讨论这个问题。
- en: 'What if we leave something such as logs inside a container and they are not
    sent out anywhere? We know there''s a `docker exec` execute command inside a running
    container, but it''s unlikely that we have access to nodes every time. Fortunately,
    `kubectl` allows us to do the same thing with the `kubectl exec` command. Its
    usage is similar to the Docker one. For example, we can run a shell inside the
    container in a pod like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们遗留了一些日志在容器内而没有发送到任何地方怎么办？我们知道有一个`docker exec`在运行的容器内执行命令，但我们不太可能每次都能访问节点。幸运的是，`kubectl`允许我们使用`kubectl
    exec`命令做同样的事情。它的用法类似于Docker。例如，我们可以像这样在pod中的容器内运行一个shell：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It's pretty much the same as logging onto a host by SSH, and it enables us to
    troubleshoot with tools we are familiar with, as we've done in non-container worlds.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这与通过SSH登录主机几乎相同，并且它使我们能够使用我们熟悉的工具进行故障排除，就像我们在非容器世界中所做的那样。
- en: Kubernetes dashboard
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes仪表板
- en: 'In addition to the command-line utility, there is a dashboard that aggregates
    almost every information we have just discussed on a decent web-UI:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了命令行实用程序之外，还有一个仪表板，它在一个体面的Web-UI上汇总了我们刚刚讨论的几乎所有信息：
- en: '![](../images/00094.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00094.jpeg)'
- en: 'It''s in fact a general purpose graphical user interface of a Kubernetes cluster,
    as it also allows us to create, edit, and delete resources. Deploying it is quite
    easy; all we need to do is apply a template:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，它是Kubernetes集群的通用图形用户界面，因为它还允许我们创建、编辑和删除资源。部署它非常容易；我们所需要做的就是应用一个模板：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This template is for the Kubernetes cluster with **RBAC** (**role-based access
    control**) enabled. Check the dashboard's project repository ([https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard))
    if you need other deployment options. Regarding RBAC, we'll talk about this in
    [Chapter 8](part0188.html#5J99O0-6c8359cae3d4492eb9973d94ec3e4f1e), *Cluster Administration*.
    Many managed Kubernetes services, such as Google Container Engine, pre-deployed
    the dashboard in the cluster so that we don't need to install it on our own. To
    determine whether the dashboard exists in our cluster or not, use `kubectl cluster-info`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此模板适用于启用了**RBAC**（基于角色的访问控制）的Kubernetes集群。如果您需要其他部署选项，请查看仪表板的项目存储库（[https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)）。关于RBAC，我们将在[第8章](part0188.html#5J99O0-6c8359cae3d4492eb9973d94ec3e4f1e)中讨论，*集群管理*。许多托管的Kubernetes服务（例如Google容器引擎）在集群中预先部署了仪表板，因此我们不需要自行安装。要确定仪表板是否存在于我们的集群中，请使用`kubectl
    cluster-info`。
- en: We'll see kubernetes-dashboard is running at ... if it's installed. The service
    for the dashboard deployed with the default template or provisioned by cloud providers
    is usually ClusterIP. In order to access it, we'll need to establish a proxy between
    our terminal and Kubernetes' API server with `kubectl proxy.` Once the proxy is
    up, we are then able to access the dashboard at `http://localhost:8001/ui`. The
    port `8001` is the default port of `kubectl proxy`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已安装，我们将看到kubernetes-dashboard正在运行...。使用默认模板部署的仪表板服务或由云提供商提供的服务通常是ClusterIP。为了访问它，我们需要在我们的终端和Kubernetes的API服务器之间建立代理，使用`kubectl
    proxy`。一旦代理启动，我们就能够在`http://localhost:8001/ui`上访问仪表板。端口`8001`是`kubectl proxy`的默认端口。
- en: As with `kubectl top`, you'll need Heapster deployed in your cluster to see
    the CPU and memory stats.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与`kubectl top`一样，您需要在集群中部署Heapster才能查看CPU和内存统计信息。
- en: Monitoring in Kubernetes
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的监控
- en: Since we now know how to examine our applications in Kubernetes, it's quite
    natural that we should have a mechanism to do so constantly to detect any incident
    at the first occurrence. To put it another way, we need a monitoring system. A
    monitoring system collects metrics from various sources, stores and analyzes data
    received, and then responds to exceptions. In a classical setup of application
    monitoring, we would gather metrics from, at the very least, three different layers
    of our infrastructure to ensure our service's availability as well as quality.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在知道如何在Kubernetes中检查我们的应用程序，所以我们应该有一种机制来不断地这样做，以便在第一次发生任何事件时检测到。换句话说，我们需要一个监控系统。监控系统从各种来源收集指标，存储和分析接收到的数据，然后响应异常。在应用程序监控的经典设置中，我们至少会从基础设施的三个不同层面收集指标，以确保我们服务的可用性和质量。
- en: Application
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序
- en: 'The data we''re concerned with at this level involves the internal states of
    an application, which can help us determine what''s going on inside our service.
    For example, the following screenshot is from Elasticsearch Marvel ([https://www.elastic.co/guide/en/marvel/current/introduction.html](https://www.elastic.co/guide/en/marvel/current/introduction.html)),
    called **Monitoring** from version 5 onward), which is a monitoring solution for
    an Elasticsearch cluster. It brings together the information about our cluster,
    particularly Elasticsearch specific metrics:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个层面关心的数据涉及应用程序的内部状态，这可以帮助我们确定服务内部发生了什么。例如，以下截图来自Elasticsearch Marvel（[https://www.elastic.co/guide/en/marvel/current/introduction.html](https://www.elastic.co/guide/en/marvel/current/introduction.html)），从版本5开始称为**监控**，这是Elasticsearch集群的监控解决方案。它汇集了关于我们集群的信息，特别是Elasticsearch特定的指标：
- en: '![](../images/00095.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00095.jpeg)'
- en: In addition, we would leverage profiling tools in conjunction with tracing tools
    to instrument our program, which augments dimensions that enables us to inspect
    our service in a finer granularity. Especially nowadays, an application might
    be composed of dozens of services in a distributed way. Without utilizing tracing
    tools, such as OpenTracing ([http://opentracing.io](http://opentracing.io)) implementations,
    identifying performance culprits can be extremely difficult.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将利用性能分析工具与跟踪工具来对我们的程序进行仪器化，这增加了我们检查服务的细粒度维度。特别是在当今，一个应用可能以分布式方式由数十个服务组成。如果不使用跟踪工具，比如OpenTracing（[http://opentracing.io](http://opentracing.io)）的实现，要识别性能问题可能会非常困难。
- en: Host
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主机
- en: Collecting tasks at the host level is usually performed by agents provided by
    the monitoring framework. The agent extracts and sends out comprehensive metrics
    about a host such as loads, disks, connections, or stats of processes that assist
    in determining a host's health.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机级别收集任务通常是由监控框架提供的代理完成的。代理提取并发送有关主机的全面指标，如负载、磁盘、连接或进程状态等，以帮助确定主机的健康状况。
- en: External resources
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部资源
- en: Aside from the aforementioned two components, we also need to check dependent
    components' statuses. For instance, say we have an application that consumes a
    queue and executes corresponding tasks; we should also take care about the metrics,
    such as the queue length and the consuming rate. If the consuming rate is low
    and the queue length keeps growing, our application is supposedly hitting trouble.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述两个组件之外，我们还需要检查依赖组件的状态。例如，假设我们有一个消耗队列并执行相应任务的应用；我们还应该关注一些指标，比如队列长度和消耗速率。如果消耗速率低而队列长度不断增长，我们的应用可能遇到了问题。
- en: These principles also apply to containers on Kubernetes, as running a container
    on a host is almost identical to running a process. Nonetheless, due to the fact
    that there is a subtle distinction between the way containers on Kubernetes and
    on traditional hosts utilize resources, we still need to take the differences
    into consideration when employing a monitoring strategy. For instance, containers
    of an application on Kubernetes would spread across multiple hosts, and also would
    not always be on the same hosts. It would be grueling to produce a consistent
    recording of one application if we are still adopting the host-centric monitoring
    approach. Therefore, rather than observing resource usages at the host level only,
    we should pile a container layer to our monitoring stack. Moreover, since Kubernetes
    is, in reality, the infrastructure to our applications, we absolutely should take
    it into account as well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则也适用于Kubernetes上的容器，因为在主机上运行容器几乎与运行进程相同。然而，由于Kubernetes上的容器和传统主机上利用资源的方式之间存在微妙的区别，当采用监控策略时，我们仍需要考虑这些差异。例如，Kubernetes上的应用的容器可能分布在多个主机上，并且也不总是在同一主机上。如果我们仍在采用以主机为中心的监控方法，要对一个应用进行一致的记录将会非常困难。因此，我们不应该仅观察主机级别的资源使用情况，而应该在我们的监控堆栈中增加一个容器层。此外，由于Kubernetes实际上是我们应用的基础设施，我们绝对应该考虑它。
- en: Container
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器
- en: 'As mentioned, metrics collected at the container level and what we get at the
    host level are pretty much the same thing, particularly the usage of system resources.
    Notwithstanding the seeming redundancy, it''s the very key which facilitates us
    to resolve difficulties on monitoring moving containers. The idea is quite simple:
    what we need to do is attach logical information to metrics, such as pod labels
    or their controller name. In this way, metrics coming out from containers across
    distinct hosts could be meaningfully grouped. Consider the following diagram;
    say we want to know how many bytes transmitted (**tx**) on **App 2**, we could
    sum up **tx** metrics over the **App 2** label and it yields **20 MB:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，容器级别收集的指标和主机级别得到的指标基本上是相同的，特别是系统资源的使用情况。尽管看起来有些多余，但这正是帮助我们解决监控移动容器困难的关键。这个想法非常简单：我们需要将逻辑信息附加到指标上，比如pod标签或它们的控制器名称。这样，来自不同主机上的容器的指标可以被有意义地分组。考虑下面的图表；假设我们想知道**App
    2**上传输的字节数（**tx**），我们可以对**App 2**标签上的**tx**指标求和，得到**20 MB**：
- en: '![](../images/00096.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00096.jpeg)'
- en: Another difference is that metrics on CPU throttling are reported at container
    level only. If performance issues are encountered at a certain application but
    the CPU resource on the host is spare, we can check if it's throttled with the
    associated metrics.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是，CPU限制的指标仅在容器级别上报告。如果在某个应用程序遇到性能问题，但主机上的CPU资源是空闲的，我们可以检查是否受到了相关指标的限制。
- en: Kubernetes
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is responsible for managing, scheduling, and orchestrating our applications.
    Accordingly, once an application has crashed, Kubernetes is certainly one of the
    first places we would want to look. In particular, when the crash happens after
    rolling out a new deployment, the state of associated objects would be reflected
    instantly on Kubernetes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes负责管理、调度和编排我们的应用程序。因此，一旦应用程序崩溃，Kubernetes肯定是我们想要查看的第一个地方。特别是在部署新版本后发生崩溃时，相关对象的状态将立即在Kubernetes上反映出来。
- en: 'To sum up, components that should be monitored are illustrated in the following
    diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，应该监控的组件如下图所示：
- en: '![](../images/00097.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00097.jpeg)'
- en: Getting monitoring essentials for Kubernetes
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Kubernetes的监控要点
- en: For every layer of the monitoring stack, we can always find a counterpart collector.
    For instance, at the application level, we can dump metrics manually; at the host
    level, we would install a metrics collector on every box; as for Kubernetes, there
    are APIs for exporting the metrics that we are interested in, and, at the very
    least, we have `kubectl` at hand.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监控堆栈的每一层，我们总是可以找到相应的收集器。例如，在应用程序级别，我们可以手动转储指标；在主机级别，我们会在每个主机上安装一个指标收集器；至于Kubernetes，有用于导出我们感兴趣的指标的API，至少我们手头上有`kubectl`。
- en: When it comes to the container level collector, what options do we have? Perhaps
    installing the host metrics collector inside the image of our application does
    the job, but we'll soon realize that it could make our container way too clumsy
    in terms of size as well as resource utilizations. Fortunately, there's already
    a solution for such needs, namely cAdvisor ([https://github.com/google/cadvisor](https://github.com/google/cadvisor)),
    the answer to the container level metrics collector. Briefly speaking, cAdvisor
    aggregates the resource usages and performance statistics of every running container
    on a machine. Notice that the deployment of cAdvisor is one per host instead of
    one per container, which is more reasonable for containerized applications. In
    Kubernetes, we don't even care about deploying cAdvisor, as it has already been
    embedded into kubelet.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor is accessible via port `4194` on every node. Prior to Kubernetes 1.7,
    the data gathered by cAdvisor was able to be collected via the kubelet port (`10250`/`10255`)
    as well. To access cAdvisor, we can access the instance port `4194` or through
    `kubectl proxy` at `http://localhost:8001/api/v1/nodes/<nodename>:4194/proxy/`
    or access `http://<node-ip>:4194/` directly.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot is from the cAdvisor Web UI. You will see a similar
    page once connected. For viewing the metrics that cAdvisor grabbed, visit the
    endpoint, `/metrics`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '>![](../images/00098.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important component in the monitoring pipeline is Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)).
    It retrieves monitoring statistics from every node, specifically kubelet on nodes
    processing, and writes to external sinks afterward. It also exposes aggregated
    metrics via the REST API. The function of Heapster sounds rather redundant with
    cAdvisor, but they play different roles in the monitoring pipeline in practice.
    Heapster gathers cluster-wide statistics; cAdvisor is a host-wide component. That
    is to say, Heapster empowers a Kubernetes cluster with the basic monitoring ability.
    The following diagram illustrates how it interacts with other components in a
    cluster:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00099.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: As a matter of fact, it's unnecessary to install Heapster if your monitoring
    framework offers a similar tool that also scrapes metrics from kubelet. However,
    since it's a default monitoring component in Kubernetes' ecosystem, many tools
    rely on it, such as `kubectl top` and the Kubernetes dashboard mentioned earlier.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果您的监控框架提供了类似的工具，也可以从 kubelet 中抓取指标，那么安装 Heapster 就不是必需的。然而，由于它是 Kubernetes
    生态系统中的默认监控组件，许多工具都依赖于它，例如前面提到的 `kubectl top` 和 Kubernetes 仪表板。
- en: 'Before deploying Heapster, check if the monitoring tool you''re using is supported
    as a Heapster sink in this document: [https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md](https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署 Heapster 之前，请检查您正在使用的监控工具是否作为此文档中的 Heapster sink 支持：[https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md](https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md)。
- en: 'If not, we can just have a standalone setup and make the dashboard and `kubectl
    top` work by applying this template:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有，我们可以使用独立的设置，并通过应用此模板使仪表板和 `kubectl top` 工作：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Remember to apply this template if RBAC is enabled:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了 RBAC，请记得应用此模板：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After Heapster is installed, the `kubectl top` command and the Kubernetes dashboard
    should display resource usages properly.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 Heapster 后，`kubectl top` 命令和 Kubernetes 仪表板应该正确显示资源使用情况。
- en: 'While cAdvisor and Heapster focus on physical metrics, we also want the logical
    states of objects being displayed on our monitoring dashboard. kube-state-metrics
    ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    is the very piece that completes our monitoring stack. It watches Kubernetes masters
    and transforms the object statues we see from `kubectl get` or `kubectl describe`
    to metrics in Prometheus format ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)).
    As long as the monitoring system supports this format, we can scrape the states
    into the metrics storage and be alerted on events such as unexplainable restart
    counts. To install kube-state-metrics, first download the templates inside the
    `kubernetes` folder under the project repository([https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes](https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes)),
    and then apply them:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 cAdvisor 和 Heapster 关注物理指标，但我们也希望在监控仪表板上显示对象的逻辑状态。kube-state-metrics ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    是完成我们监控堆栈的重要组成部分。它监视 Kubernetes 主节点，并将我们从 `kubectl get` 或 `kubectl describe` 中看到的对象状态转换为
    Prometheus 格式的指标 ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/))。只要监控系统支持这种格式，我们就可以将状态抓取到指标存储中，并在诸如无法解释的重启计数等事件上收到警报。要安装
    kube-state-metrics，首先在项目存储库的 `kubernetes` 文件夹中下载模板([https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes](https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes))，然后应用它们：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Afterwards, we can view the states inside a cluster in the metrics on its service
    endpoint:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以在其服务端点的指标中查看集群内的状态：
- en: '`http://kube-state-metrics.kube-system:8080/metrics`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://kube-state-metrics.kube-system:8080/metrics`'
- en: Hands-on monitoring
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际监控
- en: So far, we've learned lots of principles to fabricate an impervious monitoring
    system in Kubernetes toward a robust service, and it's time to implement a pragmatic
    one. Because the vast majority of Kubernetes components expose their instrumented
    metrics on a conventional path in Prometheus format, we are free to use any monitoring
    tool with which we are acquainted as long as the tool understands the format.
    In this section, we'll set up an example with an open-source project, Prometheus
    ([https://prometheus.io](https://prometheus.io)), which is a platform-independent
    monitoring tool. Its popularity in Kubernetes' ecosystem is for not only its powerfulness
    but also for its being backed by the **Cloud Native Computing Foundation** ([https://www.cncf.io/](https://www.cncf.io/)),
    who also sponsors the Kubernetes project.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学到了很多关于在Kubernetes中制造一个无懈可击的监控系统的原则，现在是时候实施一个实用的系统了。因为绝大多数Kubernetes组件都在Prometheus格式的传统路径上公开了他们的仪表盘指标，所以只要工具理解这种格式，我们就可以自由地使用我们熟悉的任何监控工具。在本节中，我们将使用一个开源项目Prometheus（[https://prometheus.io](https://prometheus.io)）来设置一个示例，它是一个独立于平台的监控工具。它在Kubernetes生态系统中的流行不仅在于其强大性，还在于它得到了**Cloud
    Native Computing Foundation**（[https://www.cncf.io/](https://www.cncf.io/)）的支持，后者也赞助了Kubernetes项目。
- en: Meeting Prometheus
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遇见Prometheus
- en: 'The Prometheus framework comprises several components, as illustrated in the
    following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus框架包括几个组件，如下图所示：
- en: '![](../images/00100.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00100.jpeg)'
- en: As with all other monitoring frameworks, Prometheus relies on agents scraping
    out statistics from the components of our system, and those agents are the exporters
    at the left of the diagram. Besides this, Prometheus adopts the pull model on
    metrics collecting, which is to say that it's not receiving metrics passively,
    but actively pulls data back from the metrics' endpoints on exporters. If an application
    exposes a metric's endpoint, Prometheus is able to scrape that data as well. The
    default storage backend is an embedded LevelDB, and can be switched to other remote
    storages such as InfluxDB or Graphite. Prometheus is also responsible for sending
    alerts according to pre-configured rules to **Alert manager**. **Alert manager**
    handles alarm sending tasks. It groups alarms received and dispatches them to
    tools that actually send messages, such as email, Slack, PagerDuty, and so on.
    In addition to alerts, we would also like to visualize collected metrics for getting
    a quick overview of our system, and Grafana is what comes in handy here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有其他监控框架一样，Prometheus依赖于从系统组件中抓取统计数据的代理，这些代理位于图表左侧的出口处。除此之外，Prometheus采用了拉取模型来收集指标，这意味着它不是被动地接收指标，而是主动地从出口处拉取数据。如果一个应用程序公开了指标的出口，Prometheus也能够抓取这些数据。默认的存储后端是嵌入式LevelDB，可以切换到其他远程存储，比如InfluxDB或Graphite。Prometheus还负责根据预先配置的规则发送警报给**Alert
    manager**。**Alert manager**负责发送警报任务。它将接收到的警报分组并将它们分发给实际发送消息的工具，比如电子邮件、Slack、PagerDuty等等。除了警报，我们还希望可视化收集到的指标，以便快速了解我们的系统情况，这时Grafana就派上用场了。
- en: Deploying Prometheus
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Prometheus
- en: 'The templates we''ve prepared for this chapter can be found here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本章准备的模板可以在这里找到：
- en: '[https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6)'
- en: 'Under 6-1_prometheus are manifests for this section, including a Prometheus
    deployment, exporters, and related resources. They''ll be settled at a dedicated
    namespace, `monitoring`, except components required to work in `kube-system` namespaces.
    Please review them carefully, and now let''s create resources in the following
    order:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在6-1_prometheus下是本节的清单，包括Prometheus部署、导出器和相关资源。它们将在专用命名空间`monitoring`中安装，除了需要在`kube-system`命名空间中工作的组件。请仔细查看它们，现在让我们按以下顺序创建资源。
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Usages of resources are confined to a relatively low level at provided setups.
    If you''d like to use them in a more formal way, adjusting parameters according
    to actual requirements is recommended. After the Prometheus server is up, we can
    connect to its Web-UI at port `9090` by `kubectl port-forward`. We can use NodePort
    or Ingress to connect to the UI if modifying its service (`prometheus/prom-svc.yml`)
    accordingly. The first page we will see when entering the UI is Prometheus'' expression
    browser, where we build queries and visualize metrics. Under the default settings,
    Prometheus will collect metrics from itself. All valid scraping targets can be
    found at path `/targets`. To speak to Prometheus, we have to gain some understanding
    of its language: **PromQL**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 资源的使用限制在提供的设置中相对较低。如果您希望以更正式的方式使用它们，建议根据实际要求调整参数。在Prometheus服务器启动后，我们可以通过`kubectl
    port-forward`连接到端口`9090`的Web-UI。如果相应地修改其服务（`prometheus/prom-svc.yml`），我们可以使用NodePort或Ingress连接到UI。当进入UI时，我们将看到Prometheus的表达式浏览器，在那里我们可以构建查询和可视化指标。在默认设置下，Prometheus将从自身收集指标。所有有效的抓取目标都可以在路径`/targets`下找到。要与Prometheus交流，我们必须对其语言**PromQL**有一些了解。
- en: Working with PromQL
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PromQL
- en: 'PromQL has three data types: instant vector, range vector, and scalar. An instant
    vector is a time series of data sampled; a range vector is a set of time series
    containing data within a certain time range; a scalar is a numeric floating value.
    Metrics stored inside Prometheus are identified with a metric name and labels,
    and we can find the name of any collected metric with the drop-down list next
    to the Execute button on the expression browser. If we query Prometheus with metric
    names, say `http_requests_total`, we''ll get lots of results as instant vectors
    match the name but with different labels. Likewise, we can also query a particular
    set of labels only with `{}` syntax. For example, the query `{code="400",method="get"}`
    means that we want any metric that has the label `code`, `method` equals to `400`,
    and `get` respectively. Combining names and labels in a query is also valid, such
    as `http_requests_total{code="400",method="get"}`. PromQL grants us the detective
    ability to inspect our applications or systems from all kinds of clues so long
    as related metrics are collected.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PromQL有三种数据类型：即时向量、范围向量和标量。即时向量是经过采样的数据时间序列；范围向量是一组包含在一定时间范围内的时间序列；标量是一个数值浮点值。存储在Prometheus中的指标通过指标名称和标签进行识别，我们可以通过表达式浏览器旁边的下拉列表找到任何收集的指标名称。如果我们使用指标名称，比如`http_requests_total`，我们会得到很多结果，因为即时向量匹配名称但具有不同的标签。同样，我们也可以使用`{}`语法仅查询特定的标签集。例如，查询`{code="400",method="get"}`表示我们想要任何具有标签`code`，`method`分别等于`400`和`get`的指标。在查询中结合名称和标签也是有效的，比如`http_requests_total{code="400",method="get"}`。PromQL赋予了我们检查应用程序或系统的侦探能力，只要相关指标被收集。
- en: 'In addition to the basic queries just mentioned, there''s so much more to PromQL,
    such as querying labels with regex and logical operators, joining and aggregating
    metrics with functions, and even performing operations between different metrics.
    For instance, the following expression gives us the total memory consumed by a
    `kube-dns` deployment in the `kube-system` namespace:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了刚才提到的基本查询之外，PromQL还有很多其他内容，比如使用正则表达式和逻辑运算符查询标签，使用函数连接和聚合指标，甚至在不同指标之间执行操作。例如，以下表达式给出了`kube-system`命名空间中`kube-dns`部署消耗的总内存：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: More detailed documents can be found at Prometheus' official site ([https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)),
    and it certainly should help you to unleash the power of Prometheus.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的文档可以在Prometheus的官方网站找到（[https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)），它肯定会帮助您释放Prometheus的力量。
- en: Discovering targets in Kubernetes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中发现目标
- en: 'Since Prometheus only pulls metrics from endpoints it knows, we have to explicitly
    tell it where we''d like to collect data from. Under the path `/config` is the
    page that lists current configured targets to pull. By default, there would be
    one job that collects the current metrics about Prometheus itself, and it''s in
    the conventional scraping path, `/metrics`. We would see a very long text page
    if connecting to the endpoint:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Prometheus只从它知道的端点中提取指标，我们必须明确告诉它我们想要从哪里收集数据。在路径`/config`下是列出当前配置的目标以进行提取的页面。默认情况下，会有一个任务来收集有关Prometheus本身的当前指标，它位于传统的抓取路径`/metrics`中。如果连接到端点，我们会看到一个非常长的文本页面：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is just the Prometheus metrics format we've mentioned several times. Next
    time when we see any page like this, we will know it's a metrics endpoint.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们已经多次提到的Prometheus指标格式。下次当我们看到这样的页面时，我们会知道这是一个指标端点。
- en: The default job to scrape Prometheus is configured as a static target. However,
    with the fact that containers in Kubernetes are created and destroyed dynamically,
    it's really troublesome to find out the exact address of a container, let alone
    set it on Prometheus. In some cases, we may utilize service DNS as a static metrics
    target, but this still cannot solve all cases. Fortunately, Prometheus helps us
    overcome the problem with its ability to discover services inside Kubernetes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取Prometheus的默认作业被配置为静态目标。然而，考虑到Kubernetes中的容器是动态创建和销毁的事实，要找出容器的确切地址，更不用说在Prometheus上设置它，真的很麻烦。在某些情况下，我们可以利用服务DNS作为静态指标目标，但这仍然不能解决所有情况。幸运的是，Prometheus通过其发现Kubernetes内部服务的能力帮助我们克服了这个问题。
- en: 'To be more specific, it''s able to query Kubernetes about the information of
    running services, and adds or deletes them to the target configuration accordingly.
    Four kinds of discovery mechanisms are currently supported:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，它能够查询Kubernetes有关正在运行的服务的信息，并根据情况将其添加或删除到目标配置中。目前支持四种发现机制：
- en: The **node** discovery mode creates one target per node, and the target port
    would be kubelet's port by default.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**发现模式为每个节点创建一个目标，默认情况下目标端口将是kubelet的端口。'
- en: The **service** discovery mode creates a target for every `service` object,
    and all defined ports in a service would become a scraping target.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**发现模式为每个`service`对象创建一个目标，并且服务中定义的所有端口都将成为抓取目标。'
- en: The **pod** discovery mode works in a similar way to the service discovery role,
    that is, it creates targets per pod and for each pod it exposes all the defined
    container ports. If there is no port defined in a pod's template, it would still
    create a scraping target with its address only.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pod**发现模式的工作方式与服务发现角色类似，也就是说，它为每个pod创建目标，并且对于每个pod，它会公开所有定义的容器端口。如果在pod的模板中没有定义端口，它仍然会只创建一个带有其地址的抓取目标。'
- en: The **endpoints** mode discovers `endpoint` objects created by a service. For
    example, if a service is backed by three pods with two ports each, then we'll
    have six scraping targets. In addition, for a pod, not only ports that expose
    to a service but also other declared container ports would be discovered.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates four discovery mechanisms: the left ones
    are the resources in Kubernetes, and those in the right list are targets created
    in Prometheus:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00101.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: 'Generally speaking, not all exposed ports are served as a metrics endpoint,
    so we certainly don''t want Prometheus to grab everything in our cluster but collect
    marked resources only. To achieve this, Prometheus utilizes annotations on resource
    manifests to distinguish which targets are to be grabbed. The annotation format
    is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '**On pod**: If a pod is created by a pod controller, remember to set Prometheus
    annotations in the pod spec rather than in the pod controller:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/scrape`: `true` indicates that this pod should be pulled.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/path`: Set this annotation to the path that exposes metrics;
    it only needs to be set if the target pod is using a path other than `/metrics`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/port`: If the defined port is different from the actual metrics
    port, override it with this annotation.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On service**: Since endpoints are mostly not created manually, endpoint discovery
    uses the annotations inherited from a service. That is to say, annotations on
    services effect service and endpoint discovery modes simultaneously. As such,
    we''d use `prometheus.io/scrape: ''true''` to denote endpoints created by a service
    that are to be scraped, and use `prometheus.io/probe: ''true''` to tag a service
    with metrics. Moreover, `prometheus.io/scheme` designates whether `http` or `https`
    is used. Other than that, the path and port annotations also work here.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following template snippet indicates Prometheus'' endpoint discovery role,
    but the service discovery role to create targets on pods is selected at: `9100/prom`.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The template `prom-config-k8s.yml` under our example repository contains the
    configuration to discover Kubernetes resources for Prometheus. Apply it with:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Because it''s a ConfigMap, it takes seconds to become consistent. Afterwards,
    reload Prometheus by sending a `SIGHUP` to the process:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The provided template is based on this example from Prometheus'' official repository;
    you can find out more usages here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml)'
- en: 'Also, the document page describes in detail how the Prometheus configuration
    works:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文档页面详细描述了Prometheus配置的工作原理：
- en: '[https://prometheus.io/docs/operating/configuration/](https://prometheus.io/docs/operating/configuration/)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/docs/operating/configuration/](https://prometheus.io/docs/operating/configuration/)'
- en: Gathering data from Kubernetes
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Kubernetes中收集数据
- en: 'The steps for implementing the five monitoring layers discussed earlier in
    Prometheus are quite clear now: installing exporters, annotating them with appropriate
    tags, and then collecting them on auto-discovered endpoints.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实施之前在Prometheus中讨论的五个监控层的步骤已经非常清晰：安装导出器，使用适当的标签对其进行注释，然后在自动发现的端点上收集它们。
- en: 'The host layer monitoring in Prometheus is done by the node exporter ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)).
    Its Kubernetes manifest can be found under the examples for this chapter, and
    it contains one DaemonSet with a scrape annotation. Install it with:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus中的主机层监控是由节点导出器（[https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)）完成的。它的Kubernetes清单可以在本章的示例中找到，其中包含一个带有抓取注释的DaemonSet。使用以下命令安装它：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Its corresponding configuration will be created by a pod discovery role.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其相应的配置将由pod发现角色创建。
- en: The container layer collector should be cAdvisor, and it has already been installed
    in kubelet. Consequently, discovering it with the node mode is the only thing
    what we need to do.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 容器层收集器应该是cAdvisor，并且已经安装在kubelet中。因此，发现它并使用节点模式是我们需要做的唯一的事情。
- en: Kubernetes monitoring is done by kube-state-metrics, which was also introduced
    previously. One even better thing is that it comes with Prometheus annotations,
    and this means that we don't need to do anything additional to configure it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes监控是由kube-state-metrics完成的，之前也有介绍。更好的是，它带有Prometheus注释，这意味着我们无需进行任何额外的配置。
- en: Up to this point, we've already set up a strong monitoring stack based on Prometheus.
    With respect to the application and external resources monitoring, there are extensive
    exporters in the Prometheus ecosystem to support monitoring various components
    inside our system. For instance, if we need statistics of our MySQL database,
    we could just install MySQL Server Exporter ([https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)),
    which offers comprehensive and useful metrics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经基于Prometheus建立了一个强大的监控堆栈。关于应用程序和外部资源的监控，Prometheus生态系统中有大量的导出器来支持监控系统内部的各种组件。例如，如果我们需要我们的MySQL数据库的统计数据，我们可以安装MySQL
    Server Exporter（[https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)），它提供了全面和有用的指标。
- en: 'In addition to those metrics already described, there are some other useful
    metrics from Kubernetes components that play a significant part in a variety of
    aspects:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了已经描述的那些指标之外，还有一些来自Kubernetes组件的其他有用的指标，在各种方面起着重要作用：
- en: '**Kubernetes API server**: The API server exposes its state at `/metrics`,
    and this target is enabled by default.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes API服务器**：API服务器在`/metrics`上公开其状态，并且此目标默认启用。'
- en: '**kube-controller-manager**: This component exposes metrics on port `10252`,
    but it''s invisible on some managed Kubernetes services such as **Google Container
    Engine** (**GKE**). If you''re on a self-hosted cluster, applying "`kubernetes/self/kube-controller-manager-metrics-svc.yml`"
    creates endpoints for Prometheus.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-controller-manager**：这个组件在端口`10252`上公开指标，但在一些托管的Kubernetes服务上是不可见的，比如**Google
    Container Engine**（**GKE**）。如果您在自托管的集群上，应用"`kubernetes/self/kube-controller-manager-metrics-svc.yml`"会为Prometheus创建端点。'
- en: '**kube-scheduler**: It uses port `10251`, and it''s not visible on clusters
    by GKE as well. "`kubernetes/self/kube-scheduler-metrics-svc.yml`" is the template
    for creating a target to Prometheus.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-scheduler**：它使用端口`10251`，在GKE集群上也是不可见的。"`kubernetes/self/kube-scheduler-metrics-svc.yml`"是创建一个指向Prometheus的目标的模板。'
- en: '**kube-dns**: There are two containers in a kube-dns pod, `dnsmasq` and `sky-dns`,
    and their metrics ports are `10054` and `10055` respectively. The corresponding
    template is `kubernetes/self/ kube-dns-metrics-svc.yml`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-dns**：kube-dns pod中有两个容器，`dnsmasq`和`sky-dns`，它们的指标端口分别是`10054`和`10055`。相应的模板是`kubernetes/self/
    kube-dns-metrics-svc.yml`。'
- en: '**etcd**: The etcd cluster also has a Prometheus metrics endpoint on port `4001`.
    If your etcd cluster is self-hosted and managed by Kubernetes, you can take "`kubernetes/self/etcd-server.yml`"
    as a reference.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd**：etcd集群也在端口`4001`上有一个Prometheus指标端点。如果您的etcd集群是自托管的并由Kubernetes管理，您可以将"`kubernetes/self/etcd-server.yml`"作为参考。'
- en: '**Nginx ingress controller**: The nginx controller publishes metrics at port
    `10254`. But the metrics contain only limited information. To get data such as
    connection counts by host or by path, you''ll need to activate the `vts` module
    in the controller to enhance the metrics collected.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nginx ingress controller**：nginx控制器在端口`10254`发布指标。但是这些指标只包含有限的信息。要获取诸如按主机或路径计算的连接计数等数据，您需要在控制器中激活`vts`模块以增强收集的指标。'
- en: Seeing metrics with Grafana
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Grafana查看指标
- en: The expression browser has a built-in graph panel that enables us to see the
    visualized metrics, but it's not designed to serve as a visualization dashboard
    for daily routines. Grafana is the best option for Prometheus. We've discussed
    how to set up Grafana in [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources*, and we also provide templates in the repository
    for this chapter; both options do the job.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式浏览器有一个内置的图形面板，使我们能够看到可视化的指标，但它并不是设计用来作为日常例行工作的可视化仪表板。Grafana是Prometheus的最佳选择。我们已经在[第4章](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e)中讨论了如何设置Grafana，*与存储和资源一起工作*，我们还为本章提供了模板；这两个选项都能胜任工作。
- en: 'To see Prometheus metrics in Grafana, we have to add a data source first. The
    following configurations are required to connect to our Prometheus server:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Grafana中查看Prometheus指标，我们首先必须添加一个数据源。连接到我们的Prometheus服务器需要以下配置：
- en: 'Type: "Prometheus"'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型："Prometheus"
- en: 'Url: `http://prometheus-svc.monitoring:9090`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网址：`http://prometheus-svc.monitoring:9090`
- en: 'Access: proxy'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问：代理
- en: 'Once it''s connected, we can import a dashboard to see something in action.
    On Grafana''s sharing page ([https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus))
    are rich off-the-shelf dashboards. The following screenshot is from the dashboard
    `#1621`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦连接上，我们就可以导入一个仪表板来看到实际的情况。在Grafana的共享页面（[https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus)）上有丰富的现成仪表板。以下截图来自仪表板`#1621`：
- en: '![](../images/00102.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00102.jpeg)'
- en: Because the graphs are drawn by data from Prometheus, we are capable of plotting
    any data with which we are concerned as long as we master PromQL.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因为图形是由Prometheus的数据绘制的，只要我们掌握PromQL，我们就能绘制任何我们关心的数据。
- en: Logging events
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录事件
- en: Monitoring with quantitative time series of a system status enables us to briskly
    dig out which components in our system failed, but it's still inadequate to diagnose
    with the root cause under syndromes. As a result, a logging system that gathers,
    persists, and searches logs is certainly helpful for uncovering the reason why
    something went wrong by means of correlating events with the anomalies detected.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用系统状态的定量时间序列进行监控，能够迅速查明系统中哪些组件出现故障，但仍然不足以诊断出症候的根本原因。因此，一个收集、持久化和搜索日志的日志系统对于通过将事件与检测到的异常相关联来揭示出出现问题的原因是非常有帮助的。
- en: 'In general, there are two main components in a logging system: the logging
    agent and the logging backend. The former is an abstract layer to a program. It
    gathers, transforms, and dispatches logs to the logging backend. A logging backend
    warehouses all logs received. As with monitoring, the most challenging part of
    building a logging system for Kubernetes is ascertaining how to gather logs from
    containers to a centralized logging backend. Typically, there are three ways to
    send out logs to a program:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，日志系统中有两个主要组件：日志代理和日志后端。前者是一个程序的抽象层。它收集、转换和分发日志到日志后端。日志后端存储接收到的所有日志。与监控一样，为Kubernetes构建日志系统最具挑战性的部分是确定如何从容器中收集日志到集中的日志后端。通常有三种方式将日志发送到程序：
- en: Dumping everything to `stdout`/`stderr`
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容转储到`stdout`/`stderr`
- en: Writing `log` files
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写`log`文件
- en: Sending logs to a logging agent or logging the backend directly; programs in
    Kubernetes are also able to emit logs in the same manner so long as we understand
    how log streams flow in Kubernetes
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志发送到日志代理或直接发送到日志后端；只要我们了解日志流在Kubernetes中的流动方式，Kubernetes中的程序也可以以相同的方式发出日志
- en: Patterns of aggregating logs
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合日志的模式
- en: For programs that log to a logging agent or backend directly, whether they are
    inside Kubernetes or not doesn't matter on the whole, as they technically don't
    output logs through Kubernetes. As for other cases, we'd use the following two
    patterns to centralize logs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直接向日志代理或后端记录日志的程序，它们是否在Kubernetes内部并不重要，因为它们在技术上并不通过Kubernetes输出日志。至于其他情况，我们将使用以下两种模式来集中日志。
- en: Collecting logs with a logging agent per node
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个节点使用一个日志代理收集日志
- en: We know messages we retrieved via `kubectl logs` are streams redirected from
    `stdout`/`stderr` of a container, but it's obviously not a good idea to collect
    logs with `kubectl logs`. Actually, `kubectl logs` gets logs from kubelet, and
    kubelet aggregates logs to the host path, `/var/log/containers/`, from the container
    engine underneath.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道通过`kubectl logs`检索到的消息是从容器的`stdout`/`stderr`重定向的流，但显然使用`kubectl logs`收集日志并不是一个好主意。实际上，`kubectl
    logs`从kubelet获取日志，kubelet将日志聚合到主机路径`/var/log/containers/`中，从容器引擎下方获取。
- en: 'Therefore, setting up logging agents on every node and configuring them to
    tail and forward `log` files under the path are just what we need for converging
    standard streams of running containers, as shown in the following diagram:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每个节点上设置日志代理并配置它们尾随和转发路径下的`log`文件，这正是我们需要的，以便汇聚运行容器的标准流，如下图所示：
- en: '![](../images/00103.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00103.jpeg)'
- en: 'In practice, we''d also configure a logging agent to tail logs from the system
    and Kubernetes, components under `/var/log` on masters and nodes such as:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们还会配置一个日志代理来从系统和Kubernetes的组件下的`/var/log`中尾随日志，比如在主节点和节点上的：
- en: '`kube-proxy.log`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy.log`'
- en: '`kube-apiserver.log`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver.log`'
- en: '`kube-scheduler.log`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler.log`'
- en: '`kube-controller-manager.log`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager.log`'
- en: '`etcd.log`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd.log`'
- en: Aside from `stdout`/`stderr`, if logs of an application are stored as files
    in the container and persisted via the `hostPath` volume, a node logging agent
    is capable of passing them to a node likewise. However, for each exported `log`
    file, we have to customize their corresponding configurations in the logging agent
    so that they can be dispatched correctly. Moreover, we also need to name `log`
    files properly to prevent any collision and take care of log rotation on our own,
    which makes it an unscalable and unmanageable logging mechanism.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`stdout`/`stderr`之外，如果应用程序的日志以文件形式存储在容器中，并通过`hostPath`卷持久化，节点日志代理可以将它们传递给节点。然而，对于每个导出的`log`文件，我们必须在日志代理中自定义它们对应的配置，以便它们可以被正确分发。此外，我们还需要适当命名`log`文件，以防止任何冲突，并自行处理日志轮换，这使得它成为一种不可扩展和不可管理的日志记录机制。
- en: Running a sidecar container to forward logs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个旁路容器来转发日志
- en: 'Sometimes it''s just difficult to modify our application to write logs to standard
    streams rather than `log` files, and we wouldn''t want to face the troubles brought
    about by logging to `hostPath` volumes. In such a situation, we could run a Sidecar
    container to deal with logging within only one pod. In other words, each application
    pod would have two containers sharing the same `emptyDir` volume so that the Sidecar
    container can follow logs from the application container and send them outside
    their pod, as shown in the following diagram:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有时修改我们的应用程序以将日志写入标准流而不是`log`文件是困难的，我们也不想面对使用`hostPath`卷带来的麻烦。在这种情况下，我们可以运行一个旁路容器来处理一个pod内的日志。换句话说，每个应用程序pod都将有两个共享相同`emptyDir`卷的容器，以便旁路容器可以跟踪应用程序容器的日志并将它们发送到他们的pod外部，如下图所示：
- en: '![](../images/00104.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00104.jpeg)'
- en: Although we don't need to worry about management of `log` files anymore, chores
    such as configuring logging agents for each pod and attaching metadata from Kubernetes
    to log entries still takes extra effort. Another choice is leveraging the Sidecar
    container to outputting logs to standard streams instead of running a dedicated
    logging agent like the following pod; the application container unremittingly
    writes messages to `/var/log/myapp.log`, and the Sidecar tails `myapp.log` in
    the shared volume.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不再需要担心管理`log`文件，但是配置每个pod的日志代理并将Kubernetes的元数据附加到日志条目中仍然需要额外的工作。另一个选择是利用旁路容器将日志输出到标准流，而不是运行一个专用的日志代理，就像下面的pod一样；应用容器不断地将消息写入`/var/log/myapp.log`，而旁路容器则在共享卷中跟踪`myapp.log`。
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can see the written log with `kubectl logs`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`kubectl logs`查看已写入的日志：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Ingesting Kubernetes events
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摄取Kubernetes事件
- en: The event messages we saw at the output of `kubectl describe` contain valuable
    information and complement the metrics gathered by kube-state-metrics, which allows
    us to know what exactly happened to our pods or nodes. Consequently, it should
    be part of our logging essentials together with system and application logs. In
    order to achieve this, we'll need something to watch Kubernetes API servers and
    aggregate events into a logging sink. And there is eventer that does what we need
    to events.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`kubectl describe`的输出中看到的事件消息包含有价值的信息，并补充了kube-state-metrics收集的指标，这使我们能够了解我们的pod或节点发生了什么。因此，它应该是我们日志记录基本要素的一部分，连同系统和应用程序日志。为了实现这一点，我们需要一些东西来监视Kubernetes
    API服务器，并将事件聚合到日志输出中。而eventer正是我们需要的事件处理程序。
- en: Eventer is part of Heapster, and it currently supports Elasticsearch, InfluxDB,
    Riemann, and Google Cloud Logging as its sink. Eventer can also output to `stdout`
    directly in case the logging system we're using is not supported.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Eventer是Heapster的一部分，目前支持Elasticsearch、InfluxDB、Riemann和Google Cloud Logging作为其输出。Eventer也可以直接输出到`stdout`，以防我们使用的日志系统不受支持。
- en: 'Deployment of eventer is similar to deploying Heapster, except for the container
    startup commands, as they are packed in the same image. The flags and options
    for each sink type can be found here: ([https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md](https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md)).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 部署eventer类似于部署Heapster，除了容器启动命令，因为它们打包在同一个镜像中。每种sink类型的标志和选项可以在这里找到：([https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md](https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md))。
- en: Example templates we provided for this chapter also include eventer, and it's
    configured to work with Elasticsearch. We'll describe it in the next section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本章提供的示例模板还包括eventer，并且它已配置为与Elasticsearch一起工作。我们将在下一节中进行描述。
- en: Logging with Fluentd and Elasticsearch
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Fluentd和Elasticsearch进行日志记录
- en: Thus far we've discussed various conditions on the logging we may encounter
    in the real world, and it's time to roll up our sleeves to fabricate a logging
    system with what we have learned.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了我们在现实世界中可能遇到的日志记录的各种条件，现在是时候动手制作一个日志系统，应用我们所学到的知识了。
- en: The architecture of a logging system and a monitoring system are pretty much
    the same in some ways--collectors, storages, and the user-interface. The corresponding
    components we're going to set up are Fluentd/eventer, Elasticsearch, and Kibana,
    respectively. Templates for this section can be found under `6-3_efk`, and they'd
    be deployed to the namespace `monitoring` from the previous part.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 日志系统和监控系统的架构在某些方面基本相同--收集器、存储和用户界面。我们将要设置的相应组件是Fluentd/eventer、Elasticsearch和Kibana。此部分的模板可以在`6-3_efk`下找到，并且它们将部署到前一部分的命名空间`monitoring`中。
- en: 'Elasticsearch is a powerful text search and analysis engine, which makes it
    an ideal choice for persisting, processing, and analyzing logs from everything
    running in our cluster. The Elasticsearch template for this chapter uses a very
    simple setup to demonstrate the concept. If you''d like to deploy an Elasticsearch
    cluster for production use, leveraging the StatefulSet controller and tuning Elasticsearch
    with the proper configuration, as we discussed in [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources,* is recommended. Let''s deploy Elasticsearch
    with the following template ([https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/)):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是一个强大的文本搜索和分析引擎，这使它成为持久化、处理和分析我们集群中运行的所有日志的理想选择。本章的Elasticsearch模板使用了一个非常简单的设置来演示这个概念。如果您想要为生产使用部署Elasticsearch集群，建议使用StatefulSet控制器，并根据我们在[第4章](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e)中讨论的适当配置来调整Elasticsearch。让我们使用以下模板部署Elasticsearch
    ([https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/))：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Elasticsearch is ready if there's a response from `es-logging-svc:9200`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从`es-logging-svc:9200`收到响应，则Elasticsearch已准备就绪。
- en: The next step is setting up a node logging agent. As we'd run it on every node,
    we definitely want it as light as possible in terms of resource usages of a node,
    hence Fluentd ([www.fluentd.org](http://www.fluentd.org)) is opted for. Fluentd
    features in lower memory footprints, which makes it a competent logging agent
    for our needs. Furthermore, since the logging requirement in the containerized
    environment is very focused, there is a sibling project, Fluent Bit (`fluentbit.io`),
    which aims to minimize the resource usages by trimming out functions that wouldn't
    be used for its target scenario. In our example, we would use the Fluentd image
    for Kubernetes ([https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset))
    to conduct the first logging pattern we mentioned previously.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置节点日志代理。由于我们会在每个节点上运行它，因此我们肯定希望它在节点资源使用方面尽可能轻量化，因此选择了Fluentd（[www.fluentd.org](http://www.fluentd.org)）。Fluentd具有较低的内存占用，这使其成为我们需求的一个有竞争力的日志代理。此外，由于容器化环境中的日志记录要求非常专注，因此有一个类似的项目，Fluent
    Bit（`fluentbit.io`），旨在通过修剪不会用于其目标场景的功能来最小化资源使用。在我们的示例中，我们将使用Fluentd镜像用于Kubernetes（[https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset)）来执行我们之前提到的第一个日志模式。
- en: 'The image is already configured to forward container logs under `/var/log/containers`
    and logs of certain system components under `/var/log`. We are absolutely able
    to further customize its logging configuration if need be. Two templates are provided
    here: `fluentd-sa.yml` is the RBAC configuration for the Fluentd DaemonSet, `fluentd-ds.yml`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像已配置为转发容器日志到`/var/log/containers`下，以及某些系统组件的日志到`/var/log`下。如果需要，我们绝对可以进一步定制其日志配置。这里提供了两个模板：`fluentd-sa.yml`是Fluentd
    DaemonSet的RBAC配置，`fluentd-ds.yml`是：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Another must-have logging component is eventer. Here we prepared two templates
    for different conditions. If you''re on a managed Kubernetes service where Heapster
    is already deployed, the template for a standalone eventer, `eventer-only.yml`,
    is used in this case. Otherwise, consider the template of running Heapster in
    combination with eventer in the same pod:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个必不可少的日志记录组件是eventer。这里我们为不同条件准备了两个模板。如果您使用的是已部署Heapster的托管Kubernetes服务，则在这种情况下使用独立eventer的模板`eventer-only.yml`。否则，考虑在同一个pod中运行Heapster和eventer的模板：
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To see logs emitted to Elasticsearch, we can invoke the search API of Elasticsearch,
    but there's a better option, namely Kibana, a web interface that allows us to
    play with Elasticsearch. The template for Kibana is `elasticsearch/kibana-logging.yml`
    under [https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看发送到Elasticsearch的日志，我们可以调用Elasticsearch的搜索API，但有一个更好的选择，即Kibana，这是一个允许我们与Elasticsearch交互的Web界面。Kibana的模板是`elasticsearch/kibana-logging.yml`，位于[https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/)下。
- en: '[PRE18]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Kibana in our example is listening to port `5601`. After exposing the service
    out of your cluster and connecting to it with any browser, you can start to search
    logs from Kubernetes. The index name of the logs sent out by eventer is `heapster-*`,
    and it's `logstash-*` for logs forwarded by Fluentd. The following screenshot
    shows what a log entry looks like in Elasticsearch.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，Kibana正在监听端口`5601`。在将服务暴露到集群外并使用任何浏览器连接后，您可以开始从Kubernetes搜索日志。由eventer发送的日志的索引名称是`heapster-*`，而由Fluentd转发的日志的索引名称是`logstash-*`。以下截图显示了Elasticsearch中日志条目的外观。
- en: The entry is from our earlier example, `myapp`, and we can find that the entry
    is already tagged with handy metadata on Kubernetes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该条目来自我们之前的示例`myapp`，我们可以发现该条目已经在Kubernetes上附带了方便的元数据标记。
- en: '![](../images/00105.jpeg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Extracting metrics from logs
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The monitoring and logging system we built around our application on top of
    Kubernetes is shown in the following diagram:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00106.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: The logging part and the monitoring part look like two independent tracks, but
    the value of the logs is much more than a collection of short texts. They are
    structured data and emitted with timestamps as usual; as such, the idea to transform
    logs into time-series data is promising. However, although Prometheus is extremely
    good at processing time-series data, it cannot ingest texts without any transformation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'An access log entry from HTTPD looks like this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '`10.1.8.10 - - [07/Jul/2017:16:47:12 0000] "GET /ping HTTP/1.1" 200 68`.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of the request IP address, time, method, handler, and so on. If
    we demarcate log segments by their meanings, counted sections can then be regarded
    as a metric sample like this: `"10.1.8.10": 1, "GET": 1, "/ping": 1, "200": 1`.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as mtail ([https://github.com/google/mtail](https://github.com/google/mtail))
    and Grok Exporter ([https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter))
    count log entries and organize those numbers to metrics so that we can further
    process them in Prometheus.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the start of this chapter, we described how to get the status of running
    containers quickly by means of built-in functions such as `kubectl`. Then we expanded
    the discussion to concepts and principles of monitoring, including why it is necessary
    to do monitoring, what to monitor, and how to monitor. Afterwards, we built a
    monitoring system with Prometheus as the core, and set up exporters to collecting
    metrics from Kubernetes. The fundamentals of Prometheus were also introduced so
    that we can leverage metrics to gain more understanding of our cluster as well
    as the applications running inside. On the logging part, we mentioned common patterns
    of logging and how to deal with them in Kubernetes, and deployed an EFK stack
    to converge logs. The system we built in this chapter facilitates the reliability
    of our service. Next, we are advancing to set up a pipeline to deliver our product
    continuously in Kubernetes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL

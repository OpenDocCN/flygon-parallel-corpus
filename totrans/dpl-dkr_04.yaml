- en: Scaling the Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展容器
- en: 'In this chapter, we will be taking our service and trying to scale it horizontally
    with multiple instances of the same container. We will cover the following topics
    in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用我们的服务，并尝试通过多个相同容器的实例来水平扩展它。我们将在本章中涵盖以下主题：
- en: Orchestration options and their pros/cons
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排选项及其优缺点
- en: Service discovery
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: State reconciliation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态协调
- en: The deployment of your own Docker Swarm cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署自己的 Docker Swarm 集群
- en: Deploying our word service from the previous chapter onto that cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们在上一章中的 word 服务部署到该集群上
- en: Service discovery
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: Before we get any further, we really need to get deeply familiar with the conceptual
    Docker container connectivity, which is, unsurprisingly, in some ways very similar
    to building high-availability services with servers in a non-containerized world.
    Because of this, covering this topic in some depth will not only expand your understanding
    of Docker networking, but also help in generally building out resilient services.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步之前，我们真的需要深入了解概念上的 Docker 容器连通性，这在某种程度上与在非容器化世界中使用服务器构建高可用服务非常相似。因此，深入探讨这个主题不仅会扩展您对
    Docker 网络的理解，还有助于通常构建出弹性服务。
- en: A recap of Docker networking
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 网络的回顾
- en: 'In the previous chapter, we covered a bit of the Docker networking layout,
    so we will cover the main points here:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了一些 Docker 网络布局，所以我们将在这里介绍主要内容：
- en: By default, Docker containers run on an isolated virtual network on the host
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，Docker 容器在主机上运行在一个隔离的虚拟网络中
- en: Each container has its own network address in that network
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个容器在该网络中都有自己的网络地址
- en: By default, `localhost` for a container is *not* the host machine's `localhost`
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，容器的 `localhost` *不是* 主机机器的 `localhost`
- en: There is high overhead of manual work in order to connect containers manually
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动连接容器存在很高的人工工作开销
- en: Manual networking connections between containers are inherently fragile
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器之间的手动网络连接本质上是脆弱的
- en: In the parallel world of setting up a local server network, the base experience
    of Docker connectivity is very much akin to hooking up your whole network with
    static IPs. While this approach is not very difficult to get working, maintaining
    it is extremely hard and laborious, which is why we need something better than
    that.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置本地服务器网络的并行世界中，Docker 连通性的基本体验非常类似于使用静态 IP 连接整个网络。虽然这种方法并不难以实现，但维护起来非常困难和费力，这就是为什么我们需要比这更好的东西。
- en: Service Discovery in depth
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解服务发现
- en: 'Since we don''t want to deal with this fragile system of keeping and maintaining
    hardcoded IP addresses, we need to figure out a way so our connections are flexible
    and require no adjustments from the client if the target service dies or a new
    one is created. It would also be nice if each connection to the same service is
    equally balanced between all instances of the same service. Ideally, our services
    would look something like this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不想处理这种脆弱的保持和维护硬编码 IP 地址的系统，我们需要找出一种方法，使我们的连接灵活，并且不需要客户端进行任何调整，如果目标服务死掉或创建一个新的服务。如果每个对同一服务的连接在所有相同服务的实例之间平衡，那就更好了。理想情况下，我们的服务看起来应该是这样的：
- en: '![](assets/533c4c3e-976c-4621-a9ad-e138dc097b9e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/533c4c3e-976c-4621-a9ad-e138dc097b9e.png)'
- en: For this exact use case for the Internet, DNS was created so that clients would
    have a way to find servers even if the IP address or network changes from anywhere
    in the world. As an added benefit, we have target addresses that are easier to
    remember (DNS names such as [https://google.com](https://google.com) instead of
    something such as `https://123.45.67.89` ) and the ability to distribute the processing
    to as many handling services as we want.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于互联网的这种确切用例，DNS被创建出来，以便客户端可以在世界各地找到服务器，即使IP地址或网络发生变化。作为一个附加好处，我们有更容易记住的目标地址（DNS名称，如[https://google.com](https://google.com)，而不是诸如`https://123.45.67.89`之类的东西），并且可以将处理分配给尽可能多的处理服务。
- en: 'If you have not worked with DNS in depth, the main principles are reduced to
    these basic steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有深入研究DNS，主要原则可以归纳为这些基本步骤：
- en: The user (or app) wants to connect to a server (that is, [google.com](http://www.google.com)).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户（或应用程序）想要连接到一个服务器（即[google.com](http://www.google.com)）。
- en: The local machine either uses its own cached DNS answer or goes out to the DNS
    system and searches for this name.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地机器要么使用自己缓存的DNS答案，要么去DNS系统搜索这个名称。
- en: The local machine gets back the IP address ( `123.45.67.89` ) that it should
    use as the target.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地机器得到应该用作目标的IP地址（`123.45.67.89`）。
- en: The local machine connects to the IP address.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地机器连接到IP地址。
- en: The DNS system is much more complicated than the single sentences mentioned
    here. While DNS is a really good thing to know about in any server-oriented tech
    position, here, it was sufficient just to know that the input to the DNS system
    is a hostname and the output is the real target (IP). If you would like to know
    more about how the DNS system actually works, I recommend that you visit [https://en.wikipedia.org/wiki/Domain_Name_System](https://en.wikipedia.org/wiki/Domain_Name_System)
    at your leisure.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DNS系统比这里提到的单个句子要复杂得多。虽然DNS是任何面向服务器的技术职位中了解的一件非常好的事情，在这里，只需要知道DNS系统的输入是主机名，输出是真正的目标（IP）就足够了。如果您想了解DNS系统实际上是如何工作的更多信息，我建议您在闲暇时访问[https://en.wikipedia.org/wiki/Domain_Name_System](https://en.wikipedia.org/wiki/Domain_Name_System)。
- en: If we coerce the DNS handling that is implemented in almost all clients already
    as a way to automatically discover services, we could make ourselves the service
    discovery mechanism that we have been looking for! If we make it smart enough,
    it can tell us where the running container is, load balance between all instances
    of the same container, and provide us with a static name to use as our target.
    As one may expect, almost all container service discovery systems have this exact
    pattern of functionality; it just generally differs if it is done as either a
    client-side discovery pattern, server-side discovery pattern, or some sort of
    a hybrid system.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们强迫几乎所有客户端中已实现的DNS处理作为一种自动发现服务的方式，我们可以使自己成为我们一直在寻找的服务发现机制！如果我们使它足够智能，它可以告诉我们正在运行的容器在哪里，平衡相同容器的所有实例，并为我们提供一个静态名称作为我们的目标使用。正如人们可能期望的那样，几乎所有容器服务发现系统都具有这种功能模式；只是通常有所不同，无论是作为客户端发现模式、服务器端发现模式，还是某种混合系统。
- en: Client-side discovery pattern
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端发现模式
- en: This type of pattern isn't used often, but it pretty much involves using a service-aware
    client to discover other services and to load balance between them. The advantage
    here is that the client can make intelligent decisions about where to connect
    to and in which manner, but the downside is that this decision making is distributed
    onto each service and hard to maintain but it is not dependent on a single source
    of truth (single service registry) that could take down a whole cluster if it
    fails.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的模式并不经常使用，但它基本上涉及使用服务感知客户端来发现其他服务并在它们之间进行负载平衡。这里的优势在于客户端可以智能地决定连接到哪里以及以何种方式，但缺点是这种决策分布在每个服务上并且难以维护，但它不依赖于单一的真相来源（单一服务注册表），如果它失败，可能会导致整个集群崩溃。
- en: 'The architecture generally looks something similar to this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 体系结构通常看起来类似于这样：
- en: '**![](assets/9e41d219-feb2-4831-bc70-82b533f54c11.png)**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](assets/9e41d219-feb2-4831-bc70-82b533f54c11.png)**'
- en: Server-side discovery pattern
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器端发现模式
- en: The more common service discovery pattern is a centralized server-side discovery
    pattern where the DNS system is used to direct the clients to the container. In
    this particular way of finding services, a container registers and de-registers
    itself from the service registry, which holds the state of the system. This state,
    in turn, is used to populate the DNS entries that the client then contacts to
    find the target(s) that it is trying to connect to. While this system is generally
    pretty stable and flexible, it sometimes suffers from really tricky issues that
    generally hamper DNS systems elsewhere, such as DNS caching, which uses stale
    IP addresses until the **time-to-live** (**TTL**) expires or when the app itself
    caches the DNS entry regardless of updates (NGINX and Java apps are notorious
    for this).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的服务发现模式是集中式服务器端发现模式，其中使用DNS系统将客户端引导到容器。在这种特定的服务发现方式中，容器会从服务注册表中注册和注销自己，该注册表保存系统的状态。这种状态反过来用于填充DNS条目，然后客户端联系这些条目以找到它试图连接的目标。虽然这个系统通常相当稳定和灵活，但有时会遇到非常棘手的问题，通常会妨碍DNS系统的其他地方，比如DNS缓存，它使用过时的IP地址，直到**生存时间**（TTL）到期，或者应用程序本身缓存DNS条目而不管更新（NGINX和Java应用程序以此著称）。
- en: '**![](assets/087d23be-79c8-4b67-bec8-0feaeb036efb.png)**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](assets/087d23be-79c8-4b67-bec8-0feaeb036efb.png)**'
- en: Hybrid systems
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合系统
- en: This grouping includes all other combinations that we haven't covered yet, but
    it covers the class of largest deployments that use a tool, HAProxy, which we
    will cover in some detail later. What it basically does is tie a specific port
    on the host (that is, `<host>:10101` ) to a load-balanced target somewhere else
    in the cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分组包括我们尚未涵盖的所有其他组合，但它涵盖了使用工具HAProxy的最大部署类别，我们稍后将详细介绍。它基本上是将主机上的特定端口（即`<host>:10101`）与集群中的负载平衡目标绑定起来。
- en: From the client perspective, they are connecting to a single and stable location
    and the HAProxy then tunnels it seamlessly to the right target.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户端的角度来看，他们连接到一个单一且稳定的位置，然后HAProxy将其无缝地隧道到正确的目标。
- en: '**![](assets/a7a98255-eb1a-47bc-a559-f6712e281787.png)**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](assets/a7a98255-eb1a-47bc-a559-f6712e281787.png)**'
- en: This setup supports both pull and push refreshing of methods and is very resilient,
    but we will take a deep dive into this type of setup in later chapters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置支持拉取和推送刷新方法，并且非常有韧性，但我们将在后面的章节中深入探讨这种类型的设置。
- en: Picking the (un)available options
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择（不）可用的选项
- en: 'With all of these types of service discoveries available, we should be able
    to handle any container scaling that we want, but we need to keep something very
    important in mind: almost all service discovery tooling is intimately bound to
    the system used for deploying and managing the containers (also known as container
    orchestration) due to the fact that updates to container endpoints are generally
    just an orchestration system implementation detail. Because of this, service discovery
    systems usually aren''t as portable as one might like, so the choice of this infrastructure
    piece usually gets decided by your orchestration tooling (with a few exceptions
    here and there).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有了所有这些类型的服务发现可用，我们应该能够处理任何我们想要的容器扩展，但我们需要牢记一件非常重要的事情：几乎所有服务发现工具都与用于部署和管理容器的系统（也称为容器编排）紧密相关，因为容器端点的更新通常只是编排系统的实现细节。因此，服务发现系统通常不像人们希望的那样可移植，因此这种基础设施的选择通常由您的编排工具决定（偶尔会有一些例外）。
- en: Container orchestration
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器编排
- en: 'As we somewhat hinted earlier, service discovery is a critical part of deploying
    a container-based system in any capacity. Without something like that, you might
    as well just use bare-metal servers as the majority of advantages gained using
    containers have been lost. To have an effective service discovery system, you
    are pretty much mandated to use some sort of container orchestration platform,
    and luckily (or maybe un-luckily?), options for container orchestration have been
    sprouting at an almost alarming rate! In general terms, though, at the time of
    writing this book (and in my humble opinion), the popular and stable choices come
    down to mainly these:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍早所暗示的，服务发现是在任何情况下部署基于容器的系统的关键部分。如果没有类似的东西，你可能会选择使用裸机服务器，因为使用容器获得的大部分优势都已经丧失了。要拥有有效的服务发现系统，你几乎必须使用某种容器编排平台，而幸运的是（或者可能是不幸的？），容器编排的选择几乎以惊人的速度不断涌现！总的来说，在撰写本书时（以及在我谦逊的意见中），流行且稳定的选择主要归结为以下几种：
- en: Docker Swarm
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: Kubernetes
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Apache Mesos/Marathon
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Mesos/Marathon
- en: Cloud-based offerings (Amazon ECS, Google Container Engine, Azure Container
    Service, and so on)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于云的服务（Amazon ECS，Google Container Engine，Azure Container Service等）
- en: 'Each one has its own vocabulary and the way in which the infrastructure pieces
    connect, so before we go any further, we need to cover the pertinent vocabulary
    in regard to orchestration services that will mostly be reusable between all of
    them:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个都有自己的词汇表和基础设施连接方式，因此在我们进一步之前，我们需要涵盖有关编排服务的相关词汇，这些词汇在所有这些服务之间大多是可重复使用的：
- en: '**Node**: An instance of Docker Engine. Generally used only when talking about
    cluster-connected instances.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：Docker引擎的一个实例。通常仅在谈论集群连接的实例时使用。'
- en: '**Service**: A functionality grouping that is composed of one or more running
    instances of the same Docker image.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**：由一个或多个运行中的相同Docker镜像实例组成的功能组。'
- en: '**Task**: A specific and unique instance of a running service. This is usually
    a single running Docker container.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：运行服务的特定和唯一实例。通常是一个运行中的Docker容器。'
- en: '**Scaling**: The count of tasks specified for a service to run. This usually
    determines how much throughput a service can support.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展**：指定服务运行的任务数量。这通常决定了服务可以支持多少吞吐量。'
- en: '**Manager node**: A node in charge of management and orchestration duties of
    the cluster.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理节点**：负责集群管理和编排任务的节点。'
- en: '**Worker node**: A node designated as a task runner.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：指定为任务运行者的节点。'
- en: State reconciliation
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态协调
- en: 'Besides our just-learned dictionary, we also need to understand the underlying
    algorithm of almost all orchestration frameworks, state reconciliation, which
    deserves its own little section here. The basic principle that this works on is
    a very simple three-step process, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们刚学到的字典，我们还需要了解几乎所有编排框架的基本算法，状态协调，它值得在这里有自己的小节。这个工作的基本原则是一个非常简单的三步过程，如下：
- en: The user setting the desired count(s) of each service or a service disappearing.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户设置每个服务或服务消失的期望计数。
- en: The orchestration framework seeing what is needed in order to change the current
    state to the desired state (delta evaluation).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排框架看到了改变当前状态到期望状态所需的内容（增量评估）。
- en: Executing whatever is needed to take the cluster to that state (known as state
    reconciliation).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行任何需要将集群带到该状态的操作（称为状态协调）。
- en: '![](assets/23380424-de88-49b9-8fe9-76b0e8f78015.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/23380424-de88-49b9-8fe9-76b0e8f78015.png)
- en: 'For example, if we currently have five running tasks for a service in the cluster
    and change the desired state to only three tasks, our management/orchestration
    system will see that the difference is `-2` and thus pick two random tasks and
    kill them seamlessly. Conversely, if we have three tasks running and we want five
    instead, the management/orchestration system will see that the desired delta is
    `+2` so it will pick two places with available resources for it and start two
    new tasks. A short explanation of two state transitions should also help clarify
    this process:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们当前在集群中为一个服务运行了五个任务，并将期望状态更改为只有三个任务，我们的管理/编排系统将看到差异为“-2”，因此选择两个随机任务并无缝地杀死它们。相反，如果我们有三个正在运行的任务，而我们想要五个，管理/编排系统将看到期望的增量为“+2”，因此它将选择两个具有可用资源的位置，并启动两个新任务。对两个状态转换的简要解释也应该有助于澄清这个过程：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this very simple but powerful logic, we can dynamically scale up and down
    our services without worrying about the intermediate stages (to a degree). Internally,
    keeping and maintaining states is such a difficult task that most orchestration
    frameworks use a special, high-speed key-value store component to do this for
    them (that is, `etcd`, `ZooKeeper`, and `Consul`).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个非常简单但强大的逻辑，我们可以动态地扩展和缩小我们的服务，而不必担心中间阶段（在一定程度上）。在内部，保持和维护状态是一个非常困难的任务，大多数编排框架使用特殊的高速键值存储组件来为它们执行此操作（即`etcd`，`ZooKeeper`和`Consul`）。
- en: Since our system only cares about where our current state is and where it needs
    to be, this algorithm also doubles as the system for building resilience as a
    dead node, or the container will reduce the current task count for applications
    and will trigger a state transition back to the desired counts automatically.
    As long as services are mostly stateless and you have the resources to run the
    new services, these clusters are resilient to almost any type of failure and now
    you can hopefully see how a few simple concepts tie together to create such a
    robust infrastructure.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的系统只关心当前状态和需要的状态，这个算法也兼作建立弹性的系统，当一个节点死掉，或者容器减少了应用程序的当前任务计数，将自动触发状态转换回到期望的计数。只要服务大多是无状态的，并且你有资源来运行新的服务，这些集群对几乎任何类型的故障都是有弹性的，现在你可以希望看到一些简单的概念如何结合在一起创建这样一个强大的基础设施。
- en: With our new understanding of management and orchestration framework basics,
    we will now take a brief look at each one of our available options (Docker Swarm,
    Kubernetes, Marathon) and see how they compare with each other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们对管理和编排框架基础的新理解，我们现在将简要地看一下我们可用选项中的每一个（Docker Swarm，Kubernetes，Marathon），并看看它们如何相互比较。
- en: Docker Swarm
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: Out of the box, Docker contains an orchestration framework and a management
    platform very architecturally similar to the one covered just a second ago, called
    Docker Swarm. Swarm allows a pretty quick and simple way to get scaling integrated
    with your platform with minimal ramp-up time and given that it is already a part
    of Docker itself, you really don't need much else to deploy a simple set of services
    in a clustered environment. As an added benefit, it contains a pretty solid service
    discovery framework, has multi-host networking capability, and uses TLS for communication
    between nodes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Docker默认包含一个编排框架和一个管理平台，其架构与刚才介绍的非常相似，称为Docker Swarm。Swarm允许以相对较快和简单的方式将扩展集成到您的平台中，而且几乎不需要时间来适应，而且它已经是Docker本身的一部分，因此您实际上不需要太多其他东西来在集群环境中部署一组简单的服务。作为额外的好处，它包含一个相当可靠的服务发现框架，具有多主机网络能力，并且在节点之间使用TLS进行通信。
- en: Multi-host networking capability is the ability of a system to create a virtual
    network across multiple physical machines that are transparent from the point
    of view of the container. Using one of these, your containers can communicate
    with each other as if they were on the same physical network, simplifying the
    connectivity logic and reducing operational costs. We will look into this aspect
    of clustering in depth a bit later.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多主机网络能力是系统在多台物理机器之间创建虚拟网络的能力，从容器的角度来看，这些物理机器是透明的。使用其中之一，您的容器可以彼此通信，就好像它们在同一个物理网络上一样，简化了连接逻辑并降低了运营成本。我们稍后将深入研究集群的这一方面。
- en: The cluster configuration for Docker Swarm can be a simple YAML file, but the
    downside is that GUI tools are, at the time of writing this, somewhat lacking,
    though Portainer ([https://portainer.io](https://portainer.io)) and Shipyard ([https://shipyard-project.com](https://shipyard-project.com))
    are getting to be pretty decent, so this might not be a problem for too long.
    Additionally, some large-scale ops tooling is missing and it seems that generally,
    features of Swarm are heavily evolving and thus in a state of flux, so my personal
    recommendation would be to use this type of orchestration if you need to get something
    up and running quickly on small-to-largish scales. As this product gets more and
    more mature (and since Docker Inc. is placing a lot of development resources behind
    this), it will probably improve significantly, and I expect it to match Kubernetes
    features in many respect so keep an eye out for its feature news.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm的集群配置可以是一个简单的YAML文件，但缺点是，在撰写本文时，GUI工具有些欠缺，尽管Portainer（[https://portainer.io](https://portainer.io)）和Shipyard（[https://shipyard-project.com](https://shipyard-project.com)）正在变得相当不错，所以这可能不会是一个长期的问题。此外，一些大规模的运维工具缺失，似乎Swarm的功能正在大幅发展，因此处于不稳定状态，因此我的个人建议是，如果您需要快速在小到大规模上运行某些东西，可以使用这种编排。随着这款产品变得越来越成熟（并且由于Docker
    Inc.正在投入大量开发资源），它可能会有显著改进，我期望它在许多方面能够与Kubernetes功能相匹敌，因此请密切关注其功能新闻。
- en: Kubernetes
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: 'Kubernetes is Google''s cloud platform and orchestration engine that currently
    provides a bit more in terms of features than does Swarm. The setup of Kubernetes
    is much more difficult as you need: a master, a node (the worker according to
    our earlier dictionary), and pods (grouping of one or more containers). Pods are
    always co-located and co-scheduled, so handling their dependencies is a bit easier
    to deal with but you do not get the same isolation. The interesting thing to keep
    in mind here is that all containers within the pod share the same IP address/ports,
    share volumes, and are generally within the same isolation group. It is almost
    better to think of a pod as a small virtual machine running many services than
    many containers running in parallel.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是谷歌的云平台和编排引擎，目前在功能方面比Swarm提供了更多。Kubernetes的设置要困难得多，因为你需要：一个主节点，一个节点（根据我们之前的词典，这是工作节点），以及pod（一个或多个容器的分组）。Pod始终是共同定位和共同调度的，因此处理它们的依赖关系会更容易一些，但你不会得到相同的隔离。在这里需要记住的有趣的事情是，pod内的所有容器共享相同的IP地址/端口，共享卷，并且通常在相同的隔离组内。几乎可以将pod视为运行多个服务的小型虚拟机，而不是并行运行多个容器。
- en: Kubernetes has been gaining a massive amount of community traction lately and
    is probably the most deployed cluster orchestration and management system in use,
    though to be fair, finding exact figures is tricky, with a majority of them being
    deployed in private clouds. Given that Google has been using this system for a
    while and on such a large scale, it has a pretty proven track record and I would
    probably recommended it for medium-to-large scales. If you don't mind the overhead
    of setting everything up, I think even smaller scales would be acceptable, but
    in that space, Docker Swarm is so easy to use that using Kubernetes for it is
    generally impractical.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最近一直在获得大量的社区关注，可能是最被部署的集群编排和管理系统，尽管要公平地说，找到确切的数字是棘手的，其中大多数被部署在私有云中。考虑到谷歌已经在如此大规模上使用这个系统，它有着相当成熟的记录，我可能会推荐它用于中大规模。如果你不介意设置一切的开销，我认为即使在较小规模上也是可以接受的，但在这个领域，Docker
    Swarm非常容易使用，因此使用Kubernetes通常是不切实际的。
- en: At the time of writing this book, both Mesos and Docker EE have included capabilities
    to support Kubernetes so if you would want to bet on an orchestration engine,
    this would probably be it.![](assets/3aae9d0c-3b9c-4f3e-9f49-b200f63fb339.png)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Mesos和Docker EE都已经包含了支持Kubernetes的功能，所以如果你想要在编排引擎上打赌，这可能就是它！
- en: Apache Mesos/Marathon
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos/Marathon
- en: When you really need to dial up the scaling to levels of Twitter and Airbnb,
    you probably need something even more powerful than Swarm or Kubernetes, which
    is where Mesos and Marathon come into play. Apache Mesos was not actually built
    with Docker in mind but as a general cluster-management tooling that provides
    resource management in a consistent way for applications that run on top of it
    with APIs. You can run anything from scripts, actual applications, and multiple
    platforms (such as HDFS and Hadoop) with relative ease. For container-based orchestration
    and scheduling on this platform these days, Marathon is the general go-to here.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你真的需要将规模扩大到Twitter和Airbnb的级别时，你可能需要比Swarm或Kubernetes更强大的东西，这就是Mesos和Marathon发挥作用的地方。Apache
    Mesos实际上并不是为Docker而建立的，而是作为一种通用的集群管理工具，以一种一致的方式为在其之上运行的应用程序提供资源管理和API。你可以相对容易地运行从脚本、实际应用程序到多个平台（如HDFS和Hadoop）的任何东西。对于这个平台上基于容器的编排和调度，Marathon是通用的选择。
- en: As mentioned a little bit earlier, Kubernetes support has been now available
    again for Mesos after being in a broken state for a while so the suggestion of
    Marathon may change by the time you read this text.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如稍早提到的，Kubernetes支持现在又可以在Mesos上使用了，之前一段时间处于破碎状态，因此在您阅读本文时，对Marathon的建议可能会改变。
- en: Marathon runs as an application (in a very loose sense of the word) on top of
    Mesos as the container orchestration platform and provides all kind of niceties,
    such as a great UI (though Kubernetes has one too), metrics, constraints, persistent
    volumes (experimental at the time of writing this), and many others. As a platform,
    Mesos and Marathon are probably the most powerful combo for handling clusters
    in the tens-of-thousands-of-nodes range, but to get everything pieced together,
    unless you use the pre-packaged DC/OS solution ([https://dcos.io/](https://dcos.io/)),
    it is in my experience really, really tricky to get up and running compared to
    the other two. If you need to cover the range of medium-to-largest of scales with
    added flexibility in order to run other platforms (such as Chronos) on it too,
    currently, I would strongly recommend this combo.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Marathon作为Mesos上的应用程序（在非常宽松的意义上）运行作为容器编排平台，并提供各种便利，如出色的用户界面（尽管Kubernetes也有一个），指标，约束，持久卷（在撰写本文时为实验性质），以及其他许多功能。作为一个平台，Mesos和Marathon可能是处理成千上万个节点的集群最强大的组合，但要将所有东西组合在一起，除非您使用预打包的DC/OS解决方案（[https://dcos.io/](https://dcos.io/)），根据我的经验，与其他两种方法相比，真的非常棘手。如果您需要覆盖中等到最大规模，并增加灵活性以便在其上运行其他平台（如Chronos），目前，我强烈推荐这种组合。
- en: '![](assets/8ceb3bee-952b-4aba-93d8-8ce66365f06b.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8ceb3bee-952b-4aba-93d8-8ce66365f06b.png)'
- en: Cloud-based offerings
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于云的服务
- en: 'If all of this seems too much trouble and you don''t mind paying a hefty premium
    every month for it, all the big cloud players have some sort of container-based
    service offering. Since these vary wildly both in functionality and feature set,
    anything that would get put onto this page in that regard will probably be outdated
    by the time it gets published, and we are more interested in deploying services
    on our own, so I will leave you links to the appropriate offerings that will have
    up-to-date information if you choose this route:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有这些似乎太麻烦，而且您不介意每个月支付高昂的费用，所有大型云服务提供商都有某种基于容器的服务提供。由于这些服务在功能和特性方面差异很大，任何放在这个页面上的内容在发布时可能已经过时，而我们更感兴趣的是部署我们自己的服务，因此我将为您提供适当的服务的链接，这些链接将提供最新的信息，如果您选择这条路线：
- en: '**Amazon ECS**: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊ECS：[https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)
- en: '**Google Container Engine**: [https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌容器引擎：[https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)
- en: '**Microsoft Azure** (**Azure Container Service**): [https://azure.microsoft.com/en-us/services/container-service/](https://azure.microsoft.com/en-us/services/container-service/)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软Azure（Azure容器服务）：[https://azure.microsoft.com/en-us/services/container-service/](https://azure.microsoft.com/en-us/services/container-service/)
- en: '**Oracle Container Cloud Service**: [https://cloud.oracle.com/container](https://cloud.oracle.com/container)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle容器云服务：[https://cloud.oracle.com/container](https://cloud.oracle.com/container)
- en: '**Docker Cloud**: [https://cloud.docker.com/](https://cloud.docker.com/)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Cloud：[https://cloud.docker.com/](https://cloud.docker.com/)
- en: Probably many others that I have missed
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能还有其他一些我错过的
- en: Personally, I would recommend this approach for small-to-medium deployments
    due to ease of use and tested environments. If your needs expand past these scales,
    implementing your service on scalable groups of virtual machines on **Virtual
    Private Clouds** (**VPCs**) with the same cloud service provider is generally
    one of the ways to go as you can tailor your infrastructure in the exact way that
    your needs expand, though the upfront DevOps costs are not small, so decide accordingly.
    A good rule of thumb to remember with pretty much any cloud offering is that with
    easy tooling already provided you get much quicker deployments counterbalanced
    by increased costs (usually hidden) and lack of flexibility/customizability.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，我会推荐这种方法用于中小型部署，因为它易于使用并且经过测试。如果您的需求超出了这些规模，通常有一种方法是在**虚拟私有云**（**VPCs**）上的可扩展虚拟机组上实施您的服务，因为您可以根据需求扩展自己的基础架构，尽管前期的DevOps成本不小，所以请据此决定。几乎任何云服务提供商提供的一个良好的经验法则是，通过提供易用的工具，您可以获得更快的部署速度，但代价是成本增加（通常是隐藏的）和缺乏灵活性/可定制性。
- en: Implementing orchestration
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施编排
- en: With our newly-gained understanding of the orchestration and management offerings
    out there, it is time to try this out ourselves. In our next exercise, we will
    first try to use Docker Swarm to create and play a bit with a local cluster and
    then we will try to deploy our service from the previous chapter onto it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们新获得的对编排和管理工具的理解，现在是时候自己尝试一下了。在我们的下一个练习中，我们将首先尝试使用Docker Swarm来创建并在本地集群上进行一些操作，然后我们将尝试将上一章的服务部署到其中。
- en: Setting up a Docker Swarm cluster
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Docker Swarm集群
- en: 'Since all the functionality to set up a Docker Swarm cluster is already included
    in the Docker installation, this is actually a really easy thing to do. Let''s
    see what commands we have available to us:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于设置Docker Swarm集群的所有功能已经包含在Docker安装中，这实际上是一件非常容易的事情。让我们看看我们可以使用哪些命令：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few things to note here--some more apparent than others:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事情需要注意--有些比其他更明显：
- en: You create a swarm with `docker swarm init`
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 `docker swarm init` 创建一个集群
- en: You join a cluster with `docker swarm join` and the machine can be a worker
    node, a manager node, or both
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 `docker swarm join` 加入一个集群，该机器可以是工作节点、管理节点或两者兼而有之
- en: Authentication is managed using tokens (unique strings that need to match)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份验证是使用令牌（需要匹配的唯一字符串）进行管理
- en: If something happens to a manager node, such as a restart or power cycle, and
    you have set up auto-locking of the swarm, you will need an unlock key to unlock
    the TLS keys
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果管理节点发生故障，例如重新启动或断电，并且您已经设置了自动锁定集群，您将需要一个解锁密钥来解锁TLS密钥
- en: So far, so good, so let's see whether we can set up a swarm with our machine
    serving both as a manager and a worker to see how this works.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，让我们看看我们是否可以设置一个同时作为管理节点和工作节点的集群，以了解其工作原理。
- en: Initializing a Docker Swarm cluster
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化Docker Swarm集群
- en: 'To create our swarm, we first need to instantiate it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的集群，我们首先需要实例化它：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have created a swarm with that command and we are automatically enrolled
    as a manager node. If you take a look at the output, the command for adding worker
    nodes is just `docker swarm join --token <token> <ip>`, but we are interested
    in a single-node deployment for now, so we won't need to worry about it. Given
    that our manager node is also a worker node, we can just use it as-is to throw
    a few services on it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经用那个命令创建了一个集群，并且我们自动注册为管理节点。如果您查看输出，添加工作节点的命令只是 `docker swarm join --token
    <token> <ip>`，但是我们现在只对单节点部署感兴趣，所以我们不需要担心这个。鉴于我们的管理节点也是工作节点，我们可以直接使用它来部署一些服务。
- en: Deploying services
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署服务
- en: 'Most of the commands we will initially need are accessible through the `docker
    services` command:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初需要的大多数命令都可以通过`docker services`命令访问：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you might be suspecting, given how similar these commands are to some of
    the ones for managing containers, once you move to an orchestration platform as
    opposed to fiddling with containers directly, the ideal management of your services
    would be done through the orchestration itself. I would probably expand this and
    go as far as to say that if you are working with containers too much while having
    an orchestration platform, you did not set something up or you did not set it
    up correctly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能怀疑的那样，考虑到这些命令与管理容器的一些命令有多么相似，一旦你转移到编排平台而不是直接操作容器，你的服务的理想管理将通过编排本身完成。我可能会扩展这一点，并且会说，如果你在拥有编排平台的同时过多地使用容器，那么你没有设置好某些东西，或者你没有正确地设置它。
- en: 'We will now try to get some sort of service running on our Swarm, but since
    we are just exploring how all this works, we can use a very slimmed down (and
    a very insecure) version of our Python web server from [Chapter 2](9b436a64-9b02-4e8c-bbe9-f3bb729152ea.xhtml),
    *Rolling Up the Sleeves*. Create a new folder and add this to a new `Dockerfile`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将尝试在我们的Swarm上运行某种服务，但由于我们只是在探索所有这些是如何工作的，我们可以使用一个非常简化（也非常不安全）的我们的Python
    Web服务器的版本。从[第2章](9b436a64-9b02-4e8c-bbe9-f3bb729152ea.xhtml) *Rolling Up the Sleeves*。创建一个新文件夹，并将其添加到新的`Dockerfile`中：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s build it so that our local registry has an image to pull from when we
    define our service:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建它，以便我们的本地注册表有一个镜像可以从中拉取，当我们定义我们的服务时：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With the image in place, let''s deploy it on our swarm:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个镜像，让我们在我们的Swarm上部署它：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The warning shown is actually very important: our service is only available
    on our local machine''s Docker registry when we built it, so using a Swarm service
    that is spread between multiple nodes will have issues since other machines will
    not be able to load the same image. For this reason, having the image registry
    available from a single source to all of the nodes is mandatory for cluster deployments.
    We will cover this issue in more detail as we progress through this and following
    chapters.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所显示的警告实际上非常重要：我们构建时服务仅在我们本地机器的Docker注册表上可用，因此使用分布在多个节点之间的Swarm服务将会出现问题，因为其他机器将无法加载相同的镜像。因此，将镜像注册表从单一来源提供给所有节点对于集群部署是强制性的。随着我们在本章和接下来的章节中的进展，我们将更详细地讨论这个问题。
- en: 'If we check out `http://127.0.0.1:8000`, we can see that our service is running!
    Let''s see this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查`http://127.0.0.1:8000`，我们可以看到我们的服务正在运行！让我们看看这个：
- en: '![](assets/de4bc4a5-6c91-4c2a-a740-66b70e72e1b6.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/de4bc4a5-6c91-4c2a-a740-66b70e72e1b6.png)'
- en: 'If we scale this service to three instances, we can see how our orchestration
    tool is handling the state transitions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这项服务扩展到三个实例，我们可以看到我们的编排工具是如何处理状态转换的：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see how this is adjusting the container instances to fit our specified
    parameters. What if we now add something in the mix that will happen in real life-a
    container death:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这是如何调整容器实例以适应我们指定的参数的。如果我们现在在其中添加一些在现实生活中会发生的事情-容器死亡：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you can see, the swarm will bounce back up like nothing happened, and this
    is exactly why containerization is so powerful: not only can we spread processing
    tasks among many machines and flexibly scale the throughput, but with identical
    services we don''t really care very much if some (hopefully small) percentage
    of services dies, as the framework will make it completely seamless for the client.
    With the built-in service discovery of Docker Swarm, the load balancer will shift
    the connection to whatever container is running/available so anyone trying to
    connect to our server should not see much of a difference.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，集群将像没有发生任何事情一样反弹回来，这正是容器化如此强大的原因：我们不仅可以在许多机器之间分配处理任务并灵活地扩展吞吐量，而且使用相同的服务，如果一些（希望很少）服务死掉，我们并不会太在意，因为框架会使客户端完全无缝地进行处理。借助Docker
    Swarm的内置服务发现，负载均衡器将把连接转移到任何正在运行/可用的容器，因此任何试图连接到我们服务器的人都不应该看到太大的差异。
- en: Cleaning up
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理
- en: 'As with any service that we are finished with, we need to make sure that we
    clean up any resources we have used up so far. In the case of Swarm, we should
    probably remove our service and destroy our cluster until we need it again. You
    can do both of those things using `docker service rm` and `docker swarm leave`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们完成的任何服务一样，我们需要确保清理我们迄今为止使用的任何资源。在Swarm的情况下，我们可能应该删除我们的服务并销毁我们的集群，直到我们再次需要它。您可以使用`docker
    service rm`和`docker swarm leave`来执行这两个操作：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The reason why we had to use the `--force` flag here is due to the fact that
    we are a manager node and we are the last one in the cluster, so by default, Docker
    will prevent this action without it. In a multi-node setup, you will not generally
    need this flag.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不得不使用`--force`标志的原因是因为我们是管理节点，也是集群中的最后一个节点，所以默认情况下，Docker会阻止这个操作。在多节点设置中，通常不需要这个标志。
- en: With this action, we are now back at where we started and are ready to do this
    with a real service.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个操作，我们现在回到了起点，并准备使用一个真正的服务。
- en: Using Swarm to orchestrate our words service
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Swarm来编排我们的单词服务
- en: In the previous chapter, we built a simple service that can be used to add and
    list words entered on a form. But if you remember, we heavily used somewhat of
    an implementation detail to connect the services together, making it extremely
    fragile if not downright hacked-up together. With our new-found knowledge of service
    discovery and our understanding of Docker Swarm orchestration, we can try to get
    our old code ready for real cluster deployment and move away from the fragile
    setup we had earlier.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了一个简单的服务，用于添加和列出在表单上输入的单词。但是如果你记得的话，我们在连接服务时大量使用了一些实现细节，如果不是完全地拼凑在一起，那就会变得非常脆弱。有了我们对服务发现的新认识和对Docker
    Swarm编排的理解，我们可以尝试准备好我们的旧代码以进行真正的集群部署，并摆脱我们之前脆弱的设置。
- en: The application server
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用服务器
- en: Copy the old application server folder from [Chapter 3](a4ccf20b-743c-4b30-b416-2303314d91ba.xhtml), *Service
    Decomposition,* to a new folder and we will change our main handler code ( `index.js`
    )  since we have to accommodate the fact that we will not be the only instance
    reading from and writing to the database anymore.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第3章](a4ccf20b-743c-4b30-b416-2303314d91ba.xhtml) *服务分解*中复制旧的应用服务器文件夹到一个新文件夹，我们将更改我们的主处理程序代码（`index.js`），因为我们必须适应这样一个事实，即我们将不再是唯一从数据库中读取和写入的实例。
- en: As always, all code can also be found at [https://github.com/sgnn7/deploying_with_docker](https://github.com/sgnn7/deploying_with_docker).
    This particular implementation can be found in `chapter_4/clustered_application`.Warning!
    As you start thinking about similar containers running in parallel, you have to
    start being extra careful about data changes that can and will occur outside of
    the container's realm of control. For this reason, keeping or caching the state
    in any form in a running container is usually a recipe for disaster and data inconsistencies.
    To avoid this issue, in general, you should try to make sure that you re-read
    the information from your upstream sources (that is, the database) before doing
    any transformation or passing of the data like we do here.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，所有代码也可以在[https://github.com/sgnn7/deploying_with_docker](https://github.com/sgnn7/deploying_with_docker)找到。这个特定的实现可以在`chapter_4/clustered_application`中找到。警告！当您开始考虑类似的容器并行运行时，您必须开始特别注意容器控制范围之外可能发生的数据更改。因此，在运行容器中保留或缓存状态通常是灾难和数据不一致的原因。为了避免这个问题，通常情况下，您应该尽量确保在进行任何转换或传递数据之前从上游源（即数据库）重新读取信息，就像我们在这里所做的那样。
- en: index.js
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: index.js
- en: 'This file is pretty much the same one from the last chapter but we will be
    making a few changes to eliminate caching:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件基本上与上一章的文件相同，但我们将进行一些更改以消除缓存：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If may have noticed, many things are similar, but there are fundamental changes
    too:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您可能已经注意到，许多事情是相似的，但也有根本性的变化：
- en: We don't pre-load the words on start as the list might change from the time
    the service initializes and the user requests data.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不会在启动时预加载单词，因为列表可能会在服务初始化和用户请求数据之间发生变化。
- en: We load the saved words on each `GET` request in order to make sure we always
    get fresh data.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在每个“GET”请求中加载保存的单词，以确保我们始终获得新鲜数据。
- en: When we save the word, we just insert it into the database and don't preserve
    it in the application as we will get new data on `GET` re-display.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们保存单词时，我们只是将其插入到数据库中，并不在应用程序中保留它，因为我们将在“GET”重新显示时获得新数据。
- en: Using this approach, any changes done to the data in the database by any app
    instances will immediately be reflected in all of them. Additionally, if a database
    administrator changed any of the data, we will also see those changes within the
    application. Since our service also uses an environment variable for the database
    host, we should not need to change it to the support service discovery.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，数据库中的数据由任何应用程序实例进行的更改将立即反映在所有实例中。此外，如果数据库管理员更改了任何数据，我们也将在应用程序中看到这些更改。由于我们的服务还使用环境变量作为数据库主机，我们不应该需要将其更改为支持服务发现。
- en: Caution! Be aware that because we read the database on each `GET` request, our
    changes to support clustering are not free and come with an increase in database
    queries, which can become a real bottleneck when the networking, cache invalidation,
    or disk transfers become overly saturated by these requests. Additionally, since
    we read the database before we display the data, slowdowns in the backend processing
    of our database `find()` will be user-visible, possibly causing undesired user
    experience, so keep these things in mind as you develop container-friendly services.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！请注意，因为我们在每个“GET”请求中读取数据库，我们对支持集群的更改并不是免费的，并且会增加数据库查询，这可能会在网络、缓存失效或磁盘传输过度饱和时成为真正的瓶颈。此外，由于我们在显示数据之前读取数据库，后端处理数据库“find（）”的减速将对用户可见，可能导致不良用户体验，因此在开发容器友好型服务时请牢记这些事情。
- en: The web server
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web服务器
- en: 'Our web server changes will be a bit trickier due to a quirk/feature of the
    NGINX configuration processing that may also impact you if you do Java-based DNS
    resolution. Essentially, NGINX caches DNS entries so hard that effectively, once
    it reads the configuration files, any new DNS resolution within that configuration
    will not actually take place at all unless some extra flags ( `resolver` ) are
    specified. With the Docker service being constantly mutable and relocatable, this
    is a serious issue that must be worked around to function properly on the Swarm.
    Here, you have a couple of options:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Run a DNS forwarder (such as `dnsmasq`) in parallel with NGINX and use that
    as the resolver. This requires running both `dnsmasq` and NGINX in the same container.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Populate the NGINX configuration container start with the same resolvers from
    the system using something such as `envsubst`: this requires all containers to
    be in the same user-defined network.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hardcode the DNS resolver IP (`127.0.0.11`): this also requires all containers
    to be in the same user-defined network.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For robustness, we will use the second option, so copy the web server from
    the previous chapter into a new folder and rename it to `nginx_main_site.conf.template`.
    We will then add a resolver configuration to it and a variable `$APP_NAME` for
    our proxy host endpoint:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since NGINX does not handle environment variable substitution in the configuration
    files, we will write a wrapper script around it. Add a new file called `start_nginx.sh`
    and include the following content in it that takes the host''s resolvers and generates
    the new main_site config:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To get this to run, we finally need to make sure we start NGINX with this script
    instead of the one built in, so we need to modify our `Dockerfile` as well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up our Dockerfile and make sure that it has the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, the main change is the start up script `CMD` override and turning the
    configuration into a template with the rest pretty much left alone.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Database
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike the other two containers, we will leave the database in one container
    due to a combination of things:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB can scale to high GB/low TB dataset sizes easily with vertical scaling.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases are extremely difficult to scale up without in-depth knowledge of
    volumes (covered in the next chapter).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharding and replica sets of databases are generally complicated enough for
    whole books to be written on this topic alone.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We may cover this topic in a later chapter, but here, it would derail us from
    our general goal of learning how to deploy services so we will just have our single
    database instance that we used in the previous chapter for now.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Deploying it all
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we did for our simple web server, we will begin by creating another Swarm
    cluster:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we need to create our overlay network for the service-discovery hostname
    resolution to work. You don''t need to know much about this other than it creates
    an isolated network that we will add all the services to:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we will build and launch our containers:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you are having trouble with getting these services up and running, you can
    check the logs with `docker service logs <service_name>` in order to figure out
    what went wrong. You can also use `docker logs <container_id>` if a specific container
    is having trouble.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'With these in place, we can now check whether our code works at `http://127.0.0.1:8080`
    (username: `user`, password: `test`):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '**![](assets/aa5b1c35-7269-44d6-85cf-1858566d42d6.png)**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Looks like it is working! Once we put in our credentials, we should be redirected
    to the main application page:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/996362dd-b32a-47dd-a7aa-06d28749de5e.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Does the database work if we put in some words?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/03ad27dd-0617-426d-9359-8ba8653640c5.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Indeed! We have really created a 1-node swarm-backed service, and it is scalable
    plus load balanced!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The Docker stack
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As it was pretty obvious from just a few paragraphs before, a manual setup
    of these services seems somewhat of a pain, so here we introduce a new tool that
    can help us do this much easier: Docker Stack. This tool uses a YAML file to get
    things to deploy all the services easily and repeatedly.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will clean up our old exercise before trying to use Docker stack configuration:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can write our YAML configuration file--you can easily notice the parallels
    that the CLI has to this configuration file:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information about all the available options usable in Docker
    stack YAML files by visiting [https://docs.docker.com/docker-cloud/apps/stack-yaml-reference/](https://docs.docker.com/docker-cloud/apps/stack-yaml-reference/).
    Generally, anything you can set with the CLI commands, you can do the same with
    the YAML configuration.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'What about starting our stack? That''s easy too! Stack has almost the same
    commands as `docker services`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you go to `http://127.0.0.1:8080` in your browser again, you will see that
    our app works just like before! We have managed to deploy our whole cluster worth
    of images with a single file on a Docker Swarm cluster!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Clean up
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are not the kind to leave useless services around, so we will remove our
    stack and stop our Swarm cluster as we prepare for the next chapter:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We won't need to clean up the network or running containers as they will automatically
    get removed by Docker once our stack is gone. With this part done, we can now
    move on to the next chapter about volumes with a clean slate.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered a multitude of things like: what service discovery
    is and why we need it, container orchestration basics and state reconciliation
    principles, as well as some major players in the orchestration world. With that
    knowledge in hand, we went on to implement a single-node full cluster using Docker
    Swarm to show how something like this can be done and near the end we used Docker
    stack to manage groups of services together, hopefully showing you how this can
    all be turned from theory to practice.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start exploring the intricate world of Docker volumes
    and data persistence, so stick with us.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL

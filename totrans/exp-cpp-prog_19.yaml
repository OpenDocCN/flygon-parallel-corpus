- en: Multithreading with GPGPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fairly recent development has been to use video cards (GPUs) for general purpose
    computing (GPGPU). Using frameworks such as CUDA and OpenCL, it is possible to
    speed up, for example, the processing of large datasets in parallel in medical,
    military, and scientific applications. In this chapter, we will look at how this
    is done with C++ and OpenCL, and how to integrate such a feature into a multithreaded
    application in C++.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenCL into a C++ based application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenges of using OpenCL in a multithreaded fashion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of latency and scheduling on multithreaded performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GPGPU processing model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 16](f600c912-487b-43d9-8569-14f8671784c6.xhtml), *Multithreading
    with Distributed Computing*, we looked at running the same task across a number
    of compute nodes in a cluster system. The main goal of such a setup is to process
    data in a highly parallel fashion, theoretically speeding up said processing relative
    to a single system with fewer CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: '**GPGPU** (**General Purpose Computing on Graphics Processing Units**) is in
    some ways similar to this, but with one major difference: while a compute cluster
    with only regular CPUs is good at scalar tasks--meaning performing one task on
    one single set of data (SISD)--GPUs are vector processors that excel at SIMD (Single
    Input, Multiple Data) tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, this means that one can send a large dataset to a GPU, along with
    a single task description, and the GPU will proceed to execute that same task
    on parts of that data in parallel on its hundreds or thousands of cores. One can
    thus regard a GPU as a very specialized kind of cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38ada78a-8f9a-4bf3-971e-de8343e0cfd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the concept of GPGPU was first coined (around 2001), the most common way
    to write GPGPU programs was using GLSL (OpenGL Shading Language) and similar shader
    languages. Since these shader languages were already aimed at the processing of
    SIMD tasks (image and scene data), adapting them for more generic tasks was fairly
    straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since that time, a number of more specialized implementations have appeared:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Since** | **Owner** | **Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2006 | NVidia | This is proprietary and only runs on NVidia GPUs |'
  prefs: []
  type: TYPE_TB
- en: '| Close to Metal | 2006 | ATi/AMD | This was abandoned in favor of OpenCL |'
  prefs: []
  type: TYPE_TB
- en: '| DirectCompute | 2008 | Microsoft | This is released with DX11, runs on DX10
    GPUs, and is limited to Windows platforms |'
  prefs: []
  type: TYPE_TB
- en: '| OpenCL | 2009 | Khronos Group | This is open standard and available across
    AMD, Intel, and NVidia GPUs on all mainstream platforms, as well as mobile platforms
    |'
  prefs: []
  type: TYPE_TB
- en: OpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of the various current GPGPU implementations, OpenCL is by far the most interesting
    GPGPU API due to the absence of limitations. It is available for virtually all
    mainstream GPUs and platforms, even enjoying support on select mobile platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Another distinguishing feature of OpenCL is that it's not limited to just GPGPU
    either. As part of its name (Open Computing Language), it abstracts a system into
    the so-called *compute devices*, each with their own capabilities. GPGPU is the
    most common application, but this feature makes it fairly easy to test implementations
    on a CPU first, for easy debugging.
  prefs: []
  type: TYPE_NORMAL
- en: One possible disadvantage of OpenCL is that it employs a high level of abstraction
    for memory and hardware details, which can negatively affect performance, even
    as it increases the portability of the code.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this chapter, we will focus on OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: Common OpenCL applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many programs incorporate OpenCL-based code in order to speed up operations.
    These include programs aimed at graphics processing, as well as 3D modelling and
    CAD, audio and video processing. Some examples are:'
  prefs: []
  type: TYPE_NORMAL
- en: Adobe Photoshop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GIMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ImageMagick
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autodesk Maya
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handbrake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vegas Pro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libav
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Cut Pro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FFmpeg
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further acceleration of certain operations is found in office applications including
    LibreOffice Calc and Microsoft Excel.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps more importantly, OpenCL is also commonly used for scientific computing
    and cryptography, including BOINC and GROMACS as well as many other libraries
    and programs.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the release of the OpenCL specification on December 8, 2008, there have
    so far been five updates, bringing it up to version 2.2\. Important changes with
    these releases are mentioned next.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL 1.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first public release was released by Apple as part of the macOS X Snow Leopard
    release on August 28, 2009.
  prefs: []
  type: TYPE_NORMAL
- en: Together with this release, AMD announced that it would support OpenCL and retire
    its own Close to Metal (CtM) framework. NVidia, RapidMind, and IBM also added
    support for OpenCL to their own frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL 1.1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenCL 1.1 specification was ratified by the Khronos Group on June 14,
    2010\. It adds additional functionality for parallel programming and performance,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: New data types including 3-component vectors and additional image formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling commands from multiple host threads and processing buffers across multiple
    devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operations on regions of a buffer including reading, writing, and copying of
    the 1D, 2D, or 3D rectangular regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhanced use of events to drive and control command execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional OpenCL built-in C functions, such as integer clamp, shuffle, and
    asynchronous-strided (not contiguous, but with gaps between the data) copies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved OpenGL interoperability through efficient sharing of images and buffers
    by linking OpenCL and OpenGL events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCL 1.2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenCL 1.2 version was released on November 15, 2011\. Its most significant
    features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Device partitioning:** This enables applications to partition a device into
    sub-devices to directly control work assignment to particular compute units, reserve
    a part of the device for use for high priority/latency-sensitive tasks, or effectively
    use shared hardware resources such as a cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Separate compilation and linking of objects**: This provides the capabilities
    and flexibility of traditional compilers enabling the creation of libraries of
    OpenCL programs for other programs to link to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced image support**: This **i**ncludes added support for 1D images and
    1D & 2D image arrays. Also, the OpenGL sharing extension now enables an OpenCL
    image to be created from OpenGL 1D textures and 1D & 2D texture arrays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Built-in kernels:** This represents the capabilities of specialized or non-programmable
    hardware and associated firmware, such as video encoder/decoders and digital signal
    processors, enabling these custom devices to be driven from and integrated closely
    with the OpenCL framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DX9 Media Surface Sharing**: This enables efficient sharing between OpenCL
    and DirectX 9 or DXVA media surfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DX11 Surface Sharing**: For seamless sharing between OpenCL and DirectX 11
    surfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCL 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenCL2.0 version was released on November 18, 2013\. This release has
    the following significant changes or additions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared Virtual Memory**: Host and device kernels can directly share complex,
    pointer-containing data structures such as trees and linked lists, providing significant
    programming flexibility and eliminating costly data transfers between host and
    devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Parallelism**: Device kernels can enqueue kernels to the same device
    with no host interaction, enabling flexible work scheduling paradigms and avoiding
    the need to transfer execution control and data between the device and host, often
    significantly offloading host processor bottlenecks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generic Address Space**: Functions can be written without specifying a named
    address space for arguments, especially useful for those arguments that are declared
    to be a pointer to a type, eliminating the need for multiple functions to be written
    for each named address space used in an application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Images**: Improved image support including sRGB images and 3D image writes,
    the ability for kernels to read from and write to the same image, and the creation
    of OpenCL images from a mip-mapped or a multi-sampled OpenGL texture for improved
    OpenGL interop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C11 Atomics**: A subset of C11 atomics and synchronization operations to
    enable assignments in one work-item to be visible to other work-items in a work-group,
    across work-groups executing on a device or for sharing data between the OpenCL
    device and host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipes**: Pipes are memory objects that store data organized as a FIFO and
    OpenCL 2.0 provides built-in functions for kernels to read from or write to a
    pipe, providing straightforward programming of pipe data structures that can be
    highly optimized by OpenCL implementers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Android Installable Client Driver Extension**: Enables OpenCL implementations
    to be discovered and loaded as a shared object on Android systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCL 2.1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenCL 2.1 revision to the 2.0 standard was released on November 16, 2015\.
    The most notable thing about this release was the introduction of the OpenCL C++
    kernel language, such as how the OpenCL language originally was based on C with
    extensions, the C++ version is based on a subset of C++14, with backwards compatibility
    for the C kernel language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updates to the OpenCL API include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Subgroups**: These enable finer grain control of hardware threading, are
    now in core, together with additional subgroup query operations for increased
    flexibility'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Copying of kernel objects and states**: clCloneKernel enables copying of
    kernel objects and state for safe implementation of copy constructors in wrapper
    classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low-latency device timer queries**: These allow for alignment of profiling
    data between device and host code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermediate SPIR-V code for the runtime**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bi-directional translator between LLVM to SPIR-V to enable flexible use of
    both intermediate languages in tool chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenCL C to LLVM compiler that generates SPIR-V through the above translator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A SPIR-V assembler and disassembler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard Portable Intermediate Representation (SPIR) and its successor, SPIR-V,
    are a way to provide device-independent binaries for use across OpenCL devices.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL 2.2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On May 16, 2017, what is now the current release of OpenCL was released. According
    to the Khronos Group, it includes the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL 2.2 brings the OpenCL C++ kernel language into the core specification
    for significantly enhanced parallel programming productivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCL C++ kernel language is a static subset of the C++14 standard and
    includes classes, templates, Lambda expressions, function overloads, and many
    other constructs for generic and meta-programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverages the new Khronos SPIR-V 1.1 intermediate language that fully supports
    the OpenCL C++ kernel language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCL library functions can now take advantage of the C++ language to provide
    increased safety and reduced undefined behavior while accessing features such
    as atomics, iterators, images, samplers, pipes, and device queue built-in types
    and address spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipe storage is a new device-side type in OpenCL 2.2 that is useful for FPGA
    implementations by making the connectivity size and type known at compile time
    and enabling efficient device-scope communication between kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenCL 2.2 also includes features for enhanced optimization of generated code:
    Applications can provide the value of specialization constant at SPIR-V compilation
    time, a new query can detect non-trivial constructors and destructors of program-scope
    global objects, and user callbacks can be set at program release time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs on any OpenCL 2.0-capable hardware (only driver update required)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a development environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regardless of which platform and GPU you have, the most important part of doing
    OpenCL development is to obtain the OpenCL runtime for one's GPU from its manufacturer.
    Here, AMD, Intel, and NVidia all provide an SDK for all mainstream platforms.
    For NVidia, OpenCL support is included in the CUDA SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the GPU vendor's SDK, one can also find details on their website
    on which GPUs are supported by this SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After installing the vendor's GPGPU SDK using the provided instructions, we
    still need to download the OpenCL headers. Unlike the shared library and runtime
    file provided by the vendor, these headers are generic and will work with any
    OpenCL implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Debian-based distributions, simply execute the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For other distributions, the package may be called the same, or something different.
    Consult the manual for one's distribution on how to find out the package name.
  prefs: []
  type: TYPE_NORMAL
- en: After installing the SDK and OpenCL headers, we are ready to compile our first
    OpenCL applications.
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On Windows, we can choose between developing with Visual Studio (Visual C++)
    or with the Windows port of GCC (MinGW). To stay consistent with the Linux version,
    we will be using MinGW along with MSYS2\. This means that we'll have the same
    compiler toolchain and same Bash shell and utilities, along with the Pacman package
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the vendor''s GPGPU SDK, as described previously, simply execute
    the following command line in an MSYS2 shell in order to install the OpenCL headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, execute the following command line when using the 32-bit version of MinGW:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With this, the OpenCL headers are in place. We now just have to make sure that
    the MinGW linker can find OpenCL library. With the NVidia CUDA SDK, you can use
    the `CUDA_PATH` environment variable for this, or browse the install location
    of the SDK and copy the appropriate OpenCL LIB file from there to the MinGW lib
    folder, making sure not to mix the 32-bit and 64-bit files.
  prefs: []
  type: TYPE_NORMAL
- en: With the shared library now also in place, we can compile the OpenCL applications.
  prefs: []
  type: TYPE_NORMAL
- en: OS X/MacOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting with OS X 10.7, an OpenCL runtime is provided with the OS. After installing
    XCode for the development headers and libraries, one can immediately start with
    OpenCL development.
  prefs: []
  type: TYPE_NORMAL
- en: A basic OpenCL application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common example of a GPGPU application is one which calculates the Fast Fourier
    Transform (FFT). This algorithm is commonly used for audio processing and similar,
    allowing you to transform, for example, from the time domain to the frequency
    domain for analysis purposes.
  prefs: []
  type: TYPE_NORMAL
- en: What it does is apply a divide and conquer approach to a dataset, in order to
    calculate the DFT (Discrete Fourier Transform). It does this by splitting the
    input sequence into a fixed, small number of smaller subsequences, computing their
    DFT, and assembling these outputs in order to compose the final sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is fairly advanced mathematics, but suffice it to say that what makes
    it so ideal for GPGPU is that it''s a highly-parallel algorithm, employing the
    subdivision of data in order to speed up the calculating of the DFT, as visualized
    in this graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1497e9bd-b3c9-4400-9ea8-45e22a4247ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each OpenCL application consists of at least two parts: the C++ code that sets
    up and configures the OpenCL instance, and the actual OpenCL code, also known
    as a kernel, such as this one based on the FFT demonstration example from Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This OpenCL kernel shows that, like the GLSL shader language, OpenCL's kernel
    language is essentially C with a number of extensions. Although one could use
    the OpenCL C++ kernel language, this one is only available since OpenCL 2.1 (2015),
    and as a result, support and examples for it are less common than the C kernel
    language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the C++ application, using which, we run the preceding OpenCL kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see here, there''s only one header we have to include in order to
    gain access to the OpenCL functions. We also specify the name of the file that
    contains the source for our OpenCL kernel. Since each OpenCL device is likely
    a different architecture, the kernel is compiled for the target device when we
    load it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to obtain a list of OpenCL devices we can use, filtering it by
    GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create an OpenCL `context` using the GPU devices we found. The context
    manages the resources on a range of devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will create the command queue that will contain the commands to
    be executed on the OpenCL devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to communicate with devices, we need to allocate buffer objects that
    will contain the data we will copy to their memory. Here, we will allocate two
    buffers, one to read and one to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now got the data on the device, but still need to load the kernel on
    it. For this, we will create a kernel using the OpenCL kernel source we looked
    at earlier, using the filename we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compile the source as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will create the actual kernel from the binary we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to pass arguments to our kernel, we have to set them here. Here, we
    will add pointers to our buffers and dimensions of the work size as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can set the work item dimensions and execute the kernel. Here, we will
    use a kernel execution method that allows us to define the size of the work group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After executing the kernel, we wish to read back the resulting information.
    For this, we tell OpenCL to copy the assigned write buffer we passed as a kernel
    argument into a newly assigned buffer. We are now free to use the data in this
    buffer as we see fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in this example, we will not use the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we free the resources we allocated and exit.
  prefs: []
  type: TYPE_NORMAL
- en: GPU memory management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using a CPU, one has to deal with a number of memory hierarchies, in the
    form of the main memory (slowest), to CPU caches (faster), and CPU registers (fastest).
    A GPU is much the same, in that, one has to deal with a memory hierarchy that
    can significantly impact the speed of one's applications.
  prefs: []
  type: TYPE_NORMAL
- en: Fastest on a GPU is also the register (or private) memory, of which we have
    quite a bit more than on the average CPU. After this, we get local memory, which
    is a memory shared by a number of processing elements. Slowest on the GPU itself
    is the memory data cache, also called texture memory. This is a memory on the
    card that is usually referred to as Video RAM (VRAM) and uses a high-bandwidth,
    but a relatively high-latency memory such as GDDR5.
  prefs: []
  type: TYPE_NORMAL
- en: The absolute slowest is using the host system's memory (system RAM), as this
    has to travel across the PCIe bus and through various other subsystems in order
    to transfer any data. Relative to on-device memory systems, host-device communication
    is best called 'glacial'.
  prefs: []
  type: TYPE_NORMAL
- en: 'For AMD, Nvidia, and similar dedicated GPU devices, the memory architecture
    can be visualized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26a187e6-07c2-4122-87ba-9911d6f052a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Because of this memory layout, it is advisable to transfer any data in large
    blocks, and to use asynchronous transfers if possible. Ideally, the kernel would
    run on the GPU core and have the data streamed to it to avoid any latencies.
  prefs: []
  type: TYPE_NORMAL
- en: GPGPU and multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Combining multithreaded code with GPGPU can be much easier than trying to manage
    a parallel application running on an MPI cluster. This is mostly due to the following
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare data: Readying the data which we want to process, such as a large set
    of images, or a single large image, by sending it to the GPU''s memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prepare kernel: Loading the OpenCL kernel file and compiling it into an OpenCL
    kernel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute kernel: Send the kernel to the GPU and instruct it to start processing
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read data: Once we know the processing has finished, or a specific intermediate
    state has been reached, we will read a buffer we passed along as an argument with
    the OpenCL kernel in order to obtain our result(s).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As this is an asynchronous process, one can treat this as a fire-and-forget
    operation, merely having a single thread dedicated to monitoring the process of
    the active kernels.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenge in terms of multithreading and GPGPU applications lies
    not with the host-based application, but with the GPGPU kernel or shader program
    running on the GPU, as it has to coordinate memory management and processing between
    both local and distant processing units, determine which memory systems to use
    depending on the type of data without causing problems elsewhere in the processing.
  prefs: []
  type: TYPE_NORMAL
- en: This is a delicate process involving a lot of trial and error, profiling and
    optimizations. One memory copy optimization or use of an asynchronous operation
    instead of a synchronous one may cut processing time from many hours to just a
    couple. A good understanding of the memory systems is crucial to preventing data
    starvation and similar issues.
  prefs: []
  type: TYPE_NORMAL
- en: Since GPGPU is generally used to accelerate tasks of significant duration (minutes
    to hours, or longer), it is probably best regarded from a multithreading perspective
    as a common worker thread, albeit with a few important complications, mostly in
    the form of latency.
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we touched upon in the earlier section on GPU memory management, it is highly
    preferable to use the memory closest to the GPU's processing units first, as they
    are the fastest. Fastest here mostly means that they have less latency, meaning
    the time taken to request information from the memory and receiving the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact latency will differ per GPU, but as an example, for Nvidia''s Kepler
    (Tesla K20) architecture, one can expect a latency of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global** memory: 450 cycles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant** memory cache: 45 â€“ 125 cycles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local** (**shared**) memory: 45 cycles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These measurements are all on the CPU itself. For the PCIe bus one would have
    to expect something on the order of multiple milliseconds per transfer once one
    starts to transfer multi-megabyte buffers. To fill for example the GPU's memory
    with a gigabyte-sized buffer could take a considerable amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: For a simple round-trip over the PCIe bus one would measure the latency in microseconds,
    which for a GPU core running at 1+ GHz would seem like an eternity. This basically
    defines why communication between the host and GPU should be absolutely minimal
    and highly optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Potential issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common mistake with GPGPU applications is reading the result buffer before
    the processing has finished. After transferring the buffer to the device and executing
    the kernel, one has to insert synchronization points to signal the host that it
    has finished processing. These generally should be implemented using asynchronous
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: As we just covered in the section on latency, it's important to keep in mind
    the potentially very large delays between a request and response, depending on
    the memory sub-system or bus. Failure to do so may cause weird glitches, freezes
    and crashes, as well as data corruption and an application which will seemingly
    wait forever.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to profile a GPGPU application to get a good idea of what the
    GPU utilization is, and whether the process flow is anywhere near being optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging GPGPU applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The biggest challenge with GPGPU applications is that of debugging a kernel.
    CUDA comes with a simulator for this reason, which allows one to run and debug
    a kernel on a CPU. OpenCL allows one to run a kernel on a CPU without modification,
    although this may not get the exact same behavior (and bugs) as when run on a
    specific GPU device.
  prefs: []
  type: TYPE_NORMAL
- en: A slightly more advanced method involves the use of a dedicated debugger such
    as Nvidia's Nsight, which comes in versions both for Visual Studio ([https://developer.nvidia.com/nvidia-nsight-visual-studio-edition](https://developer.nvidia.com/nvidia-nsight-visual-studio-edition))
    and Eclipse ([https://developer.nvidia.com/nsight-eclipse-edition](https://developer.nvidia.com/nsight-eclipse-edition)).
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the marketing blurb on the Nsight website:'
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA Nsight Visual Studio Edition brings GPU computing into Microsoft Visual
    Studio (including multiple instances of VS2017). This application development
    environment for GPUs allows you to build, debug, profile and trace heterogeneous
    compute, graphics, and virtual reality applications built with CUDA C/C++, OpenCL,
    DirectCompute, Direct3D, Vulkan API, OpenGL, OpenVR, and the Oculus SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an active CUDA debug session:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74e349e0-dec7-41a7-809f-f167a52a522f.png)'
  prefs: []
  type: TYPE_IMG
- en: A big advantage of such a debugger tool is that it allows one to monitor, profile
    and optimize one's GPGPU application by identifying bottlenecks and potential
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to integrate GPGPU processing into a C++ application
    in the form of OpenCL. We also looked at the GPU memory hierarchy and how this
    impacts performance, especially in terms of host-device communication.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be familiar with GPGPU implementations and concepts, along with
    how to create an OpenCL application, and how to compile and run it. How to avoid
    common mistakes should also be known.
  prefs: []
  type: TYPE_NORMAL
- en: As this is the final chapter of this book, it is hoped that all major questions
    have been answered, and that the preceding chapters, along with this one, have
    been informative and helpful in some fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on from this book, the reader may be interested in pursuing any of the
    topics covered in more detail, for which many resources are available both online
    and offline. The topic of multithreading and related areas is very large and touches
    upon many applications, from business to scientific, artistic and personal applications
  prefs: []
  type: TYPE_NORMAL
- en: The reader may want to set up a Beowulf cluster of tehir own, or focus on GPGPU,
    or combine the two. Maybe there is a complex application they have wanted to write
    for a while, or perhaps just have fun with programming.
  prefs: []
  type: TYPE_NORMAL

- en: Chapter 8. Introduction to Gos Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just finished with the *Gang Of Four* design patterns that are commonly
    used in object oriented programming languages. They have been used extensively
    for the last few decades (even before they were explicitly defined in a book).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to see concurrency in the Go language. We will,
    learn that with multiple cores and multiple processes, applications can help us
    to achieve better performance and endless possibilities. We will look at how to
    use some of the already known patterns in concurrently safe ways.
  prefs: []
  type: TYPE_NORMAL
- en: A little bit of history and theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about Go's concurrency, it's impossible not to talk about history.
    In the last decades, we saw an improvement in the speed of CPUs until we reached
    the hardware limits imposed by current hardware materials, design, and architectures.
    When we reached this point, we started to play with the first multicore computers,
    the first double CPU motherboards, and then single CPUs with more than one core
    in their heart.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the languages we are using are still the ones created when we
    had single core CPUs, such as Java or C++. While being terrific systems languages,
    they lack a proper concurrency support by design. You can develop concurrent apps
    in both of the languages used in your project by using third party tools or by
    developing your own (not a very easy task).
  prefs: []
  type: TYPE_NORMAL
- en: Go's concurrency was designed with these caveats in mind. The creators wanted
    garbage collected and procedural language that is familiar for newcomers, but
    which, at the same time, can be used to write concurrent applications easily and
    without affecting the core of the language.
  prefs: []
  type: TYPE_NORMAL
- en: We have experienced this in the early chapters. We have developed more than
    20 design patterns without a word about concurrency. This clearly shows that the
    concurrent features of the Go language are completely separated from the core
    language while being part of it, a perfect example of abstraction and encapsulation.
  prefs: []
  type: TYPE_NORMAL
- en: There are many concurrency models in computer science, the most famous being
    the actor model present in languages such as **Erlang** or **Scala**. Go, on the
    other side, uses **Communicating Sequential Processes** (**CSP**), which has a
    different approach to concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many people have misunderstood the differences between both, even thinking
    that they are the same. There is a popular speech by Rob Pike, one of the creators
    of Go, *Concurrency is not parallelism*, which I really agree with. As a quick
    summary of the talk, we can extract the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is about dealing with many things at once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelism is about doing many things at the same time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency enables parallelism by designing a correct structure of concurrency
    work.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can think of the mechanism of a bike. When we pedal, we usually
    push down the pedal to produce force (and this push, raises our opposite leg on
    the opposite pedal). We cannot push with both legs at the same time because the
    cranks don't allow us to do it. But this design allows the construction of a parallel
    bike, commonly called a **tandem bike**. A tandem bike is a bike that two people
    can ride at the same time; they both pedal and apply force to the bike.
  prefs: []
  type: TYPE_NORMAL
- en: In the bike example, concurrency is the design of a bike that, with two legs
    (Goroutines), you can produce power to move the bike by yourself. The design is
    concurrent and correct. If we use a tandem bike and two people (two cores), the
    solution is concurrent, correct, and parallel. But the key thing is that with
    a concurrent design, we don't have to worry about parallelism; we can think about
    it as an extra feature if our concurrent design is correct. In fact, we can use
    the tandem bike with only one person, but the concurrent design of the legs, pedals,
    chain, wheels of a bike is still correct.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concurrency versus parallelism](img/B05557_08_01-1-300x255.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With concurrency, on the left side, we have a design and a structure that is
    executed sequentially by the same CPU core. Once we have this design and structure,
    parallelism can be achieved by simply repeating this structure on a different
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: This is how Go eases the reasoning about concurrent and parallel programs by
    simply not worrying too much about parallel execution and focusing much more on
    concurrent design and structure. Breaking a big task into smaller tasks that can
    be run concurrently usually provides much better performance in a single-core
    computer, but, if this design can also be run in parallel, we could achieve an
    even higher throughput (or not, depending on the design).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we can set the number of cores in use in a Go app by setting the environment
    variable `GOMAXPROCS` to the number of cores we want. This is not only useful
    when using schedulers, such as **Apache Mesos**, but it gives us more control
    about how a Go app works and performs.
  prefs: []
  type: TYPE_NORMAL
- en: So, to recap, it is very important to keep in mind that concurrency is about
    structure and parallelism is about execution. We must think about making our programs
    concurrent in a better way, by breaking them down into smaller pieces of work,
    and Go's scheduler will try to make them parallel if it's possible and allowed.
  prefs: []
  type: TYPE_NORMAL
- en: CSP versus actor-based concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common and, perhaps, intuitive way to think about concurrency is close
    to the way the actor model works.
  prefs: []
  type: TYPE_NORMAL
- en: '![CSP versus actor-based concurrency](img/B05557_08_02-1-300x164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the actor model, if **Actor 1** wants to communicate with **Actor 2**, then
    **Actor 1** must know **Actor 2** first; for example, it must have its process
    ID, maybe from the creation step, and put a message on its inbox queue. After
    placing the message, **Actor 1** can continue its tasks without getting blocked
    if **Actor 2** cannot process the message immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'CSP, on the other side, introduces a new entity into the equation-channels.
    Channels are the way to communicate between processes because they are completely
    anonymous (unlike actors, where we need to know their process IDs). In the case
    of CSP, we don''t have a process ID to use to communicate. Instead, we have to
    create a channel to the processes to allow incoming and outgoing communication.
    In this case, what we know that the receiver is the channel it uses to receive
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CSP versus actor-based concurrency](img/B05557_08_03-1-300x37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this diagram, we can see that the processes are anonymous, but we have a
    channel with ID 1, that is, **Channel 1**, which connects them together. This
    abstraction does not tell us how many processes are on each side of the channel;
    it simply connects them and allows communication between processes by using the
    channel.
  prefs: []
  type: TYPE_NORMAL
- en: The key here is that channels isolate both extremes so that process A can send
    data through a channel that will be handled by potentially one or more processes
    that' are transparent to A. It also works the same in reverse; process B can receive
    data from many channels one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Goroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Go, we achieve concurrency by working with Goroutines. They are like processes
    that run applications in a computer concurrently; in fact, the main loop of Go
    could be considered a Goroutine, too. Goroutines are used in places where we would
    use actors. They execute some logic and die (or keep looping if necessary).
  prefs: []
  type: TYPE_NORMAL
- en: But Goroutines are not threads. We can launch thousands of concurrent Goroutines,
    even millions. They are incredibly cheap, with a small growth stack. We will use
    Goroutines to execute code that we want to work concurrently. For example, three
    calls to three services to compose a response can be designed concurrently with
    three Goroutines to do the service calls potentially in parallel and a fourth
    Goroutine to receive them and compose the response. What's the point here? That
    if we have a computer with four cores, we could potentially run this service call
    in parallel, but if we use a one-core computer, the design will still be correct
    and the calls will be executed concurrently in only one core. By designing concurrent
    applications, we don't need to worry about parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the bike analogy, we were pushing the pedals of the bike with our
    two legs. That's two Goroutines concurrently pushing the pedals. When we use the
    tandem, we had a total of four Goroutines, possibly working in parallel. But we
    also have two hands to handle the front and rear brakes. That's a total of eight
    Goroutines for our two threads bike. Actually, we don't pedal when we brake and
    we don't brake when we pedal; that's a correct concurrent design. Our nervous
    system transports the information about when to stop pedaling and when to start
    braking. In Go, our nervous system is composed of channels; we will see them after
    playing a bit with Goroutines first.
  prefs: []
  type: TYPE_NORMAL
- en: Our first Goroutine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Enough of the explanations now. Let''s get our hands dirty. For our first Goroutine,
    we will print the message `Hello World!` in a Goroutine. Let''s start with what
    we''ve been doing up until now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this small snippet of code will simply output `Hello World!` in the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Not impressive at all. To run it in a new Goroutine, we just need to add the
    keyword `go` at the beginning of the call to the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With this simple word, we are telling Go to start a new Goroutine running the
    contents of the `helloWorld` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What? It printed nothing! Why is that? Things get complicated when you start
    to deal with concurrent applications. The problem is that the `main` function
    finishes before the `helloWorld` function gets executed. Let's analyse it step
    by step. The `main` function starts and schedules a new Goroutine that will execute
    the `helloWorld` function, but the function isn't executed when the function finishes--it
    is still in the scheduling process.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our `main` problem is that the `main` function has to wait for the Goroutine
    to be executed before finishing. So let''s pause for a second to give some room
    to the Goroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `time.Sleep` function effectively sleeps the main Goroutine for one second
    before continuing (and exiting). If we run this now, we must get the message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I suppose you must have noticed by now the small gap of time where the program
    is freezing before finishing. This is the function for sleeping. If you are doing
    a lot of tasks, you might want to raise the waiting time to whatever you want.
    Just remember that in any application the `main` function cannot finish before
    the rest of the Goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Anonymous functions launched as new Goroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have defined the `helloWorld` function so that it can be launched with a
    different Goroutine. This is not strictly necessary because you can launch snippets
    of code directly in the function''s scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is also valid. We have used an anonymous function and we have launched
    it in a new Goroutine using the `go` keyword. Take a closer look at the closing
    braces of the function-they are followed by opening and closing parenthesis, indicating
    the execution of the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also pass data to anonymous functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is also valid. We had defined an anonymous function that received a string,
    which then printed the received string. When we called the function in a different
    Goroutine, we passed the message we wanted to print. In this sense, the following
    example would also be valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have defined a function within the scope of our `main` function
    and stored it in a variable called `messagePrinter`. Now we can concurrently print
    as many messages as we want by using the `messagePrinter(string)` signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We have just scratched the surface of concurrent programming in Go, but we can
    already see that it can be quite powerful. But we definitely have to do something
    with that sleeping period. WaitGroups can help us with this problem.
  prefs: []
  type: TYPE_NORMAL
- en: WaitGroups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'WaitGroup comes in the synchronization package (the `sync` package) to help
    us synchronize many concurrent Goroutines. It works very easily--every time we
    have to wait for one Goroutine to finish, we add `1` to the group, and once all
    of them are added, we ask the group to wait. When the Goroutine finishes, it says
    `Done` and the WaitGroup will take one from the group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is the simplest possible example of a WaitGroup. First, we created a variable
    to hold it called the `wait` variable. Next, before launching the new Goroutine,
    we say to the WaitGroup `hey, you'll have to wait for one thing to finish` by
    using the `wait.Add(1)` method. Now we can launch the `1` that the WaitGroup has
    to wait for, which in this case is the previous Goroutine that prints `Hello World`
    and says `Done` (by using the `wait.Done()` method) at the end of the Goroutine.
    Finally, we indicate to the WaitGroup to wait. We have to remember that the function
    `wait.Wait()` was probably executed before the Goroutine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the code again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now it just waits the necessary time and not one millisecond more before exiting
    the application. Remember that when we use the `Add(value)` method, we add entities
    to the WaitGroup, and when we use the `Done()` method, we subtract one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, the `Add` function takes a delta value, so the following code is
    equivalent to the previous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we added `1` before launching the Goroutine and we added `-1`
    (subtracted 1) at the end of it. If we know in advance how many Goroutines we
    are going to launch, we can also call the `Add` method just once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are going to create five Goroutines (as stated in the `goroutines`
    variable). We know it in advance, so we simply add them all to the WaitGroup.
    We are then going to launch the same amount of `goroutine` variables by using
    a `for` loop. Every time one Goroutine finishes, it calls the `Done()` method
    of the WaitGroup that is effectively waiting at the end of the main loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, in this case, the code reaches the end of the `main` function before
    all Goroutines are launched (if any), and the WaitGroup makes the execution of
    the main flow wait until all `Done` messages are called. Let''s run this small
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We haven''t mentioned it before, but we have passed the iteration index to
    each Goroutine as the parameter `GoroutineID` to print it with the message `Hello
    goroutines!` You might also have noticed that the Goroutines aren''t executed
    in order. Of course! We are dealing with a scheduler that doesn''t guarantee the
    order of execution of the Goroutines. This is something to keep in mind when programming
    concurrent applications. In fact, if we execute it again, we won''t necessarily
    get the same order of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to use WaitGroups, we can also introduce the concept of
    callbacks. If you have ever worked with languages like JavaScript that use them
    extensively, this section will be familiar to you. A callback is an anonymous
    function that will be executed within the context of a different function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we want to write a function to convert a string to uppercase,
    as well as making it asynchronous. How do we write this function so that we can
    work with callbacks? There''s a little trick-we can have have a function that
    takes a string and returns a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'So take the returning type of this function (a string) and put it as the second
    parameter in an anonymous function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, the `toUpperSync` function returns nothing, but also takes a function that,
    by coincidence, also takes a string. We can execute this function with the result
    we will usually return.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute the `f` function with the result of calling the `strings.ToUpper`
    method with the provided word (which returns the word `parameter` in uppercase).
    Let''s write the `main` function too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In our main code, we have defined our callback. As you can see, we passed the
    test `Hello Callbacks!` to convert it to uppercase. Next we pass the callback
    to be executed with the result of passing our string to uppercase. In this case,
    we simply print the text in the console with the text `Callback` in front of it.
    When we execute this code, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Strictly speaking, this is a synchronous callback. To make it asynchronous
    we have to introduce some concurrent handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same code executed asynchronously. We use WaitGroups to handle
    concurrency (we will see later that channels can also be used for this). Now,
    our function `toUpperAsync` is, as its name implies, asynchronous. We launched
    the callback in a different Goroutine by using the keyword `go` when calling the
    callback. We write a small message to show the ordering nature of the concurrent
    execution more precisely. We wait until the callback signals that it''s finished
    and we can exit the program safely. When we execute this, we get the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the program reaches the end of the `main` function before executing
    the callback in the `toUpperAsync` function. This pattern brings many possibilities,
    but leaves us open to one big problem called callback hell.
  prefs: []
  type: TYPE_NORMAL
- en: Callback hell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The term **callback hell** is commonly used to refer to when many callbacks
    have been stacked within each other. This makes them difficult to reason with
    and handle when they grow too much. For example, using the same code as before,
    we could stack another asynchronous call with the contents that we previously
    printed to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '(We have omitted imports, the package name, and the `toUpperAsync` function
    as they have not changed.) Now we have the `toUpperAsync` function within a `toUpperAsync`
    function, and we could embed many more if we want. In this case, we again pass
    the text that we previously printed on the console to use it in the following
    callback. The inner callback finally prints it on the console, giving the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we can assume that the outer callback will be executed before
    the inner one. That's why we don't need to add one more to the WaitGroup.
  prefs: []
  type: TYPE_NORMAL
- en: The point here is that we must be careful when using callbacks. In very complex
    systems, too many callbacks are hard to reason with and hard to deal with. But
    with care and rationality, they are powerful tools.
  prefs: []
  type: TYPE_NORMAL
- en: Mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are working with concurrent applications, you have to deal with more
    than one resource potentially accessing some memory location. This is usually
    called **race condition**.
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, a race condition is similar to that moment where two people
    try to get the last piece of pizza at exactly the same time--their hands collide.
    Replace the pizza with a variable and their hands with Goroutines and we'll have
    a perfect analogy.
  prefs: []
  type: TYPE_NORMAL
- en: There is one character at the dinner table to solve this issues--a father or
    mother. They have kept the pizza on a different table and we have to ask for permission
    to stand up before getting our slice of pizza. It doesn't matter if all the kids
    ask at the same time--they will only allow one kid to stand.
  prefs: []
  type: TYPE_NORMAL
- en: Well, a mutex is like our parents. They'll control who can access the pizza--I
    mean, a variable--and they won't allow anyone else to access it.
  prefs: []
  type: TYPE_NORMAL
- en: To use a mutex, we have to actively lock it; if it's already locked (another
    Goroutine is using it), we'll have to wait until it's unlocked again. Once we
    get access to the mutex, we can lock it again, do whatever modifications are needed,
    and unlock it again. We'll look at this using an example.
  prefs: []
  type: TYPE_NORMAL
- en: An example with mutexes - concurrent counter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mutexes are widely used in concurrent programming. Maybe not so much in Go because
    it has a more idiomatic way of concurrent programming in its use of channels,
    but it's worth seeing how they work for the situations where channels simply don't
    fit so well.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we are going to develop a small concurrent counter. This counter
    will add one to an integer field in a `Counter` type. This should be done in a
    concurrent-safe way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `Counter` structure is defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `Counter` structure has a field of `int` type that stores the current value
    of the count. It also embeds the `Mutex` type from the `sync` package. Embedding
    this field will allow us to lock and unlock the entire structure without actively
    calling a specific field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `main` function launches 10 Goroutines that try to add one to the field
    value of `Counter` structure. All of this is done concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We have created a type called `Counter`. Using a `for` loop, we have launched
    a total of 10 Goroutines, as we saw in the *Anonymous functions launched as new
    Goroutines* section. But inside every Goroutine, we are locking the counter so
    that no more Goroutines can access it, adding one to the field value, and unlocking
    it again so others can access it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll print the value held by the counter. It must be 10 because we
    have launched 10 Goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: But how can we know that this program is thread safe? Well, Go comes with a
    very handy built-in feature called the "race detector".
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the race detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already know what a race condition is. To recap, it is used when two processes
    try to access the same resource at the same time with one or more writing operations
    (both processes writing or one process writing while the other reads) involved
    at that precise moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go has a very handy tool to help diagnose race conditions, that you can run
    in your tests or your main application directly. So let''s reuse the example we
    just wrote for the *mutexes* section and run it with the race detector. This is
    as simple as adding the `-race` command-line flag to the command execution of
    our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, not very impressive is it? But in fact it is telling us that it has not
    detected a potential race condition in the code of this program. Let''s make the
    detector of `-race` flag warn us of a possible race condition by not locking `counter`
    before we modify it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the `for` loop, comment the `Lock` and `Unlock` calls before and after
    adding `1` to the field value. This will introduce a race condition. Let''s run
    the same program again with the race flag activated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'I have reduced the output a bit to see things more clearly. We can see a big,
    uppercase message reading `WARNING: DATA RACE`. But this output is very easy to
    reason with. First, it is telling us that some memory position represented by
    *line 19* on our `main.go` file is reading some variable. But there is also a
    write operation in *line 19* of the same file!'
  prefs: []
  type: TYPE_NORMAL
- en: This is because a "`++`" operation requires a read of the current value and
    a write to add one to it. That's why the race condition is in the same line, because
    every time it's executed it reads and writes the field in the `Counter` structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s keep in mind that the race detector works at runtime. It doesn''t
    analyze our code statically! What does it mean? It means that we can have a potential
    race condition in our design that the race detector will not detect. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will leave the code as shown in the preceding example. We will take all
    locks and unlocks from the code and launch a single Goroutine to update the `value`
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'No warnings, so the code is correct. Well, we know, by design, it''s not. We
    can raise the number of Goroutines executed to two and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s execute the program again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now yes, the race condition is detected. But what if we reduce the number of
    processors in use to just one? Will we have a race condition too?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: It seems that no race condition has been detected. This is because the scheduler
    executed one Goroutine first and then the other, so, finally, the race condition
    didn't occur. But with a higher number of Goroutines it will also warn us about
    a race condition, even using only one core.
  prefs: []
  type: TYPE_NORMAL
- en: So, the race detector can help us to detect race conditions that are happening
    in our code, but it won't protect us from a bad design that is not immediately
    executing race conditions. A very useful feature that can save us from lots of
    headaches.
  prefs: []
  type: TYPE_NORMAL
- en: Channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Channels are the second primitive in the language that allows us to write concurrent
    applications. We have talked a bit about channels in the *Communicating sequential
    processes* section.
  prefs: []
  type: TYPE_NORMAL
- en: Channels are the way we communicate between processes. We could be sharing a
    memory location and using mutexes to control the processes' access. But channels
    provide us with a more natural way to handle concurrent applications that also
    produces better concurrent designs in our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Our first channel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working with many Goroutines seems pretty difficult if we can't create some
    synchronization between them. The order of execution could be irrelevant as soon
    as they are synchronized. Channels are the second key feature to write concurrent
    applications in Go.
  prefs: []
  type: TYPE_NORMAL
- en: A TV channel in real life is something that connects an emission (from a studio)
    to millions of TVs (the receivers). Channels in Go work in a similar fashion.
    One or more Goroutines can work as emitters, and one or more Goroutine can act
    as receivers.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing channels, by default, block the execution of Goroutines until
    something is received. It is as if our favourite TV show delays the emission until
    we turn the TV on so we don't miss anything.
  prefs: []
  type: TYPE_NORMAL
- en: How is this done in Go?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: To create channels in Go, we use the same syntax that we use to create slices.
    The `make` keyword is used to create a channel, and we have to pass the keyword
    `chan` and the type that the channel will transport, in this case, strings. With
    this, we have a blocking channel with the name `channel`. Next, we launch a Goroutines
    that sends the message `Hello World!` to the channel. This is indicated by the
    intuitive arrow that shows the flow--the `Hello World!` text going to (`<-`) a
    channel. This works like an assignment in a variable, so we can only pass something
    to a channel by first writing the channel, then the arrow, and finally the value
    to pass. We cannot write `"Hello World!" -> channel`.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, this channel is blocking the execution of Gorountines
    until a message is received. In this case, the execution of the `main` function
    is stopped until the message from the launched Goroutines reaches the other end
    of the channel in the line `message := <-channel`. In this case, the arrow points
    in the same direction, but it's placed before the channel, indicating that the
    data is being extracted from the channel and assigned to a new variable called
    `message` (using the new assignment "`:=`" operator).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we don''t need to use a WaitGroup to synchronize the `main` function
    with the created Goroutines, as the default nature of channels is to block until
    data is received. But does it work the other way around? If there is no receiver
    when the Goroutine sends the message, does it continue? Let''s edit this example
    to see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use the `Sleep` function again. In this case, we print a message
    when the Goroutine is finished. The big difference is in the `main` function.
    Now we wait one second before we listen to the channel for data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The output can differ because, again, there are no guarantees in the order of
    execution, but now we can see that no message is printed until one second has
    passed. After the initial delay, we start listening to the channel, take the data,
    and print it. So the emitter also has to wait for a cue from the other side of
    the channel to continue its execution.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, channels are ways to communicate between Goroutines by sending data
    through one end and receiving it at the other (like a pipe). In their default
    state, an emitter Goroutine will block its execution until a receiver Goroutine
    takes the data. The same goes for a receiver Goroutine, which will block until
    some emitter sends data through the channel. So you can have passive listeners
    (waiting for data) or passive emitters (waiting for listeners).
  prefs: []
  type: TYPE_NORMAL
- en: Buffered channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A buffered channel works in a similar way to default unbuffered channels. You
    also pass and take values from them by using the arrows, but, unlike unbuffered
    channels, senders don''t need to wait until some Goroutine picks the data that
    they are sending:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This example is like the first example we used for channels, but now we have
    set the capacity of the channel to one in the `make` statement. With this, we
    tell the compiler that this channel has a capacity of one string before getting
    blocked. So the first string doesn''t block the emitter, but the second would.
    Let''s run this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now we can run this small program as many times as we want--the output will
    always be in the same order. This time, we have launched the concurrent function
    and waited for one second. Previously, the anonymous function wouldn't continue
    until the second has passed and someone can pick the sent data. In this case,
    with a buffered channel, the data is held in the channel and frees the Goroutine
    to continue its execution. In this case, the Goroutine is always finishing before
    the wait time passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This new channel has a size of one, so a second message would block the Goroutine
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we add a second `Hello world! 2` message, and we provide it with an index.
    In this case, the output of this program could be like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Indicating that we have just taken one message from the channel buffer, we have
    printed it, and the `main` function finished before the launched Goroutine could
    finish. The Goroutine got blocked when sending the second message and couldn't
    continue until the other end took the first message. Then it prints it so quickly
    that it doesn't have time to print the message to show the ending of the Goroutine.
    If you keep executing the program on the console, sooner or later the scheduler
    will finish the Goroutine execution before the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: Directional channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One cool feature about Go channels is that, when we use them as parameters,
    we can restrict their directionality so that they can be used only to send or
    to receive. The compiler will complain if a channel is used in the restricted
    direction. This feature applies a new level of static typing to Go apps and makes
    code more understandable and more readable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll take a simple example with channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The line where we launch the new Goroutine `go func(ch chan<- string)` states
    that the channel passed to this function can only be used as an input channel,
    and you can't listen to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also pass a channel that will be used as a receiver channel only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the arrow is on the opposite side of the keyword `chan`, indicating
    an extracting operation from the channel. Keep in mind that the channel arrow
    always points left, to indicate a receiving channel, it must go on the left, and
    to indicate an inserting channel, it must go on the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to send a value through this *receive only* channel, the compiler
    will complain about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This function has a receive only channel that we will try to use to send the
    message `hello` through. Let''s see what the compiler says:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: It doesn't like it and asks us to correct it. Now the code is even more readable
    and safe, and we have just placed an arrow in front or behind the `chan` argument.
  prefs: []
  type: TYPE_NORMAL
- en: The select statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The select statement is also a key feature in Go. It is used to handle more
    than one channel input within a Goroutine. In fact, it opens lots of possibilities,
    and we will use it extensively in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '![The select statement](img/B05557_08_04-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the `select` structure, we ask the program to choose between one or more
    channels to receive their data. We can save this data in a variable and make something
    with it before finishing the select. The `select` structure is just executed once;
    it doesn't matter if it is listening to more channels, it will be executed only
    once and the code will continue executing. If we want it to handle the same channels
    more than once, we have to put it in a `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: We will make a small app that will send the message `hello` and the message
    `goodbye` to the same Goroutine, which will print them and exit if it doesn't
    receive anything else in five seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will make a generic function that sends a string over a channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can send a string over a channel by simply calling the `sendString`
    method. It''s time for the receiver. The receiver will take messages from both
    channels--the one that sends `hello` messages and the one that sends `goodbye`
    messages. You can also see this in the previous diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Let's start with the arguments. This function takes three channels--two receiving
    channels and one to send something through it. Then, it starts an infinite loop
    with the `for` keyword. This way we can keep listening to both channels forever.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the scope of `select` block, we have to use a case for each channel
    we want to handle (have you realized how similar it is to the `switch` statement?).
    Let''s see the three cases step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: The first case takes the incoming data from the `helloCh` argument and saves
    it in a variable called `msg`. Then it prints the contents of this variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second case takes the incoming data from the `goodbyeCh` argument and saves
    it in a variable called `msg` too. Then it also prints the content of this variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third case is quite interesting. It calls the `time` function. After that,
    if we check its signature, it accepts a time and duration value and returns a
    receiving channel. This receiving channel will receive a time, the value of `time`
    after the specified duration has passed. In our example, we use the channel it
    returns as a timeout. Because the select is restarted after each handle, the timer
    is restarted too. This is a very simple way to set a timer to a Goroutine waiting
    for the response of one or many channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Everything is ready for the `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Again, step by step, we created the three channels that we'll need in this exercise.
    Then, we launched our `receiver` function in a different Goroutine. This Goroutine
    is handled by Go's scheduler and our program continues. We launched a new Goroutine
    to send the message `hello` to the `helloCh` arguments. Again, this is going to
    occur eventually when the Go's scheduler decides.
  prefs: []
  type: TYPE_NORMAL
- en: Our program continues again and waits for a second. In this break, Go's scheduler
    will have time to execute the receiver and the first message (if it hasn't done
    so yet), so the `hello!` message will appear on the console during the break.
  prefs: []
  type: TYPE_NORMAL
- en: A new message is sent over the `goodbye` channel with the `goodbye!` text in
    a new Goroutine, and our program continues again to a line where we wait for an
    incoming message in the `quitCh` argument.
  prefs: []
  type: TYPE_NORMAL
- en: We have launched three Goroutines already--the receiver that it is still running,
    the first message that had finished when the message was handled by the `select`
    statement, and the second message was been printed almost immediately and had
    finished too. So just the receiver is running at this moment, and if it doesn't
    receive any other message in the following two seconds, it will handle the incoming
    message from the `time` structure. After `channel` type, print a message to say
    that it is quitting, send a `true` to the `quitCh`, and break the infinite loop
    where it was looping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run this small app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The result  may not be very impressive, but the concept is clear. We can handle
    many incoming channels in the same Goroutine by using the select statement.
  prefs: []
  type: TYPE_NORMAL
- en: Ranging over channels too!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last feature about channels that we will see is ranging over channels.
    We are talking about the range keyword. We have used it extensively to range over
    lists, and we can use it to range over a channel too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we have created an unbuffered channel, but it would work with
    a buffered one too. We launched a function in a new Goroutine that sends the number
    "1" over a channel, waits a second, sends the number "2", and closes the channel.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to range over the channel. The syntax is quite similar to a
    list range. We store the incoming data from the channel in the variable `v` and
    we print this variable to the console. The range keeps iterating until the channel
    is closed, taking data from the channel.
  prefs: []
  type: TYPE_NORMAL
- en: Can you guess the output of this little program?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Again, not very impressive. It prints the number "1", then waits a second, prints
    the number "2", and exits the application.
  prefs: []
  type: TYPE_NORMAL
- en: According to the design of this concurrent app, the range was iterates over
    possible incoming data from the
  prefs: []
  type: TYPE_NORMAL
- en: channel
  prefs: []
  type: TYPE_NORMAL
- en: until the concurrent Goroutine closes this channel. At that moment, the range
    finishes and the app can exit.
  prefs: []
  type: TYPE_NORMAL
- en: Range is very useful in taking data from a channel, and it's commonly used in
    fan-in patterns where many different Goroutines send data to the same channel.
  prefs: []
  type: TYPE_NORMAL
- en: Using it all - concurrent singleton
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to create Goroutines and channels, we'll put all our knowledge
    in a single package. Think back to the first few chapter, when we explained the
    singleton pattern-it was some structure or variable that could only exist once
    in our code. All access to this structure should be done using the pattern described,
    but, in fact, it wasn't concurrent safe.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will write with concurrency in mind. We will write a concurrent counter,
    like the one we wrote in the *mutexes* section, but this time we will solve it
    with channels.
  prefs: []
  type: TYPE_NORMAL
- en: Unit test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To restrict concurrent access to the `singleton` instance, just one Goroutine
    will be able to access it. We'll access it using channels--the first one to add
    one, the second one to get the current count, and the third one to stop the Goroutine.
  prefs: []
  type: TYPE_NORMAL
- en: We will add one 10,000 times using 10,000 different Goroutines launched from
    two different `singleton` instances. Then, we'll introduce a loop to check the
    count of the `singleton` until it is 5,000, but we'll write how much the count
    is before starting the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the count has reached 5,000, the loop will exit and quit the running Goroutine-the
    test code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see the full test we'll use. After creating two instances of the
    `singleton`, we have created a `for` loop that launches the `AddOne` method 5,000
    times from each instance. This is not happening yet; they are being scheduled
    and will be executed eventually. We are printing the count of the `singleton`
    instance to clearly see this eventuality; depending on the computer, it will print
    some number greater than 0 and lower than 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: The last step before stopping the Goroutine that is holding the count is to
    enter a loop that checks the value of the count and waits 10 milliseconds if the
    value is not the expected value (10,000). Once it reaches this value, the loop
    will exit and we can stop the `singleton` instance.
  prefs: []
  type: TYPE_NORMAL
- en: We'll jump directly to the implementation as the requirement is quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First of all, we''ll create the Goroutine that will hold the count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We created three channels, as we mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: The `addCh` channel is used to communicate with the action of adding one to
    the count, and receives a `bool` type just to signal "add one" (we don't need
    to send the number, although we could).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getCountCh` channel will return a channel that will receive the current
    value of the count. Take a moment to reason about the `getCountCh` channel-it's
    a channel that receives a channel that receives integer types. It sounds a bit
    complicated, but it will make more sense when we finish the example, don't worry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `quitCh` channel will communicate to the Goroutine that it should end its
    infinite loop and finish itself too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we have the channels that we need to perform the actions we want. Next,
    we launch the Goroutine passing the channels as arguments. As you can see, we
    are restricting the direction of the channels to provide more type safety. Inside
    this Goroutine, we create an infinite `for` loop. This loop won't stop until a
    break is executed within it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `select` statement, if you remember, was a way to receive data
    from different channels at the same time. We have three cases, so we listen to
    the three incoming channels that entered as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The `addCh` case will add one to the count. Remember that only one case can
    be executed on each iteration so that no Goroutine could be accessing the current
    count until we finish adding one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getCountCh` channel receives a channel that receives an integer, so we
    capture this new channel and send the current value through it to the other end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `quitCh` channel breaks the `for` loop, so the Goroutine ends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One last thing. The `init()` function in any package will get executed on program
    execution, so we don't need to worry about executing this function specifically
    from our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll create the type that the tests are expecting. We will see that
    all the magic and logic is hidden from the end user in this type (as we have seen
    in the code of the test):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The `singleton` type works similar to the way it worked in [Chapter 2](ch02.html
    "Chapter 2. Creational Patterns - Singleton, Builder, Factory, Prototype, and
    Abstract Factory Design Patterns") , *Creational Patterns - Singleton, Builder,
    Factory, Prototype, and Abstract Factory*, but this time it won't hold the count
    value. We created a local value for it called `instance`, and we return the pointer
    to this instance when we call the `GetInstance()` method. It is not strictly necessary
    to do it this way, but we don't need to allocate a new instance of the `singleton`
    type every time we want to access the count variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the `AddOne()` method will have to add one to the current count. How?
    By sending `true` to the `addCh` channel. That''s simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This small snippet will trigger the `addCh` case in our Goroutine in turn.
    The `addCh` case simply executes `count++` and finishes, letting `select` channel
    control flow that is executed on `init` function above to execute the next instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The `GetCount` method creates a channel every time it's called and defers the
    action of closing it at the end of the function. This channel is unbuffered as
    we have seen previously in this chapter. An unbuffered channel blocks the execution
    until it receives some data. So we send this channel to `getCountCh` which is
    a channel too and, effectively, expects a `chan int` type to send the current
    count value back through it. The `GetCount()` method will not return until the
    value of `count` variable arrives to the `resCh` channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be thinking, why aren''t we using the same channel in both directions
    to receive the value of the count? This way we will avoid an allocation. Well,
    if we use the same channel inside the `GetCount()` method, we will have two listeners
    in this channel--one in `select` statement, at the beginning of the file on the
    `init` function, and one there, so it could resolve to any of them when sending
    the value back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we have to stop the Goroutine at some moment. The `Stop` method sends
    the value to the `singleton` type Goroutine so that the `quitCh` case is triggered
    and the `for` loop is broken. The next step is to close all channels so that no
    more data can be sent through them. This is very convenient when you know that
    you won't be using some of your channels anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time to execute the tests and take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Very little code output, but everything has worked as expected. In the test,
    we printed the value of the count before entering the loop that iterates until
    it reaches the value 10,000\. As we saw previously, the Go scheduler will try
    to run the content of the Goroutines using as many OS threads as you configured
    by using the `GOMAXPROCS` configuration. In my computer, it is set to `4` because
    my computer has four cores. But the point is that we can see that a lot of things
    can happen after launching a Goroutine (or 10,000) and the next execution line.
  prefs: []
  type: TYPE_NORMAL
- en: But what about its use of mutexes?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the code is much leaner. As we saw previously, we can embed the
    mutex within the `singleton` structure. The count is also held in the `count`
    field and the `AddOne()` and `GetCount()` methods lock and unlock the value to
    be concurrently safe.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing. In this `singleton` instance, we are using the `RWMutex` type
    instead of the already known `sync.Mutex` type. The main difference here is that
    the `RWMutex` type has two types of locks--a read lock and a write lock. The read
    lock, executed by calling the `RLock` method, only waits if a write lock is currently
    active. At the same time, it only blocks a write lock, so that many read actions
    can be done in parallel. It makes a lot of sense; we don't want to block a Goroutine
    that wants to read a value just because another Goroutine is also reading the
    value-it won't change. The `sync.RWMutex` type helps us to achieve this logic
    in our code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how to write a concurrent Singleton using mutexes and channels.
    While the channels example was more complex, it also shows the core power of Go's
    concurrency, as you can achieve complex levels of event-driven architectures by
    simply using channels.
  prefs: []
  type: TYPE_NORMAL
- en: Just keep in mind that, if you haven't written concurrent code in the past,
    it can take some time to start thinking concurrently in a comfortable way. But
    it's nothing that practice cannot solve.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the importance of designing concurrent apps to achieve parallelism
    in our programs. We have dealt with most of Go's primitives to write concurrent
    applications, and now we can write common concurrent design patterns.
  prefs: []
  type: TYPE_NORMAL

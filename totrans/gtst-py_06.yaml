- en: Principles of Algorithm Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we want to study algorithm design? There are of course many reasons,
    and our motivation for learning something is very much dependent on our own circumstances.
    There are without doubt important professional reasons for being interested in
    algorithm design. Algorithms are the foundations of all computing. We think of
    a computer as being a piece of hardware, a hard drive, memory chips, processors,
    and so on. However, the essential component, the thing that, if missing, would
    render modern technology impossible, is algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical foundation of algorithms, in the form of the Turing machine,
    was established several decades before digital logic circuits could actually implement
    such a machine. The Turing machine is essentially a mathematical model that, using
    a predefined set of rules, translates a set of inputs into a set of outputs. The
    first implementations of Turing machines were mechanical and the next generation
    may likely see digital logic circuits replaced by quantum circuits or something
    similar. Regardless of the platform, algorithms play a central predominant role.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect is the effect algorithms have in technological innovation. As
    an obvious example, consider the page rank search algorithm, a variation of which
    the Google search engine is based on. Using this and similar algorithms allows
    researchers, scientists, technicians, and others to quickly search through vast
    amounts of information extremely quickly. This has a massive effect on the rate
    at which new research can be carried out, new discoveries made, and new innovative
    technologies developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The study of algorithms is also important because it trains us to think very
    specifically about certain problems. It can serve to increase our mental and problem
    solving abilities by helping us isolate the components of a problem and define
    relationships between these components. In summary, there are four broad reasons
    for studying algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: They are essential for computer science and *intelligent* systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are important in many other domains (computational biology, economics,
    ecology, communications, ecology, physics, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They play a role in technology innovation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They improve problem solving and analytical thinking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithms, in their simplest form, are just a sequence of actions, a list of
    instructions. It may just be a linear construct of the form do *x*, then do *y*,
    then do *z*, then finish. However, to make things more useful we add clauses to
    the effect of, *x* then do *y*, in Python the `if-else` statements. Here, the
    future course of action is dependent on some conditions; say the state of a data
    structure. To this we also add the operation, iteration, the while, and for statements.
    Expanding our algorithmic literacy further we add recursion. Recursion can often
    achieve the same result as iteration, however, they are fundamentally different.
    A recursive function calls itself, applying the same function to progressively
    smaller inputs. The input of any recursive step is the output of the previous
    recursive step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, we can say that algorithms are composed of the following four
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions based on the state of a data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteration, repeating an action a number of times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursion, calling itself on a subset of inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm design paradigms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, we can discern three broad approaches to algorithm design. They
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the name suggests, the divide and conquer paradigm involves breaking a problem
    into smaller sub problems, and then in some way combining the results to obtain
    a global solution. This is a very common and natural problem solving technique,
    and is, arguably, the most commonly used approach to algorithm design.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy algorithms often involve optimization and combinatorial problems; the
    classic example is applying it to the traveling salesperson problem, where a greedy
    approach always chooses the closest destination first. This shortest path strategy
    involves finding the best solution to a local problem in the hope that this will
    lead to a global solution.
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic programming approach is useful when our sub problems overlap. This
    is different from divide and conquer. Rather than break our problem into independent
    sub problems, with dynamic programming, intermediate results are cached and can
    be used in subsequent operations. Like divide and conquer it uses recursion; however,
    dynamic programming allows us to compare results at different stages. This can
    have a performance advantage over divide and conquer for some problems because
    it is often quicker to retrieve a previously calculated result from memory rather
    than having to recalculate it.
  prefs: []
  type: TYPE_NORMAL
- en: Recursion and backtracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recursion is particularly useful for divide and conquer problems; however,
    it can be difficult to understand exactly what is happening, since each recursive
    call is itself spinning off other recursive calls. At the core of a recursive
    function are two types of cases: base cases, which tell the recursion when to
    terminate, and recursive cases that call the function they are in. A simple problem
    that naturally lends itself to a recursive solution is calculating factorials.
    The recursive factorial algorithm defines two cases: the base case when *n* is
    zero, and the recursive case when *n* is greater than zero. A typical implementation
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints out the digits 1, 2, 4, 24\. To calculate 4 requires four
    recursive calls plus the initial parent call. On each recursion, a copy of the
    methods variables is stored in memory. Once the method returns it is removed from
    memory. The following is a way we can visualize this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ceaff8fe-caa4-42cf-8dd5-f0e739a5d7fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It may not necessarily be clear if recursion or iteration is a better solution
    to a particular problem; after all they both repeat a series of operations and
    both are very well suited to divide and conquer approaches to algorithm design.
    Iteration churns away until the problem is done. Recursion breaks the problem
    down into smaller and smaller chunks and then combines the results. Iteration
    is often easier for programmers, because control stays local to a loop, whereas
    recursion can more closely represent mathematical concepts such as factorials.
    Recursive calls are stored in memory, whereas iterations are not. This creates
    a trade off between processor cycles and memory usage, so choosing which one to
    use may depend on whether the task is processor or memory intensive. The following
    table outlines the key differences between recursion and iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Recursion** | **Iteration** |'
  prefs: []
  type: TYPE_TB
- en: '| Terminates when a base case is reached | Terminates when a defined condition
    is met |'
  prefs: []
  type: TYPE_TB
- en: '| Each recursive call requires space in memory | Each iteration is not stored
    in memory |'
  prefs: []
  type: TYPE_TB
- en: '| An infinite recursion results in a stack overflow error | An infinite iteration
    will run while the hardware is powered |'
  prefs: []
  type: TYPE_TB
- en: '| Some problems are naturally better suited to recursive solutions | Iterative
    solutions may not always be obvious |'
  prefs: []
  type: TYPE_TB
- en: Backtracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backtracking is a form of recursion that is particularly useful for types of
    problems such as traversing tree structures, where we are presented with a number
    of options at each node, from which we must choose one. Subsequently we are presented
    with a different set of options, and depending on the series of choices made either
    a goal state or a dead end is reached. If it is the latter, we must backtrack
    to a previous node and traverse a different branch. Backtracking is a divide and
    conquer method for exhaustive search. Importantly backtracking **prunes** branches
    that cannot give a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of back tracking is given in the following example. Here, we have
    used a recursive approach to generating all the possible permutations of a given
    string, *s*, of a given length *n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b1ab5929-7ac9-4dec-b033-e9fde81b5b2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice the double list compression and the two recursive calls within this comprehension.
    This recursively concatenates each element of the initial sequence, returned when
    `*n* = 1`, with each element of the string generated in the previous recursive
    call. In this sense it is *backtracking* to uncover previously ingenerated combinations.
    The final string that is returned is all *n* letter combinations of the initial
    string.
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer - long multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For recursion to be more than just a clever trick, we need to understand how
    to compare it to other approaches, such as iteration, and to understand when its
    use will lead to a faster algorithm. An iterative algorithm that we are all familiar
    with is the procedure we learned in primary math classes, used to multiply two
    large numbers. That is, long multiplication. If you remember, long multiplication
    involved iterative multiplying and carry operations followed by a shifting and
    addition operation.
  prefs: []
  type: TYPE_NORMAL
- en: Our aim here is to examine ways to measure how efficient this procedure is and
    attempt to answer the question; is this the most efficient procedure we can use
    for multiplying two large numbers together?
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we can see that multiplying two 4 digit numbers together
    requires 16 multiplication operations, and we can generalize to say that an *n*
    digit number requires, approximately, *n²* multiplication operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a349984a-9f69-4c49-b418-7ff885bc6a42.png)'
  prefs: []
  type: TYPE_IMG
- en: This method of analyzing algorithms, in terms of the number of computational
    primitives such as multiplication and addition, is important because it gives
    us a way to understand the relationship between the time it takes to complete
    a certain computation and the size of the input to that computation. In particular,
    we want to know what happens when the input, the number of digits, n, is very
    large. This topic, called asymptotic analysis, or time complexity, is essential
    to our study of algorithms and we will revisit it often during this chapter and
    the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Can we do better? A recursive approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out that in the case of long multiplication the answer is yes, there
    are in fact several algorithms for multiplying large numbers that require less
    operations. One of the most well-known alternatives to long multiplication is
    the **Karatsuba algorithm**, first published in 1962\. This takes a fundamentally
    different approach: rather than iteratively multiplying single digit numbers,
    it recursively carries out multiplication operations on progressively smaller
    inputs. Recursive programs call themselves on smaller subsets of the input. The
    first step in building a recursive algorithm is to decompose a large number into
    several smaller numbers. The most natural way to do this is to simply split the
    number in to two halves, the first half of most significant digits, and a second
    half of least significant digits. For example, our four-digit number, 2345, becomes
    a pair of two-digit numbers, 23 and 45\. We can write a more general decomposition
    of any 2 *n* digit numbers, *x* and *y* using the following, where *m* is any
    positive integer less than *n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0ca172df-86c2-4780-bf6a-9570e18aab94.png)![](assets/55df9f14-ece8-4cbb-ae31-8b23c3267211.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So now we can rewrite our multiplication problem *x*, *y* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/35b80ce5-cbcc-4638-8799-346532ee2154.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we expand and gather like terms we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d5f89699-bf11-480c-9db2-0deae29ac8eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'More conveniently, we can write it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ad955c99-182e-4f77-a7d2-df5800b6215f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7bb07889-0803-436c-84b5-8edb5a6eb21d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It should be pointed out that this suggests a recursive approach to multiplying
    two numbers since this procedure does itself involve multiplication. Specifically,
    the products *ac*, *ad*, *bc*, and *bd* all involve numbers smaller than the input
    number and so it is conceivable that we could apply the same operation as a partial
    solution to the overall problem. This algorithm, so far, consists of four recursive
    multiplication steps and it is not immediately clear if it will be faster than
    the classic long multiplication approach.
  prefs: []
  type: TYPE_NORMAL
- en: What we have discussed so far in regards to the recursive approach to multiplication,
    has been well known to mathematicians since the late 19^(th) century. The Karatsuba
    algorithm improves on this is by making the following observation. We really only
    need to know three quantities: *z[2]*= *ac* ; *z[1]=ad +bc*, and *z[0]*= *bd*
    to solve equation 3.1\. We need to know the values of *a, b, c, d* only in so
    far as they contribute to the overall sum and products involved in calculating
    the quantities *z[2]*, *z[1]*, and *z[0]*. This suggests the possibility that
    perhaps we can reduce the number of recursive steps. It turns out that this is
    indeed the situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the products *ac* and *bd* are already in their simplest form, it seems
    unlikely that we can eliminate these calculations. We can however make the following
    observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/319a0a26-74e5-4319-9eff-7b0e7dea8bef.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When we subtract the quantities *ac* and *bd,* which we have calculated in
    the previous recursive step, we get the quantity we need, namely (*ad* + *bc*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/03bddaec-240b-473a-a996-0718fc542efd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This shows that we can indeed compute the sum of *ad + bc* without separately
    computing each of the individual quantities. In summary, we can improve on equation
    3.1 by reducing from four recursive steps to three. These three steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Recursively calculate *ac.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively calculate *bd.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively calculate (*a* +*b*)(*c* + *d*) and subtract *ac* and *bd.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following example shows a Python implementation of the Karatsuba algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To satisfy ourselves that this does indeed work, we can run the following test
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Runtime analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be becoming clear that an important aspect to algorithm design is
    gauging the efficiency both in terms of space (memory) and time (number of operations).
    This second measure, called runtime performance, is the subject of this section.
    It should be mentioned that an identical metric is used to measure an algorithm's
    memory performance. There are a number of ways we could, conceivably, measure
    run time and probably the most obvious is simply to measure the time the algorithm
    takes to complete. The major problem with this approach is that the time it takes
    for an algorithm to run is very much dependent on the hardware it is run on. A
    platform-independent way to gauge an algorithm's runtime is to count the number
    of operations involved. However, this is also problematic in that there is no
    definitive way to quantify an operation. This is dependent on the programming
    language, the coding style, and how we decide to count operations. We can use
    this idea, though, of counting operations, if we combine it with the expectation
    that as the size of the input increases the runtime will increase in a specific
    way. That is, there is a mathematical relationship between *n*, the size of the
    input, and the time it takes for the algorithm to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much of the discussion that follows will be framed by the following three guiding
    principles. The rational and importance of these principles should become clearer
    as we proceed. These principles are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Worst case analysis. Make no assumptions on the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignore or suppress constant factors and lower order terms. At large inputs higher
    order terms dominate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on problems with large input sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst case analysis is useful because it gives us a tight upper bound that our
    algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower
    order terms is really just about ignoring the things that, at large values of
    the input size, *n*, do not contribute, in a large degree, to the overall run
    time. Not only does it make our work mathematically easier, it also allows us
    to focus on the things that are having the most impact on performance.
  prefs: []
  type: TYPE_NORMAL
- en: We saw with the Karatsuba algorithm that the number of multiplication operations
    increased to the square of the size, *n*, of the input. If we have a four-digit
    number the number of multiplication operations is 16; an eight-digit number requires
    64 operations. Typically, though, we are not really interested in the behavior
    of an algorithm at small values of *n*, so we most often ignore factors that increase
    at slower rates, say linearly with *n*. This is because at high values of *n*,
    the operations that increase the fastest as we increase *n*, will dominate.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain this in more detail with an example, the merge sort algorithm.
    Sorting is the subject of [Chapter 13](40b124ee-3b32-4a76-9524-463dbe813217.xhtml),
    *Sorting*, however, as a precursor and as a useful way to learn about runtime
    performance, we will introduce merge sort here.
  prefs: []
  type: TYPE_NORMAL
- en: The merge sort algorithm is a classic algorithm developed over 60 years ago.
    It is still used widely in many of the most popular sorting libraries. It is relatively
    simple and efficient. It is a recursive algorithm that uses a divide and conquer
    approach. This involves breaking the problem into smaller sub problems, recursively
    solving them, and then somehow combining the results. Merge sort is one of the
    most obvious demonstrations of the divide and conquer paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The merge sort algorithm consists of three simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Recursively sort the left half of the input array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively sort the right half of the input array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge two sorted sub arrays into one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A typical problem is sorting a list of numbers into a numerical order. Merge
    sort works by splitting the input into two halves and working on each half in
    parallel. We can illustrate this process schematically with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/250a5ae4-7c53-4800-9d19-06c6c7f8dc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the Python code for the merge sort algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We run this program for the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/038ce698-89ab-4d99-82b2-ac75765f7e84.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem that we are interested in is how we determine the running time performance,
    that is, what is the rate of growth in the time it takes for the algorithm to
    complete relative to the size of *n*. To understand this a bit better, we can
    map each recursive call onto a tree structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each node in the tree is a recursive call working on progressively smaller
    sub problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bdb1209e-db62-4741-8468-d4809f3d0f48.png)'
  prefs: []
  type: TYPE_IMG
- en: Each invocation of merge-sort subsequently creates two recursive calls, so we
    can represent this with a binary tree. Each of the child nodes receives a sub
    set of the input. Ultimately we want to know the total time it takes for the algorithm
    to complete relative to the size of *n*. To begin with we can calculate the amount
    of work and the number of operations at each level of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on the runtime analysis, at level 1, the problem is split into two
    *n*/2 sub problems, at level 2 there is four *n*/4 sub problems, and so on. The
    question is when does the recursion bottom out, that is, when does it reach its
    base case. This is simply when the array is either zero or one.
  prefs: []
  type: TYPE_NORMAL
- en: The number of recursive levels is exactly the number of times you need to divide
    *n* by 2 until you get a number that is at most 1\. This is precisely the definition
    of log2\. Since we are counting the initial recursive call as level 0, the total
    number of levels is log[2]*n* + 1.
  prefs: []
  type: TYPE_NORMAL
- en: Let's just pause to refine our definitions. So far we have been describing the
    number of elements in our input by the letter *n*. This refers to the number of
    elements in the first level of the recursion, that is, the length of the initial
    input. We are going to need to differentiate between the size of the input at
    subsequent recursive levels. For this we will use the letter *m* or specifically
    *m[j]* for the length of the input at recursive level *j.*
  prefs: []
  type: TYPE_NORMAL
- en: Also there are a few details we have overlooked, and I am sure you are beginning
    to wonder about. For example, what happens when *m*/2 is not an integer, or when
    we have duplicates in our input array. It turns out that this does not have an
    important impact on our analysis here.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using a recursion tree to analyze algorithms is that we can
    calculate the work done at each level of the recursion. How to define this work
    is simply as the total number of operations and this of course is related to the
    size of the input. It is important to measure and compare the performance of algorithms
    in a platform independent way. The actual run time will of course be dependent
    on the hardware on which it is run. Counting the number of operations is important
    because it gives us a metric that is directly related to an algorithm's performance,
    independent of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, since each invocation of merge sort is making two recursive calls,
    the number of calls is doubling at each level. At the same time each of these
    calls is working on an input that is half of its parents. We can formalize this
    and say that:'
  prefs: []
  type: TYPE_NORMAL
- en: For level j , where *j* is an integer 0, 1, 2 ... log[2]*n*, there are two ^j
    sub problems each of size *n*/2^j.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the total number of operations, we need to know the number of operations
    encompassed by a single merge of two sub arrays. Let's count the number of operations
    in the previous Python code. What we are interested in is all the code after the
    two recursive calls have been made. Firstly, we have the three assignment operations.
    This is followed by three while loops. In the first loop we have an if else statement
    and within each of are two operations, a comparison followed by an assignment.
    Since there are only one of these sets of operations within the if else statements,
    we can count this block of code as two operations carried out *m* times. This
    is followed by two while loops with an assignment operation each. This makes a
    total of 4*m* + 3 operations for each recursion of merge sort.
  prefs: []
  type: TYPE_NORMAL
- en: Since *m* must be at least 1, the upper bound for the number of operations is
    7*m*. It has to be said that this has no pretense at being an exact number. We
    could of course decide to count operations in a different way. We have not counted
    the increment operations or any of the housekeeping operations; however, this
    is not so important as we are more concerned with the rate of growth of the runtime
    with respect to *n* at high values of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem a little daunting since each call of a recursive call itself spins
    off more recursive calls, and seemingly explodes exponentially. The key fact that
    makes this manageable is that as the number of recursive calls doubles, the size
    of each sub problem halves. These two opposing forces cancel out nicely as we
    can demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the maximum number of operations at each level of the recursion
    tree we simply multiply the number of sub problems by the number of operations
    in each sub problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a6d82944-b30f-4e9c-b46e-9e467765e528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Importantly this shows that, because the 2^j cancels out the number of operations
    at each level is independent of the level. This gives us an upper bound to the
    number of operations carried out on each level, in this example, 7*n*. It should
    be pointed out that this includes the number of operations performed by each recursive
    call on that level, not the recursive calls made on subsequent levels. This shows
    that the work done, as the number of recursive calls doubles with each level,
    is exactly counter balanced by the fact that the input size for each sub problem
    is halved.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the total number of operations for a complete merge sort we simply
    multiply the number of operations on each level by the number of levels. This
    gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ac440c24-f7c5-48a7-a083-75d67cff8b8f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When we expand this out, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d4c151e6-ea4e-4fb6-804c-648353a68837.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The key point to take from this is that there is a logarithmic component to
    the relationship between the size of the input and the total running time. If
    you remember from school mathematics, the distinguishing characteristic of the
    logarithm function is that it flattens off very quickly. As an input variable,
    *x*, increases in size, the output variable, *y* increases by smaller and smaller
    amounts. For example, compare the log function to a linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/983df66e-1889-4df2-98ba-d00d8853a7c0.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous example, multiplying the *n*log[2]*n* component and comparing
    it to *n*² .
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0c0197f3-2cd7-4b71-91f8-c4bb4a4f5f8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how for very low values of *n*, the time to complete, *t* , is actually
    lower for an algorithm that runs in n² time. However, for values above about 40,
    the log function begins to dominate, flattening the output until at the comparatively
    moderate size *n* = 100, the performance is more than twice than that of an algorithm
    running in *n*² time. Notice also that the disappearance of the constant factor,
    + 7 is irrelevant at high values of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used to generate these graphs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to install the matplotlib library, if it is not installed already,
    for this to work. Details can be found at the following address; I encourage you
    to experiment with this list comprehension expression used to generate the plots.
    For example, adding the following plot statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4129159f-5b74-4138-94d0-02abb037cc78.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows the difference between counting six operations or
    seven operations. We can see how the two cases diverge, and this is important
    when we are talking about the specifics of an application. However, what we are
    more interested in here is a way to characterize growth rates. We are not so much
    concerned with the absolute values, but how these values change as we increase
    *n*. In this way we can see that the two lower curves have similar growth rates,
    when compared to the top (*x*²) curve. We say that these two lower curves have
    the same **complexity class**. This is a way to understand and describe different
    runtime behaviors. We will formalize this performance metric in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are essentially three things that characterize an algorithm''s runtime
    performance. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Worst case - Use an input that gives the slowest performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best case - Use an input that give, the best results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average case - Assumes the input is random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To calculate each of these, we need to know the upper and lower bounds. We have
    seen a way to represent an algorithm's runtime using mathematical expressions,
    essentially adding and multiplying operations. To use asymptotic analyses, we
    simply create two expressions, one each for the best and worst cases.
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The letter "O" in big *O* notation stands for order, in recognition that rates
    of growth are defined as the order of a function. We say that one function *T*(*n*)
    is a big O of another function, *F*(*n*), and we define this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4b0a93a3-4b55-4fd3-8226-64d255fca9cc.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The function, *g*(*n*), of the input size, *n*, is based on the observation
    that for all sufficiently large values of *n*, *g*(*n*) is bounded above by a
    constant multiple of *f*(*n*). The objective is to find the smallest rate of growth
    that is less than or equal to *f*(*n*). We only care what happens at higher values
    of *n*. The variable *n[0]*represents the threshold below which the rate of growth
    is not important, The function T(n) represents the **tight upper bound** F(n).
    In the following plot we see that *T*(*n*) = *n²* + 500 = *O*(*n²*) with *C* =
    2 and *n[0]* is approximately 23:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ba4595b-ba4b-4157-9a3f-632eaa3e382a.png)'
  prefs: []
  type: TYPE_IMG
- en: You will also see the notation *f*(*n*) = *O*(*g*(*n*)). This describes the
    fact that *O*(*g*(*n*)) is really a set of functions that include all functions
    with the same or smaller rates of growth than *f*(n). For example, *O*(*n²*) also
    includes the functions *O*(*n*), *O*(*n*log*n*), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we list the most common growth rates in order from
    lowest to highest. We sometimes call these growth rates the **time complexity**
    of a function, or the complexity class of a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Complexity Class** | **Name** | **Example operations** |'
  prefs: []
  type: TYPE_TB
- en: '| O(1) | Constant | append, get item, set item. |'
  prefs: []
  type: TYPE_TB
- en: '| O(log*n*) | Logarithmic | Finding an element in a sorted array. |'
  prefs: []
  type: TYPE_TB
- en: '| O(n) | Linear | copy, insert, delete, iteration. |'
  prefs: []
  type: TYPE_TB
- en: '| *n*Log*n* | Linear-Logarithmic | Sort a list, merge - sort. |'
  prefs: []
  type: TYPE_TB
- en: '| *n²* | Quadratic | Find the shortest path between two nodes in a graph. Nested
    loops. |'
  prefs: []
  type: TYPE_TB
- en: '| *n³* | Cubic | Matrix multiplication. |'
  prefs: []
  type: TYPE_TB
- en: '| 2*^n* | Exponential | ''Towers of Hanoi'' problem, backtracking. |'
  prefs: []
  type: TYPE_TB
- en: Composing complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, we need to find the total running time of a number of basic operations.
    It turns out that we can combine the complexity classes of simple operations to
    find the complexity class of more complex, combined operations. The goal is to
    analyze the combined statements in a function or method to understand the total
    time complexity of executing several operations. The simplest way to combine two
    complexity classes is to add them. This occurs when we have two sequential operations.
    For example, consider the two operations of inserting an element into a list and
    then sorting that list. We can see that inserting an item occurs in O(*n*) time
    and sorting is O(*n*log*n*) time. We can write the total time complexity as O(*n*
    + *n*log*n*), that is, we bring the two functions inside the O(...). We are only
    interested in the highest order term, so this leaves us with just O(*n*log*n*).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat an operation, for example, in a while loop, then we multiply the
    complexity class by the number of times the operation is carried out. If an operation
    with time complexity O(*f*(*n*)) is repeated O(*n*) times then we multiply the
    two complexities:'
  prefs: []
  type: TYPE_NORMAL
- en: O(*f*(*n*) * O(*n*)) = O(*nf*(*n*)).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose the function f(...) has a time complexity of O(*n*²) and
    it is executed *n* times in a while loop as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The time complexity of this loop then becomes O(*n*²) * O(*n*) = O(*n * n²*)
    = O(*n³*). Here we are simply multiplying the time complexity of the operation
    with the number of times this operation executes. The running time of a loop is
    at most the running time of the statements inside the loop multiplied by the number
    of iterations. A single nested loop, that is, one loop nested inside another loop,
    will run in *n*² time assuming both loops run *n* times. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Each statement is a constant, c, executed *n**n* times, so we can express the
    running time as ; *c**n* *n* = *cn*² = O(*n*2).
  prefs: []
  type: TYPE_NORMAL
- en: 'For consecutive statements within nested loops we add the time complexities
    of each statement and multiply by the number of times the statement executed.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This can be written as *c*[0] +*c*[1]*n* + *cn*² = O(*n*²).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define (base 2) logarithmic complexity, reducing the size of the problem
    by ½, in constant time. For example, consider the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that `i` is doubling on each iteration, if we run this with *n* = 10
    we see that it prints out four numbers; 2, 4, 8, and 16\. If we double *n* we
    see it prints out five numbers. With each subsequent doubling of n the number
    of iterations is only increased by 1\. If we assume *k* iterations, we can write
    this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e7efcb8e-d08f-4c5c-8ba8-ff765a96cdbf.png)'
  prefs: []
  type: TYPE_IMG
- en: From this we can conclude that the total time = **O**(*log(n)*).
  prefs: []
  type: TYPE_NORMAL
- en: Although Big O is the most used notation involved in asymptotic analysis, there
    are two other related notations that should be briefly mentioned. They are Omega
    notation and Theta notation.
  prefs: []
  type: TYPE_NORMAL
- en: Omega notation (Ω)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a similar way that Big O notation describes the upper bound, Omega notation
    describes a **tight lower bound**. The definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0be56dbf-c79b-4d73-b1f6-7b1411bb36f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The objective is to give the largest rate of growth that is equal to or less
    than the given algorithms, T(*n*), rate of growth.
  prefs: []
  type: TYPE_NORMAL
- en: Theta notation (ϴ)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is often the case where both the upper and lower bounds of a given function
    are the same and the purpose of Theta notation is to determine if this is the
    case. The definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1da75a4c-29dc-4ab9-a25e-ae5b5209db08.png)'
  prefs: []
  type: TYPE_IMG
- en: Although Omega and Theta notations are required to completely describe growth
    rates, the most practically useful is Big O notation and this is the one you will
    see most often.
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often we are not so interested in the time complexity of individual operations,
    but rather the time averaged running time of sequences of operations. This is
    called amortized analysis. It is different from average case analysis, which we
    will discuss shortly, in that it makes no assumptions regarding the data distribution
    of input values. It does, however, take into account the state change of data
    structures. For example, if a list is sorted it should make any subsequent find
    operations quicker. Amortized analysis can take into account the state change
    of data structures because it analyzes sequences of operations, rather then simply
    aggregating single operations.
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis finds an upper bound on runtime by imposing an artificial
    cost on each operation in a sequence of operations, and then combining each of
    these costs. The artificial cost of a sequence takes in to account that the initial
    expensive operations can make subsequent operations cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: When we have a small number of expensive operations, such as sorting, and lots
    of cheaper operations such as lookups, standard worst case analysis can lead to
    overly pessimistic results, since it assumes that each lookup must compare each
    element in the list until a match is found. We should take into account that once
    we sort the list we can make subsequent find operations cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in our runtime analysis we have assumed that the input data was completely
    random and have only looked at the effect the size of the input has on the runtime.
    There are two other common approaches to algorithm analysis; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Average case analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average case analysis finds the average running time based on some assumptions
    regarding the relative frequencies of various input values. Using real-world data,
    or data that replicates the distribution of real-world data, is many times on
    a particular data distribution and the average running time is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking is simply having an agreed set of typical inputs that are used
    to measure performance. Both benchmarking and average time analysis rely on having
    some domain knowledge. We need to know what the typical or expected datasets are.
    Ultimately we will try to find ways to improve performance by fine-tuning to a
    very specific application setting.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a straightforward way to benchmark an algorithm's runtime performance.
    This can be done by simply timing how long the algorithm takes to complete given
    various input sizes. As we mentioned earlier, this way of measuring runtime performance
    is dependent on the hardware that it is run on. Obviously faster processors will
    give better results, however, the relative growth rates as we increase the input
    size will retain characteristics of the algorithm itself rather than the hardware
    it is run on. The absolute time values will differ between hardware (and software)
    platforms; however, their relative growth will still be bound by the time complexity
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a simple example of a nested loop. It should be fairly obvious
    that the time complexity of this algorithm is O(n²) since for each n iterations
    in the outer loop there are also n iterations in the inter loop. For example,
    our simple nested for loop consists of a simple statement executed on the inner
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is a simple test function that runs the nest function with
    increasing values of `n`. With each iteration we calculate the time this function
    takes to complete using the `timeit.timeit` function. The `timeit` function, in
    this example, takes three arguments, a string representation of the function to
    be timed, a setup function that imports the nest function, and an `int` parameter
    that indicates the number of times to execute the main statement. Since we are
    interested in the time the nest function takes to complete relative to the input
    size, `n`, it is sufficient, for our purposes, to call the nest function once
    on each iteration. The following function returns a list of the calculated runtimes
    for each value of n:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code we run the test2 function and graph the results, together
    with the appropriately scaled n² function for comparison, represented by the dashed
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/44c2f6b6-a425-4914-93b4-03030c9acde8.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, this gives us pretty much what we expect. It should be remembered
    that this represents both the performance of the algorithm itself as well as the
    behavior of underlying software and hardware platforms, as indicated by both the
    variability in the measured runtime and the relative magnitude of the runtime.
    Obviously a faster processor will result in faster runtimes, and also performance
    will be affected by other running processes, memory constraints, clock speed,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have taken a general overview of algorithm design. Importantly,
    we saw a platform independent way to measure an algorithm's performance. We looked
    at some different approaches to algorithmic problems. We looked at a way to recursively
    multiply large numbers and also a recursive approach for merge sort. We saw how
    to use backtracking for exhaustive search and generating strings. We also introduced
    the idea of benchmarking and a simple platform-dependent way to measure runtime.
    In the following chapters, we will revisit many of these ideas with reference
    to specific data structures. In the next chapter, we will discuss linked lists
    and other pointer structures.
  prefs: []
  type: TYPE_NORMAL

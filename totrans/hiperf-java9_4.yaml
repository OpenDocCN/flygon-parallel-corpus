- en: Chapter 4. Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As long as we kept talking about the designing, implementation, and tuning of
    one process, we were able to keep illustrating it with vivid images (albeit in
    our imagination only) of pyramid building. Multiple thread management, based on
    the democratic principle of equality between thread pool members, had also a sense
    of centralized planning and supervision. Different priorities were assigned to
    threads programmatically, hardcoded (for most cases) after thoughtful consideration
    by the programmer in accordance with the expected load, and adjusted after monitoring.
    The upper limits of the available resources were fixed, although they could be
    increased after, again, a relatively big centralized decision.
  prefs: []
  type: TYPE_NORMAL
- en: Such systems had great success and still constitute the majority of the web
    applications currently deployed to production. Many of them are monoliths, sealed
    inside a single `.ear` or `.war` file. This works fine for relatively small applications
    and a corresponding team size that supports them. They are easy (if the code is
    well structured) to maintain, build, and if the production load is not very high,
    they can be easily deployed. If the business does not grow or has little impact
    on the company's internet presence, they continue to do the job and will do so
    probably for the foreseeable future. Many service providers are eager to host
    such websites by charging a small fee and relieving the website owner of the technical
    worries of production maintenance not directly related to the business. But that
    is not the case for everybody.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the load, the more difficult and expensive the scaling becomes unless
    the code and the overall architecture is restructured in order to become more
    flexible and resilient to the growing load. This lesson describes the solution
    many leaders of the industry have adopted while addressing the issue and the motivation
    behind it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The particular aspects of the microservices we are going to discuss in this
    lesson include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The motivation for the microservices rising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frameworks that were developed recently in support of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of microservices development with practical examples, including
    the considerations and decision-making process during microservices building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros and cons of the three main deployment methods such as container-less, self-contained,
    and in-container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Microservices?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some businesses have a higher demand for the deployment plan because of the
    need to keep up with the bigger volume of traffic. The natural answer to this
    challenge would be and was to add servers with the same `.ear` or `.war` file
    deployed and join all the servers into a cluster. So, one failed server could
    be automatically replaced with another one from the cluster, and the site user
    would never experience disconnect of the service. The database that backed all
    the clustered servers could be clustered too. A connection to each of the clusters
    went through a load balancer, making sure that none of the cluster members worked
    more than the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The web server and database clustering help but only to a degree, because as
    the code base grows, its structure can create one or several bottlenecks unless
    such and similar issues are addressed with a scalable design. One of the ways
    to do it is to split the code into tiers: front end (or web tier), middle tier
    (or app tier) and back end (or backend tier). Then, again, each tier can be deployed
    independently (if the protocol between tiers has not changed) and in its own cluster
    of servers, as each tier can grow horizontally as needed independently of other
    tiers. Such a solution provides more flexibility for scaling up, but makes the
    deployment plan more complex, especially if the new code introduces breaking changes.
    One of the approaches is to create a second cluster that will host a new code,
    then take the servers one by one from the old cluster, deploy the new code, and
    put them in the new cluster. The new cluster would be turned on as soon as at
    least one server in each tier has the new code. This approach worked fine for
    the web and app tiers but was more complex for the backend, which once in a while
    required data migration and similar joyful exercises. Add to it unexpected outages
    in the middle of the deployment caused by human errors, defects in the code, pure
    accidents, or some combination of all the earlier mentioned (one time, for example,
    an electric power cable was cut by an excavator in the nearby construction site),
    and it is easy to understand why very few people love a deployment of a major
    release to production.'
  prefs: []
  type: TYPE_NORMAL
- en: Programmers, being by nature problem solvers, tried to prevent the earlier scenario
    as best as they could by writing defensive code, deprecating instead of changing,
    testing, and so on. One of the approaches was to break the application into more
    independently deployable parts with the hope of avoiding deploying everything
    at the same time. They called these independent units **services**, and **Service-Oriented
    Architecture** (**SOA**) was born.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, in many companies, the natural growth of the code base was not
    adjusted to the new challenges in a timely manner. Like the frog that was eventually
    boiled in a slowly heated pot of water, they never had time to jump out of the
    hot spot by changing the design. It was always cheaper to add another feature
    to the blob of the existing functionality than redesign the whole app. Business
    metrics of the time-to-market and keeping the bottom line in the black always
    were and will remain the main criterion for the decision making, until the poorly
    structured source code eventually stops working, pulling down all the business
    transactions with it or, if the company is lucky, allows them to weather the storm
    and shows the importance of the investment in the redesign.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of all that, some lucky companies remained in the business with
    their monolithic application still running as expected (maybe not for long, but
    who knows), some went out of business, some learned from their mistakes and progressed
    into the brave world of the new challenges, and others learned from their mistakes
    and designed their systems to be SOA upfront.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to observe similar tendencies in the social sphere. Society
    moved from the strong centralized governments to more loosely coupled confederations
    of semi-independent states tied together by the mutually beneficial economic and
    cultural exchange.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, maintaining such a loose structure comes with a price. Each participant
    has to be more responsible in maintaining the contract (social, in the case of
    a society, and API, in the case of the software) not only formally but also in
    spirit. Otherwise, for example, the data flowing from a new version of one component,
    although correct by type, might be unacceptable to another component by value
    (too big or too small). Maintaining a cross-team understanding and overlapping
    of responsibility requires constant vigilance in keeping the culture alive and
    enlightening. Encouraging innovation and risk taking, which can lead to a business
    breakthrough, contradict the protecting tendencies for stability and risk aversion
    coming from the same business people.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from monolithic single-team development to multiple teams and an independent
    components-based system requires an effort on all levels of the enterprise. What
    do you mean by **No more Quality Assurance Department**? Who then will care about
    the professional growth of the testers? And what about the IT group? What do you
    mean by **The developers are going to support production**? Such changes affect
    human lives and are not easy to implement. That's why SOA architecture is not
    just a software principle. It affects everybody in the company.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the industry leaders, who have managed to grow beyond anything we
    could imagine just a decade ago, were forced to solve even more daunting problems
    and came back to the software community with their solutions. And that is where
    our analogy with the pyramid building does not work anymore. Because the new challenge
    is not just to build something so big that was never built before but also to
    do it quickly not in a matter of years, but in a few weeks and even days. And
    the result has to last not for a thousand years but has to be able to evolve constantly
    and be flexible enough to adapt to new, unexpected requirements in real time.
    If only one aspect of the functionality has changed, we should be able to redeploy
    only this one service. If the demand for any service grows, we should be able
    to scale only along this one service and release resources when the demand drops.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid big deployments with all hands on deck and to come closer to the continuous
    deployment (which decreases time-to-market and is thus supported by business),
    the functionality continued to split into smaller chunks of services. In response
    to the demand, more sophisticated and robust cloud environments, deployment tools
    (including containers and container orchestration), and monitoring systems supported
    this move. The reactive streams, described in the previous lesson, started to
    develop even before the Reactive Manifesto came out and plugged a snag into the
    stack of modern frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting an application into independent deployment units brought several not
    quite expected benefits that have increased the motivation for plowing ahead.
    The physical isolation of services allows more flexibility in choosing a programming
    language and platform of implementation. It helps not only to select technology
    that is the best for the job but also to hire people able to implement it, not
    being bound by a certain technological stack of the company. It also helped the
    recruiters to spread the net wider and use smaller cells for bringing in new talent,
    which is not a small advantage with a limited number of available specialists
    and the unlimited demand of the fast-growing data processing industry.
  prefs: []
  type: TYPE_NORMAL
- en: Also, such architecture enforced a discussion and explicit definition of the
    interfaces between smaller parts of the complex system, thus creating a solid
    foundation for further growth and tuning of the processing sophistication.
  prefs: []
  type: TYPE_NORMAL
- en: And that is how microservices came into the picture and were put to work by
    giants of traffic such as Netflix, Google, Twitter, eBay, Amazon, and Uber. Now,
    let's talk about the results of this effort and the lessons learned.
  prefs: []
  type: TYPE_NORMAL
- en: Building Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into the building process, let''s revisit the characteristics
    a chunk of code has to possess in order to be qualified as a microservice. We
    will do it in no particular order:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the source code of one microservice should be smaller to that of
    an SOA, and one development team should be able to support several of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to be deployed independently of other services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each has to have its own database (or schema or set of tables), although this
    statement is still under debate, especially in cases when several services modify
    the same data set or the inter-dependent data sets; if the same team owns all
    of the related services, it is easier to accomplish. Otherwise, there are several
    possible strategies we will discuss later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to be stateless and idempotent. If one instance of the service has failed,
    another should be able to accomplish what was expected from the service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should provide a way to check its **health**, meaning that the service is
    up and running and ready to do the job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing resources has to be considered during the design, development, and,
    after deployment, monitored for validation of the assumptions. In the previous
    lesson, we talked about threads synchronization. You could see that this problem
    was not easy to solve, and we have presented several possible ways to do it. Similar
    approaches can be applied toward microservices. Although they are run in different
    processes, they can communicate to each other if need be, so they can coordinate
    and synchronize their actions.
  prefs: []
  type: TYPE_NORMAL
- en: Special care has to be taken during modification of the same persistent data
    whether shared across databases, schemas, or tables within the same schema. If
    an eventual consistency is acceptable (which is often the case for larger sets
    of data, used for statistical purposes, for example) then no special measures
    are necessary. However, the need for transactional integrity poses a more difficult
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: One way to support a transaction across several microservices is to create a
    service that would play the role of a **Distributed Transaction Manager** (**DTM**).
    Other services that need coordination would pass to it the new modified values.
    The DTM service could keep the concurrently modified data temporarily in a database
    table and would move it into the main table(s) in one transaction after all the
    data is ready (and consistent).
  prefs: []
  type: TYPE_NORMAL
- en: If the time to access the data is an issue or you need to protect the database
    from an excessive number of concurrent connections, dedicating a database to some
    services may be an answer. Alternatively, if you would like to try another option,
    memory cache could be the way to go. Adding a service that provides access to
    the cache (and updates it as needed) increases isolation from the services that
    use it, but requires (sometimes difficult) synchronization between the peers that
    are managing the same cache too.
  prefs: []
  type: TYPE_NORMAL
- en: After considering all the options and possible solutions for data sharing, it
    is often helpful to revisit the idea of creating its own database (or schema)
    for each microservice. One may discover that the effort of the data isolation
    (and subsequent synchronization on the database level) does not look as daunting
    as before if compared with the effort to synchronize the data dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: That said, let's look over the field of the frameworks for microservices implementation.
    One can definitely write the microservices from scratch, but before doing that,
    it is always worth looking at what is out there already, even if to find eventually
    that nothing fits your particular needs.
  prefs: []
  type: TYPE_NORMAL
- en: There are more than a dozen frameworks that are currently used for building
    microservices. Two most popular are Spring Boot ([https://projects.spring.io/spring-boot/](https://projects.spring.io/spring-boot/))
    and raw J2EE. The J2EE community founded the initiative MicroProfile ([https://microprofile.io/](https://microprofile.io/))
    with a declared goal of **Optimizing Enterprise Java** for a microservices architecture.
    KumuluzEE ([https://ee.kumuluz.com/](https://ee.kumuluz.com/)) is a lightweight
    open-source microservice framework coplined with MicroProfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of some other frameworks include the following (in alphabetical order):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Akka**: This is a toolkit for building highly concurrent, distributed, and
    resilient message-driven applications for Java and Scala ([akka.io](https://akka.io/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootique**: This is a minimally opinionated framework for runnable Java apps
    ([bootique.io](http://bootique.io))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropwizard**: This is a Java framework for developing ops-friendly, high-performance,
    RESTful web services ([www.dropwizard.io](http://www.dropwizard.io))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jodd**: This is a set of Java microframeworks, tools, and utilities, under
    1.7 MB ([jodd.org](http://jodd.org))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lightbend Lagom**: This is an opinionated microservice framework built on
    Akka and Play ([www.lightbend.com](http://www.lightbend.com))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ninja**: This is a full stack web framework for Java ([www.ninjaframework.org](http://www.ninjaframework.org))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spotify Apollo**: This is a set of Java libraries used at Spotify for writing
    microservices ([spotify.github.io/apollo](http://spotify.github.io/apollo))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vert.x**: This is a toolkit for building reactive applications on the JVM
    ([vertx.io](http://vertx.io))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All frameworks support HTTP/JSON communication between microservices; some of
    them also have an additional way to send messages. If not the latter, any lightweight
    messaging system can be used. We mentioned it here because, as you may recall,
    message-driven asynchronous processing is a foundation for elasticity, responsiveness,
    and resilience of a reactive system composed of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the process of microservices building, we will use Vert.x, an
    event-driven, non-blocking, lightweight, and polyglot toolkit (components can
    be written in Java, JavaScript, Groovy, Ruby, Scala, Kotlin, and Ceylon). It supports
    an asynchronous programming model and a distributed event bus that reaches even
    into in-browser JavaScript (thus allowing the creation of real-time web applications).
  prefs: []
  type: TYPE_NORMAL
- en: 'One starts using Vert.x by creating a `Verticle` class that implements the
    interface `io.vertx.core.Verticle`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The method names previously mentioned are self-explanatory. The method `getVertex()`
    provides access to the `Vertx` object the entry point into the Vert.x Core API.
    It provides access to the following functionality necessary for the microservices
    building:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating TCP and HTTP clients and servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DNS clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Datagram sockets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating periodic services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing access to the event bus and file system API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing access to the shared data API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and undeploying verticles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this Vertx object, various verticles can be deployed, which talk to each
    other, receive an external request, and process and store data as any other Java
    application, thus forming a system of microservices. Using RxJava implementation
    from the package `io.vertx.rxjava`, we will show how one can create a reactive
    system of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'A verticle is a building block in Vert.`x` world. It can easily be created
    by extending the `io.vertx.rxjava.core.AbstractVerticle` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The earlier mentioned class, in turn, extends `io.vertx.core.AbstractVerticle`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A verticle can be created by extending the class `io.vertx.core.AbstractVerticle`,
    too. However, we will write reactive microservices, so we will extend its rx-fied
    version, `io.vertx.rxjava.core.AbstractVerticle`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Vert.x and run the provided example, all you need to do is to add the
    following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Other Vert.x functionality can be added as needed by including other Maven dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: What makes `Vert.x` `Verticle` reactive is the underlying implementation of
    an event loop (a thread) that receives an event and delivers it a `Handler` (we
    will show how to write the code for it). When a `Handler` gets the result, the
    event loop invokes the callback.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you see, it is important not to write a code that blocks the event loop,
    thus the Vert.x golden rule: don''t block the event loop.'
  prefs: []
  type: TYPE_NORMAL
- en: If not blocked, the event loop works very quickly and delivers a huge number
    of events in a short period of time. This is called the reactor pattern ([https://en.wikipedia.org/wiki/Reactor_pattern](https://en.wikipedia.org/wiki/Reactor_pattern)).
    Such an event-driven non-blocking programming model is a very good fit for reactive
    microservices. For certain types of code that are blocking by nature (JDBC calls
    and long computations are good examples) a worker verticle can be executed asynchronously
    (not by the event loop, but by a separate thread using the method `vertx.executeBlocking()`),
    which keeps the golden rule intact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a few examples. Here is a `Verticle` class that works as an
    HTTP server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, the server is created, and the stream of data from a
    possible request is wrapped into an `Observable`. We then subscribed to the data
    coming from the `Observable` and passed in a function (a request handler) that
    will process the request and generate a necessary response. We also told the server
    which port to listen. Using this `Verticle`, we can deploy several instances of
    an HTTP server listening on different ports. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this application, the output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the same thread is listening on both ports. If we now place
    a request to each of the running servers, we will get the response we have hardcoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We ran our examples from the `main()` method. A plugin `maven-shade-plugin`
    allows you to specify which verticle you would like to be the starting point of
    your application. Here is an example from [http://vertx.io/blog/my-first-vert-x-3-application](http://vertx.io/blog/my-first-vert-x-3-application):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It will generate a specified JAR file (called `target/my-first-app-1.0-SNAPSHOT-fat.jar`,
    in this example). It is called `fat` because it contains all the necessary dependencies.
    This file will also contain `MANIFEST.MF` with the following entries in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use any verticle instead of `io.vertx.blog.first.MyFirstVerticle`,
    used in this example, but `io.vertx.core.Starter` has to be there because that
    is the name of the `Vert.x` class that knows how to read the manifest and execute
    the method `start()` of the specified verticle. Now, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This command will execute the `start()` method of the `MyFirstVerticle` class
    the same way the `main()` method is executed in our example, which we will continue
    to use for the simplicity of demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compliment the HTTP server, we can create an HTTP client too. However, first,
    we will modify the method `start()` in the `server` verticle to accept the parameter
    `name`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create an HTTP `client` verticle that sends a request and prints
    out the response every second for 3 seconds, then stops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume we deploy both verticles as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this last example, we demonstrated how to create an HTTP client and periodic
    service. Now, let's add more functionality to our system. For example, let's add
    another verticle that will interact with the database and use it via the HTTP
    server we have already created.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to add this dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The newly added JAR file allows us to create an in-memory database and a handler
    to access it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Those familiar with RxJava can see that Vert.x code closely follows the style
    and naming convention of RxJava. Nevertheless, we encourage you to go through
    Vert.x documentation, because it has a very rich API that covers many more cases
    than just demonstrated. In the previous code, the operation `flatMap()` receives
    the function that runs the script and then closes the connection. The operation
    `doAfterTerminate()` in this case acts as if it was placed inside a finally block
    in a traditional code and closes the connection either in case of success or if
    an exception is generated. The `subscribe()` method has several overloaded versions.
    For our code, we have selected the one that takes two functions one is going to
    be executed in the case of success (we print a message about the table being created)
    and another in the case of an exception (we just print the stack trace then).
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the created database, we can add to `DbHandler` methods `insert()`,
    `process()`, and `readProcessed()` that will allow us to demonstrate how to build
    a reactive system. The code for the method `insert()` can look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `insert()` method, as well as other methods we are going to write, takes
    full advantage of Java functional interfaces. It creates a record in the table
    `who_called` (using the passed in parameter `name`). Then, the operation `subscribe()`
    executes one of the two functions passed in by the code that calls this method.
    We use the method `printAction()` only for better traceability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The method `process()` also accepts two functions but does not need other parameters.
    It processes all the records from the table `who_called` that are not processed
    yet (not listed in the table `processed`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If two threads are reading the table `who_called` for the purpose of selecting
    records not processed yet, the clause `for update` in the SQL query makes sure
    that only one gets each record, so they are not going to be processed twice. The
    significant advantage of the method `process()` code is its usage of the `rxQUeryStream()`
    operation that emits the found records one at a time so that they are processed
    independently of each other. In the case of a big number of not processed records,
    such a solution guarantees a smooth delivery of the results without the spiking
    of the resources consumption. The following `flatMap()` operation does processing
    using the function passed in. The only requirement for that function is that it
    must return one integer value (in `JsonArray`) that is going to be used as a parameter
    for the `SQL_INSERT_PROCESSED` statement. So, it is up to the code that calls
    this method to decide the nature of the processing. The rest of the code is similar
    to the method `insert()`. The code indentation helps to follow the nesting of
    the operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method `readProcessed()` has code that looks very similar to the code of
    the method `insert()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code reads the specified number of the latest processed records.
    The difference from the method `process()` is that the method `readProcessed()`
    returns all the read records in one result set, so it is up to the user of this
    method to decide how to process the result in bulk or one at a time. We show all
    these possibilities just to demonstrate the variety of the possible options. With
    the `DbHandler` class in place, we are ready to use it and create the `DbServiceHttp`
    microservice, which allows a remote access to the `DbHandler` capabilities by
    wrapping around it an HTTP server. Here is the constructor of the new microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the earlier mentioned code, you can see how the URL mapping is done in Vert.x.
    For each possible route, a corresponding `Verticle` method is assigned, each accepting
    the `RoutingContext` object that contains all the data of HTTP context, including
    the `HttpServerRequest` and `HttpServerResponse` objects. A variety of convenience
    methods allows us to easily access the URL parameters and other data necessary
    to process the request. Here is the method `insert()` referred in the `start()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'All it does is extracts the parameter `name` from the request and constructs
    the two functions necessary to call method `insert()` of `DbHandler` we discussed
    earlier. The method `process()` looks similar to the previous method `insert()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The function `process` mentioned earlier defines what should be done with the
    records coming from the `SQL_SELECT_TO_PROCESS` statement inside the method `process()`
    in `DbHandler`. In our case, it calculates the length of the caller's name and
    passes it as a parameter along with the name itself (as a return value) to the
    next SQL statement that inserts the result into the table `processed`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the method `readProcessed()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That is where (in the previous code in the function `onSuccess()`) the result
    set from the query `SQL_READ_PROCESSED` is read and used to construct the response.
    Notice that we do it by creating an `Observable` first, then subscribing to it
    and passing the result of the subscription as the response into method `end()`.
    Otherwise, the response can be returned without waiting for the response to be
    constructed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can launch our reactive system by deploying the `DbServiceHttp` verticle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do that, in the output we will see the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In another window, we can issue the command that generates an HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we read the processed records now, there should be none:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The log messages show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/4_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can request processing of the existing records and then read the results
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In principle, it is enough already to build a reactive system. We can deploy
    many `DbServiceHttp` microservices on different ports or cluster them to increase
    processing capacity, resilience, and responsiveness. We can wrap other services
    inside an HTTP client or an HTTP server and let them talk to each other, processing
    the input and passing the results along the processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: However, Vert.x also has a feature that even better suits the message-driven
    architecture (without using HTTP). It is called an event bus. Any verticle has
    access to the event bus and can send any message to any address (which is just
    a string) using either method `send()` (`rxSend()` in the case of reactive programming)
    or method `publish()`. One or many verticles can register themselves as a consumer
    for a certain address.
  prefs: []
  type: TYPE_NORMAL
- en: If many verticles are consumers for the same address, then the method `send()`
    (`rxSend()`) delivers the message only to one of them (using a round-robin algorithm
    to pick the next consumer). The method `publish()`, as you would expect, delivers
    the message to all consumers with the same address. Let's see an example, using
    the already familiar `DbHandler` as the main working horse.
  prefs: []
  type: TYPE_NORMAL
- en: 'A microservice, based on an event bus, looks very similar to the one based
    on the HTTP protocol we discussed already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We simplified the preceding code by skipping some sections (that are very similar
    to the `DbServiceHttp` class) and trying to highlight the code structure. For
    demo purposes, we will deploy two instances of this class and send three messages
    to each of the addresses `INSERT`, `PROCESS`, and `READ_PROCESSED`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the delay for 200 ms we inserted using the method `delayMs()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The delay is necessary to let the `DbServiceBus` verticle to be deployed and
    started (and the consumers registered with the address). Otherwise, an attempt
    to send a message may fail because the consumer is not registered with the address
    yet. The `PeriodicServiceBusSend()` verticle code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code sends a message to an address every `delaySec` seconds as
    many times as the length of the array `caller[]`, and then undeploys the verticle
    (itself). If we run the demo, the beginning of the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, for each address, only `DbServiceBus(1)` was a receiver of
    the first message. The second message to the same address was received by `DbServiceBus(2)`.
    That was the round-robin algorithm (which we mentioned earlier) in action. The
    final section of the output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can deploy as many verticles of the same type as needed. For example, let''s
    deploy four verticles that send messages to the address `INSERT`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the results, we will also ask the reading Verticle to read the last
    eight records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The result (the final section of the output) then will be as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Four verticles have sent the same messages, so each name was sent four times
    and processed that is what we see in the previous output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now return to one inserting periodic verticle but will change it from
    using the method `rxSend()` to the method `publish()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This change would mean that the message has to be sent to all verticles that
    are registered as the consumers at that address. Now, let''s run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We have included another delay for 200 ms to give the publishing verticle time
    to send the message. The output (in the final section) now shows that each message
    was processed twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservices](img/04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is because two consumers `DbServiceBus(1)` and `DbServiceBus(2)` were deployed,
    and each received a message to the address `INSERT` and inserted it in the table
    `who_called`.
  prefs: []
  type: TYPE_NORMAL
- en: All the previous examples we have run in one JVM process. If necessary, Vert.x
    instances can be deployed in different JVM processes and clustered by adding the
    `-cluster` option to the run command. Therefore, they share the event bus and
    the addresses are visible to all Vert.x instances. This way, the resources can
    be added to each address as needed. For example, we can increase the number of
    processing microservices only and compensate the load's increase.
  prefs: []
  type: TYPE_NORMAL
- en: Other frameworks we mentioned earlier have similar capabilities. They make microservices
    creation easy and may encourage breaking the application into tiny single-method
    operations with an expectation of assembling a very resilient and responsive system.
  prefs: []
  type: TYPE_NORMAL
- en: However, these are not the only criteria of good quality. System decomposition
    increases the complexity of its deployment. Also, if one development team is responsible
    for many microservices, the complexity of versioning so many pieces in different
    stages (development, test, integration test, certification, staging, production)
    may lead to confusion and a very challenging deployment process, which, in turn,
    may slow down the rate of changes necessary to keep the system in sync with the
    market requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the developing of the microservices, many other aspects have
    to be addressed to support the reactive system:'
  prefs: []
  type: TYPE_NORMAL
- en: A monitoring system has to be designed to provide an insight into the state
    of the application, but it should not be so complex as to pull the development
    resources away from the main application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts have to be installed to warn the team about possible and actual issues
    in a timely manner, so they can be addressed before affecting the business.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If possible, self-correcting automated processes have to be implemented. For
    example, the system should be able to add and release resources in accordance
    with the current load; the retry logic has to be implemented with a reasonable
    upper limit of a attempts before declaring the failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A layer of circuit breakers has to protect the system from the domino effect
    when failure of one component deprives other components of the necessary resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An embedded testing system should be able to introduce disruptions and simulate
    processing load to ensure that the application resilience and responsiveness do
    not degrade over time. For example, the Netflix team has introduced a **chaos
    monkey** a system that is able to shut down various parts of the production system
    to test the ability to recover. They use it even in production because a production
    environment has a specific configuration, and no test in another environment can
    guarantee that all possible issues are found.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the main considerations of a reactive system design is the selection
    of the deployment methodology that can be either container-less, self-contained,
    or in-container. We will look into the pros and cons of each of these approaches
    in the following sections of this lesson.
  prefs: []
  type: TYPE_NORMAL
- en: Container-Less Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: People use the term **container** to refer to very different things. In the
    original usage, a container was something that carried its content from one location
    to another without changing anything inside. However, when servers were introduced,
    only one aspect was emphasized the ability to hold an application to contain it.
    Also, another meaning was added to provide life-supportive infrastructure so that
    the container's content (an application) can not only survive but also be active
    and respond to the external requests. Such a redefined notion of a container was
    applied to web servers (servlet container), application servers (an application
    container with or without an EJB container), and other software facilities that
    provided the supportive environment for applications. Sometimes, even the JVM
    itself was called a container, but this association did not survive, probably,
    because the ability to actively engage (execute) the content does not align well
    with the original meaning of a container.
  prefs: []
  type: TYPE_NORMAL
- en: That is why, later, when people started talking about container-less deployment,
    they typically meant the ability to deploy an application into a JVM directly,
    without first installing WebSphere, WebLogic, JBoss, or any other mediating software
    that provides the runtime environment for the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous sections, we described many frameworks that allow us to build
    and deploy an application (or rather a reactive system of microservices) without
    the need for any other container beyond the JVM itself. All you need to do is
    to build a fat JAR file that includes all the dependencies (except those that
    come from the JVM itself) and then run it as a standalone Java process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Well, you also need to make sure that `MANIFEST.MF` in your JAR file has an
    entry `main` class that points to the fully qualified class name that has the
    `main()` method and will be run at the startup. We have described how to do it
    in the previous section, *Building Microservices*.
  prefs: []
  type: TYPE_NORMAL
- en: That is the promised compile-once-run-everywhere of Java, everywhere meaning
    everywhere where JVM of a certain version or higher is installed. There are several
    advantages and disadvantages of this approach. We will discuss them not relative
    to the traditional deployment in a server container. The advantages of deployment
    without using the traditional containers are quite obvious, starting with much
    fewer (if any) licensing costs and ending up with much a lighter deployment and
    scalability process, not even mentioning much less consumption of resources. Instead,
    we will compare container-less deployment not with the traditional one, but with
    a self-contained and an in-container in a new generation of containers that have
    been developed a few years ago.
  prefs: []
  type: TYPE_NORMAL
- en: They allow the ability not only to contain and execute the contained code, which
    the traditional containers did too, but also to move it to a different location
    without any change to the contained code. From now on, by a container, we are
    going to mean only the new ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of container-less deployment are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to add more Java processes either inside the same physical (or virtual
    or in the cloud) machine or on new hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An isolation level between processes is high, which is especially important
    in the shared environment when you have no control over other co-deployed applications,
    and it is possible that a rogue application would try to penetrate the neighboring
    execution environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a small footprint since it does not include anything else beyond the
    application itself or a group of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of container-less deployment are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Each JAR file requires the JVM of a certain version or higher, which may force
    you to bring up a new physical or virtual machine just for this reason, to deploy
    one particular JAR file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of an environment you do not control, your code might be deployed
    with a wrong version of JVM, which could lead to unpredictable results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes in the same JVM compete for resources, which are especially hard to
    manage in the case of the environments shared by different teams or different
    companies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When several microservices are bundled into the same JAR file, they might require
    different versions of a third-party library or even incompatible libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices can be deployed one per JAR or bundled together by a team, by
    related services, by the unit of scale, or using another criterion. Not the least
    important consideration is the total number of such JAR files. As this number
    grows (Google today deals with hundreds of thousands of deployment units at a
    time), it may become impossible to handle deployment via simple bash script and
    require a complex process that allows account ability for possible incompatibilities.
    If that is the case, then it is reasonable to consider using virtual machines
    or containers (in their new incarnation, see the following section) for better
    isolation and management.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Contained Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Self-contained microservices look much similar to container-less. The only difference
    is that the JVM (or JRE, actually) or any other external frameworks and servers
    necessary for the application to run are included in the fat JAR file too. There
    are many ways to build such an all-inclusive JAR file.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Boot, for example, provides a convenient GUI with checkbox list that
    allows you to select which parts of your Spring Boot application and the external
    tools you would like to package. Similarly, WildFly Swarm allows you to choose
    which parts of the Java EE components you would like to bundle along with your
    application. Alternatively, you can do it yourself using the `javapackager` tool.
    It compiles and packages the application and JRE in the same JAR file (it can
    also be `.exe` or `.dmg`) for distribution. You can read about the tool on the
    Oracle website [https://docs.oracle.com/javase/9/tools/javapackager.htm](https://docs.oracle.com/javase/9/tools/javapackager.htm)
    or you can just run the command `javapackager` on a computer where JDK is installed
    (it comes with Java 8 too) you will get the list of tool options and their brief
    description.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, to use the `javapackager` tool, all you need to do is to prepare
    a project with everything you would like to package together, including all the
    dependencies (packaged in JAR files), and run the `javapackager` command with
    the necessary options that allow you to specify the type of output you would like
    to have (`.exe` or `.dmg`, for example), the JRE location you would like to bundle
    together, the icon to use, the `main` class entry for `MANIFEST.MF`, and so on.
    There are also Maven plugins that make the packaging command simpler because much
    of the setup has to be configured in `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of self-contained deployment are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is one file (with all the microservices that compose the reactive system
    or some part of it) to handle, which is simpler for a user and for a distributor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need to pre-install JRE and no risk of mismatching the required
    version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The isolation level is high because your application has a dedicated JRE, so
    the risk of an intrusion from a co-deployed application is minimal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have full control over the dependencies included in the bundle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the file is bigger, which might be an impediment if it has to be
    downloaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The configuration is more complex than in the case of a container-less JAR file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bundle has to be generated on a platform that matches the target one, which
    might lead to mismatch if you have no control over the installation process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other processes deployed on the same hardware or virtual machine can hog the
    resources critical for your application needs, which are especially hard to manage
    if your application is downloaded and run not by the team that has developed it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-Container Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Those who are familiar with **Virtual Machine** (**VM**) and not familiar with
    modern containers (such as Docker, Rocket by CoreOS, VMware Photon, or similar)
    could get the impression that we were talking about VM while saying that a container
    could not only contain and execute the contained code, but also to move it to
    a different location without any change to the contained code. If so, that would
    be quite an apt assumption. VM does allow all of that, and a modern container
    can be considered a lightweight VM as it also allows the allocation of resources
    and provides the feeling of a separate machine. Yet, a container is not a full-blown
    isolated virtual computer.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference is that the bundle that can be passed around as a VM includes
    an entire operating system (with the application deployed). So, it is quite possible
    that a physical server running two VMs would have two different operating systems
    running on it. By contrast, a physical server (or a VM) running three containerized
    applications has only one operating system running, and the two containers share
    (read-only) the operating system kernel, each having its own access (mount) for
    writing to the resources they do not share. This means, for example, a much shorter
    start time, because starting a container does not require us to boot the operating
    system (as in the case of a VM).
  prefs: []
  type: TYPE_NORMAL
- en: For an example, let's take a closer look at Docker the community leader in container.
    In 2015, an initiative called **Open Container Project** was announced, later
    renamed the **Open Container Initiative** (**OCI**), which was supported by Google,
    IBM, Amazon, Microsoft, Red Hat, Oracle, VMware, HP, Twitter, and many other companies.
    Its purpose was to develop industry standards for a container format and container
    runtime software for all platforms. Docker has donated about 5 percent of its
    code base to the project because its solution was chosen as the starting point.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an extensive Docker documentation at: [https://docs.docker.com](https://docs.docker.com).
    Using Docker, one can include in the package all the Java EE Container and the
    application as a Docker image, achieving essentially the same result as with a
    self-contained deployment. Then, you can launch your application by starting the
    Docker image in the Docker engine using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: It starts a process that looks like running an OS on a physical computer, although
    it can also be happening in a cloud inside a VM that is running on the physical
    Linux server shared by many different companies and individuals. That is why an
    isolation level (which, in the case of containers, is almost as high as in a VM)
    may be critical in choosing between different deployment models.
  prefs: []
  type: TYPE_NORMAL
- en: A typical recommendation would be to put one microservice in each container,
    but nothing prevents you from putting several microservices in one Docker image
    (or any other container for that matter). However, there are already mature systems
    of container management (in the world of containers called **orchestration**)
    that can help you with deployment, so the complexity of having many containers,
    although a valid consideration, should not be a big obstacle if resilience and
    responsiveness are at stake. One of the popular orchestrations called **Kubernetes**
    supports microservice registry, discovery, and load balancing. Kubernetes can
    be used in any cloud or in a private infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Containers allow a fast, reliable, and consistent deployment in practically
    any of the current deployment environments, whether it is your own infrastructure
    or a cloud at Amazon, Google, or Microsoft. They also allow the easy movement
    of an application through the development, testing, and production stages. Such
    infrastructure independence allows you, if necessary, to use a public cloud for
    development and testing and your own computers for production.
  prefs: []
  type: TYPE_NORMAL
- en: Once a base operating image is created, each development team can then build
    their application on top, thus avoiding the complexities of environment configuration.
    The versions of a container can also be tracked in a version control system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of using containers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The level of isolation is the highest if compared with container-less and self-contained
    deployment. In addition, more efforts were put recently into adding security to
    containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each container is managed, distributed, deployed, started, and stopped by the
    same set of commands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need to pre-install JRE and risk of mismatching the required version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have full control over the dependencies included in the container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is straightforward to scale up/down each microservice by adding/removing
    container instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of using containers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You and your team have to learn a whole new set of tools and become involved
    more heavily in the production stage. On the other hand, that seems to be the
    general tendency in recent years.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices is a new architectural and design solution for highly loaded processing
    systems that became popular after being successfully used in production by such
    giants as Amazon, Google, Twitter, Microsoft, IBM, and others. It does not mean
    though that you must adopt it too, but you can consider the new approach and see
    if some or any of it can help your applications to be more resilient and responsive.
  prefs: []
  type: TYPE_NORMAL
- en: Using microservices can provide a substantial value, but it is not free. It
    comes with increased complexity of the need to manage many more units through
    all the lifecycle from requirements and development through testing to production.
    Before committing to the full-scale microservice architecture, give it a shot
    by implementing just a few microservices and move them all the way to production.
    Then, let it run for some time and gauge the experience. It will be very specific
    to your organization. Any successful solution must not be blindly copied but adopted
    as fit for your particular needs and abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Better performance and overall efficiency often can be achieved by gradual improvements
    of what is already in place than by radical redesign and re-architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the next lesson, we will discuss and demonstrate new API that can improve
    your code by making it more readable and faster performing.
  prefs: []
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the _________ object, various verticles can be deployed, which talk to
    each other, receive an external request, and process and store data as any other
    Java application, thus forming a system of microservices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following is advantage of container-less deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each JAR file requires the JVM of a certain version or higher, which may force
    you to bring up a new physical or virtual machine just for this reason, to deploy
    one particular JAR file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of an environment you do not control, your code might be deployed
    with a right version of JVM, which could lead to unpredictable results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processes in the same JVM compete for resources, which are especially hard to
    manage in the case of the environments shared by different teams or different
    companies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has a small footprint since it does not include anything else beyond the
    application itself or a group of microservices
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether True or False: One way to support a transaction across several
    microservices is to create a service that would play the role of a Parallel Transaction
    Manager.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following are the Java frameworks that are included in Java 9?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Akka
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ninja
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Orange
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selenium
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether True or False: The level of isolation in a container is the highest
    if compared with container-less and self-contained deployment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

- en: Getting Started with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we set up our programming environment. Now, with our drivers
    and compilers firmly in place, we will begin the actual GPU programming! We will
    start by learning how to use PyCUDA for some basic and fundamental operations.
    We will first see how to query our GPU—that is, we will start by writing a small
    Python program that will tell us what the characteristics of our GPU are, such
    as the core count, architecture, and memory. We will then spend some time getting
    acquainted with how to transfer memory between Python and the GPU with PyCUDA's
    `gpuarray` class and how to use this class for basic computations. The remainder
    of this chapter will be spent showing how to write some basic functions (which
    we will refer to as **CUDA Kernels**) that we can directly launch onto the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning outcomes for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Determining GPU characteristics, such as memory capacity or core count, using
    PyCUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the difference between host (CPU) and device (GPU) memory and
    how to use PyCUDA's `gpuarray` class to transfer data between the host and device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do basic calculations using only `gpuarray` objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to perform basic element-wise operations on the GPU with the PyCUDA `ElementwiseKernel`
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the functional programming concept of reduce/scan operations and
    how to make a basic reduction or scan CUDA kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016 onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0 onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the prerequisites, check the *Preface* of this book;
    for the software and hardware requirements, check the `README` section in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: Querying your GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we begin to program our GPU, we should really know something about its
    technical capacities and limits. We can determine this by doing what is known
    as a **GPU query**. A GPU query is a very basic operation that will tell us the
    specific technical details of our GPU, such as available GPU memory and core count.
    NVIDIA includes a command-line example written in pure CUDA-C called `deviceQuery`
    in the `samples` directory (for both Windows and Linux) that we can run to perform
    this operation. Let''s take a look at the output that is produced on the author''s
    Windows 10 laptop (which is a Microsoft Surface Book 2 with a GTX 1050 GPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ef6b22de-9871-49b2-ad73-4e7aff2017ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at some of the essentials of all of the technical information displayed
    here. First, we see that there is only one GPU installed, Device 0—it is possible
    that a host computer has multiple GPUs and makes use of them, so CUDA will designate
    each *GPU device* an individual number. There are some cases where we may have
    to be specific about the device number, so it is always good to know. We can also
    see the specific type of device that we have (here, GTX 1050), and which CUDA
    version we are using. There are two more things we will take note of for now:
    the total number of cores (here, 640), and the total amount of global memory on
    the device (in this case, 2,048 megabytes, that is, 2 gigabytes).'
  prefs: []
  type: TYPE_NORMAL
- en: While you can see many other technical details from `deviceQuery`, the core
    count and amount of memory are usually the first two things your eyes should zero
    in on the first time you run this on a new GPU, since they can give you the most
    immediate idea of the capacity of your new device.
  prefs: []
  type: TYPE_NORMAL
- en: Querying your GPU with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, finally, we will begin our foray into the world of GPU programming by writing
    our own version of `deviceQuery` in Python. Here, we will primarily concern ourselves
    with only the amount of available memory on the device, the compute capability,
    the number of multiprocessors, and the total number of CUDA cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by initializing CUDA as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that we will always have to initialize PyCUDA with `pycuda.driver.init()`
    or by importing the PyCUDA `autoinit` submodule with `import pycuda.autoinit`!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now immediately check how many GPU devices we have on our host computer
    with this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s type this into IPython and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9c6850ad-552d-48ed-a6d5-4145c4f7407f.png)'
  prefs: []
  type: TYPE_IMG
- en: Great! So far, I have verified that my laptop does indeed have one GPU in it.
    Now, let's extract some more interesting information about this GPU (and any other
    GPU on the system) by adding a few more lines of code to iterate over each device
    that can be individually accessed with `pycuda.driver.Device` (indexed by number).
    The name of the device (for example, GeForce GTX 1050) is given by the `name`
    function. We then get the **compute capability** of the device with the `compute_capability`
    function and total amount of device memory with the `total_memory` function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute capability** can be thought of as a *version number* for each NVIDIA
    GPU architecture; this will give us some important information about the device
    that we can''t otherwise query, as we will see in a minute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we will write it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to look at some of the remaining attributes of our GPU, which
    PyCUDA yields to us in the form of a Python dictionary type. We will use the following
    lines to convert this into a dictionary that is indexed by strings indicating
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now determine the number of *multiprocessors* on our device with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A GPU divides its individual cores up into larger units known as **Streaming** **Multiprocessors
    (SMs)**; a GPU device will have several SMs, which will each individually have
    a particular number of CUDA cores, depending on the compute capability of the
    device. To be clear: the number of cores per multiprocessor is not indicated directly
    by the GPU—this is given to us implicitly by the compute capability. We will have
    to look up some technical documents from NVIDIA to determine the number of cores
    per multiprocessor (see [http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)),
    and then create a lookup table to give us the number of cores per multiprocessor.
    We do so as such, using the `compute_capability` variable to look up the number
    of cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now finally determine the total number of cores on our device by multiplying
    these two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now can finish up our program by iterating over the remaining keys in our
    dictionary and printing the corresponding values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we finally completed our first true GPU program of the text! (Also
    available at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py)).
    Now, we can run it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/59a5907a-0a76-4c08-bfe6-349d9ce48c71.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now have a little pride that we can indeed write a program to query our
    GPU! Now, let's actually begin to learn to *use* our GPU, rather than just observe
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyCUDA's gpuarray class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much like how NumPy's `array` class is the cornerstone of numerical programming
    within the NumPy environment, PyCUDA's `gpuarray` class plays an analogously prominent
    role within GPU programming in Python. This has all of the features you know and
    love from NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing,
    array unraveling, and overloaded operators for point-wise computations (for example,
    `+`, `-`, `*`, `/`, and `**`).
  prefs: []
  type: TYPE_NORMAL
- en: '`gpuarray` is really an indispensable tool for any budding GPU programmer.
    We will spend this section going over this particular data structure and gaining
    a strong grasp of it before we move on.'
  prefs: []
  type: TYPE_NORMAL
- en: Transferring data to and from the GPU with gpuarray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we note from writing our prior `deviceQuery` program in Python, a GPU has
    its own memory apart from the host computer's memory, which is known as **device
    memory**. (Sometimes this is known more specifically as **global device memory***,*
    to differentiate this from the additional cache memory, shared memory, and register
    memory that is also on the GPU.) For the most part, we treat (global) device memory
    on the GPU as we do dynamically allocated heap memory in C (with the `malloc`
    and `free` functions) or C++ (as with the `new` and `delete` operators); in CUDA
    C, this is complicated further with the additional task of transferring data back
    and forth between the CPU to the GPU (with commands such as `cudaMemcpyHostToDevice`
    and `cudaMemcpyDeviceToHost`), all while keeping track of multiple pointers in
    both the CPU and GPU space and performing proper memory allocations (`cudaMalloc`)
    and deallocations (`cudaFree`).
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation,
    and data transfers with the `gpuarray` class. As stated, this class acts similarly
    to NumPy arrays, using vector/ matrix/tensor shape structure information for the
    data. `gpuarray` objects even perform automatic cleanup based on the lifetime,
    so we do not have to worry about *freeing* any GPU memory stored in a `gpuarray`
    object when we are done with it.
  prefs: []
  type: TYPE_NORMAL
- en: How exactly do we use this to transfer data from the host to the GPU? First,
    we must contain our host data in some form of NumPy array (let's call it `host_data`),
    and then use the `gpuarray.to_gpu(host_data)` command to transfer this over to
    the GPU and create a new GPU array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now perform a simple computation within the GPU (pointwise multiplication
    by a constant on the GPU), and then retrieve the GPU data into a new with the
    `gpuarray.get` function. Let''s load up IPython and see how this works (note that
    here we will initialize PyCUDA with `import pycuda.autoinit`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/14eef3e5-273f-45c9-b42f-99d07628f9d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One thing to note is that we specifically denoted that the array on the host
    had its type specifically set to a NumPy `float32` type with the `dtype` option
    when we set up our NumPy array; this corresponds directly with the float type
    in C/C++. Generally speaking, it''s a good idea to specifically set data types
    with NumPy when we are sending data to the GPU. The reason for this is twofold:
    first, since we are using a GPU for increasing the performance of our application,
    we don''t want any unnecessary overhead of using an unnecessary type that will
    possibly take up more computational time or memory, and second, since we will
    soon be writing portions of code in inline CUDA C, we will have to be very specific
    with types or our code won''t work correctly, keeping in mind that C is a statically-typed
    language.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember to specifically set data types for NumPy arrays that will be transferred
    to the GPU. This can be done with the `dtype` option in the constructor of the
    `numpy.array` class.
  prefs: []
  type: TYPE_NORMAL
- en: Basic pointwise arithmetic operations with gpuarray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last example, we saw that we can use the (overloaded) Python multiplication
    operator (`*` ) to multiply each element in a `gpuarray` object by a scalar value
    (here it was 2); note that a pointwise operation is intrinsically parallelizable,
    and so when we use this operation on a `gpuarray` object PyCUDA is able to offload
    each multiplication operation onto a single thread, rather than computing each
    multiplication in serial, one after the other (in fairness, some versions of NumPy
    can use the advanced SSE instructions found in modern x86 chips for these computations,
    so in some cases the performance will be comparable to a GPU). To be clear: these
    pointwise operations performed on the GPU are in parallel since the computation
    of one element is not dependent on the computation of any other element.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a feel for how the operators work, I would suggest that the reader load
    up IPython and create a few `gpuarray` objects on the GPU, and then play around
    with these operations for a few minutes to see that these operators do work similarly
    to arrays in NumPy. Here is some inspiration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fd5469e2-c573-472e-a1a9-6da1dc61ddc5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can see that `gpuarray` objects act predictably and are in accordance
    with how NumPy arrays act. (Notice that we will have to pull the output off the
    GPU with the `get` function!) Let's now do some comparison between CPU and GPU
    computation time to see if and when there is any advantage to doing these operations
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: A speed test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s write up a little program (`time_calc0.py`) that will do a speed comparison
    test between a scalar multiplication on the CPU and then the same operation on
    the GPU. We will then use NumPy''s `allclose` function to compare the two output
    values. We will generate an array of 50 million random 32-bit floating point values
    (this will amount to roughly 48 megabytes of data, so this should be entirely
    feasible with several gigabytes of memory on any somewhat modern host and GPU
    device), and then we will time how long it takes to scalar multiply the array
    by two on both devices. Finally, we will compare the output values to ensure that
    they are equal. Here''s how it''s done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: (You can find the `time_calc0.py` file on the repository provided to you earlier.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s load up IPython and run this a few times to get an idea of the
    general speed of these, and see if there is any variance. (Here, this is being
    run on a 2017-era Microsoft Surface Book 2 with a Kaby Lake i7 processor and a
    GTX 1050 GPU.):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a278a2f1-e099-4907-805f-708f2884a7c3.png)'
  prefs: []
  type: TYPE_IMG
- en: We first notice that the CPU computation time is about the same for each computation
    (roughly 0.08 seconds). Yet, we notice that the GPU computation time is far slower
    than the CPU computation the first time we run this (1.09 seconds), and it becomes
    much faster in the subsequent run, which remains roughly constant in every following
    run (in the range of 7 or 9 milliseconds). If you exit IPython, and then run the
    program again, the same thing will occur. What is the reason for this phenomenon?
    Well, let's do some investigative work using IPython's built-in `prun` profiler.
    (This works similarly to the `cProfiler` module that was featured in [Chapter
    1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*.)
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load our program as text within IPython with the following lines,
    which we can then run with our profiler via Python''s `exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now type `%prun -s cumulative exec(time_calc_code)` into our IPython console
    (with the leading `%`) and see what operations are taking the most time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7dfbfc79-dcc1-4cc8-b7b6-7f11103f54e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, there are a number of suspicious calls to a Python module file, `compiler.py`; these
    take roughly one second total, a little less than the time it takes to do the
    GPU computation here. Now let''s run this again and see if there are any differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/da5995b8-f05d-45d7-950c-f921d79b3886.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that this time, there are no calls to `compiler.py`. Why is this? By
    the nature of the PyCUDA library, GPU code is often compiled and linked with NVIDIA's
    `nvcc` compiler the first time it is run in a given Python session; it is then
    cached and, if the code is called again, then it doesn't have to be recompiled.
    This may include even *simple* operations such as this scalar multiply! (We will
    see eventually see that this can be ameliorated by using the pre-compiled code
    in, [Chapter 10](5383b46f-8dc6-4e17-ab35-7f6bd35f059f.xhtml), *Working with Compiled
    GPU Code*, or by using NVIDIA's own linear algebra libraries with the Scikit-CUDA
    module, which we will see in [Chapter 7](55146879-4b7e-4774-9a8b-cc5c80c04ed8.xhtml),
    *Using the CUDA Libraries with Scikit-CUDA*).
  prefs: []
  type: TYPE_NORMAL
- en: In PyCUDA, GPU code is often compiled at runtime with the NVIDIA `nvcc` compiler
    and then subsequently called from PyCUDA. This can lead to an unexpected slowdown,
    usually the first time a program or GPU operation is run in a given Python session.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyCUDA's ElementWiseKernel for performing pointwise computations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now see how to program our own point-wise (or equivalently, *element-wise*)
    operations directly onto our GPU with the help of PyCUDA's `ElementWiseKernel`
    function. This is where our prior knowledge of C/C++ programming will become useful—we'll
    have to write a little bit of *inline code* in CUDA C, which is compiled externally
    by NVIDIA's `nvcc` compiler and then launched at runtime by our code via PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: We use the term **kernel** quite a bit in this text; by *kernel*, we always
    mean a function that is launched directly onto the GPU by CUDA. We will use several
    functions from PyCUDA that generate templates and design patterns for different
    types of kernels, easing our transition into GPU programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dive right in; we''re going to start by explicitly rewriting the code
    to multiply each element of a `gpuarray` object by 2 in CUDA-C; we will use the
    `ElementwiseKernel` function from PyCUDA to generate our code. You should try
    typing the following code directly into an IPython console. (The less adventurous
    can just download this from this text''s Git repository, which has the filename
    `simple_element_kernel_example0.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at how this is set up; this is, of course, several lines of
    inline C. We first set the input and output variables in the first line ( `"float
    *in, float *out"` ), which will generally be in the form of C pointers to allocated
    memory on the GPU. In the second line, we define our element-wise operation with
    `"out[i] = 2*in[i];"`, which will multiply each point in `in` by two and place
    this in the corresponding index of `out`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that PyCUDA automatically sets up the integer index `i` for us. When we
    use `i` as our index, `ElementwiseKernel` will automatically parallelize our calculation
    over `i` among the many cores in our GPU. Finally, we give our piece of code its
    internal CUDA C kernel name ( `"gpu_2x_ker"` ). Since this refers to CUDA C's
    namespace and not Python's, it's fine (and also convenient) to give this the same
    name as in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s do a speed comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run this program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/02db7f9f-e682-41fb-af4c-2833d054a746.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Whoa! That doesn''t look good. Let''s run the `speedcomparison()` function
    a few times from IPython:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9514e819-e7cd-42c3-b1af-d5ea832c6864.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the speed increases dramatically after the first time we use
    a given GPU function. Again, as with the prior example, this is because PyCUDA
    compiles our inline CUDA C code the first time a given GPU kernel function is
    called using the `nvcc` compiler. After the code is compiled, then it is cached
    and re-used for the remainder of a given Python session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s cover something else important before we move on, which is very
    subtle. The little kernel function we defined operates on C float pointers; this
    means that we will have to allocate some empty memory on the GPU that is pointed
    to by the `out` variable. Take a look at this portion of code again from the `speedcomparison()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did before, we send a NumPy array over to the GPU (`host_data`) via the
    `gpuarray.to_gpu` function, which automatically allocates data onto the GPU and
    copies it over from the CPU space. We will plug this into the `in` part of our
    kernel function. In the next line, we allocate empty memory on the GPU with the
    `gpuarray.empty_like` function. This acts as a plain `malloc` in C, allocating
    an array of the same size and data type as `device_data`, but without copying
    anything. We can now use this for the `out` part of our kernel function. We now
    look at the next line in `speedcomparison()` to see how to launch our kernel function
    onto the GPU (ignoring the lines we use for timing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, the variables we set correspond directly to the first line we defined
    with `ElementwiseKernel` (here being, `"float *in, float *out"`).
  prefs: []
  type: TYPE_NORMAL
- en: Mandelbrot revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s again look at the problem of generating the Mandelbrot set from [Chapter
    1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*. The original
    code is available under the `1` folder in the repository, with the filename `mandelbrot0.py`,
    which you should take another look at before we continue. We saw that there were
    two main components of this program: the first being the generation of the Mandelbrot
    set, and the second concerning dumping the Mandelbrot set into a PNG file. In
    the first chapter, we realized that we could parallelize only the generation of
    the Mandelbrot set, and considering that this takes the bulk of the time for the
    program to do, this would be a good candidate for an algorithm to offload this
    onto a GPU. Let''s figure out how to do this. (We will refrain from re-iterating
    over the definition of the Mandelbrot set, so if you need a deeper review, please
    re-read the *Mandelbrot* *revisited* section of [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why
    GPU Programming?*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make a new Python function based on `simple_mandelbrot` from
    the original program. We''ll call it `gpu_mandelbrot`, and this will take in the
    same exact input as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We will proceed a little differently from here. We will start by building a
    complex lattice that consists of each point in the complex plane that we will
    analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we''ll use some tricks with the NumPy matrix type to easily generate
    the lattice, and then typecast the result from a NumPy `matrix` type to a two-dimensional
    NumPy `array` (since PyCUDA can only handle NumPy `array` types, not `matrix`
    types). Notice how we are very carefully setting our NumPy types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we now have a two-dimensional complex array that represents the lattice
    from which we will generate our Mandelbrot set; as we will see, we can operate
    on this very easily within the GPU. Let''s now transfer our lattice to the GPU
    and allocate an array that we will use to represent our Mandelbrot set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To reiterate—the `gpuarray.to_array` function only can operate on NumPy `array`
    types, so we were sure to have type-cast this beforehand before we sent it to
    the GPU. Next, we have to allocate some memory on the GPU with the `gpuarray.empty` function,
    specifying the size/shape of the array and the type. Again, you can think of this
    as acting similarly to `malloc` in C; remember that we won't have to deallocate
    or `free` this memory later, due to the `gpuarray` object destructor taking care
    of memory clean-up automatically when the end of the scope is reached.
  prefs: []
  type: TYPE_NORMAL
- en: When you allocate memory on the GPU with the PyCUDA functions `gpuarray.empty`
    or `gpuarray.empty_like`, you do not have to deallocate this memory later due
    to the destructor of the `gpuarray `object managing all memory clean up.
  prefs: []
  type: TYPE_NORMAL
- en: We're now ready to launch the kernel; the only change we have to make is to
    change the
  prefs: []
  type: TYPE_NORMAL
- en: 'We haven''t written our kernel function yet to generate the Mandelbrot set,
    but let''s just write how we want the rest of this function to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So this is how we want our new kernel to act—the first input will be the complex
    lattice of points (NumPy `complex64` type) we generated, the second will be a
    pointer to a two-dimensional floating point array (NumPy `float32` type) that
    will indicate which elements are members of the Mandelbrot set, the third will
    be an integer indicating the maximum number of iterations for each point, and
    the final input will be the upper bound for each point used for determining membership
    in the Mandelbrot class. Notice that we are *very* careful in typecasting everything
    that goes into the GPU!
  prefs: []
  type: TYPE_NORMAL
- en: The next line retrieves the Mandelbrot set we generated from the GPU back into
    CPU space, and the end value is returned. (Notice that the input and output of
    `gpu_mandelbrot` is exactly the same as that of `simple_mandelbrot`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at how to properly define our GPU kernel. First, let''s add
    the appropriate `include` statements to the header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to write our GPU kernel! We''ll show it here and then go over
    this line-by-line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: First, we set our input with the first string passed to `ElementwiseKernel`.
    We have to realize that when we are working in CUDA-C, particular C datatypes
    will correspond directly to particular Python NumPy datatypes. Again, note that
    when arrays are passed into a CUDA kernel, they are seen as C pointers by CUDA.
    Here, a CUDA C `int` type corresponds exactly to a NumPy `int32` type, while a
    CUDA C `float` type corresponds to a NumPy `float32` type. An internal PyCUDA
    class template is then used for complex types—here PyCUDA `::complex<float>` corresponds
    to Numpy `complex64`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the content of the second string, which is deliminated with three
    quotes (`"""`). This allows us to use multiple lines within the string; we will
    use this when we write larger inline CUDA kernels in Python.
  prefs: []
  type: TYPE_NORMAL
- en: While the arrays we have passed in are two-dimensional arrays in Python, CUDA
    will only see these as being one-dimensional and indexed by `i`. Again, `ElementwiseKernel`
    indexes `i` across multiple cores and threads for us automatically. We initialize
    each point in the output to one with `mandelbrot_graph[i] = 1;`, as `i` will be
    indexed over every single element of our Mandelbrot set; we're going to assume
    that every point will be a member unless proven otherwise. (Again, the Mandelbrot
    set is over two dimensions, real and complex, but `ElementwiseKernel` will automatically
    translate everything into a one-dimensional set. When we interact with the data
    again in Python, the two-dimensional structure of the Mandelbrot set will be preserved.)
  prefs: []
  type: TYPE_NORMAL
- en: We set up our `c` value as in Python to the appropriate lattice point with `pycuda::complex<float>
    c = lattice[i];` and initialize our `z` value to `0` with `pycuda::complex<float>
    z(0,0);` (the first zero corresponds to the real part, while the second corresponds
    to the imaginary part). We then perform a loop over a new iterator, `j`, with `for(int
    j = 0; j < max_iters; j++)`. (Note that this algorithm will not be parallelized
    over `j` or any other index—only `i`! This `for` loop will run serially over `j`—but
    the entire piece of code will be parallelized across `i`.)
  prefs: []
  type: TYPE_NORMAL
- en: We then set the new value of `*z*` with `z = z*z + c;` as per the Mandelbrot
    algorithm. If the absolute value of this element exceeds the upper bound ( `if(abs(z)
    > upper_bound)` ), we set this point to 0 ( `mandelbrot_graph[i] = 0;` ) and break
    out of the loop with the `break` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: In the final string passed into `ElementwiseKernel` we give the kernel its internal
    CUDA C name, here `"mandel_ker"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now ready to launch the kernel; the only change we have to make is to
    change the reference from `simple_mandelbrot` in the main function to `gpu_mandelbrot`,
    and we''re ready to go. Let''s launch this from IPython:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f00d5080-4975-4023-9f14-397a8e007ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s check the dumped image to make sure this is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6fa6851a-bcce-46d0-a63d-8023766da21a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is certainly the same Mandelbrot image that is produced in the first chapter,
    so we have successfully implemented this onto a GPU! Let''s now look at the speed
    increase we''re getting: in the first chapter, it took us 14.61 seconds to produce
    this graph; here, it only took 0.894 seconds. Keep in mind that PyCUDA also has
    to compile and link our CUDA C code at runtime, and the time it takes to make
    the memory transfers to and from the GPU. Still, even with all of that extra overhead,
    it is a very worthwhile speed increase! (You can view the code for our GPU Mandelbrot
    with the file named `gpu_mandelbrot0.py` in the Git repository.)'
  prefs: []
  type: TYPE_NORMAL
- en: A brief foray into functional programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we continue, let's briefly do a review of two functions available in
    Python for **functional programming***—*`map` and `reduce`. These are both considered
    to be *functional* because they both act on *functions* for their operation. We
    find these interesting because these both correspond to common design patterns
    in programming, so we can swap out different functions in the input to get a multitude
    of different (and useful) operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first recall the `lambda` keyword in Python. This allows us to define
    an **anonymous function**—in most cases, these can be thought of as a `throwaway` function
    that we may only wish to use once, or functions that are able to be defined on
    a single line. Let''s open up IPython right now and define a little function that
    squares a number as such—`pow2 = lambda x : x**2`. Let''s test it out on a few
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f7154a51-4486-4292-9ed0-89415e526394.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s recall that `map` acts on two input values: a function and a `list`
    of objects that the given function can act on. `map` outputs a list of the function''s
    output for each element in the original list. Let''s now define our squaring operation
    as an anonymous function which we input into map, and a list of the last few numbers
    we checked with the following—`map(lambda x : x**2, [2,3,4])`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c41f370e-9cba-40ba-a5df-e84857044437.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that `map` acts as `ElementwiseKernel`! This is actually a standard
    design pattern in functional programming. Now, let''s look at `reduce`; rather
    than taking in a list and outputting a directly corresponding list, reduce takes
    in a list, performs a recursive binary operation on it, and outputs a singleton.
    Let''s get a notion of this design pattern by typing `reduce(lambda x, y : x +
    y, [1,2,3,4])`. When we type this in IPython, we will see that this will output
    a single number, 10, which is indeed the sum of *1+2+3+4*. You can try replacing
    the summation above with multiplication, and seeing that this indeed works for
    recursively multiplying a long list of numbers together. Generally speaking, we
    use reduce operations with *associative binary operations*; this means that, no
    matter the order we perform our operation between sequential elements of the list,
    will always invariably give the same result, provided that the list is kept in
    order. (This is not to be confused with the *commutative property*.)'
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how PyCUDA handles programming patterns akin to `reduce`—with
    **parallel scan** and **reduction kernels**.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel scan and reduction kernel basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at a basic function in PyCUDA that reproduces the functionality
    of reduce—`InclusiveScanKernel`. (You can find the code under the `simple_scankernal0.py` filename.)
    Let''s execute a basic example that sums a small list of numbers on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We construct our kernel by first specifying the input/output type (here, NumPy
    `int32`) and in the string, `"a+b"`. Here, `InclusiveScanKernel` sets up elements
    named `a` and `b` in the GPU space automatically, so you can think of this string
    input as being analogous to `lambda a,b: a + b` in Python. We can really put any
    (associative) binary operation here, provided we remember to write it in C.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run `sum_gpu`, we see that we will get an array of the same size as
    the input array. Each element in the array represents the value for each step
    in the calculation (the NumPy `cumsum` function gives the same output, as we can
    see). The last element will be the final output that we are seeking, which corresponds
    to the output of reduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/98e28110-698a-4ab5-a827-9c1a8a2e31d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try something a little more challenging; let''s find the maximum value
    in a `float32` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: (You can find the complete code in the file named `simple_scankernal1.py`.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the main change we made is to replace the `a + b` string with `a > b
    ? a : b`. (In Python, this would be rendered within a `reduce` statement as `lambda
    a, b:  max(a,b)`). Here, we are using a trick to give the max among `a` and `b`
    with the C language''s `?` operator. We finally display the last value of the
    resulting element in the output array, which will be exactly the last element
    (which we can always retrieve with the `[-1]` index in Python).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s finally look one more PyCUDA function for generating GPU kernels—`ReductionKernel`.
    Effectively, `ReductionKernel` acts like a `ElementwiseKernel` function followed
    by a parallel scan kernel. What algorithm is a good candidate for implementing
    with a `ReductionKernel`? The first that tends to come to mind is the dot product
    from linear algebra. Let''s remember computing the dot product of two vectors
    has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiply the vectors pointwise
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum the resulting pointwise multiples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These two steps are also called *multiply and accumulate*. Let''s set up a
    kernel to do this computation now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: First, note the datatype we use for our kernel (a `float32`). We then set up
    the input arguments to our CUDA C kernel with `arguments`, (here two float arrays
    representing each vector designated with `float *`) and set the pointwise calculation
    with `map_expr`, here it is pointwise multiplication. As with `ElementwiseKernel`,
    this is indexed over `i`. We set up `reduce_expr` the same as with `InclusiveScanKernel`.
    This will take the resulting output from the element-wise operation and perform
    a reduce-type operation on the array. Finally, we set the *neutral element* with
    neutral. This is an element that will act as an identity for `reduce_expr`; here,
    we set `neutral=0`, because `0` is always the identity under addition (under multiplication,
    one is the identity). We'll see why exactly we have to set this up when we cover
    parallel prefix in greater depth later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We first saw how to query our GPU from PyCUDA, and with this re-create the CUDA
    `deviceQuery` program in Python. We then learned how to transfer NumPy arrays
    to and from the GPU's memory with the PyCUDA `gpuarray` class and its `to_gpu`
    and `get` functions. We got a feel for using `gpuarray` objects by observing how
    to use them to do basic calculations on the GPU, and we learned to do a little
    investigative work using IPython's `prun` profiler. We saw there is sometimes
    some arbitrary slowdown when running GPU functions from PyCUDA for the first time
    in a session, due to PyCUDA launching NVIDIA's `nvcc` compiler to compile inline
    CUDA C code. We then saw how to use the `ElementwiseKernel` function to compile
    and launch element-wise operations, which are automatically parallelized onto
    the GPU from Python. We did a brief review of functional programming in Python
    (in particular the `map` and `reduce` functions), and finally, we covered how
    to do some basic reduce/scan-type computations on the GPU using the `InclusiveScanKernel`
    and `ReductionKernel` functions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the absolute basics down about writing and launching kernel
    functions, we should realize that PyCUDA has covered the vast amount of the overhead
    in writing a kernel for us with its templates. We will spend the next chapter
    learning about the principles of CUDA kernel execution, and how CUDA arranges
    concurrent threads in a kernel into abstract **grids** and **blocks**.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In `simple_element_kernel_example0.py`, we don't consider the memory transfers
    to and from the GPU in measuring the time for the GPU computation. Try measuring
    the time that the `gpuarray` functions, `to_gpu` and `get`, take with the Python
    time command. Would you say it's worth offloading this particular function onto
    the GPU, with the memory transfer times in consideration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*,
    we had a discussion of Amdahl's Law, which gives us some idea of the gains we
    can potentially get by offloading portions of a program onto a GPU. Name two issues
    that we have seen in this chapter that Amdahl's law does not take into consideration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify `gpu_mandel0.py` to use smaller and smaller lattices of complex numbers,
    and compare this to the same lattices CPU version of the program. Can we choose
    a small enough lattice such that the CPU version is actually faster than the GPU
    version?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a kernel with `ReductionKernel` that takes two `complex64` arrays on
    the GPU of the same length and returns the absolute largest element among both
    arrays.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens if a `gpuarray` object reaches end-of-scope in Python?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do you think we need to define `neutral` when we use `ReductionKernel`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If in `ReductionKernel` we set `reduce_expr ="a > b ? a : b"`, and we are operating
    on int32 types, then what should we set "`neutral`" to?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

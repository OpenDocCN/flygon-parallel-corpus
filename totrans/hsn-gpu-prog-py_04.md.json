["```py\nimport pycuda.autoinit\nimport pycuda.driver as drv\nimport numpy as np\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\n```", "```py\nker = SourceModule(\"\"\"\n__global__ void scalar_multiply_kernel(float *outvec, float scalar, float *vec)\n{\n int i = threadIdx.x;\n outvec[i] = scalar*vec[i];\n}\n\"\"\")\n```", "```py\nscalar_multiply_gpu = ker.get_function(\"scalar_multiply_kernel\")\n```", "```py\ntestvec = np.random.randn(512).astype(np.float32)\ntestvec_gpu = gpuarray.to_gpu(testvec)\noutvec_gpu = gpuarray.empty_like(testvec_gpu)\n```", "```py\nscalar_multiply_gpu( outvec_gpu, np.float32(2), testvec_gpu, block=(512,1,1), grid=(1,1,1))\n```", "```py\nprint \"Does our kernel work correctly? : {}\".format(np.allclose(outvec_gpu.get() , 2*testvec) )\n```", "```py\nimport pycuda.autoinit\nimport pycuda.driver as drv\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport matplotlib.animation as animation\n```", "```py\nker = SourceModule(\"\"\"\n#define _X  ( threadIdx.x + blockIdx.x * blockDim.x )\n#define _Y  ( threadIdx.y + blockIdx.y * blockDim.y )\n```", "```py\n#define _WIDTH  ( blockDim.x * gridDim.x )\n#define _HEIGHT ( blockDim.y * gridDim.y  )\n\n#define _XM(x)  ( (x + _WIDTH) % _WIDTH )\n#define _YM(y)  ( (y + _HEIGHT) % _HEIGHT )\n```", "```py\n#define _INDEX(x,y)  ( _XM(x)  + _YM(y) * _WIDTH )\n```", "```py\n__device__ int nbrs(int x, int y, int * in)\n{\n     return ( in[ _INDEX(x -1, y+1) ] + in[ _INDEX(x-1, y) ] + in[ _INDEX(x-1, y-1) ] \\\n                   + in[ _INDEX(x, y+1)] + in[_INDEX(x, y - 1)] \\\n                   + in[ _INDEX(x+1, y+1) ] + in[ _INDEX(x+1, y) ] + in[ _INDEX(x+1, y-1) ] );\n}\n\n```", "```py\n__global__ void conway_ker(int * lattice_out, int * lattice  )\n{\n   // x, y are the appropriate values for the cell covered by this thread\n   int x = _X, y = _Y;\n\n   // count the number of neighbors around the current cell\n   int n = nbrs(x, y, lattice);\n\n    // if the current cell is alive, then determine if it lives or dies for the next generation.\n    if ( lattice[_INDEX(x,y)] == 1)\n       switch(n)\n       {\n          // if the cell is alive: it remains alive only if it has 2 or 3 neighbors.\n          case 2:\n          case 3: lattice_out[_INDEX(x,y)] = 1;\n                  break;\n          default: lattice_out[_INDEX(x,y)] = 0;                   \n       }\n    else if( lattice[_INDEX(x,y)] == 0 )\n         switch(n)\n         {\n            // a dead cell comes to life only if it has 3 neighbors that are alive.\n            case 3: lattice_out[_INDEX(x,y)] = 1;\n                    break;\n            default: lattice_out[_INDEX(x,y)] = 0;         \n         }\n\n}\n\"\"\")\n\nconway_ker = ker.get_function(\"conway_ker\")\n\n```", "```py\ndef update_gpu(frameNum, img, newLattice_gpu, lattice_gpu, N):    \n```", "```py\n    conway_ker(newLattice_gpu, lattice_gpu, grid=(N/32,N/32,1), block=(32,32,1) )    \n```", "```py\n    img.set_data(newLattice_gpu.get() )    \n    lattice_gpu[:] = newLattice_gpu[:]\n\n    return img\n```", "```py\nif __name__ == '__main__':\n    # set lattice size\n    N = 256\n\n    lattice = np.int32( np.random.choice([1,0], N*N, p=[0.25, 0.75]).reshape(N, N) )\n    lattice_gpu = gpuarray.to_gpu(lattice)\n```", "```py\nlattice_gpu = gpuarray.to_gpu(lattice)\n    lattice_gpu = gpuarray.to_gpu(lattice)\n    newLattice_gpu = gpuarray.empty_like(lattice_gpu) \n\n    fig, ax = plt.subplots()\n    img = ax.imshow(lattice_gpu.get(), interpolation='nearest')\n    ani = animation.FuncAnimation(fig, update_gpu, fargs=(img, newLattice_gpu, lattice_gpu, N, ) , interval=0, frames=1000, save_count=1000) \n\n    plt.show()\n```", "```py\nimport pycuda.autoinit\nimport pycuda.driver as drv\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\nimport numpy as np\nimport matplotlib.pyplot as plt \n```", "```py\nker = SourceModule(\"\"\"\n```", "```py\n__global__ void conway_ker(int * lattice, int iters) {\n```", "```py\n int x = _X, y = _Y; \n for (int i = 0; i < iters; i++)\n {\n     int n = nbrs(x, y, lattice); \n     int cell_value;\n```", "```py\n if ( lattice[_INDEX(x,y)] == 1)\n switch(n)\n {\n // if the cell is alive: it remains alive only if it has 2 or 3 neighbors.\n case 2:\n case 3: cell_value = 1;\n break;\n default: cell_value = 0; \n }\n else if( lattice[_INDEX(x,y)] == 0 )\n switch(n)\n {\n // a dead cell comes to life only if it has 3 neighbors that are alive.\n case 3: cell_value = 1;\n break;\n default: cell_value = 0; \n } \n __syncthreads();\n lattice[_INDEX(x,y)] = cell_value; \n __syncthreads();\n } \n}\n\"\"\")\n```", "```py\nconway_ker = ker.get_function(\"conway_ker\")\nif __name__ == '__main__':\n # set lattice size\n N = 32\n lattice = np.int32( np.random.choice([1,0], N*N, p=[0.25, 0.75]).reshape(N, N) )\n lattice_gpu = gpuarray.to_gpu(lattice)\n conway_ker(lattice_gpu, np.int32(1000000), grid=(1,1,1), block=(32,32,1))\n fig = plt.figure(1)\n plt.imshow(lattice_gpu.get())\n```", "```py\n int x = _X, y = _Y; \n```", "```py\n__global__ void conway_ker_shared(int * p_lattice, int iters)\n{\n int x = _X, y = _Y;\n __shared__ int lattice[32*32];\n```", "```py\n lattice[_INDEX(x,y)] = p_lattice[_INDEX(x,y)];\n __syncthreads();\n```", "```py\n __syncthreads();\n p_lattice[_INDEX(x,y)] = lattice[_INDEX(x,y)];\n __syncthreads();\n} \"\"\")\n```", "```py\ninput: x0, ..., xn-1 initialize:\nfor k=0 to n-1:\n    yk := xk begin:\nparfor i=0 to n-1 :\n    for j=0 to log2(n):\n        if i >= 2j :\n            yi := yi  yi - 2^j else:\n            continue\n        end if\n    end for\nend parfor\nend\noutput: y0, ..., yn-1\n```", "```py\nimport pycuda.autoinit\nimport pycuda.driver as drv\nimport numpy as np\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\nfrom time import time\n\nnaive_ker = SourceModule(\"\"\"\n__global__ void naive_prefix(double *vec, double *out)\n{\n     __shared__ double sum_buf[1024]; \n     int tid = threadIdx.x; \n     sum_buf[tid] = vec[tid];\n\n```", "```py\n int iter = 1;\n for (int i=0; i < 10; i++)\n {\n     __syncthreads();\n     if (tid >= iter )\n     {\n         sum_buf[tid] = sum_buf[tid] + sum_buf[tid - iter]; \n     } \n     iter *= 2;\n }\n __syncthreads();\n```", "```py\n __syncthreads();\n out[tid] = sum_buf[tid];\n __syncthreads();\n\n}\n\"\"\")\nnaive_gpu = naive_ker.get_function(\"naive_prefix\")\n\n```", "```py\nif __name__ == '__main__':\n testvec = np.random.randn(1024).astype(np.float64)\n testvec_gpu = gpuarray.to_gpu(testvec)\n\n outvec_gpu = gpuarray.empty_like(testvec_gpu)\n naive_gpu( testvec_gpu , outvec_gpu, block=(1024,1,1), grid=(1,1,1))\n\n total_sum = sum( testvec)\n total_sum_gpu = outvec_gpu[-1].get()\n\n print \"Does our kernel work correctly? : {}\".format(np.allclose(total_sum_gpu , total_sum) )\n```", "```py\ninput: x0, ..., xn-1initialize:\n    for i = 0 to n - 1:\n        yi := xi\nbegin:\nfor k=0 to log2(n) - 1:\n    parfor j=0 to n - 1: \n        if j is divisible by 2k+1:\n            yj+2^(k+1)-1 = yj+2^k-1  yj +2^(k+1) -1else:\n            continue\nend\noutput: y0, ..., yn-1\n\n```", "```py\ninput: x0, ..., xn-1 initialize:\n    for i = 0 to n - 2:\n        yi := xi\n    yn-1 := 0\nbegin:\nfor k = log2(n) - 1 to 0:\n    parfor j = 0 to n - 1: \n        if j is divisible by 2k+1:\n            temp := yj+2^k-1\n            yj+2^k-1 := yj+2^(k+1)-1\n            yj+2^(k+1)-1 := yj+2^(k+1)-1  temp\n        else:\n            continue\nend\noutput: y0 , y1 , ..., yn-1\n```", "```py\nup_ker = SourceModule(\"\"\"\n__global__ void up_ker(double *x, double *x_old, int k)\n{\n```", "```py\nint tid =  blockIdx.x*blockDim.x + threadIdx.x;\n```", "```py\n int _2k = 1 << k;\n int _2k1 = 1 << (k+1);\n\n int j = tid* _2k1;\n```", "```py\n\n x[j + _2k1 - 1] = x_old[j + _2k -1 ] + x_old[j + _2k1 - 1];\n}\n\"\"\")\n```", "```py\n\nup_gpu = up_ker.get_function(\"up_ker\")\n\ndef up_sweep(x):\n    x = np.float64(x)\n    x_gpu = gpuarray.to_gpu(np.float64(x) )\n    x_old_gpu = x_gpu.copy()\n    for k in range( int(np.log2(x.size) ) ) : \n        num_threads = int(np.ceil( x.size / 2**(k+1)))\n        grid_size = int(np.ceil(num_threads / 32))\n\n        if grid_size > 1:\n            block_size = 32\n        else:\n            block_size = num_threads\n\n        up_gpu(x_gpu, x_old_gpu, np.int32(k) , block=(block_size,1,1), grid=(grid_size,1,1))\n        x_old_gpu[:] = x_gpu[:]\n\n    x_out = x_gpu.get()\n    return(x_out)\n```", "```py\ndown_ker = SourceModule(\"\"\"\n__global__ void down_ker(double *y, double *y_old, int k)\n{\n int j = blockIdx.x*blockDim.x + threadIdx.x;\n\n int _2k = 1 << k;\n int _2k1 = 1 << (k+1);\n\n int j = tid*_2k1;\n\n y[j + _2k - 1 ] = y_old[j + _2k1 - 1];\n y[j + _2k1 - 1] = y_old[j + _2k1 - 1] + y_old[j + _2k - 1];\n}\n\"\"\")\n\ndown_gpu = down_ker.get_function(\"down_ker\")\n```", "```py\n\ndef down_sweep(y):\n    y = np.float64(y)\n    y[-1] = 0\n    y_gpu = gpuarray.to_gpu(y)\n    y_old_gpu = y_gpu.copy()\n    for k in reversed(range(int(np.log2(y.size)))):\n        num_threads = int(np.ceil( y.size / 2**(k+1)))\n        grid_size = int(np.ceil(num_threads / 32))\n\n        if grid_size > 1:\n            block_size = 32\n        else:\n            block_size = num_threads\n\n        down_gpu(y_gpu, y_old_gpu, np.int32(k), block=(block_size,1,1), grid=(grid_size,1,1))\n        y_old_gpu[:] = y_gpu[:]\n    y_out = y_gpu.get()\n    return(y_out)\n```", "```py\ndef efficient_prefix(x):\n        return(down_sweep(up_sweep(x)))\n\n```"]
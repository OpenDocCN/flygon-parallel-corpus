["```py\n// Copy an array of floats from the host to the device.\ncudaMemcpy(device_array, host_array, size_of_array*sizeof(float), cudaMemcpyHostToDevice);\n// Block execution until memory transfer to device is complete.\ncudaDeviceSynchronize();\n// Launch CUDA kernel.\nSome_CUDA_Kernel <<< block_size, grid_size >>> (device_array, size_of_array);\n// Block execution until GPU kernel function returns.\ncudaDeviceSynchronize();\n// Copy output of kernel to host.\ncudaMemcpy(host_array,  device_array, size_of_array*sizeof(float), cudaMemcpyDeviceToHost); // Block execution until memory transfer to host is complete.\ncudaDeviceSynchronize();\n```", "```py\nimport pycuda.autoinit\nimport pycuda.driver as drv\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\nimport numpy as np\nfrom time import time\n```", "```py\nnum_arrays = 200\narray_len = 1024**2\n```", "```py\nker = SourceModule(\"\"\" \n__global__ void mult_ker(float * array, int array_len)\n{\n     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n     int num_iters = array_len / blockDim.x;\n\n     for(int j=0; j < num_iters; j++)\n     {\n         int i = j * blockDim.x + thd;\n\n         for(int k = 0; k < 50; k++)\n         {\n              array[i] *= 2.0;\n              array[i] /= 2.0;\n         }\n     }\n}\n\"\"\")\n\nmult_ker = ker.get_function('mult_ker')\n```", "```py\ndata = []\ndata_gpu = []\ngpu_out = []\n\n# generate random arrays.\nfor _ in range(num_arrays):\n    data.append(np.random.randn(array_len).astype('float32'))\n\nt_start = time()\n\n# copy arrays to GPU.\nfor k in range(num_arrays):\n    data_gpu.append(gpuarray.to_gpu(data[k]))\n\n# process arrays.\nfor k in range(num_arrays):\n    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1))\n\n# copy arrays from GPU.\nfor k in range(num_arrays):\n    gpu_out.append(data_gpu[k].get())\n\nt_end = time()\n\nfor k in range(num_arrays):\n    assert (np.allclose(gpu_out[k], data[k]))\n\nprint 'Total time: %f' % (t_end - t_start)\n```", "```py\ndata = []\ndata_gpu = []\ngpu_out = []\nstreams = []\n```", "```py\nfor _ in range(num_arrays):\n    streams.append(drv.Stream())\n```", "```py\nfor k in range(num_arrays):\n    data_gpu.append(gpuarray.to_gpu_async(data[k], stream=streams[k]))\n```", "```py\nfor k in range(num_arrays):\n    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1), stream=streams[k])\n```", "```py\nfor k in range(num_arrays):\n    gpu_out.append(data_gpu[k].get_async(stream=streams[k]))\n```", "```py\nif __name__ == '__main__':\n\n    N = 128\n    num_concurrent = 4\n```", "```py\nstreams = []\nlattices_gpu = []\nnewLattices_gpu = []\n\nfor k in range(num_concurrent):\n    streams.append(drv.Stream())\n    lattice = np.int32( np.random.choice([1,0], N*N, p=[0.25, 0.75]).reshape(N, N) )\n    lattices_gpu.append(gpuarray.to_gpu(lattice)) \n    newLattices_gpu.append(gpuarray.empty_like(lattices_gpu[k])) \n```", "```py\nfig, ax = plt.subplots(nrows=1, ncols=num_concurrent)\nimgs = []\n\nfor k in range(num_concurrent):\n    imgs.append( ax[k].imshow(lattices_gpu[k].get_async(stream=streams[k]), interpolation='nearest') )\n\n```", "```py\nani = animation.FuncAnimation(fig, update_gpu, fargs=(imgs, newLattices_gpu, lattices_gpu, N, streams, num_concurrent) , interval=0, frames=1000, save_count=1000)    \n```", "```py\nfor k in range(num_concurrent):\n    conway_ker( newLattices_gpu[k], lattices_gpu[k], grid=(N/32,N/32,1), block=(32,32,1), stream=streams[k] )\n     imgs[k].set_data(newLattices_gpu[k].get_async(stream=streams[k]) )\n     lattices_gpu[k].set_async(newLattices_gpu[k], stream=streams[k])\n\n return imgs\n```", "```py\narray_len = 100*1024**2\ndata = np.random.randn(array_len).astype('float32')\ndata_gpu = gpuarray.to_gpu(data)\n```", "```py\nstart_event = drv.Event()\nend_event = drv.Event()\n```", "```py\nstart_event.record()\nmult_ker(data_gpu, np.int32(array_len), block=(64,1,1), grid=(1,1,1))\nend_event.record()\n```", "```py\nprint 'Has the kernel started yet? {}'.format(start_event.query())\n print 'Has the kernel ended yet? {}'.format(end_event.query())\n```", "```py\nend_event.synchronize()\n\nprint 'Has the kernel started yet?  {}'.format(start_event.query())\n\nprint 'Has the kernel ended yet? {}'.format(end_event.query())\n```", "```py\nprint 'Kernel execution time in milliseconds: %f ' % start_event.time_till(end_event)\n```", "```py\ndata = []\ndata_gpu = []\ngpu_out = []\nstreams = []\nstart_events = []\nend_events = []\n\nfor _ in range(num_arrays):\n    streams.append(drv.Stream())\n    start_events.append(drv.Event())\n    end_events.append(drv.Event())\n```", "```py\nfor k in range(num_arrays):\n    start_events[k].record(streams[k])\n    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1), stream=streams[k])\n\nfor k in range(num_arrays):\n    end_events[k].record(streams[k])\n```", "```py\nkernel_times = []\nfor k in range(num_arrays):\n   kernel_times.append(start_events[k].time_till(end_events[k]))\n```", "```py\nprint 'Mean kernel duration (milliseconds): %f' % np.mean(kernel_times)\nprint 'Mean kernel standard deviation (milliseconds): %f' % np.std(kernel_times)\n```", "```py\nmandelbrot_lattice_gpu = gpuarray.to_gpu_async(mandelbrot_lattice) pycuda.autoinit.context.synchronize()\n```", "```py\nmandel_ker( mandelbrot_lattice_gpu, mandelbrot_graph_gpu, np.int32(max_iters), np.float32(upper_bound))\npycuda.autoinit.context.synchronize()\nmandelbrot_graph = mandelbrot_graph_gpu.get_async()\npycuda.autoinit.context.synchronize()\n```", "```py\nimport numpy as np\nfrom pycuda import gpuarray\nimport pycuda.driver as drv\n```", "```py\ndrv.init()\n```", "```py\ndev = drv.Device(0)\n```", "```py\nctx = dev.make_context()\n```", "```py\nx = gpuarray.to_gpu(np.float32([1,2,3]))\nprint x.get()\n```", "```py\nctx.pop()\n```", "```py\nimport threading\nclass PointlessExampleThread(threading.Thread):\n```", "```py\ndef __init__(self):\n    threading.Thread.__init__(self)\n    self.return_value = None\n```", "```py\ndef run(self):\n    print 'Hello from the thread you just spawned!'\n    self.return_value = 123\n```", "```py\ndef join(self):\n    threading.Thread.join(self)\n    return self.return_value\n```", "```py\nNewThread = PointlessExampleThread()\nNewThread.start()\nthread_output = NewThread.join()\nprint 'The thread completed and returned this value: %s' % thread_output\n```", "```py\nimport pycuda\nimport pycuda.driver as drv\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\nimport numpy as np\nfrom time import time\nimport threading \n```", "```py\nnum_arrays = 10\narray_len = 1024**2\n```", "```py\nkernel_code = \"\"\" \n__global__ void mult_ker(float * array, int array_len)\n{\n     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n     int num_iters = array_len / blockDim.x;\n    for(int j=0; j < num_iters; j++)\n     {\n     int i = j * blockDim.x + thd;\n     for(int k = 0; k < 50; k++)\n     {\n         array[i] *= 2.0;\n         array[i] /= 2.0;\n     }\n }\n}\n\"\"\"\n```", "```py\nclass KernelLauncherThread(threading.Thread):\n    def __init__(self, input_array):\n        threading.Thread.__init__(self)\n        self.input_array = input_array\n        self.output_array = None\n```", "```py\ndef run(self):\n    self.dev = drv.Device(0)\n    self.context = self.dev.make_context()\n    self.ker = SourceModule(kernel_code)\n    self.mult_ker = self.ker.get_function('mult_ker')\n```", "```py\nself.array_gpu = gpuarray.to_gpu(self.input_array)\nself.mult_ker(self.array_gpu, np.int32(array_len), block=(64,1,1), grid=(1,1,1))\nself.output_array = self.array_gpu.get()\nself.context.pop()\n```", "```py\n def join(self):\n     threading.Thread.join(self)\n     return self.output_array\n```", "```py\ndata = []\ngpu_out = []\nthreads = []\nfor _ in range(num_arrays):\n    data.append(np.random.randn(array_len).astype('float32'))\nfor k in range(num_arrays):\n threads.append(KernelLauncherThread(data[k]))\n```", "```py\nfor k in range(num_arrays):\n    threads[k].start()\n\nfor k in range(num_arrays):\n    gpu_out.append(threads[k].join())\n```", "```py\nfor k in range(num_arrays):\n    assert (np.allclose(gpu_out[k], data[k]))\n\n```"]
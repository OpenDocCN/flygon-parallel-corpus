- en: Streams, Events, Contexts, and Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the prior chapters, we saw that there are two primary operations we perform
    from the host when interacting with the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: Copying memory data to and from the GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching kernel functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know that *within* a single kernel, there is one level of concurrency among
    its many threads; however, there is another level of concurrency *over* multiple
    kernels *and* GPU memory operations that are also available to us. This means
    that we can launch multiple memory and kernel operations at once, without waiting
    for each operation to finish. However, on the other hand, we will have to be somewhat
    organized to ensure that all inter-dependent operations are synchronized; this
    means that we shouldn't launch a particular kernel until its input data is fully
    copied to the device memory, or we shouldn't copy the output data of a launched
    kernel to the host until the kernel has finished execution.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we have what are known as **CUDA** **streams**—a **stream** is
    a sequence of operations that are run in order on the GPU. By itself, a single
    stream isn't of any use—the point is to gain concurrency over GPU operations issued
    by the host by using multiple streams. This means that we should interleave launches
    of GPU operations that correspond to different streams, in order to exploit this
    notion.
  prefs: []
  type: TYPE_NORMAL
- en: We will be covering this notion of streams extensively in this chapter. Additionally,
    we will look at **events**, which are a feature of streams that are used to precisely
    time kernels and indicate to the host as to what operations have been completed
    within a given stream.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will briefly look at CUDA **contexts**. A **context** can be thought
    of as analogous to a process in your operating system, in that the GPU keeps each
    context's data and kernel code *walled off *and encapsulated away from the other
    contexts currently existing on the GPU. We will see the basics of this near the
    end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the learning outcomes for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concepts of device and stream synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to effectively use streams to organize concurrent GPU operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to effectively use CUDA events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding CUDA contexts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to explicitly synchronize within a given context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to explicitly create and destroy a CUDA context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to use contexts to allow for GPU usage among multiple processes
    and threads on the host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter''s code is also available on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the prerequisites, check the *Preface* of this book,
    and for the software and hardware requirements, check the README in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: CUDA device synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can use CUDA streams, we need to understand the notion of **device
    synchronization**. This is an operation where the host blocks any further execution
    until all operations issued to the GPU (memory transfers and kernel executions)
    have completed. This is required to ensure that operations dependent on prior
    operations are not executed out-of-order—for example, to ensure that a CUDA kernel
    launch is completed before the host tries to read its output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In CUDA C, device synchronization is performed with the `cudaDeviceSynchronize`
    function. This function effectively blocks further execution on the host until
    all GPU operations have completed. `cudaDeviceSynchronize` is so fundamental that
    it is usually one of the very first topics covered in most books on CUDA C—we
    haven''t seen this yet, because PyCUDA has been invisibly calling this for us
    automatically as needed. Let''s take a look at an example of CUDA C code to see
    how this is done manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this block of code, we see that we have to synchronize with the device directly
    after every single GPU operation. If we only have a need to call a single CUDA
    kernel at a time, as seen here, this is fine. But if we want to concurrently launch
    multiple independent kernels and memory operations operating on different arrays
    of data, it would be inefficient to synchronize across the entire device. In this
    case, we should synchronize across multiple streams. We'll see how to do this
    right now.
  prefs: []
  type: TYPE_NORMAL
- en: Using the PyCUDA stream class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start with a simple PyCUDA program; all this will do is generate a series
    of random GPU arrays, process each array with a simple kernel, and copy the arrays
    back to the host. We will then modify this to use streams. Keep in mind this program
    will have no point at all, beyond illustrating how to use streams and some basic
    performance gains you can get. (This program can be seen in the `multi-kernel.py` file,
    under the `5` directory in the GitHub repository.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we''ll start by importing the appropriate Python modules, as well
    as the `time` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now will specify how many arrays we wish to process—here, each array will
    be processed by a different kernel launch. We also specify the length of the random
    arrays we will generate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a kernel that operates on each array; all this will do is iterate
    over each point in the array, and multiply and divide it by 2 for 50 times, ultimately
    leaving the array intact. We want to restrict the number of threads that each
    kernel launch will use, which will help us gain concurrency among many kernel
    launches on the GPU so that we will have each thread iterate over different parts
    of the array with a `for` loop. (Again, remember that this kernel function will
    be completely useless for anything other than for learning about streams and synchronization!)
    If each kernel launch uses too many threads, it will be harder to gain concurrency
    later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will generate some random data array, copy these arrays to the GPU,
    iteratively launch our kernel over each array across 64 threads, and then copy
    the output data back to the host and assert that the same with NumPy''s `allclose`
    function. We will time the duration of all operations from start to finish by
    using Python''s `time` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now prepared to run this program. I will run it right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e590fec8-0e98-4fce-bd4e-091871758825.png)'
  prefs: []
  type: TYPE_IMG
- en: So, it took almost three seconds for this program to complete. We will make
    a few simple modifications so that our program can use streams, and then see if
    we can get any performance gains (this can be seen in the `multi-kernel_streams.py`
    file in the repository).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we note that for each kernel launch we have a separate array of data
    that it processes, and these are stored in Python lists. We will have to create
    a separate stream object for each individual array/kernel launch pair, so let''s
    first add an empty list, entitled `streams`, that will hold our stream objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now generate a series of streams that we will use to organize the kernel
    launches. We can get a stream object from the `pycuda.driver` submodule with the
    `Stream` class. Since we''ve imported this submodule and aliased it as `drv`,
    we can fill up our list with new stream objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will have to first modify our memory operations that transfer data
    to the GPU. Consider the following steps for it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Look for the first loop that copies the arrays to the GPU with the `gpuarray.to_gpu`
    function. We will want to switch to the asynchronous and stream-friendly version
    of this function, `gpu_array.to_gpu_async`, instead. (We must now also specify
    which stream each memory operation should use with the `stream` parameter):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now launch our kernels. This is exactly as before, only we must specify
    what stream to use by using the `stream` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to pull our data off the GPU. We can do this by switching
    the `gpuarray get` function to `get_async`, and again using the `stream` parameter,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to run our stream-friendly modified program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1e11ea75-4947-459b-a915-363bdad7b241.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we have a triple-fold performance gain, which is not too bad considering
    the very few numbers of modifications we had to make. But before we move on, let's
    try to get a deeper understanding as to why this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the case of two CUDA kernel launches. We will also perform
    GPU memory operations corresponding to each kernel before and after we launch
    our kernels, for a total of six operations. We can visualize the operations happening
    on the GPU with respect to time with a graph as such—moving to the right on the
    *x*-axis corresponds to time duration, while the *y*-axis corresponds to operations
    being executed on the GPU at a particular time. This is depicted with the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9c272bdc-c6e1-4438-96ad-392af521175d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s not too hard to visualize why streams work so well in performance increase—since
    operations in a single stream are blocked until only all *necessary* prior operations
    are competed, we will gain concurrency among distinct GPU operations and make
    full use of our device. This can be seen by the large overlap of concurrent operations.
    We can visualize stream-based concurrency over time as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d01793e3-d5a6-4ff8-b3fc-f22c198c7962.png)'
  prefs: []
  type: TYPE_IMG
- en: Concurrent Conway's game of life using CUDA streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now see a more interesting application—we will modify the LIFE (Conway's
    *Game of Life*) simulation from the last chapter, so that we will have four independent
    windows of animation displayed concurrently. (It is suggested you look at this
    example from the last chapter, if you haven't yet.)
  prefs: []
  type: TYPE_NORMAL
- en: Let's get a copy of the old LIFE simulation from the last chapter in the repository,
    which should be under `conway_gpu.py` in the `4` directory. We will now modify
    this into our new CUDA-stream based concurrent LIFE simulation. (This new streams-based
    simulation that we will see in a moment is also available in the `conway_gpu_streams.py`
    file in this chapter's directory, `5`.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the main function at the end of the file. We will set a new variable
    that indicates how many concurrent animations we will display at once with `num_concurrent`
    (where `N` indicates the height/width of the simulation lattice, as before). We
    will set it to `4` here, but you can feel free to try other values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now need a collection of `num_concurrent` stream objects, and will
    also need to allocate a collection of input and output lattices on the GPU. We''ll
    of course just store these in lists and initialize the lattices as before. We
    will set up some empty lists and fill each with the appropriate objects over a
    loop, as such (notice how we set up a new initial state lattice on each iteration,
    send it to the GPU, and concatenate it to `lattices_gpu`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Since we're only doing this loop once during the startup of our program and
    the virtually all of the computational work will be in the animation loop, we
    really don't have to worry about actually using the streams we just immediately
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now set up the environment with Matplotlib using the subplots function;
    notice how we can set up multiple animation plots by setting the `ncols` parameter.
    We will have another list structure that will correspond to the images that are
    required for the animation updates in `imgs`. Notice how we can now set this up
    with `get_async` and the appropriate corresponding stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing to change in the main function is the penultimate line starting
    with `ani = animation.FuncAnimation`. Let''s modify the arguments to the `update_gpu`
    function to reflect the new lists we are using and add two more arguments, one
    to pass our `streams` list, plus a parameter to indicate how many concurrent animations
    there should be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now duly make the required modifications to the `update_gpu` function to
    take these extra parameters. Scroll up a bit in the file and modify the parameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`def update_gpu(frameNum, imgs, newLattices_gpu, lattices_gpu, N, streams,
    num_concurrent)`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to modify this function to iterate `num_concurrent` times and set
    each element of `imgs` as before, before finally returning the whole `imgs` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice the changes we made—each kernel is launched in the appropriate stream,
    while `get` has been switched to a `get_async` synchronized with the same stream.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last line in the loop copies GPU data from one device array to
    another without any re-allocation. Before, we could use the shorthand slicing
    operator `[:]` to directly copy the elements between the arrays without re-allocating
    any memory on the GPU; in this case, the slicing operator notation acts as an
    alias for the PyCUDA `set` function for GPU arrays. (`set`, of course, is the
    function that copies one GPU array to another of the same size, without any re-allocation.)
    Luckily, there is indeed a stream-synchronized also version of this function, `set_async`,
    but we need to use this specifically to call this function, explicitly specifying
    the array to copy and the stream to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now finished and ready to run this. Go to a Terminal and enter `python
    conway_gpu_streams.py` at the command line to enjoy the show:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/93d56393-5968-409d-bcab-d56330f6bc91.png)'
  prefs: []
  type: TYPE_IMG
- en: Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Events** are objects that exist *on the GPU*, whose purpose is to act as
    milestones or progress markers for a stream of operations. Events are generally
    used to provide measure time duration *on the device side* to precisely time operations;
    the measurements we have been doing so far have been with host-based Python profilers
    and standard Python library functions such as `time`. Additionally, events they
    can also be used to provide a status update for the host as to the state of a
    stream and what operations it has already completed, as well as for explicit stream-based
    synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an example that uses no explicit streams and uses events to
    measure only one single kernel launch. (If we don't explicitly use streams in
    our code, CUDA actually invisibly defines a default stream that all operations
    will be placed into).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use the same useless multiply/divide loop kernel and header as
    we did at the beginning of the chapter, and modify most of the following contents.
    We want a single kernel instance to run a long time for this example, so we will
    generate a huge array of random numbers for the kernel to process, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We now construct our events using the `pycuda.driver.Event` constructor (where,
    of course, `pycuda.driver` has been aliased as `drv` by our prior import statement).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create two event objects here, one for the start of the kernel launch,
    and the other for the end of the kernel launch, (We will always need *two* event
    objects to measure any single GPU operation, as we will see soon):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are about ready to launch our kernel, but first, we have to mark the `start_event`
    instance''s place in the stream of execution with the event record function. We
    launch the kernel and then mark the place of `end_event` in the stream of execution,
    and also with `record`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Events have a binary value that indicates whether they were reached or not
    yet, which is given by the function query. Let''s print a status update for both
    events, immediately after the kernel launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run this right now and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b7cd2aa1-cb0c-485a-939b-cf2d7fe35d1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Our goal here is to ultimately measure the time duration of our kernel execution,
    but the kernel hasn't even apparently launched yet. Kernels in PyCUDA have launched
    asynchronously (whether they exist in a specific stream or not), so we have to
    have to ensure that our host code is properly synchronized with the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `end_event` comes last, we can block further host code execution until
    the kernel completes by this event object''s synchronize function; this will ensure
    that the kernel has completed before any further lines of host code are executed.
    Let''s add a line a line of code to do this in the appropriate place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are ready to measure the execution time of the kernel; we do this
    with the event object''s `time_till` or `time_since `operations to compare to
    another event object to get the time between these two events in milliseconds.
    Let''s use the `time_till `operation of `start_event` on `end_event`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Time duration can be measured between two events that have already occurred
    on the GPU with the `time_till` and `time_since` functions. Note that these functions
    always return a value in terms of milliseconds!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try running our program again now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/29aadcbf-d395-487a-ac70-f3c422ae6f12.png)'
  prefs: []
  type: TYPE_IMG
- en: (This example is also available in the `simple_event_example.py` file in the
    repository.)
  prefs: []
  type: TYPE_NORMAL
- en: Events and streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now see how to use event objects with respect to streams; this will
    give us a highly intricate level of control over the flow of our various GPU operations,
    allowing us to know exactly how far each individual stream has progressed via
    the `query` function, and even allowing us to synchronize particular streams with
    the host while ignoring the other streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, though, we have to realize this—each stream has to have its own dedicated
    collection of event objects; multiple streams cannot share an event object. Let''s
    see what this means exactly by modifying the prior example, `multi_kernel_streams.py`.
    After the kernel definition, let''s add two additional empty lists—`start_events`
    and `end_events`. We will fill these lists up with event objects, which will correspond
    to each stream that we have. This will allow us to time one GPU operation in each
    stream, since every GPU operation requires two events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can time each kernel launch individually by modifying the second loop
    to use the record of the event at the beginning and end of the launch. Notice
    that here, since there are multiple streams, we have to input the appropriate
    stream as a parameter to each event object''s `record` function. Also, notice
    that we can capture the end events in a second loop; this will still allow us
    to capture kernel execution duration perfectly, without any delay in launching
    the subsequent kernels. Now consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''re going to extract the duration of each individual kernel launch.
    Let''s add a new empty list after the iterative assert check, and fill it with
    the duration by way of the `time_till` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now add two `print` statements at the very end, to tell us the mean
    and standard deviation of the kernel execution times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/40d38973-beee-4d08-8e0b-3847ae757c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: (This example is also available as `multi-kernel_events.py` in the repository.)
  prefs: []
  type: TYPE_NORMAL
- en: We see that there is a relatively low degree of standard deviation in kernel
    duration, which is good, considering each kernel processes the same amount of
    data over the same block and grid size—if there were a high degree of deviation,
    then that would mean that we were making highly uneven usage of the GPU in our
    kernel executions, and we would have to re-tune parameters to gain a greater level
    of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Contexts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A CUDA **context** is usually described as being analogous to a process in an
    operating system. Let's review what this means—a process is an instance of a single
    program running on a computer; all programs outside of the operating system kernel
    run in a process. Each process has its own set of instructions, variables, and
    allocated memory, and is, generally speaking, blind to the actions and memory
    of other processes. When a process ends, the operating system kernel performs
    a cleanup, ensuring that all memory that the process allocated has been de-allocated,
    and closing any files, network connections, or other resources the process has
    made use of. (Curious Linux users can view the processes running on their computer
    with the command-line `top` command, while Windows users can view them with the
    Windows Task Manager).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a process, a context is associated with a single host program that
    is using the GPU. A context holds in memory all CUDA kernels and allocated memory
    that is making use of and is blind to the kernels and memory of other currently
    existing contexts. When a context is destroyed (at the end of a GPU based program,
    for example), the GPU performs a cleanup of all code and allocated memory within
    the context, freeing resources up for other current and future contexts. The programs
    that we have been writing so far have all existed within a single context, so
    these operations and concepts have been invisible to us.
  prefs: []
  type: TYPE_NORMAL
- en: Let's also remember that a single program starts as a single process, but it
    can fork itself to run across multiple processes or threads. Analogously, a single
    CUDA host program can generate and use multiple CUDA contexts on the GPU. Usually,
    we will create a new context when we want to gain host-side concurrency when we
    fork new processes or threads of a host process. (It should be emphasized, however,
    that there is no exact one-to-one relation between host processes and CUDA contexts).
  prefs: []
  type: TYPE_NORMAL
- en: As in many other areas of life, we will start with a simple example. We will
    first see how to access a program's default context and synchronize across it.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing the current context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to see how to explicitly synchronize our device within a context
    from within Python as in CUDA C; this is actually one of the most fundamental
    skills to know in CUDA C, and is covered in the first or second chapters in most
    other books on the topic. So far, we have been able to avoid this topic, since
    PyCUDA has performed most synchronizations for us automatically with `pycuda.gpuarray`
    functions such as `to_gpu` or `get`; otherwise, synchronization was handled by
    streams in the case of the `to_gpu_async` or `get_async` functions, as we saw
    at the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will be humble and start by modifying the program we wrote in [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA, *which generates an image of the Mandelbrot set
    using explicit context synchronization. (This is available here as the file `gpu_mandelbrot0.py`
    under the `3` directory in the repository.)
  prefs: []
  type: TYPE_NORMAL
- en: We won't get any performance gains over our original Mandelbrot program here;
    the only point of this exercise is just to help us understand CUDA contexts and
    GPU synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the header, we, of course, see the `import pycuda.autoinit` line.
    We can access the current context object with `pycuda.autoinit.context`, and we
    can synchronize in our current context by calling the `pycuda.autoinit.context.synchronize()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s modify the `gpu_mandelbrot` function to handle explicit synchronization.
    The first GPU-related line we see is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now change this to be explicitly synchronized. We can copy to the GPU
    asynchronously with `to_gpu_async`, and then synchronize as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We then see the next line allocates memory on the GPU with the `gpuarray.empty`
    function. Memory allocation in CUDA is, by the nature of the GPU architecture,
    automatically synchronized; there is no *asynchronous* memory allocation equivalent
    here. Hence, we keep this line as it was before.
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocation in CUDA is always synchronized!
  prefs: []
  type: TYPE_NORMAL
- en: 'We now see the next two lines—our Mandelbrot kernel is launched with an invocation
    to `mandel_ker`, and we copy the contents of our Mandelbrot `gpuarray` object
    with an invocation to `get`. We synchronize after the kernel launch, switch `get`
    to `get_async`, and finally synchronize one last line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can now run this, and it will produce a Mandelbrot image to disk, exactly
    as in [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml), *Getting Started
    with PyCUDA.*
  prefs: []
  type: TYPE_NORMAL
- en: (This example is also available as `gpu_mandelbrot_context_sync.py` in the repository.)
  prefs: []
  type: TYPE_NORMAL
- en: Manual context creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have been importing `pycuda.autoinit` at the beginning of all of
    our PyCUDA programs; this effectively creates a context at the beginning of our
    program and has it destroyed at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try doing this manually. We will make a small program that just copies
    a small array to the GPU, copies it back to the host, prints the array, and exits.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we initialize CUDA with the `pycuda.driver.init` function, which is
    here aliased as `drv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we choose which GPU we wish to work with; this is necessary for the cases
    where one has more than one GPU. We can select a specific GPU with  `pycuda.driver.Device`;
    if you only have one GPU, as I do, you can access it with `pycuda.driver.Device(0)`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a new context on this device with `make_context`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a new context, this will automatically become the default
    context. Let''s copy an array into the GPU, copy it back to the host, and print
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are done. We can destroy the context by calling the `pop` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: That's it! We should always remember to destroy contexts that we explicitly
    created with `pop` before our program exists.
  prefs: []
  type: TYPE_NORMAL
- en: (This example can be seen in the `simple_context_create.py` file under this
    chapter's directory in the repository.)
  prefs: []
  type: TYPE_NORMAL
- en: Host-side multiprocessing and multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, we may seek to gain concurrency on the host side by using multiple
    processes or threads on the host's CPU. Let's make the distinction right now between
    a host-side operating system process and thread with a quick overview.
  prefs: []
  type: TYPE_NORMAL
- en: Every host-side program that exists outside the operating system kernel is executed
    as a process, and can also exist in multiple processes. A process has its own
    address space, as it runs concurrently with, and independently of, all other processes.
    A process is, generally speaking, blind to the actions of other processes, although
    multiple processes can communicate through sockets or pipes. In Linux and Unix,
    new processes are spawned with the fork system call.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a host-side thread exists within a single process, and multiple
    threads can also exist within a single process. Multiple threads in a single process
    run concurrently. All threads in the same process share the same address space
    within the process and have access to the same shared variables and data. Generally,
    resource locks are used for accessing data among multiple threads, so as to avoid
    race conditions. In compiled languages such as C, C++, or Fortran, multiple process
    threads are usually managed with the Pthreads or OpenMP APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Threads are much more lightweight than processes, and it is far faster for an
    operating system kernel to switch tasks between multiple threads in a single process,
    than to switch tasks between multiple processes. Normally, an operating system
    kernel will automatically execute different threads and processes on different
    CPU cores to establish true concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: A peculiarity of Python is that while it supports multi-threading through the
    `threading` module, all threads will execute on the same CPU core. This is due
    to technicalities of Python being an interpreted scripting language, and is related
    to Python's Global Identifier Lock (GIL). To achieve true multi-core concurrency
    on the host through Python, we, unfortunately, must spawn multiple processes with
    the `multiprocessing` module. (Unfortunately, the multiprocessing module is currently
    not fully functional under Windows, due to how Windows handles processes. Windows
    users will sadly have to stick to single-core multithreading here if they want
    to have any form of host-side concurrency.)
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how to use both threads in Python to use GPU based operations;
    Linux users should note that this can be easily extended to processes by switching
    references of `threading` to `multiprocessing`, and references to `Thread` to
    `Process`, as both modules look and act similarly. By the nature of PyCUDA, however,
    we will have to create a new CUDA context for every thread or process that we
    will use that will make use of the GPU. Let's see how to do this right now.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple contexts for host-side concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first briefly review how to create a single host thread in Python that
    can return a value to the host with a simple example. (This example can also be
    seen in the `single_thread_example.py` file under `5` in the repository.) We will
    do this by using the `Thread` class in the `threading` module to create a subclass
    of `Thread`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We now set up our constructor. We call the parent class''s constructor and
    set up an empty variable within the object that will be the return value from
    the thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We now set up the run function within our thread class, which is what will
    be executed when the thread is launched. We''ll just have it print a line and
    set the return value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We finally have to set up the join function. This will allow us to receive
    a return value from the thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are done setting up our thread class. Let''s start an instance of this
    class as the `NewThread` object, spawn the new thread by calling the `start` method,
    and then block execution and get the output from the host thread by calling `join`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9dc8f524-03ac-4f2c-a21c-8736c1feb1cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can expand this idea among multiple concurrent threads on the host to
    launch concurrent CUDA operations by way of multiple contexts and threading. We
    will now look at one last example. Let's re-use the pointless multiply/divide
    kernel from the beginning of this chapter and launch it within each thread that
    we spawn.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the imports. Since we are making explicit contexts, remember
    to remove `pycuda.autoinit` and add an import `threading` at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same array size as before, but this time we will have a direct
    correspondence between the number of the threads and the number of the arrays.
    Generally, we don''t want to spawn more than 20 or so threads on the host, so
    we will only go for `10` arrays. So, consider now the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will store our old kernel as a string object; since this can only be
    compiled within a context, we will have to compile this in each thread individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can begin setting up our class. We will make another subclass of `threading.Thread`
    as before, and set up the constructor to take one parameter as the input array.
    We will initialize an output variable with `None`, as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now write the `run` function. We choose our device, create a context
    on that device, compile our kernel, and extract the kernel function reference.
    Notice the use of the `self` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We now copy the array to the GPU, launch the kernel, and copy the output back
    to the host. We then destroy the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set up the join function. This will return `output_array` to the
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now done with our subclass. We will set up some empty lists to hold
    our random test data, thread objects, and thread output values, similar to before.
    We will then generate some random arrays to process and set up a list of kernel
    launcher threads that will operate on each corresponding array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now launch each thread object, and extract its output into the `gpu_out`
    list by using `join`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we just do a simple assert on the output arrays to ensure they are
    the same as the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This example can be seen in the `multi-kernel_multi-thread.py` file in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by learning about device synchronization and the importance
    of synchronization of operations on the GPU from the host; this allows dependent
    operations to allow antecedent operations to finish before proceeding. This concept
    has been hidden from us, as PyCUDA has been handling synchronization for us automatically
    up to this point. We then learned about CUDA streams, which allow for independent
    sequences of operations to execute on the GPU simultaneously without synchronizing
    across the entire GPU, which can give us a big performance boost; we then learned
    about CUDA events, which allow us to time individual CUDA kernels within a given
    stream, and to determine if a particular operation in a stream has occurred. Next,
    we learned about contexts, which are analogous to processes in a host operating
    system. We learned how to synchronize across an entire CUDA context explicitly
    and then saw how to create and destroy contexts. Finally, we saw how we can generate
    multiple contexts on the GPU, to allow for GPU usage among multiple threads or
    processes on the host.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the launch parameters for the kernel in the first example, our kernels were
    each launched over 64 threads. If we increase the number of threads to and beyond
    the number of cores in our GPU, how does this affect the performance of both the
    original to the stream version?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the CUDA C example that was given at the very beginning of this chapter,
    which illustrated the use of `cudaDeviceSynchronize`. Do you think it is possible
    to get some level of concurrency among multiple kernels without using streams
    and only using `cudaDeviceSynchronize`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are a Linux user, modify the last example that was given to operate over
    processes rather than threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the `multi-kernel_events.py` program; we said it is good that there
    was a low standard deviation of kernel execution durations. Why would it be bad
    if there were a high standard deviation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We only used 10 host-side threads in the last example. Name two reasons why
    we have to use a relatively small number of threads or processes for launching
    concurrent GPU operations on the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

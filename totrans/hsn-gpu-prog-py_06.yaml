- en: Debugging and Profiling Your CUDA Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will finally learn how to debug and profile our GPU code
    using several different methods and tools. While we can easily debug pure Python
    code using IDEs such as Spyder and PyCharm, we can't use these tools to debug
    the actual GPU code, remembering that the GPU code itself is written in CUDA-C
    with PyCUDA providing an interface. The first and easiest method for debugging
    a CUDA kernel is the usage of `printf` statements, which we can actually call
    directly in the middle of a CUDA kernel to print to the standard output. We will
    see how to use `printf` in the context of CUDA and how to apply it effectively for
    debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will fill in some of the gaps in our CUDA-C programming so that we
    can directly write CUDA programs within the NVIDIA Nsight IDE, which will allow
    us to make test cases in CUDA-C for some of the code we have been writing. We
    will take a look at how to compile CUDA-C programs, both from the command line
    with `nvcc` and also with the Nsight IDE. We will then see how to debug within
    Nsight and use Nsight to understand the CUDA lockstep property. Finally, we will
    have an overview of the NVIDIA command line and Visual Profilers for profiling
    our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning outcomes for this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `printf` effectively as a debugging tool for CUDA kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing complete CUDA-C programs outside of Python, especially for creating
    test cases for debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling CUDA-C programs on the command line with the `nvcc` compiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing and debugging CUDA programs with the NVIDIA Nsight IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the CUDA warp lockstep property and why we should avoid branch
    divergence within a single CUDA warp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to effectively use the NVIDIA command line and Visual Profilers for GPU
    code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the prerequisites, check the *Preface* of this book,
    and for the software and hardware requirements, check the README in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: Using printf from within CUDA kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It may come as a surprise, but we can actually print text to the standard output
    from directly within a CUDA kernel; not only that, each individual thread can
    print its own output. This will come in particularly handy when we are debugging
    our kernels, as we may need to monitor the values of particular variables or computations
    at particular points in our code and it will also free us from the shackles of
    using a debugger to go through step by step. Printing output from a CUDA kernel
    is done with none other than the most fundamental function in all of C/C++ programming,
    the function that most people will learn when they write their first `Hello world` program
    in C: `printf`. Of course, `printf` is the standard function that prints a string
    to the standard output, and is really the equivalent in the C programming language
    of Python's `print` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now briefly review how to use `printf` before we see how to use it in
    CUDA. The first thing to remember is that `printf` always takes a string as its
    first parameter; so printing "Hello world!" in C is done with `printf("Hello world!\n");`.
    (Of course, `\n` indicates "new line" or "return", which moves the output in the
    Terminal to the next line.) `printf` can also take a variable number of parameters
    in the case that we want to print any constants or variables from directly within
    C: if we want to print the `123` integers to the output, we do this with `printf("%d",
    123);` (where `%d` indicates that an integer follows the string.)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we use `%f`, `%e`, or `%g` to print floating-point values (where
    `%f` is the decimal notation, `%e` is the scientific notation, and `%g` is the
    shortest representation whether decimal or scientific). We can even print several
    values in a row, remembering to place these specifiers in the correct order: `printf("%d
    is a prime number, %f is close to pi, and %d is even.\n", 17, 3.14, 4);` will
    print "17 is a prime number, 3.14 is close to pi, and 4 is even." on the Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, nearly halfway through this book, we will finally embark on creating our
    first parallel `Hello world` program in CUDA! We start by importing the appropriate
    modules into Python and then write our kernel. We will start out by printing the
    thread and grid identification of each individual thread (we will only launch
    this in one-dimensional blocks and grids, so we only need the `x` values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's stop for a second and note that we wrote `\\n` rather than `\n`. This
    is due to the fact that the triple quote in Python itself will interpret `\n` as
    a "new line", so we have to indicate that we mean this literally by using a double
    backslash so as to pass the `\n` directly into the CUDA compiler.
  prefs: []
  type: TYPE_NORMAL
- en: We will now print some information about the block and grid dimensions, but
    we want to ensure that it is printed after every thread has already finished its
    initial `printf` command. We can do this by putting in `__syncthreads();` to ensure
    each individual thread will be synchronized after the first `printf` function
    is executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we only want to print the block and grid dimensions to the terminal only
    once; if we just place `printf` statements here, every single thread will print
    out the same information. We can do this by having only one specified thread print
    to the output; let''s go with the 0th thread of the 0th block, which is the only
    thread that is guaranteed to exist no matter the block and grid dimensionality
    we choose. We can do this with a C `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now print the dimensionality of our block and grid and close up the
    `if` statement, and that will be the end of our CUDA kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now extract the kernel and then launch it over a grid consisting of
    two blocks, where each block has five threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run this right now (this program is also available in `hello-world_gpu.py`
    under `6` in the repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2c945416-8c8a-4eae-8d57-ef1479dd2798.png)'
  prefs: []
  type: TYPE_IMG
- en: Using printf for debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go over an example to see how we can approach debugging a CUDA kernel
    with `printf` with an example before we move on. There is no exact science to
    this method, but it is a skill that can be learned through experience. We will
    start with a CUDA kernel that is for matrix-matrix multiplication, but that has
    several bugs in it. (The reader is encouraged to go through the code as we go
    along, which is available as the `broken_matrix_ker.py` file in the `6` directories
    within the repository.)
  prefs: []
  type: TYPE_NORMAL
- en: Let's briefly review matrix-matrix multiplication before we continue. Suppose
    we have two matrices ![](assets/0e03608a-7c0c-4629-b61b-ec90b15d0cf7.png), *A*
    and *B*, and we multiply these together to get another matrix, *C*, of the same
    size as follows: ![](assets/60101910-7794-4484-b69f-7ddaad3994db.png). We do this
    by iterating over all tuples ![](assets/f550491f-6b50-49dd-be38-817b363525b9.png) and
    setting the value of ![](assets/54c310e8-02c5-4bbc-b645-9f33465d4e77.png) to the
    dot product of the *i*^(th) row of *A* and the *j*^(th) column of *B*: ![](assets/615e6730-f5b3-48d2-9ac2-fb9a7dc535de.png).
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we set each *i, j* element in the output matrix *C* as follows:   ![](assets/8870cb22-35f8-4612-aa78-b1cc92fd38f9.png)
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we already wrote a kernel that is to perform matrix-matrix multiplication,
    which takes in two arrays representing the input matrices, an additional pre allocated
    float array that the output will be written to, and an integer that indicates
    the height and width of each matrix (we will assume that all matrices are the
    same size and square-shaped). These matrices are all to be represented as one-dimensional
    `float *` arrays in a row-wise one-dimensional layout. Furthermore, this will
    be implemented so that each CUDA thread will handle a single row/column tuple
    in the output matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We make a small test case and check it against the output of the matrix multiplication
    in CUDA, and it fails as an assertion check on two 4 x 4 matrices, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run this program right now, and unsurprisingly get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7fb29cbf-af3a-4fd1-a75a-cd05d3f56a44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now look at the CUDA C code, which consists of a kernel and a device
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to place `printf` invocations intelligently throughout our CUDA
    code so that we can monitor a number of appropriate values and variables in the
    kernel and device function; we should also be sure to print out the thread and
    block numbers alongside these values at every `printf` invocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start at the entry point of our kernel. We see two variables, `row`
    and `col`, so we should check these right away. Let''s put the following line
    right after we set them (since this is parallelized over two dimensions, we should
    print the *x* and *y* values of `threadIdx` and `blockIdx`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code again, we get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/579c6bb8-3bee-4788-b606-aff8d5d15812.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two things that are immediately salient: that there are repeated
    values for row and column tuples (every individual tuple should be represented
    only once), and that the row and column values never exceed two, when they both
    should reach three (since this unit test is using 4 x 4 matrices). This should
    indicate to us that we are calculating the row and column values wrongly; indeed,
    we are forgetting to multiply the `blockIdx` values by the `blockDim` values to
    find the objective row/column values. We fix this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the program again, though, we still get an assertion error. Let''s
    keep our original `printf` invocation in place, so we can monitor the values as
    we continue. We see that there is an invocation to a device function in the kernel, `rowcol_dot`,
    so we decide to look into there. Let''s first ensure that the variables are being
    passed into the device function correctly by putting this `printf` invocation
    at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run our program, even more lines will come out, however, we will see
    one that says—`threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 2, col is 0.` and
    yet another that says—`threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 0, col is
    2, N is 4`. By the `threadIdx` and `blockIdx` values, we see that this is the
    same thread in the same block, but with the `row` and `col` values reversed. Indeed,
    when we look at the invocation of the `rowcol_dot` device function, we see that
    `row` and `col` are indeed reversed from that in the declaration of the device
    function. We fix this, but when we run the program again, we get yet another assertion
    error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s place another `printf` invocation in the device function, within the
    `for` loop; this, of course, is the *dot product* that is to perform a dot product
    between rows of matrix `A` with columns of matrix `B`. We will check the values
    of the matrices we are multiplying, as well as `k`; we will also only look at
    the values of the very first thread, or else we will get an incoherent mess of
    an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the values of the `A` and `B` matrices that are set up for our
    unit tests before we continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b2580d2a-f34e-4d83-abbd-43ebc163aba7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that both matrices vary when we switch between columns but are constant
    when we change between rows. Therefore, by the nature of matrix multiplication,
    the values of matrix `A` should vary across `k` in our `for` loop, while the values
    of `B` should remain constant. Let''s run the program again and check the pertinent
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/47f4367a-3282-4718-9ec9-e818e0a16ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, it appears that we are not accessing the elements of the matrices in a
    correct way; remembering that these matrices are stored in a row-wise format,
    we modify the indices so that their values are accessed in the proper manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Running the program again will yield no assertion errors. Congratulations, you
    just debugged a CUDA kernel using the only `printf`!
  prefs: []
  type: TYPE_NORMAL
- en: Filling in the gaps with CUDA-C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now go through the very basics of how to write a full-on CUDA-C program.
    We'll start small and just translate the *fixed* version of the little matrix
    multiplication test program we just debugged in the last section to a pure CUDA-C
    program, which we will then compile from the command line with NVIDIA's `nvcc`
    compiler into a native Windows or Linux executable file (we will see how to use
    the Nsight IDE in the next section, so we will just be doing this with only a
    text editor and the command line for now). Again, the reader is encouraged to
    look at the code we are translating from Python as we go along, which is available
    as the `matrix_ker.py` file in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's open our favorite text editor and create a new file entitled `matrix_ker.cu`.
    The extension will indicate that this is a CUDA-C program, which can be compiled
    with the `nvcc` compiler.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA-C program and library source code filenames always use the `.cu` file extension.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start at the beginning—as Python uses the `import` keyword at the beginning
    of a program for libraries, we recall the C language uses `#include`. We will
    need to include a few import libraries before we continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's briefly think about what we need these for: `cuda_runtime.h` is the header
    file that has the declarations of all of the particular CUDA datatypes, functions,
    and structures that we will need for our program. We will need to include this
    for any pure CUDA-C program that we write. `stdio.h`, of course, gives us all
    of the standard I/O functions for the host such as `printf`, and we need `stdlib.h`
    for using the `malloc` and `free` dynamic memory allocation functions on the host.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to always put `#include <cuda_runtime.h>` at the beginning of every
    pure CUDA-C program!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, before we continue, we remember that we will ultimately have to check
    the output of our kernel with a correct known output, as we did with NumPy''s
    `allclose` function. Unfortunately, we don''t have a standard or easy-to-use numerical
    math library in C as Python has with NumPy. More often than not, it''s just easier
    to write your own equivalent function if it''s something simple, as in this case.
    This means that we will now explicitly have to make our own equivalent to NumPy''s
    `allclose`. We will do so as such: we will use the `#define` macro in C to set
    up a value called `_EPSILON`, which will act as a constant to indicate the minimum
    value between the output and expected output to be considered the same, and we
    will also set up a macro called `_ABS`, which will tell us the absolute difference
    between two numbers. We do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our own version of `allclose`. This will take in two float
    pointers and an integer value, `len`. We loop through both arrays and check them:
    if any points differ by more than `_EPSILON`, we return -1, otherwise we return
    0 to indicate that the two arrays do indeed match.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We note one thing: since we are using CUDA-C, we precede the definition of
    the function with `__host__`, to indicate that this function is intended to be
    run on the CPU rather than on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now can cut and paste the device and kernel functions exactly as they appear
    in our Python version here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Again, in contrast with `__host__`, notice that the CUDA device function is
    preceded by `__device__`, while the CUDA kernel is preceded by `__global__`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, as in any C program, we will need to write the `main` function, which
    will run on the host, where we will set up our test case and from which we explicitly
    launch our CUDA kernel onto GPU. Again, in contrast to vanilla C, we will have
    explicitly to specify that this is also to be run on the CPU with `__host__`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we will have to do is select and initialize our GPU. We do
    so with `cudaSetDevice` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`cudaSetDevice(0)` will select the default GPU. If you have multiple GPUs installed
    in your system, you can select and use them instead with `cudaSetDevice(1)`, `cudaSetDevice(2)`,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now set up `N` as in Python to indicate the height/width of our matrix.
    Since our test case will consist only of 4 x 4 matrices, we set it to `4`. Since
    we will be working with dynamically allocated arrays and pointers, we will also
    have to set up a value that will indicate the number of bytes our test matrices
    will require. The matrices will consist of *N* x *N* floats, and we can determine
    the number of bytes required by a float with the `sizeof` keyword in C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now set up our test matrices as such; these will correspond exactly to the
    `test_a` and `test_b` matrices that we saw in our Python test program (notice
    how we use the `h_` prefix to indicate that these arrays are stored on the host,
    rather than on the device):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We now set up another array, which will indicate the expected output of the
    matrix multiplication of the prior test matrices. We will have to calculate this
    explicitly and put these values into our C code. Ultimately, we will compare this
    to the GPU output at the end of the program, but let''s just set it up and get
    it out of the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We now declare some pointers for arrays that will live on the GPU, and for
    that we will copy the values of `h_A` and `h_B` and pointer to the GPU''s output.
    Notice how we just use standard float pointers for this. Also, notice the prefix
    `d_`— this is another standard CUDA-C convention that indicates that these will
    exist on the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will allocate some memory on the device for `d_A` and `d_B` with `cudaMalloc`,
    which is almost the same as `malloc` in C; this is what PyCUDA `gpuarray` functions
    such as `empty` or `to_gpu` have been calling us invisibly to allocate memory
    arrays on the GPU throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s think a bit about how this works: in C functions, we can get the address
    of a variable by preceding it with an ampersand (`&`); if you have an integer, `x`,
    we can get its address with `&x`. `&x` will be a pointer to an integer, so its
    type will be `int *`. We can use this to set values of parameters into a C function,
    rather than use only pure return values.'
  prefs: []
  type: TYPE_NORMAL
- en: Since `cudaMalloc` sets the pointer through a parameter rather than with the
    return value (in contrast to the regular `malloc`), we have to use the ampersand
    operator, which will be a pointer to a pointer, as it is a pointer to a float
    pointer as here (`float **`). We have to typecast this value explicitly with the
    parenthesis since `cudaMalloc` can allocate arrays of any type. Finally, in the
    second parameter, we have to indicate how many bytes to allocate on the GPU; we
    already set up `num_bytes` previously to be the number of bytes we will need to
    hold a 4 x 4 matrix consisting of floats, so we plug this in and continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now copy the values from `h_A` and `h_B` to `d_A` and `d_B` respectively
    with two invocations of the function `cudaMemcpy`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`cudaMemcpy` always takes a destination pointer as the first argument, a source
    pointer as the second, the number of bytes to copy as the third argument, and
    a final parameter. The last parameter will indicate if we are copying from the
    host to the GPU with `cudaMemcpyHostToDevice` , from the GPU to the host with
    `cudaMemcpyDeviceToHost`, or between two arrays on the GPU with `cudaMemcpyDeviceToDevice`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now allocate an array to hold the output of our matrix multiplication
    on the GPU with another invocation of `cudaMalloc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will have to have some memory set up on the host that will store
    the output of the GPU when we want to check the output of our kernel. Let''s set
    up a regular C float pointer and allocate memory with `malloc` as we would normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are almost ready to launch our kernel. CUDA uses a data structure called
    `dim3` to indicate block and grid sizes for kernel launches; we will set these
    up as such, since we want a grid with a dimension of 2 x 2 and blocks that are
    also of a dimension of 2 x 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to launch our kernel; we use the triple-triangle brackets
    to indicate to the CUDA-C compiler the block and grid sizes that the kernel should
    be launched over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, of course, before we can copy the output of the kernel back to the host,
    we have to ensure that the kernel has finished executing. We do this by calling
    `cudaDeviceSynchronize`, which will block the host from issuing any more commands
    to the GPU until the kernel has finished execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We now can copy the output of our kernel to the array we''ve allocated on the
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we synchronize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we check the output, we realize that we no longer need any of the arrays
    we allocated on the GPU. We free this memory by calling `cudaFree` on each array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re done with the GPU, so we call `cudaDeviceReset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we finally check the output we copied onto the host with the `allclose`
    function we wrote at the beginning of this chapter. If the actual output doesn''t
    match the expected output, we print an error and return `-1`, otherwise, we print
    that it does match and we return 0. We then put a closing bracket on our program''s
    `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we make one final invocation to the standard C free function since
    we have allocated memory to `h_output `, in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now save our file, and compile it into a Windows or Linux executable file
    from the command line with `nvcc matrix_ker.cu -o matrix_ker`. This should output
    a binary executable file, `matrix_ker.exe` (in Windows) or `matrix_ker` (in Linux).
    Let''s try compiling and running it right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ada8e5c8-9443-40ce-81d9-03100f570855.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations, you've just created your first pure CUDA-C program! (This example
    is available as `matrix_ker.cu` in the repository, under `7`.)
  prefs: []
  type: TYPE_NORMAL
- en: Using the Nsight IDE for CUDA-C development and debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now learn how to use the Nsight IDE for developing CUDA-C programs. We
    will see how to import the program we just wrote, and compile and debug it from
    within Nsight. Note that there are differences between the Windows and Linux versions
    of Nsight, since it is effectively a plugin of the Visual Studio IDE under Windows
    and in the Eclipse IDE under Linux. We will cover both in the following two subsections;
    feel free to skip whatever operating system does not apply to you here.
  prefs: []
  type: TYPE_NORMAL
- en: Using Nsight with Visual Studio in Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open up Visual Studio, and click on File, then choose New | Project.... A window
    will pop up where you set the type of project: choose the NVIDIA drop-down item,
    and then choose CUDA 9.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e5b3c450-388f-44ac-9d77-8f7cf574bf69.png)'
  prefs: []
  type: TYPE_IMG
- en: Give the project some appropriate name and then click OK. A project should appear
    in the solution explorer window with a simple premade CUDA test program, consisting
    of one source file, `kernel.cu`, which consists of a simple parallel add kernel
    with test code. If you want to see whether this compiles and runs, click the green
    right-pointing arrow at the top marked Local Windows Debugger. A Terminal should
    pop up with some text output from the kernel and then close immediately.
  prefs: []
  type: TYPE_NORMAL
- en: If you have problems with a Windows Terminal-based application closing after
    you run it from Visual Studio, try adding `getchar();` to the end of the main
    function, which will keep the Terminal open until you press a key. (Alternatively,
    you can also use a debugger breakpoint at the end of the program.)
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's add the CUDA-C program we just wrote. In the Solution Explorer window,
    right-click  `kernel.cu`, and click Remove on `kernel.cu`. Now, right-click on
    the project name, and choose Add, and then choose Existing item. We will now be
    able to select an existing file, so find where the path is to `matrix_ker.cu` and
    add it to the project. Click on the green arrow marked Local Windows Debugger
    at the top of the IDE and the program should compile and run, again in a Windows
    Terminal. So, that's it—we can set up and compile a complete CUDA program in Visual
    Studio now, just from those few steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see how to debug our CUDA kernel. Let''s start by adding one breakpoint
    to our code at the entry point of the kernel `matrix_mult_ker`, where we set the
    value of `row` and `col`. We can add this breakpoint by clicking on the gray column
    left of the line numbers on the window; a red dot should appear there for every
    breakpoint we add. (You can ignore any red squiggly lines that the Visual Studio
    editor may place under your code; this is due to the fact that CUDA is not a *native* language
    to Visual Studio):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b4408001-ab3d-42fb-9741-1e1a3c896491.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now start debugging. From the top menu, choose the Nsight drop-down menu
    and choose Start CUDA Debugging. There may be two options here, Start CUDA Debugging
    (Next-Gen) and Start CUDA Debugging (Legacy). It doesn't matter which one, but
    you may have issues with Next-Gen depending on your GPU; in that case, choose
    Legacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your program should start up, and the debugger should halt at the breakpoint
    in our kernel that we just set. Let''s press *F10* to step over the line, and
    now see if the `row` variable gets set correctly. Let''s look at the Locals window
    in the Variable Explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4bbfaa46-46cb-44cc-ac0b-055dc9ce89ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that we are currently in the very first thread in the very first
    block in the grid by checking the values of `threadIdx` and `blockIdx`; `row`
    is set to `0`, which does indeed correspond to the correct value. Now, let''s
    check the value of row for some different thread. To do this, we have to switch
    the **thread focus** in the IDE; we do this by clicking the Nsight drop-down menu
    above, then choosing Windows|CUDA Debug Focus.... A new menu should appear allowing
    you to choose a new thread and block. Change thread from 0, 0, 0 to 1, 0, 0 in
    the menu, and click OK:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c5f2b196-8ce2-45b6-807a-11f8d86fe86b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When you check the variables again, you should see the correct value is set
    for `row` for this thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f2f27821-5df5-4ea7-a847-385967ba54a0.png)'
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, that is how you debug with Nsight in Visual Studio. We now have
    the basics of how to debug a CUDA program from Nsight/Visual Studio in Windows,
    and we can use all of the regular conventions as we would for debugging a regular
    Windows program as with any other IDE (setting breakpoints, starting the debugger,
    continue/resume, step over, step in, and step out). Namely, the main difference
    is you have to know how to switch between CUDA threads and blocks to check variables,
    otherwise, it's pretty much the same.
  prefs: []
  type: TYPE_NORMAL
- en: Using Nsight with Eclipse in Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now see how to use Nsight in Linux. You can open Nsight from either
    your desktop by selecting it or you can run it from a command line with the `nsight` command.
    The Nsight IDE will open. From the top of the IDE, click on File, then choose
    New... from the drop-down menu, and from there choose New CUDA C/C++ Project.
    A new window will appear, and from here choose CUDA Runtime Project. Give the
    project some appropriate name, and then click Next. You''ll be prompted to give
    further settings options, but the defaults will work fine for our purposes for
    now. (Be sure to note where the source folder and project paths will be located
    in the third and fourth screens here.) You''ll get to a final screen, where you
    can press Finish to create the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e2f72961-a2fe-4c06-a4c2-2ab6653c140b.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, you'll end up at a project view with your new project and some placeholder
    code open; as of CUDA 9.2, this will consist of a reciprocal kernel example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now import our code. Either you can just use the editor in Nsight to
    delete all of the code in the default source file and cut and paste it in, or
    you can manually delete the file from the project''s source directory, manually
    copy the `matrix_ker.cu` file into the source directory, and then choose to refresh
    the source directory view in Nsight by selecting it and then pressing *F5*. You
    can now build the project with *Ctrl* + *B*, and run it with *F11*. The output
    of our program should appear within the IDE itself within the Console subwindow,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1e287f83-ed1a-416e-aca7-511b375c1568.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now set a breakpoint within our CUDA code; let's set it at the entry
    point of our kernel where the row value is set. We set the cursor onto that row
    in the Eclipse editor, and then press *Ctrl* + *Shift* + *B* to set it.
  prefs: []
  type: TYPE_NORMAL
- en: We can now begin debugging by pressing *F11* (or clicking the bug icon). The
    program should be paused at the very beginning of the `main` function, so press
    *F8* to *resume* to the first breakpoint. You should see the first line in our
    CUDA kernel highlighted with an arrow pointing to it in the IDE; let's step over
    the current line by pressing *F6*, which will ensure that the row has been set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can easily switch between different threads and blocks in our CUDA
    grid to check the current values that they hold as follows: from the top of the
    IDE, click on the Window drop-down menu, then click Show view, and then choose
    CUDA. A window with the currently running kernel should open, and from here you
    can see a list of all of the blocks that this kernel is running over.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the first one and from here you will be able to see all of the individual
    threads that are running within the block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/429490aa-3a80-48af-8833-9cc6a4988a7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can look at the variable corresponding to the very first thread in
    the very first block by clicking on the Variables tab—here, row should be 0, as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2d42ba20-c3bb-4f76-acc9-054fa1cc3960.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can check the values for a different thread by again going to the CUDA
    tab, choosing the appropriate thread, and switching back. Let''s stay in the same
    block, but choose thread (1, 0, 0) this time, and check the value of row again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ca99eb7a-04e5-40a0-982e-8a605b3cc697.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the value of row is now 1, as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: We now have the basics of how to debug a CUDA program from Nsight/Eclipse in
    Linux, and we can use all of the regular conventions as you would for debugging
    a regular Linux program as with any other IDE (setting breakpoints, starting the
    debugger, continue/resume, step over, step in, and step out). Namely, the main
    difference here is we have to know how to switch between CUDA threads and blocks
    to check variables, otherwise, it's pretty much the same.
  prefs: []
  type: TYPE_NORMAL
- en: Using Nsight to understand the warp lockstep property in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now use Nsight to step through some code to help us better understand
    some of the CUDA GPU architecture, and how **branching** within a kernel is handled.
    This will give us some insight about how to write more efficient CUDA kernels.
    By branching, we mean how the GPU handles control flow statements such as `if`,
    `else`, or `switch` within a CUDA kernel. In particular, we are interested in
    how **branch divergence** is handled within a kernel, which is what happens when
    one thread in a kernel satisfies the conditions to be an `if` statement, while
    another doesn''t and is an `else` statement: they are divergent because they are
    executing different pieces of code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a small CUDA-C program as an experiment: we will start with a
    small kernel that prints one output if its `threadIdx.x` value is even and another
    if it is odd. We then write a `main` function that will launch this kernel over
    one single block consisting of 32 different threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: (This code is also available as `divergence_test.cu` in the repository.)
  prefs: []
  type: TYPE_NORMAL
- en: If we compile and run this from the command line, we might naively expect there
    to be an interleaved sequence of strings from even and odd threads; or maybe they
    will be randomly interleaved—since all of the threads run concurrently and branch
    about the same time, this would make sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, every single time we run this, we always get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/38c855ca-4bd8-4ab6-9e6b-371a9c9c0bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: All of the strings corresponding to even threads are printed first, while all
    of the odd strings are printed second. Perhaps the Nsight debugger can shed some
    light on this; let's import this little program into an Nsight project as we did
    in the last section, putting a breakpoint at the first `if` statement in our kernel.
    We will then do a *step over*, so that the debugger stops where the first `printf`
    statement is. Since the default thread in Nsight is (0,0,0), this should have
    satisfied the first `if` statement so it will be stuck there until the debugger
    continues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s switch over to an odd thread, say (1,0,0), and see where it is in our
    program now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/58816167-07da-44ae-954a-4b6c3e911bbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Very strange! Thread (1,0,0) is also at the same place in execution as thread
    (0,0,0). Indeed, if we check every single other odd thread here, it will be stuck
    in the same place—at a `printf` statement that all of the odd threads should have
    skipped right past.
  prefs: []
  type: TYPE_NORMAL
- en: What gives? This is known as the **warp lockstep property**. A **warp** in the
    CUDA architecture is a unit of 32 "lanes" within which our GPU executes kernels
    and grids over, where each lane will execute a single thread. A major limitation
    of warps is that all threads executing on a single warp must step through the
    same exact code in **lockstep**; this means that not every thread does indeed
    run the same code, but just ignores steps that are not applicable to it. (This
    is called lockstep because it's like a group of soldiers marching *lockstep* in
    unison—whether they want to march, or not!)
  prefs: []
  type: TYPE_NORMAL
- en: The lockstep property implies that if one single thread running on a warp diverges
    from all 31 other threads in a single `if` statement, all 31 other threads have
    their execution delayed until this single anomalous thread finishes and returns
    from its solitary `if` divergence. This is a property that you should always keep
    in mind when writing kernels, and why branch divergence should be minimized as
    much as possible as a general rule in CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: Using the NVIDIA nvprof profiler and Visual Profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will end with a brief overview of the command-line Nvidia `nvprof` profiler.
    In contrast to the Nsight IDE, we can freely use any Python code that we have
    written—we won't be compelled here to write full-on, pure CUDA-C test function
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do a basic profiling of a binary executable program with the `nvprof
    program` command; we can likewise profile a Python script by using the `python`
    command as the first argument, and the script as the second as follows: `nvprof
    python program.py`. Let''s profile the simple matrix-multiplication CUDA-C executable
    program that we wrote earlier, with `nvprof matrix_ker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/41fb00d0-582b-4b22-81ea-b9ae1988ad84.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that this is very similar to the output of the Python cProfiler module
    that we first used to analyze a Mandelbrot algorithm way back in [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?*—only now, this exclusively tells us only about all of the
    CUDA operations that were executed. So, we can use this when we specifically want
    to optimize on the GPU, rather than concern ourselves with any of the Python or
    other commands that executed on the host. (We can further analyze each individual
    CUDA kernel operation with block and grid size launch parameters if we add the
    command-line option, `--print-gpu-trace`.)
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at one more trick to help us *visualize* the execution time of all
    of the operations of a program; we will use `nvprof` to dump a file that can then
    be read by the NVIDIA Visual Profiler, which will show this to us graphically.
    Let's do this using an example from the last chapter, `multi-kernel_streams.py`
    (this is available in the repository under `5`). Let's recall that this was one
    of our introductory examples to the idea of CUDA streams, which allow us to execute
    and organize multiple GPU operations concurrently. Let's dump the output to a
    file with the `.nvvp` file suffix with the `-o` command-line option as follows: `nvprof
    -o m.nvvp python multi-kernel_streams.py`. We can now load this file into the
    NVIDIA Visual Profiler with the `nvvp m.nvvp` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should see a timeline across all CUDA streams as such (remembering that
    the name of the kernel used in this program is called `mult_ker`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/37df8c10-9b93-4d86-bb74-ff00920e7470.png)'
  prefs: []
  type: TYPE_IMG
- en: Not only can we see all kernel launches, but also memory allocations, memory
    copies, and other operations. This can be useful for getting an intuitive and
    visual understanding of how your program is using your GPU over time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started out in this chapter by seeing how `printf` can be used within a
    CUDA kernel to output data from individual threads; we saw in particular how useful
    this can be for debugging code. We then covered some of the gaps in our knowledge
    in CUDA-C, so that we can write full test programs that we can compile into proper
    executable binary files: there is a lot of overhead here that was hidden from
    us before that we have to be meticulous about. Next, we saw how to create and
    compile a project in the Nsight IDE and how to use it for debugging. We saw how
    to stop at any breakpoint we set in a CUDA kernel and switch between individual
    threads to see the different local variables. We also used the Nsight debugger
    to learn about the warp lockstep property and why it is important to avoid branch
    divergence in CUDA kernels. Finally, we had a very brief overview of the NVIDIA
    command-line `nvprof` profiler and Visual Profiler for analyzing our GPU code.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first CUDA-C program that we wrote, we didn''t use a `cudaDeviceSynchronize`
    command after the calls we made to allocate memory arrays on the GPU with `cudaMalloc`.
    Why was this not necessary? (Hint: Review the last chapter.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we have a single kernel that is launched over a grid consisting of two
    blocks, where each block has 32 threads. Suppose all of the threads in the first
    block execute an `if` statement, while all of the threads in the second block
    execute the corresponding `else` statement. Will all of the threads in the second
    block have to "lockstep" through the commands in the `if` statement as the threads
    in the first block are actually executing them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What if we executed a similar piece of code, only over a grid consisting of
    one single block executed over 64 threads, where the first 32 threads execute
    an `if` and the second 32 execute an `else` statement?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can the `nvprof` profiler measure for us that Python's cProfiler cannot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name some contexts where we might prefer to use `printf` to debug a CUDA kernel
    and other contexts where it might be easier to use Nsight to debug a CUDA kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of the `cudaSetDevice` command in CUDA-C?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we have to use `cudaDeviceSynchronize` after every kernel launch or memory
    copy in CUDA-C?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

- en: The CUDA Device Function Libraries and Thrust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, looking at a fairly broad overview of the libraries that
    are available in CUDA through the Scikit-CUDA wrapper module. We will now look
    at a few other libraries that we will have to use directly from within CUDA C
    proper, without the assistance of wrappers like those in Scikit-CUDA. We will
    start by looking at two standard libraries that consist of device functions that
    we may invoke from any CUDA C kernel cuRAND and the CUDA Math API. By the end
    of learning how to use these libraries, we will know how to use these libraries
    in the context of Monte Carlo integration. Monte Carlo integration is a well-known
    randomized method that provides estimates for the values of definite integrals
    from calculus. We will first look at a basic example of how to implement a simple
    Monte Carlo method with cuRAND to do a basic estimate of the value of Pi (as in
    the well-known constant, π=3.14159...), and then we'll embark on a more ambitious
    project where we will construct a Python class that can perform definite integration
    on any arbitrary mathematical function, and use the Math API for creating such
    functions. We'll also look at how to effectively use some ideas from metaprogramming
    in our design of this class.
  prefs: []
  type: TYPE_NORMAL
- en: We will then take another look at writing some pure CUDA programs with the help
    of the Thrust C++ library. Thrust is a library that provides C++ template containers,
    similar to those in the C++ Standard Template Library (STL). This will enable
    us to manipulate CUDA C arrays from C++ in a more natural way that is closer to
    PyCUDA's `gpuarray` and the STL's vector container. This will save us from having
    to constantly use pointers, such as *mallocs* and *frees*, that plagued us before
    in CUDA C.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the purpose that a seed has in generating lists of pseudo-random
    numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cuRAND device functions for generating random numbers in a CUDA kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the concept of Monte Carlo integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using dictionary-based string formatting in Python for metaprogramming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the CUDA Math API device function library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding what a functor is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Thrust vector container when programming in pure CUDA C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's code is also available on GitHub, and can be found at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA.](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the prerequisites for this chapter, check the preface
    of this book. For the software and hardware requirements, check out the README
    at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: The cuRAND device function library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with cuRAND. This is a standard CUDA library that is used for generating
    pseudo-random values within a CUDA kernel on a thread-by-thread basis, which is
    initialized and invoked by calling device functions from each individual thread
    within a kernel. Let's emphasize again that this is a **pseudo-random** sequence
    of values—since the digital hardware is always deterministic and never random
    or arbitrary, we use algorithms to generate a sequence of apparently random values
    from an initial **seed value**. Usually, we can set the seed value to a truly
    random value (such as the clock time in milliseconds), which will yield us with
    a nicely arbitrary sequence of *random* values. These generated random values
    have no correlation with prior or future values in the sequence generated by the
    same seed, although there can be correlations and repeats when you combine values
    generated from different seeds. For this reason, you have to be careful that the
    values you wish to be mutually *random* are generated by the same seed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by looking at the function prototype for `curand_init`, which
    we will initialize with an appropriate seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, all of the inputs are unsigned long, which in C is an unsigned (non-negative
    valued) 64-bit integer. First, we can see the `seed`, which is, of course, the
    seed value. Generally speaking, you'll set this with the clock value or some variation.
    We then see a value called `sequence` and as we stated previously, values generated
    by cuRAND will only be truly mathematically mutually random if they are generated
    by the same seed value. So, if we have multiple threads using the same seed value,
    we use `sequence` to indicate which sub-sequence of random numbers of length 2^(190 )for
    the current thread to use, while we use `offset` to indicate at which point to
    start within this sub-sequence; this will generate values in each thread that
    are all mathematically mutually random with no correlation. Finally, the last
    parameter is for a pointer to a `curandState_t` object; this keeps track of where
    we are in the sequence of pseudo-random numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you initialize a class object, you will then generate random values from
    the appropriate random distribution by calling the appropriate device function.
    The two most common distributions are uniform and normal (Gaussian). A uniform
    distribution (`curand_uniform`, in cuRAND) is a function that outputs values that
    are all equally probable over a given range: that is to say, for a uniform distribution
    over 0 to 1, there is a 10% chance that a value will fall between 0 and 0.1, or
    between 0.9 to 1, or between any two points that are spaced .1 away from each
    other. The normal distribution (`curand_normal`, in cuRAND) has values that are
    centered at a particular mean, which will be distributed according to the well-known
    bell-shaped curve that is defined by the distribution''s standard deviation. (The
    default mean of `curand_normal` is `0` and the standard deviation is 1 in cuRAND,
    so this will have to be shifted and scaled manually for other values.) Another
    well-known distribution supported by cuRAND is the Poisson distribution (`curand_poisson`),
    which is used for modeling the occurrences of random events over time.'
  prefs: []
  type: TYPE_NORMAL
- en: We will be primarily looking at how to use cuRAND in the context of uniform
    distributions in the next section, due to their applicability to Monte Carlo integration.
    Readers interested in learning how to use more features in cuRAND are encouraged
    to look at the official documentation from NVIDIA.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating π with Monte Carlo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will apply our new knowledge of cuRAND to perform an estimate of the
    well-known mathematical constant π, or Pi, which is, of course, the never-ending
    irrational number 3.14159265358979...
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an estimate, though, we need to take a moment to think about what this
    means. Let''s think about a circle. Remember that the radius of a circle is the
    length from the center of the circle to any point in the circle; usually, this
    is designated with *R*. The diameter is defined as *D = 2R*, and the circumference
    *C* is the length around the circle. Pi is then defined as *π = C / D* . We can
    use Euclidean geometry to find a formula for the area of the circle, which turns
    out being *A = πR²* . Now, let''s think about a circle with radius *R* being circumscribed
    in a square with all sides of length *2R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/310fec25-8742-4878-a6e6-e5d917ef29bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, of course, we know that the area of the square is *(2R)² = 4R²*. Let''s
    consider *R=1*, so that we have known that the area of the circle is exactly π,
    while the area of the square is exactly 4\. Let''s make a further assumption and
    state that both the circle and square are centered at (0,0) in the Cartesian plane.
    Now, let''s take a completely random value within the square, (*x,y*), and see
    if it falls within the circle. How can we do this? By applying the Pythagorean
    formula: we do this by checking whether *x² + y²* is less than or equal to 1\.
    Let''s designate the total number of random points we choose with *iters*, and
    the number of hits with *hits*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a little bit more thinking about this: the probability of picking
    a point within the circle should be proportionate to the area of the circle divided
    by the area of the rectangle; here, this is π / 4\. However, if we choose a very
    large value of random points, notice that we will get the following approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/36f08e8a-fac6-413e-9276-a44a18fba9a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is exactly how we will estimate π! The number of iterations we will have
    to do will be very high before we can come up with a decent estimate of Pi, but
    notice how nicely parallelizable this is: we can check the "hits" in different
    threads, splitting the total number of iterations among different threads. At
    the end of the day, we can just sum up the total number of hits among all of the
    threads to get our estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin to write a program to make our Monte Carlo estimate. Let''s
    first import the usual Python modules that we will need for a PyCUDA program,
    with one addition from SymPy:'
  prefs: []
  type: TYPE_NORMAL
- en: SymPy is used for perfect *symbolic* computations that are to be made in Python
    so that when we have very large integers, we can use the `Rational` function to
    make a much more accurate floating-point estimate of a division.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to do something a little different than normal when we build our
    kernel: we need to set the option `no_extern_c=True` in `SourceModule`. This modifies
    how the code is compiled so that our code can properly link with C++ code, as
    required by the cuRAND library. We then begin writing our kernel and include the
    appropriate header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s include a macro for the Pythagorean distance. Since we are just
    checking if this value is equal to or below `1`, we can, therefore, omit the square
    root. We will be using a lot of unsigned 64-bit integers, so let''s make another
    macro to save us from typing `unsigned long long` over and over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now set up our kernel. By the nature of PyCUDA, this will have to be
    compiled to the interface as a bonafide C function rather than as a C++ function.
    We do this with an `extern "C"` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define our kernel. We will have two parameters: one for `iters`,
    which is the total number of iterations for each thread, and another for an array
    that will hold the total number of hits for each thread. We will need a `curandState`
    object for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s hold the global thread ID in an integer called `tid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`clock()` is a device function that outputs the current time down to the millisecond.
    We can add `tid` to the output of `clock()` to get a unique seed for each thread.
    We don''t need to use different subsequences or offsets, so let''s set them both
    to 0\. We will also carefully typecast everything here to 64-bit unsigned integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up the `x` and `y` values to hold a random point in the rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then iterate `iters` times to see how many hits in the circle we get.
    We generate these with `curand_uniform(&cr_state)`. Notice that we can generate
    them over 0 to 1, rather than from -1 to 1, since the squaring of these in the
    `_PYTHAG` macro will remove any negative values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now end and close off our kernel, as well as the `extern "C"` block
    with another final `}` bracket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s get the Python wrapper function to our kernel with `get_function`.
    We will also set up the block and grid sizes: 32 threads per block, and 512 blocks
    per grid. Let''s calculate the total number of threads and set up an array on
    the GPU to hold all of the hits (initialized to 0s, of course):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up the total number of iterations per thread to 2^(24):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now launch the kernel as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s sum over the number of hits in the array, which gives us the total
    number of hits. Let''s also calculate the total number of iterations among all
    of the threads in the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now make our estimate with `Rational`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now convert this into a floating point value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check our estimate against NumPy''s constant value, `numpy.pi`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now done. Let''s run this from IPython and check it out (This program is
    also available as the `monte_carlo_pi.py` file under `Chapter08` in this book''s
    repository.):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5d4875e3-6231-4fc4-a2e2-3004f3933636.png)'
  prefs: []
  type: TYPE_IMG
- en: The CUDA Math API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will take a look at the **CUDA Math API**. This is a library that consists
    of device functions similar to those in the standard C `math.h` library that can
    be called from individual threads in a kernel. One difference here is that single
    and double valued floating-point operations are overloaded, so if we use `sin(x)`
    where `x` is a float, the sin function will yield a 32-bit float as the output,
    while if `x` were a 64-bit double, then the output of `sin` would also be a 64-bit
    value (Usually, this is the proper name for a 32-bit function, but it has an `f` at
    the end, such as `sinf`). There are also additional **instrinsic** functions.
    Intrinsic functions are less accurate but faster math functions that are built
    into the NVIDIA CUDA hardware; generally, they have similar names to the original
    function, except that they are preceded with two underscores—therefore, the intrinsic,
    32-bit sin function is `__sinf`.
  prefs: []
  type: TYPE_NORMAL
- en: A brief review of definite integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we''re going to use some object-oriented programming in Python to set
    up a class that we can use to evaluate definite integrals of functions using a
    Monte Carlo method. Let''s stop for a moment and talk about what we mean: suppose
    we have a mathematical function (as in the type you might see in a calculus class)
    that we call *f(x)*. When we graph this out on the Cartesian plane between points
    *a* and *b*, it may look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/35cc22f9-0a21-45ae-89b9-367edd83aa59.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's review exactly what definite integration means—let's denote the first
    gray area in this graph as *I*, the second gray area as *II*, and the third gray
    area as *III*. Notice that the second gray area here is below zero. The definite
    integral of *f* here, from *a* to *b,* will be the value *I - II + III*, and we
    will denote this mathematically as  ![](assets/6fbf2855-0340-4589-9e3e-a007a7276e06.png). In
    general, the definite integral from *a* to *b* is just the sum of all of the total
    "positive" area bounded by the *f* function and x-axis with y > 0 between *a*
    and *b*, minus all of the "negative" area bounded by the *f* function and the
    x-axis with y < 0 between *a* and *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to calculate or estimate the definite integral of a function
    between two points. One that you may have seen in a calculus class is to find
    a closed-form solution: find the anti-derivative of *f*, *F*, and calculate *F(b)
    - F(a)*. In many areas, though, we won''t be able to find an exact anti-derivative,
    and we will have to determine the definite integral numerically. This is exactly
    the idea behind Monte Carlo integration: we evaluate *f* at many, many random
    points between *a* and *b*, and then use those to make an estimate of the definite
    integral.'
  prefs: []
  type: TYPE_NORMAL
- en: Computing definite integrals with the Monte Carlo method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now going to use the CUDA Math API for representing an arbitrary mathematical
    function, *f*, while using the cuRAND library to implement the Monte Carlo integral.
    We will do this with **metaprogramming**: we will use Python to generate the code
    for a device function from a code template, which will plug into an appropriate
    Monte Carlo kernel for integration.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is that it will look and act similarly to some of the metaprogramming
    tools we've seen with PyCUDA, such as `ElementwiseKernel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the appropriate modules into our new project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to use a trick in Python called **dictionary based string formatting**.
    Let''s go over this for a minute before we continue. Suppose we are writing a
    chunk of CUDA C code, and we are unsure of whether we want a particular collection
    of variables to be float or double; perhaps it looks like this: `code_string="float
    x, y; float * z;"`. We might actually want to format the code so that we can switch
    between floats and doubles on the fly. Let''s change all references from `float` in
    the string to `%(precision)s`—`code_string="%(precision)s x, y; %(precision)s
    * z;"`. We can now set up an appropriate dictionary that will swap `%(presision)s` with
    `double`, which is, `code_dict = {''precision'' : ''double''}`, and get the new
    double string with `code_double = code_string % code_dict`. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/508d1ba2-6545-46ad-b443-f494ecbeba3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's think for a moment about how we want our new Monte Carlo integrator
    to work. We will also have it take a string that is a math equation that is written
    using the CUDA Math API to define the function we want to integrate. We can then
    fit this string into the code using the dictionary trick we just learned, and
    use this to integrate arbitrary functions. We will also use the template to switch
    between `float` and `double` precision, as per the user's discretion.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin our CUDA C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will keep the unsigned 64-bit integer macro from before, `ULL`. Let''s define
    some new macros for a reciprocal of x (`_R`), and for squaring (`_P2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define a device function that our equation string will plug into.
    We will use the `math_function` value when we have to swap the text from a dictionary.
    We will have another value called `p`, for precision (which will either be a `float`
    or `double`). We''ll call this device function `f`. We''ll put an `inline` in
    the declaration of the function, which will save us a little time from branching
    when this is called from the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's think about how this will work— We declare a 32 or 64-bit floating
    point value called `y`, call `math_function`, and then return `y`. `math_function`, which
    will only make sense if it's some code that acts on the input parameter `x` and
    sets some value to `y`, such as `y = sin(x)`. Let's keep this in mind and continue.
  prefs: []
  type: TYPE_NORMAL
- en: We will now begin writing our Monte Carlo integration kernel. Let's remember
    that we have to make our CUDA kernel visible from plain C with the `extern "C"` keyword.
    We will then set up our kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will indicate how many random samples each thread in the kernel should
    take with `iters`; we then indicate the lower bound of integration (*b*) with
    `lo` and the upper bound (*a*) with `hi`, and pass in an array, `ys_out`, to store
    the collection of partial integrals for each thread (we will later sum over `ys_out`
    to get the value of the complete definite integral from `lo` to `hi` on the host
    side). Again, notice how we are referring to the precision as `p`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need a `curandState` object for generating random values. We will also
    need to find the global thread ID and the total number of threads. Since we are
    working with a one-dimensional mathematical function, it makes sense to set up
    our block and grid parameters in one dimension, `x`, as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now calculate the amount of area there is between `lo` and `hi` that
    a single thread will process. We''ll do this by dividing up the entire length
    of the integration (which will be `hi - lo`) by the total number of threads.:'
  prefs: []
  type: TYPE_NORMAL
- en: Again, note how we are using templating tricks so that this value can be multi-precision.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that we have a parameter called `iters`; this indicates how many random
    values each thread will sample. We need to know what the density of the samples
    is in a little bit; that is, the average number of samples per unit distance.
    We calculate it like so, remembering to typecast the integer `iters` into a floating-point
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that we are dividing the area we are integrating over by the number
    of threads. This means that each thread will have its own start and end point.
    Since we are dividing up the lengths fairly for each thread, we calculate this
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now initialize cuRAND like we did previously, making sure that each
    thread is generating random values from its own individual seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start sampling, we will need to set up some additional floating point
    values. `y` will hold the final value for the integral estimate from `t_lo` to
    `t_hi`, and `y_sum` will hold the sum of all of the sampled values. We will also
    use the `rand_val` variable to hold the raw random value we generate, and `x`
    to store the scaled random value from the area that we will be sampling from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s loop to the sample values from our function, adding the values
    into `y_sum`. The one salient thing to notice is the `%(p_curand)`s at the end
    of `curand_uniform—`the 32-bit floating point version of this function is `curand_uniform`,
    while the 64-bit version is `curand_uniform_double`. We will have to swap this
    with either `_double` or an empty string later, depending on what level of precision
    we go with here. Also, notice how we scale `rand_val` so that `x` falls between
    `t_lo` and `t_hi`, remembering that random uniform distributions in cuRAND only
    yields values between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now calculate the value of the subintegral from `t_lo` to `t_hi` by
    dividing `y_sum` by density:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We output this value into the array and close off our CUDA kernel, as well
    as the `extern "C"`, with the final closing bracket. We''re done writing CUDA
    C, so we will close off this section with a triple-quote:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We will now do something a little different—we're going to set up a class to
    handle our definite integrals. Let's call it `MonteCarloIntegrator`. We will start,
    of course, by writing the constructor, that is, the `__init__` function. This
    is where we will input the object reference, `self`. Let's set up the default
    value for `math_function` to be `'y = sin(x)'`, with the default precision as
    `'d'`, for double. We'll also set the default value for `lo` as 0 and `hi` as
    the NumPy approximation of π . Finally, we'll have values for the number of random
    samples each thread will take (`samples_per_thread`), and the grid size that we
    will launch our kernel over (`num_blocks`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start this function by storing the text string `math_function` within
    the `self` object for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s set up the values related to our choice of floating-point precision
    that we will need for later, particularly for setting up our template dictionary.
    We will also store the `lo` and `hi` values within the object. Let''s also be
    sure to raise exception errors if the user inputs an invalid datatype, or if `hi`
    is actually smaller than `lo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now set up our code template dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now generate the actual final code using dictionary-based string formatting,
    and compile. Let''s also turn off warnings from the `nvcc` compiler by setting
    `options=[''-w'']` in `SourceModule`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now set up a function reference in our object to our compiled kernel
    with `get_function`. Let''s save the remaining two parameters within our object
    before we continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, while we will need different instantiations of `MonteCarloIntegrator` objects
    to evaluate definite integrals of different mathematical functions or floating
    point precision, we might want to evaluate the same integral over different `lo`
    and `hi` bounds, change the number of threads/grid size, or alter the number of
    samples we take at each thread. Thankfully, these are easy alterations to make,
    and can all be made at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll set up a specific function for evaluating the integral of a given object.
    We will set the default values of these parameters to be those that we stored
    during the call to the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can finish this function off by setting up an empty array to store the partial
    sub-integrals and launching the kernel. We then need to sum over the sub-integrals
    to get the final value, which we return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to try this out. Let''s just set up a class with the default values—this
    will integrate `y = sin(x)` from 0 to π. If you remember calculus, the anti-derivative
    of *sin(x)* is *-cos(x)*, so we can evaluate the definite integral like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0f237e46-f4f9-459c-a1db-29766ce488b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we should get a numerical value close to 2\. Let''s see what we
    get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/49e7d08f-4958-4e3e-96f6-2168b71e628a.png)'
  prefs: []
  type: TYPE_IMG
- en: Writing some test cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will finally get to see how to use the CUDA Math API to write some test
    cases for our class by way of the `math_function` parameter. These will be fairly
    straightforward if you have any experience with the C/C++ standard math library.
    Again, these functions are overloaded so that we don't have to change the names
    of anything when we switch between single and double precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already seen one example, namely *y = sin(x)*. Let''s try something
    a little more ambitious:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/94a6e5b9-688f-4fee-9cde-34949e6f0386.png)'
  prefs: []
  type: TYPE_IMG
- en: We will integrate this function from *a=*11.733 to *b=*18.472, and then check
    the output of our Monte Carlo integrator against the known value of this integral
    from another source. Here, Mathematica indicates that the value of this definite
    integral is 8.9999, so we will check against that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s think of how to represent this function: here, *log* refers to
    the base-*e* logarithm (also known as *ln*), and this is just `log(x)` in the
    Math API. We already set up a macro for squaring, so we can represent *sin²(x)*
    as `_P2(sin(x))`. We can now represent the entire function with `y = log(x)*_P2(sin(x))`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the following equation, integrating from *a=.9* to *b=4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/41e43277-2c80-488e-b8ab-78355f6c0c53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remembering that `_R` is the macro we set up for a reciprocal, we can write
    the function with the Math API like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Before we move on, let's note that Mathematica tells us that the value of this
    definite integral is .584977.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check on one more function. Let''s be a little ambitious and say that
    it''s this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b33f7b41-e613-4e37-96ac-9b12a223a7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: We can represent this as `'y = (cosh(x)*sin(x))/ sqrt( pow(x,3) + _P2(sin(x)))'`;
    naturally `sqrt` is the square root in the denominator, and `pow` allows us to
    take a value of arbitrary power. Of course, `sin(x)` is *sin(x)* and `cosh(x)`
    is *cosh(x)*. We integrate this from *a*=1.85 to *b*=4.81; Mathematica tells us
    that the true value of this integral is -3.34553.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to check some test cases and verify that our Monte Carlo integral
    is working! Let''s iterate over a list, whose first value is a string indicating
    the function (using the Math API), the second value indicates the lower bound
    of integration, the third indicates the upper bound of integration, and the last
    value indicates the expected value that was calculated with Mathematica:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now iterate over this list and see how well our algorithm works compared
    to Mathematica:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run this right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b788faff-a6b9-4c86-ad21-26edd0e614e6.png)'
  prefs: []
  type: TYPE_IMG
- en: This is also available as the `monte_carlo_integrator.py` file under the `Chapter08`
    directory in this book's repository.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA Thrust library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at the CUDA Thrust Library. This library's central feature
    is a high-level vector container that is similar C++'s own vector container. While
    this may sound trivial, this will allow us to program in CUDA C with less reliance
    on pointers, mallocs, and frees. Like the C++ vector container, Thrust's vector
    container handles the resizing and concatenation of elements automatically, and
    with the magic of C++ destructors, *freeing* is also handled automatically when
    a Thrust vector object goes out of scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thrust actually provides two vector containers: one for the host-side, and
    one for the device-side. The host-side Thrust vector is more or less identical
    to the STL vector, with the main difference being that it can interact more easily
    with the GPU. Let''s write a little bit of code in proper CUDA C to get a feel
    for how this works.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the include statements. We''ll be using the headers for both
    the host and device side vectors, and we''ll also include the C++ `iostream` library,
    which will allow us to perform basic I/O operations on the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s just use the standard C++ namespace (this is so that we don''t have
    to type in the `std::` resolution operator when checking the output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now make our main function and set up an empty Thrust vector on the
    host side. Again, these are C++ templates, so we have to choose the datatype upon
    declaration with the `< >` brackets. We will set this up to be an array of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s append some integers to the end of `v` by using `push_back`, exactly
    how we would do so with a regular STL vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now iterate through all of the values in the vector, and output each
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: The output here should be `v[0] == 1` through `v[3] == 4`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This may have seemed trivial so far. Let''s set up a Thrust vector on the GPU
    and then copy the contents from `v`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Yes, that's all—only one line, and we're done. All of the content of `v` on
    the host will now be copied to `v_gpu` on the device! (If this doesn't amaze you,
    please take another look at [Chapter 6](6d1c808f-1dc2-4454-b0b8-d0a36bc3c908.xhtml),
    *Debugging and Profiling Your CUDA Code*, and think about how many lines this
    would have taken us before.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try using `push_back` on our new GPU vector, and see if we can concatenate
    another value to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now check the contents of `v_gpu`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This part should output `v_gpu[0] == 1` through `v_gpu[4] == 5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, thanks to the destructors of these objects, we don''t have to do any
    cleanup in the form of freeing any chunks of allocated memory. We can now just
    return from the program, and we are done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Using functors in Thrust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can use a concept known as **functors** in Thrust. In C++,
    a **functor** is a class or struct object that looks and acts like a function;
    this lets us use something that looks and acts like a function, but can hold some
    parameters that don't have to be set every time it is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start a new Thrust program with the appropriate include statements,
    and use the standard namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s set up a basic functor. We will use a `struct` to represent this,
    rather than `class`. This will be a weighted multiplication function, and we will
    store the weight in a float called `w`. We will make a constructor that sets up
    the weight with a default value of `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now set up our functor with the `operator()` keyword; this will indicate
    to the compiler to treat the following block of code as the `default` function
    for objects of this type. Remember that this will be running on the GPU as a device
    function, so we precede the whole thing with `__device__`. We indicate the inputs
    with parentheses and output the appropriate value, which is just a scaled multiple.
    Now, we can close off the definition of our struct with `};`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use this to make a basic dot product function; recall that this
    requires a pointwise multiplication between two arrays, followed by a `reduce` type
    sum. Let''s start by declaring our function and creating a new vector, `z`, that
    will hold the values of the point-wise multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use Thrust''s `transform` operation, which will act on the inputs
    of `v` and `w` point-wise, and output into `z`. Notice how we input the functor
    into the last slot of transform; by using the plain closed parentheses like so,
    it will use the default value of the constructor (w = 1) so that this will act
    as a normal, non-weighted/scaled dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now sum over `z` with Thrust''s reduce function. Let''s just return
    the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We're done. Now, let's write some test code—we'll just take the dot product
    of the vectors `[1,2,3]` and `[1,1,1]`, which will be easy for us to check. (This
    will be 6.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just set up the first vector, `v`, using `push_back`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now declare a vector, `w`, to be of size `3`, and we can set its default
    values to `1` using Thrust''s fill function, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do a check to make sure that our values are set correctly by outputting
    their values to `cout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check the output of our dot product, and then return from the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile this (from the command line in both Linux or Windows by using
    `nvcc thrust_dot_product.cu -o thrust_dot_product`) and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/03884f6c-d05f-4127-a8b1-3f66dd0cbbf9.png)'
  prefs: []
  type: TYPE_IMG
- en: The code for this is also available in the `thrust_dot_product.cu` file in the
    `Chapter08` directory in this book's repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to initialize a stream of random numbers in
    cuRAND by choosing the appropriate seed. Since computers are deterministic devices,
    they can only generate lists of pseudo-random numbers, so our seed should be something
    truly random; generally, adding a thread ID to the clock time in milliseconds
    will work well enough for most purposes.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how we can use the uniform distribution from cuRAND to do
    a basic estimate of Pi. Then we took on a more ambitious project of creating a
    Python class that can compute definite integrals of arbitrary functions; we used
    some ideas from metaprogramming coupled with the CUDA Math API to define these
    `arbitrary` functions. Finally, we had a brief overview of the CUDA Thrust library,
    which is generally used for writing pure CUDA C programs outside of Python. Thrust
    most notably provides a `device_vector` container that is similar to the standard
    C++ `vector`. This reduces some of the cognitive overhead from using pointers
    in CUDA C.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at a brief example of how to use Thrust with an appropriate
    functor to do simple `point-wise` and `reduce` operations, in the form of the
    implementation of a simple dot product function.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try rewriting the Monte Carlo integration examples (in the `__main__` function
    in `monte_carlo_integrator.py`) to use the CUDA `instrinsic` functions. How does
    the accuracy compare to before?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We only used the uniform distribution in all of our cuRAND examples. Can you
    name one possible use or application of using the normal (Gaussian) random distribution
    in GPU programming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose that we use two different seeds to generate a list of 100 pseudo-random
    numbers. Should we ever concatenate these into a list of 200 numbers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the last example, try adding `__host__` before `__device__` in the definition
    of our `operator()` function in the `multiply_functor` struct. Now, see if you
    can directly implement a host-side dot-product function using this functor without
    any further modifications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a look at the `strided_range.cu` file in the Thrust `examples` directory.
    Can you think of how to use this to implement a general matrix-matrix multiplication
    using Thrust?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the importance of the `operator()` function when defining a functor?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

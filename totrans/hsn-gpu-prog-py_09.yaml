- en: Implementation of a Deep Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now use our accumulated knowledge of GPU programming to implement our
    very own deep neural network (DNN) with PyCUDA. DNNs have attracted a lot of interest
    in the last decade, as they provide a robust and elegant model for machine learning
    (ML). DNNs was also one of the first applications (outside of rendering graphics)
    that were able to show the true power of GPUs by leveraging their massive parallel
    throughput, which ultimately helped NVIDIA rise to become a major player in the
    field of artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: In the course of this book, we have mostly been covering individual topics in
    a *bubble* on a chapter-by-chapter basis—here, we will build on many of the subjects
    we have learned about thus far for our very own implementation of a DNN. While
    there are several open source frameworks for GPU-based DNNs currently available
    to the general public—for example, Google's TensorFlow and Keras, Microsoft's
    CNTK, Facebook's Caffe2, and PyTorch—it is very instructive to go through an implementation
    of one from scratch, which will give us a greater insight and appreciation of
    the underlying technologies required for DNNs. We have a lot of material to cover
    here, so we'll cut right to the chase after a brief introduction to some of the
    basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be looking at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what an **artificial neuron** (**AN**) is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how many ANs can be combined together in a **deep neural network**
    (**DNN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a DNN from scratch in CUDA and Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how cross-entropy loss can be used to evaluate the output of a
    neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing gradient descent to train an NN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to train and test an NN on a small dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the prerequisites for this chapter, check out the
    preface of this book. For the software and hardware requirements, check out the
    README file in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons and neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's briefly go over some of the basics of **machine learning (ML)** and **neural networks
    (NNs)**. In Machine Learning, our goal is to take a collection of data with a
    particular set of labeled classes or characteristics and use these examples to
    train our system to predict the values of future data. We call a program or function
    that predicts classes or labels of future data based on prior training data a
    **classifier**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many types of classifiers, but here we will be focusing on NNs. The
    idea behind NNs is that they (allegedly) work in a way that is similar to the
    human brain, in that they learn and classify data using a collection of **artificial
    neurons (ANs)**, all connected together to form a particular structure. Let''s
    step back for a moment, though, and look at what an individual AN is. In mathematics,
    this is just an *affine* function from the linear space **R^n** to **R**, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/25478608-c882-49f1-85a9-9e3f6b0d472d.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that this can be characterized as a dot product between a constant
    weight vector ***w*** and an input vector ***x***, with an additional bias constant *b*
    added to the end. (Again, the only *input* into this function here is *x*; the
    other values are constants!)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, individuall*y* a single AN is fairly useless (and stupid), as their *intelligence* only
    emerges when acting in cooperation with a large number of other ANs. Our first
    step is to stack a collection of *m* similar ANs on top of each other so as to
    form what we will call a **dense layer (DL)**. This is dense because each neuron
    will process every single input value from *x – *each AN will take in an array
    or vector value from **R^n** and output a single value in **R.** Since there are
    *m* neurons, this means that we can say their output collectively is in the space
    **R^m**. We will notice that if we stack the weights for each neuron in our layer,
    so as to form an *m x n* matrix of weights, we can then just calculate the output
    of each neuron with a matrix multiplication followed by the addition of the appropriate
    biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's suppose that we want to build an NN classifier that can classify
    *k* different classes; we can create a new additional dense layer that takes in
    the *m* values from the prior dense layer, and outputs *k* values. Supposing that
    we have the appropriate weight and bias values for each layer (which are certainly
    not trivial to find), and that we also have the appropriate **activation function**
    set up after each layer (which we will define later), this will act as a classifier
    between our *k* distinct classes, giving us the probability of *x* falling into
    each respective class based on the outputs of the final layer. Of course, we're
    getting way ahead of ourselves here, but that is, in a nutshell, how an NN works.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it seems like we can just keep connecting dense layers to each other into
    long chains to achieve classifications. This is what is known as a DNN. When we
    have a layer that is not directly connected to the inputs or outputs, that is
    known as a hidden layer. The strength of a DNN is that the additional layers allow
    the NN to capture abstractions and subtleties of the data that a shallow NN could
    not pick up on.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a dense layer of artificial neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s implement the most important building block of an NN, the **dense
    layer**. Let''s start by declaring a CUDA kernel, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's go over the inputs, one by one. `num_outputs`, of course, indicates the
    total number of outputs this layer has; this is exactly the number of neurons
    in the layer. `num_inputs` tells us the size of the input data. Setting a positive
    value for `relu` and `sigmoid` will indicate that we should use the corresponding
    activation function on the output of this layer, which we will define later. `w`
    and `b` are arrays containing the weights and biases of this layer, while `x`
    and `y` will act as our inputs and outputs. Oftentimes, we wish to classify more
    than one piece of data at a time. We can indicate this by setting `batch_size`
    to be the number of points we wish to predict. Finally, `w_t`, `b_t`, and `delta`
    will be used in the training process to determine the appropriate weights and
    biases for this layer by means of **gradient descent**. (We will see more on gradient
    descent in a later section.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start writing our kernel. We will parallelize the computations
    over each output, so we will set an integer `i` to be the global thread ID to
    this end, and have any unnecessary extra threads which happen to be running this
    kernel to just not do anything with the appropriate `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s iterate over each data point in the batch with the appropriate
    `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will multiply and accumulate the 32-bit floats from the weights and inputs
    into a 64-bit double `temp` and then add the appropriate bias point. We will then
    typecast this back to a 32-bit float and put the value in the output array, and
    then close off the loop over `k`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Multiply and accumulate* types of operations are generally subject to a great
    loss of numerical precision. This can be mitigated by using a temporary variable
    of higher precision to store values in the course of the operation, and then typecasting
    this variable back to the original precision after the operation is completed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To train an NN, we will ultimately have to calculate the derivative (from calculus)
    of our NN with respect to each weight and bias within each individual layer, which
    is with respect to a particular batch of inputs. Remember that the derivative
    of a mathematical function *f* at the value *x* can be estimated as *f**(x + δ)
    - f(x) / δ*, where delta (δ) is some sufficiently small positive value. We will
    use the input values `w_t` and `b_t` to indicate to the kernel whether we want
    to calculate the derivative with respect to a particular weight or bias; otherwise,
    we will set these input values to a negative value to evaluate only for this layer.
    We will also set delta to be an appropriately small value for the calculation
    of the derivative, and use this to increment the value of the appropriate bias
    or weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will add some code for what is known as the **rectified linear unit**
    (or **ReLU**) and **sigmoid activation functions**. These are used for processing
    the immediate output of a dense neural layer. ReLU just sets all negative values
    to 0, while acting as an identity for positive inputs, while sigmoid just computes
    the value of the `sigmoid` function on each value ( *1 / (1 + e^(-x))* ). ReLU
    (or any other activation function) is used between hidden layers in an NN as a
    means to make the entire NN act as a nonlinear function; otherwise, the entire
    NN would constitute a trivial (and inefficiently computed) matrix operation. (While
    there are many other nonlinear activation functions that can be used between layers,
    ReLU has been found to be a particularly effective function for training.) Sigmoid
    is used as a final layer in an NN intended for **labeling**, that is, one that
    may assign multiple labels for a given input, as opposed to assigning an input
    to a single class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go up a little bit in the file, before we even begin to define this
    CUDA kernel, and define these operations as C macros. We will also remember to
    put in the CUDA-C code we''ve just written while we are at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use the kernel inputs `relu` and `sigmoid` to indicate whether
    we should use these additional layers; we will take a positive input from these
    to indicate that they should be used, respectively. We can add this, close off
    our kernel, and compile it into a usable Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s go to the beginning of the file and set up the appropriate import
    statements. Notice that we will include the `csv` module, which will be used for
    processing data inputs for testing and training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s continue setting up our dense layer; we will want to wrap this
    within a Python class for ease of use, which will make our lives much easier when
    we start connecting these dense layers together into a full-blown NN. We''ll call `class
    DenseLayer` and start by writing a constructor. Most of the inputs and setup here
    should be self-explanatory: we should definitely add an option to load weights
    and biases from a pre-trained network, and we''ll also include the option to specify
    a default *delta *value as well as a default stream. (If no weights or biases
    are given, weights are initialized to random values, while all biases are set
    to 0.) We will also specify whether to use ReLU or sigmoid layers here, as well.
    Toward the end, notice how we set up the block and grid sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will set up a function in this class to evaluate inputs from this layer;
    we will meticulously check the input (x) to determine if it is already on the
    GPU (transferring it over to a `gpuarray` if not), and we will let the user specify
    a preallocated `gpuarray` for output (y), manually allocating an output array
    if one is not specified. We will also check the delta and `w_t`/`b_t` values for
    the case of training, as well as `batch_size`. We will then run the kernel on
    the `x` input with outputs going into `y`, and finally return `y` as the output
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There we go. We have fully implemented a dense layer!
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of the softmax layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at how we can implement a **softmax layer**. As we have already
    discussed, a sigmoid layer is used for assigning labels to a class—that is, if
    you want to have multiple nonexclusive characteristics that you want to infer
    from an input, you should use a sigmoid layer. A **softmax layer** is used when
    you only want to assign a single class to a sample by inference—this is done by
    computing a probability for each possible class (with probabilities over all classes,
    of course, summing to 100%). We can then select the class with the highest probability
    to give the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see exactly what the softmax layer does—given a set of a collection
    of *N* real numbers (*c[0], ..., c[N-1]*) , we first compute the sum of the exponential
    function on each number (![](assets/90e15296-aedc-4f2b-a9bf-8099d5a28a07.png)),
    and then calculate the exponential of each number divided by this sum to yield
    the softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ce20fc54-36ee-46a9-86a9-40387fb73545.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start with our implementation. We will start by writing two very short
    CUDA kernels: one that takes the exponential of each input, and another that takes
    the mean over all of the points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write a Python wrapper class, like we did previously. First, we
    will start with the constructor, and we will indicate the number of both inputs
    and outputs with `num`. We can also specify a default stream, if we wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write `eval_ function` in a way that is similar to the dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Implementation of Cross-Entropy loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s implement what is known as the **cross-entropy loss** function.
    This is used to measure how accurate an NN is on a small subset of data points
    during the training process; the bigger the value that is output by our loss function,
    the more inaccurate our NN is at properly classifying the given data. We do this
    by calculating a standard mean log-entropy difference between the expected output
    and the actual output of the NN. For numerical stability, we will limit the value
    of the output to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Implementation of a sequential network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's implement one final class that will combine multiple dense layer
    and softmax layer objects into a single coherent feed-forward sequential neural
    network. This will be implemented as another class, which will subsume the other
    classes. Let's first start by writing the constructor—we will be able to set the
    max batch size here, which will affect how much memory is allocated for the use
    of this network – we'll store some allocated memory used for weights and input/output
    for each layer in the list variable, `network_mem`. We will also store the `DenseLayer`
    and `SoftmaxLayer` objects in the list network, and information about each layer
    in the NN in `network_summary`. Notice how we can also set up some training parameters
    here, including the delta, how many streams to use for gradient descent (we'll
    see this later), as well as the number of training epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see one other input at the beginning called layers. Here, we can
    indicate the construction of the NN by describing each layer, which the constructor
    will create by iterating through each element of layers and calling the `add_layer`
    method, which we will implement next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s implement the `add_layer` method. We will use a dictionary data
    type to pass all of the relevant information about the layer to the sequential
    network—including the type of layer (dense, softmax, and so on), the number of
    inputs/outputs, weights, and biases. This will append the appropriate object and
    information to the object''s network and `network_summary` list variables, as
    well as appropriately allocate `gpuarray` objects to the `network_mem` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Implementation of inference methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now add two methods for inference to our `SequentialNetwork` class—that
    is, for predicting an output given for a particular input. The first method we
    will just call `predict`, which will be used by the end user. In the course of
    the training process, we will have to make predictions based on a partial result
    from only some of the layers, and we will make another method to this end called
    `partial_predict`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by implementing *predict*.  This will take two inputs—a collection
    of samples in the form of a one- or two-dimensional NumPy array, and possibly
    a user-defined CUDA stream. We will start by doing some type-checks and formatting
    on the samples (here, called `x`), remembering that the samples will be stored
    row-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform the actual inference step. We just have to iterate through
    our entire neural network, performing an `eval_` on each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now pull the final output of the NN, the GPU, and return it to the
    user. If the number of samples in `x` is actually smaller than the maximum batch
    size, we will slice the output array appropriately before it is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with that done, let''s implement `partial_predict`. Let''s briefly discuss
    the idea behind this. When we are in the training process, we will evaluate a
    collection of samples, and then look at how a subtle change of adding *delta* to
    each weight and bias individually will affect the outputs. To save time, we can
    calculate the outputs of each layer and store them for a given collection of samples,
    and then only recompute the output for the layer where we change the weight, as
    well as for all subsequent layers. We''ll see the idea behind this in a little
    more depth soon, but for now, we can implement this like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now make a full implementation of the training method for our NN in
    the form of **batch-stochastic gradient descent (BSGD)**. Let's think about what
    this means, word by word. **Batch** means that this training algorithm will operate
    on a collection of training samples at once, rather than all of the samples simultaneously,
    while **stochastic** indicates that each batch is chosen randomly. **Gradient** means
    that we will be using a gradient from calculus—which, here, is the collection
    of derivatives for each weight and bias on the loss function. Finally, **descent** means
    that we are trying to reduce the loss function—we do this by iteratively making
    subtle changes on the weights and biases by *subtracting* the Gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Remember from calculus that the gradient of a point always points in the direction
    of the greatest *increase*, with its opposite direction being that of the greatest
    *decrease*. Since we want a *decrease*, we subtract the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now implement BSGD as the `bsgd` method in our `SequentialNetwork`
    class. Let''s go over the input parameters of `bsgd`, one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`training` will be a two-dimensional NumPy array of training samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` will be the desired output of the final layer of the NN corresponding
    to each training sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delta` will indicate how much we should increase a weight for the calculation
    of derivatives by'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_streams` will indicate the maximum number of concurrent CUDA streams that
    BSGD will perform calculations over'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` will indicate how large we want the batches that we will calculate
    the loss function on for each update of the weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs` will indicate how many times we shuffle the order of the current set
    of samples, break into a collection of batches, and then perform BSGD on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_rate` will indicate the rate at which we will update our weights
    and biases with our gradient calculations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll start out this method as usual and perform some checks and typecasting,
    set up the collection of CUDA stream objects into a Python list, and allocate
    some additional needed GPU memory in another list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can begin training. We will start by doing an iteration of the entire
    BSGD for each `epoch`, performing a random shuffle of the entire dataset for each
    epoch. We''ll print some information to the terminal as well so that the user
    will have some status updates in the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will make a loop that iterates over each batch in the shuffled dataset.
    We start by calculating the entropy from the current batch, and we will print
    this as well. If the user sees decreases in entropy, then they will know that
    gradient descent is working here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now iterate through each dense layer of our NN, calculating the gradient
    for the entire set of weights and biases. We will store these derivatives for
    the weights and biases in *flattened* (one-dimensional) arrays, which will correspond
    to the `w_t` and `b_t` indices in our CUDA kernels, which are also flattened.
    Since we will have multiple streams process different outputs for different weights,
    we will use a Python Queue container to store the set of weights and biases that
    are yet to be processed for this batch: we can then just pop values off the top
    of this container to the next available stream (we''ll store these as tuples,
    with the first element indicating whether this is a weight or bias, in particular):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to iterate over each and every weight and bias, which we can do
    with a `while` loop that checks if the `queue` object we just set up is empty.
    We will set up another queue, `stream_weights`, that will help us organize which
    weights and biases each stream has processed. After setting up the weight and
    bias inputs appropriately, we can now use `partial_predict` by using the current
    stream and corresponding GPU memory arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we already performed a `predict` for this batch of samples to calculate
    the entropy, so we are now able to perform `partial_predict` on this batch, provided
    we are careful about which memory and layers we use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We have only computed the prediction of the output for alterations of a small
    set of weights and biases. We will have to compute the entropy for each, and then
    store the value of the derivative in the flattened arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now finished the `while` loop. Once we reach the outside of this, we
    will know that we''ve calculated the derivatives for all weights and biases for
    this particular layer. Before we iterate to the next layer, we will append the
    calculated values for the gradient of the current set of weights and biases into
    the `all_grad` list. We will also reshape the flattened list of weights back into
    the original shape while we''re at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After we are done iterating over every layer, we can perform the optimization
    of the weights and biases of our NN on this batch. Notice how if the `training_rate` variable
    is far less than `1`, this will reduce how fast the weights are updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We have fully implemented a (very simple) GPU-based DNN!
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning and normalizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we move on to training and testing our brand-new NN, we need to step
    back for a moment and talk about **conditioning** and **normalizing** data. NNs
    are highly susceptible to numerical error, especially when inputs have a large
    variance in scale. This can be mitigated by properly **conditioning** our training
    data; this means that for each point in an input sample, we will calculate the
    mean and variance of each point over all samples, and then subtract the mean and
    divide by the standard deviation for each point in each sample before it is input
    into the NN for either training or inference (prediction). This method is known
    as n**ormalization**. Let''s put together a small Python function that can do
    this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The Iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now construct our very own DNN for a real-life problem: classification
    of flower types based on the measurements of petals. We will be working with the
    well-known *Iris dataset* for this. This dataset is stored as a comma-separated
    value (CSV) text file, with each line containing four different numerical values
    (petal measurements), followed by the flower type (here, there are three classes—*Irissetosa*, *Irisversicolor*,
    and *Irisvirginica*). We will now design a small DNN that will classify the type
    of iris, based on this set.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we continue, please download the Iris dataset and put it into your working
    directory.  This is available from the UC Irvine Machine Learning repository,
    which can be found here: [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by processing this file into appropriate data arrays that we
    can use for training and validating our DNN. Let''s start by opening up our main
    function; we will need to translate the names of the flowers into actual classes
    that a DNN can output, so let''s make a small dictionary that will give us a corresponding
    label for each class. We will also set up some empty lists to store our training
    data and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s read from the CSV file. We will use the `reader` function from
    Python''s `csv` module, which we imported earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now randomly shuffle the data and use two-third of these samples as
    training data. The remaining one-third will be used for test (validation) data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, finally, we can begin building our DNN! First, let''s create a `SequentialNetwork`
    object. We''ll set the `max_batch_size` to `32`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create our NN. This will consist of four dense layers (two hidden)
    and a softmax layer. We will increment the number of neurons in each layer until
    the final layer, which will only have three outputs (one for each class). This
    increasing amount of neurons per layer allows us to capture some of the subtleties
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now condition our training data and begin the training with our BSGD
    method that we just implemented. We will train with `batch_size` set to `16`, `max_streams` set
    to `10`, the number of `epochs` set to 100, the `delta` set to 0.0001, and the
    `training_rate` set to 1—these will be admissible parameters for virtually any
    modern GPU. We will also time the training procedure while we''re at it, which
    can be rather time-consuming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our DNN is fully trained. We are ready to begin the validation process!
    Let''s set up a Python variable called `hits` to count the total number of correct
    classifications. We will also need to condition the validation/testing data too.
    One more thing—we determine the class by the index corresponding to the largest
    value of the softmax layer of our DNN. We can check whether this gives us the
    correct classification by using NumPy''s `argmax` function, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to check how well our DNN actually works. Let''s output the
    accuracy as well as the total training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are done. We can now fully implement a DNN with Python and CUDA! Generally
    speaking, you can expect an accuracy ranging from 80%-97% for this particular
    problem, with a training time of 10-20 minutes on any Pascal-level GPU.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is available in the `deep_neural_network.py` file,
    under the appropriate directory in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by giving the definition of an artificial neural
    network, and showed you how individual ANs can be combined into dense layers,
    which combine together into a full-on deep neural network. We then implemented
    a dense layer in CUDA-C and made an appropriate corresponding Python wrapper class.
    We also included functionality to add ReLU and sigmoid layers on the outputs of
    a dense layer. We saw the definition and motivation of using a softmax layer,
    which is used for classification problems, and then implemented this in CUDA-C
    and Python. Finally, we implemented a Python class so that we could build a sequential
    feed-forward DNN from the prior classes;  we implemented a cross-entropy loss
    function, and then used this in our loss function in our implementation of gradient
    descent to train the weights and biases in our DNN. Finally, we used our implementation
    to construct, train, and test a DNN on a real-life dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a great deal of self-confidence in our CUDA programming abilities,
    since we can write our own GPU-based DNN! We will now move on to some very advanced
    material in the next two chapters, where we will look at how we can write our
    own interfaces to compiled CUDA code, as well as some of the very technical ins
    and outs of NVIDIA GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you construct a DNN and after training it, it yields only garbage. After
    inspection, you find that all of the weights and biases are either huge numbers
    or NaNs. What might the problem be?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name one possible problem with a small `training_rate` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name one possible problem with a large `training_rate` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we want to train a DNN that will assign multiple labels to an image
    of an animal ("slimey", "furry", "red", "brown", and so on). Should we use a sigmoid
    or softmax layer at the end of the DNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we want to classify an image of a single animal as either a cat or dog.
    Do we use sigmoid or softmax?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we decrease the batch size, will there be more or less updates to the weights
    and biases during gradient descent training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

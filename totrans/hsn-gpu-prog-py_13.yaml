- en: Assessment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1, Why GPU Programming?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first two `for` loops iterate over every pixel, whose outputs are invariant
    to each other; we can thus parallelize over these two `for` loops. The third `for`
    loop calculates the final value of a particular pixel, which is intrinsically
    recursive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amdahl's Law doesn't account for the time it takes to transfer memory between
    the GPU and the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 512 x 512 amounts to 262,144 pixels. This means that the first GPU can only
    calculate the outputs of half of the pixels at once, while the second GPU can
    calculate all of the pixels at once; this means the second GPU will be about twice
    as fast as the first here. The third GPU has more than sufficient cores to calculate
    all pixels at once, but as we saw in problem 1, the extra cores will be of no
    use to us here. So the second and third GPUs will be equally fast for this problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One issue with generically designating a certain segment of code as parallelizable
    with regards to Amdahl's Law is that this makes the assumption that the computation
    time for this piece of code will be close to 0 if the number of processors, *N*,
    is very large. As we can see from the last problem, this is not the case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, using *time* consistently can be cumbersome, and it might not zero in
    on the bottlenecks of your program. Second, a profiler can tell you the exact
    computation time of all of your code from the perspective of Python, so you can
    tell whether some library function or background activity of your operating system
    is at fault rather than your code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 2, Setting Up Your GPU Programming Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No, CUDA only supports Nvidia GPUs, not Intel HD or AMD Radeon
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This book only uses Python 2.7 examples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Device Manager
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lspci`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`free`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.run`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 3, Getting Started with PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Memory transfers between host/device, and compilation time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can, but this will vary depending on your GPU and CPU setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do this using the C `?` operator for both the point-wise and reduce operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a `gpuarray` object goes out of scope its destructor is called, which will
    deallocate (free) the memory it represents on the GPU automatically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ReductionKernel` may perform superfluous operations, which may be necessary
    depending on how the underlying GPU code is structured. A *neutral element* will
    ensure that no values are altered as a result of these superfluous operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should set `neutral` to the smallest possible value of a signed 32-bit integer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 4, Kernels, Threads, Blocks, and Grids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of the threads don't operate on the GPU simultaneously. Much like a CPU
    switching between tasks in an OS, the individual cores of the GPU switch between
    the different threads for a kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: O( n/640 log n), that is, O(n log n).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is actually no internal grid-level synchronization in CUDA—only block-level
    (with `__syncthreads).` We have to synchronize anything above a single block with
    the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Naive: 129 addition operations. Work-efficient: 62 addition operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, we can't use `__syncthreads` if we need to synchronize over a large grid
    of blocks. We can also launch over fewer threads on each iteration if we synchronize
    on the host, freeing up more resources for other operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of a naive parallel sum, we will likely be working with only a small
    number of data points that should be equal to or less than the total number of
    GPU cores, which can likely fit in the maximum size of a block (1032); since a
    single block can be synchronized internally, we should do so. We should use the
    work-efficient algorithm only if the number of data points are far greater than
    the number of available cores on the GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 5, Streams, Events, Contexts, and Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance improves for both; as we increase the number of threads, the
    GPU reaches peak utilization in both cases, reducing the gains made through using
    streams.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, you can launch an arbitrary number of kernels asynchronously and synchronize
    them to with `cudaDeviceSynchronize`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open up your text editor and try it!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: High standard deviation would mean that the GPU is being used unevenly, overwhelming
    the GPU at some points and under-utilizing it at others. A low standard deviation
    would mean that all launched operations are running generally smoothly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i. The host can generally handle far fewer concurrent threads than a GPU. ii.
    Each thread requires its own CUDA context. The GPU can become overwhelmed with
    excessive contexts, since each has its own memory space and has to handle its
    own loaded executable code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 6, Debugging and Profiling Your CUDA Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory allocations are automatically synchronized in CUDA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `lockstep` property only holds in single blocks of size 32 or less. Here,
    the two blocks would properly diverge without any `lockstep`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The same thing would happen here. This 64-thread block would actually be split
    into two 32-thread warps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nvprof can time individual kernel launches, GPU utilization, and stream usage;
    any host-side profiler would only see CUDA host functions being launched.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Printf is generally easier to use for small-scale projects with relatively short,
    inline kernels. If you write a very involved CUDA kernel with thousands of lines,
    then probably you would want to use the IDE to step through and debug your kernel
    line by line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This tells CUDA which GPU we want to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cudaDeviceSynchronize` will ensure that interdependent kernel launches and
    mem copies are indeed synchronized, and that they won''t launch before all necessary
    operations have finished.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 7, Using the CUDA Libraries with Scikit-CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SBLAH starts with an S, so this function uses 32-bit real floats. ZBLEH starts
    with a Z, which means it works with 128-bit complex floats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hint: set `trans = cublas._CUBLAS_OP[''T'']`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hint: use the Scikit-CUDA wrapper to the dot product, `skcuda.cublas.cublasSdot`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hint: build upon the answer to the last problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can put the cuBLAS operations in a CUDA stream and use event objects with
    this stream to precisely measure the computation times on the GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the input appears as being complex to cuFFT, it will calculate all of
    the values as NumPy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dark edge is due to the zero-buffering around the image. This can be mitigated
    by *mirroring* the image on its edges rather than by using a zero-buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 8, The CUDA Device Function Libraries and Thrust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try it. (It's actually more accurate than you'd think.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One application: a Gaussian distribution can be used to add `white noise` to
    samples to augment a dataset in machine learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No, since they are from different seeds, these lists may have a strong correlation
    if we concatenate them together. We should use subsequences of the same seed if
    we plan to concatenate them together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hint: remember that matrix multiplication can be thought of as a series of
    matrix-vector multiplications, while matrix-vector multiplication can be thought
    of as a series of dot products.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Operator()` is used to define the actual function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 9, Implementation of a Deep Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One problem could be that we haven't normalized our training inputs. Another
    could be that the training rate was too large.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With a small training rate a set of weights might converge very slowly, or not
    at all.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A large training rate can lead to a set of weights being over-fit to particular
    batch values or this training set. Also, it can lead to numerical overflows/underflows
    as in the first problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sigmoid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Softmax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More updates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 10, Working with Compiled GPU Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Only the EXE file will have the host functions, but both the PTX and EXE will
    contain the GPU code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cuCtxDestory`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`printf` with arbitrary input parameters. (Try looking up the `printf` prototype.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With a Ctypes `c_void_p` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will allow us to link to the function with its original name from Ctypes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Device memory allocations and memcopies between device/host are automatically
    synchronized by CUDA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 11, Performance Optimization in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fact that `atomicExch` is thread-safe doesn't guarantee that all threads
    will execute this function at the same time (which is not the case since different
    blocks in a grid can be executed at different times).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A block of size 100 will be executed over multiple warps, which will not be
    synchronized within the block unless we use `__syncthreads`. Thus, `atomicExch`
    may be called at multiple times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since a warp executes in lockstep by default, and blocks of size 32 or less
    are executed with a single warp, `__syncthreads` would be unnecessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a naïve parallel sum within the warp, but otherwise, we are doing as
    many sums with`atomicAdd` as we would do with a serial sum. While CUDA automatically
    parallelizes many of these `atomicAdd` invocations, we could reduce the total
    number of required `atomicAdd` invocations by implementing a work-efficient parallel
    sum.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Definitely `sum_ker`. It's clear that PyCUDA's sum doesn't use the same hardware
    tricks as we do since ours performs better on smaller arrays, but by scaling the
    size to be much larger, the only explanation as to why PyCUDA's version is better
    is that it performs fewer addition operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 12, Where to Go from Here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two examples: DNA analysis and physics simulations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Two examples: OpenACC, Numba.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TPUs are only used for machine learning operations and lack the components required
    to render graphics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ethernet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

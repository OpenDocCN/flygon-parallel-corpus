- en: Architecting a Kubernetes Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the basics of what composes a Kubernetes cluster, we
    still need to understand how to place all the Kubernetes components together,
    and how to suit their requirements to provision a production-ready Kubernetes
    cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will examine how to determine these requirements and how
    they will help us maintain steady workloads and achieve a successful deployment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Kube-sizing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining storage considerations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining network requirements
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing kube objects
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube-sizing
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing a Kubernetes cluster, we don't just need to worry about how we
    are going to configure our deployment objects to host our applications, or how
    we are going to configure our service objects to provide communication across
    our pods—where all this is hosted is also important. Therefore, we also need to
    take into account the resources that are required to bring balance to our application
    workloads and our control plane.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: etcd considerations
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will require at least a three-node `etcd` cluster in order for it to be able
    to support itself in case one node fails. Because `etcd` uses a distributed census
    algorithm called **Raft**, odd-numbered clusters are recommended. This is because,
    in order for an action to be allowed, more than 50% of the members of the cluster
    have to agree on it. In a scenario with a two-node cluster, for example, if one
    of the nodes fails, the other node's vote is only 50% of the cluster, and therefore,
    the cluster loses quorum. Now, when we have a three-node cluster, a single node
    failure represents only a 33.33% vote loss and the two remaining nodes' votes
    still 66.66% for the action to be allowed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The following link is for a great website where you can learn exactly how the
    Raft algorithm works: [http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: For `etcd`, we can choose from two deployment models for our cluster. We can
    either run it on the same node as our kube-apiserver, or we can have a separate
    set of clusters running our key-value store. Either way, this will not change
    how `etcd` reaches quorum, so you will still have to install `etcd` in odd numbers
    across your control-plane manager nodes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: For a Kubernetes use case, `etcd` won't consume lots of compute resources such
    as CPU or memory. Although `etcd` does aggressively cache key-value data and uses
    most of its memory-tracking watchers, two cores and 8 GB of RAM will be more than
    enough.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to the disk, this is where you need to be more critical. The `etcd` cluster
    relies heavily on disk latency, because of the way the consensus protocol persistently
    stores metadata in the log. Every member of the `etcd` cluster has to store every
    request, and any major spike in latency can trigger a cluster leader election,
    which will cause instability for the cluster. A **hard disk drive** (**HDD**) for
    `etcd` is out of the question unless you are running 15k RPM disks in a Raid 0
    disk to squeeze the highest performance possible out of a magnetic drive. A **solid
    state drive** (**SSD**) is the way to go and, with extremely low latency and higher
    **input/output operations per second** (**IOPS**), they are the perfect candidate
    to host your key-value store. Thankfully, all major cloud providers offer SSD
    solutions to satisfy this need.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: kube-apiserver sizing
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The remaining resources that are required for the control-plane components will
    depend on the number of nodes that they will be managing and which add-ons you
    will be running on them. One additional thing to take into account is the fact
    that you can put these master nodes behind a load balancer to ease the load and
    provide high availability. In addition to this, you can always horizontally scale
    your master nodes in periods of contention.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Taking all this into consideration, and taking into account that `etcd` will
    be hosted alongside our master nodes, we can say that a three-master node cluster
    with **virtual machines** (**VMs**) with 2 to 4 vCPUs, and between 8 and 16 GB
    of RAM will be more than enough to handle greater than or equal to 100 worker
    nodes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The worker nodes, on the other hand, are the ones that'll be doing the heavy
    lifting—these guys are the ones that will be running our application workloads.
    Standardizing the size of these nodes will be impossible as they fall into a *What
    if?* scenario. We are required to know exactly what type of applications we will
    be running on our nodes, and what their resource requirements are, for us to be
    able to size them correctly. Nodes will not only be sized on the application resource
    requirements, but we will also have to consider periods where we will have more
    than our planned pods running on them. For instance, you can perform a rolling
    update on a deployment to use a newer image depending on how you have configured
    your `maxSurge`; this node will have to handle 10% to 25% more load.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Containers are really lightweight, but when orchestrators come into play, you
    can have 30, 40, or even 100 containers running on a single node! This exponentially
    increases your resource consumption per host. While pods come with resource-limiting
    functionalities and specifications to limit the container's resource consumption,
    you still need to account for the required resources of those containers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Nodes can always be scaled horizontally during periods of contention and high-resource
    demand. However, it's always good to have those extra resources available to avoid
    any undesirable **out of memory** (**OOMs**)killers. So, plan for the future and
    for the *What if?* scenario by having a pool of extra resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer considerations
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our nodes still need to talk to our API server and, as we mentioned before,
    having several master nodes requires a load balancer. When it comes to load balancing
    requests from our nodes to the masters, we have several options to pick from,
    depending on where you are running your cluster. If you are running Kubernetes
    in a public cloud, you can go ahead with your cloud provider's load balancer option,
    as they are usually elastic. This means that they autoscale as needed and offer
    more features than you actually require. Essentially, load balancing requests
    to the API server will be the only task that your load balancer will perform.
    This leads us to the on-premises scenario—as we are sticking to open source solutions
    here, then you can configure a Linux box running either HAProxy or NGINX to satisfy
    your load balancing needs. There is no wrong answer in choosing between HAProxy
    and NGINX, as they provide you with exactly what you need.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, the basic architecture will look like the following screenshot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae11e5e7-863f-4eb1-aaab-b5be6cdd8b81.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Storage considerations
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Storage needs are not as straightforward as they are for a regular host or hypervisor.
    There are several types of storage that our nodes and pods will be consuming,
    and we need to tier them properly. Because you are running Linux, tiering the
    storage into different filesystems and storage backends will be extremely easy—nothing
    that **logical volume manager** (**LVM**) or different mount points can't solve.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The basic Kubernetes binaries, such as `kubelet` and `kube-proxy`, can run on
    basic storage alongside the OS files; nothing very high-end is required, as any
    SSD will be enough to satisfy their needs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Now, on the other hand, we have the storage in which our container images will
    be stored and run from. Going back to the [Chapter 6](6da53f60-978c-43a4-9dc9-f16b14405709.xhtml),
    *Creating a Highly Available Self-Healing Architecture*, we learned that containers
    are composed of read-only layers. This means that when the disks are running tens
    or even hundreds of containers in a single node, they will be hit very hard on
    read requests. The storage backend for this will have to serve read requests with
    very low latency. Specific numbers in terms of IOPS and latency will vary across
    each environment, but the basis will be the same. This is because of the nature
    of containers—disks that provide a higher read performance over writes will be
    preferable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们有存储，容器镜像将存储和运行。回到[第6章]（6da53f60-978c-43a4-9dc9-f16b14405709.xhtml），*创建高可用的自愈架构*，我们了解到容器由只读层组成。这意味着当磁盘在单个节点上运行数十甚至数百个容器时，它们将受到读取请求的严重打击。用于此的存储后端将必须以非常低的延迟提供读取请求。在IOPS和延迟方面的具体数字将因环境而异，但基础将是相同的。这是因为容器的性质——提供更高读取性能而非写入的磁盘将更可取。
- en: 'Storage performance is not the only factor to take into account. Storage space
    is also very important. Calculating how much space you require will depend on
    the following two things:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 存储性能并不是唯一需要考虑的因素。存储空间也非常重要。计算所需空间将取决于以下两个因素：
- en: How big are the images that you are going to be running?
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将要运行的镜像有多大？
- en: How many different images will you be running and what are their sizes?
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将运行多少不同的镜像，它们的大小是多少？
- en: 'This will directly consume the space in `/var/lib/docker` or `/var/lib/containerd`.
    With this in mind, a separate mount point for `/var/lib/docker` or `containerd/`
    with enough space to store all the images that you are going to be running on
    the pods will be a good option. Take into account that these images are ephemeral
    and will not live on your node forever. Kubernetes does have garbage collection
    strategies embedded in kubelet, which will delete old images that are no longer
    in use if you reach a specified threshold for disk usage. These options are `HighThresholdPercent`
    and `LowThresholdPercent.` You can set them with a kubelet flag: `--eviction-hard=imagefs.available`
    or `--eviction-soft=imagefs.available`. These flags are already configured by
    default to garbage collect when free storage reaches less than 15%, however, you
    can adjust them to your needs. `eviction-hard` is the threshold that it needs
    to reach in order to start deleting images, while `eviction-soft` is the percentage
    or amount that it needs to reach to stop deleting images.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将直接消耗`/var/lib/docker`或`/var/lib/containerd`中的空间。考虑到这一点，为`/var/lib/docker`或`containerd/`设置一个单独的挂载点，具有足够的空间来存储你将在pod上运行的所有镜像，将是一个不错的选择。请注意，这些镜像是临时的，不会永远存在于你的节点上。Kubernetes确实在kubelet中嵌入了垃圾收集策略，当达到指定的磁盘使用阈值时，将删除不再使用的旧镜像。这些选项是`HighThresholdPercent`和`LowThresholdPercent`。你可以使用kubelet标志进行设置：`--eviction-hard=imagefs.available`或`--eviction-soft=imagefs.available`。这些标志已经默认配置为在可用存储空间低于15%时进行垃圾收集，但是你可以根据需要进行调整。`eviction-hard`是需要达到的阈值，以开始删除镜像，而`eviction-soft`是需要达到的百分比或数量，以停止删除镜像。
- en: 'Some containers will still require some sort of read/write volume for persistent
    data. As discussed in [Chapter 7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml), *Understanding
    the Core Components of a Kubernetes Cluster*, there are several storage provisioners,
    and all of them will suit different scenarios. All you need to know is that you
    have a series of options available to you, thanks to the Kubernetes storage classes.
    Some of the open source software-defined storage solutions that are worth mentioning
    are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一些容器仍将需要某种读/写卷以用于持久数据。如[第7章]（d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml）中所讨论的，Kubernetes集群的核心组件，有几种存储供应商，它们都适用于不同的场景。你需要知道的是，由于Kubernetes存储类别的存在，你有一系列可用的选项。以下是一些值得一提的开源软件定义存储解决方案：
- en: Ceph
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceph
- en: GlusterFS
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS
- en: OpenStack Cinder
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack Cinder
- en: '**Network File System** (**NFS**)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络文件系统**（**NFS**）'
- en: Each storage provisioner will have its benefits and downsides, but it is beyond
    the scope of this book to go through each one in detail. We have offered a good
    overview of Gluster in previous chapters, as it is what we are going to use in
    later chapters for our example deployment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个存储供应商都有其优势和劣势，但详细介绍每个存储供应商已超出了本书的范围。我们在之前的章节中对Gluster进行了很好的概述，因为在后续章节中我们将用它来进行示例部署。
- en: Network requirements
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络要求
- en: 'In order to understand the network requirements of our cluster, we first need
    to understand the Kubernetes networking model and what problems it aims to solve.
    Container networking can be very hard to grasp; however, it has three essential
    problems:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们集群的网络要求，我们首先需要了解Kubernetes网络模型以及它旨在解决的问题。容器网络可能很难理解；然而，它有三个基本问题：
- en: How do containers talk to each other (on the same host and on different hosts)?
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器如何相互通信（在同一台主机上和在不同的主机上）？
- en: How do containers talk to the outside world, and how does the outside world
    talk to the containers?
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器如何与外部世界通信，外部世界如何与容器通信？
- en: Who allocates and configures each container's unique IP address?
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谁分配和配置每个容器的唯一IP地址？
- en: Containers on the same host can talk to each other through a virtual bridge
    that you can see with the `brctl` utility from the `bridge-utils` package. This
    is handled by the Docker engine and it's called the Docker networking model. Containers
    are attached to the virtual bridge named `docker0` through a `veth` virtual interface that
    is allocated an IP from a private subnet address. In this way, all containers
    can talk to each other through their `veth` virtual interface. The problem with
    the Docker model arises when containers are allocated on different hosts, or when
    external services want to communicate with them. To solve this, Docker provides
    a method where containers are exposed to the outside world through the host's
    ports. Requests come into a certain port in the host's IP address and are then
    proxied to the container behind that port.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同一主机上的容器可以通过虚拟桥相互通信，您可以使用`bridge-utils`软件包中的`brctl`实用程序看到这一点。这由Docker引擎处理，称为Docker网络模型。容器通过分配IP来附加到名为`docker0`的虚拟桥上的`veth`虚拟接口。这样，所有容器都可以通过它们的`veth`虚拟接口相互通信。Docker模型的问题出现在容器分配在不同主机上，或者外部服务想要与它们通信时。为解决这个问题，Docker提供了一种方法，其中容器通过主机的端口暴露给外部世界。请求进入主机IP地址的某个端口，然后被代理到该端口后面的容器。
- en: This method is useful but not ideal. You can't configure services to specific
    ports or in a dynamic port allocation scenario—our services will require flags
    to connect to the correct ports each time we deploy them. This can get really
    messy very quickly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法很有用，但并非理想。您无法将服务配置为特定端口或在动态端口分配方案中—我们的服务将需要标志每次部署时连接到正确的端口。这可能会很快变得非常混乱。
- en: 'To avoid this, Kubernetes have implemented their own networking model that
    has to comply with the following rules:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，Kubernetes实现了自己的网络模型，必须符合以下规则：
- en: All pods can communicate with all other pods without **network address translation**
    (**NAT**)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有pod可以在没有网络地址转换（NAT）的情况下与所有其他pod通信
- en: All nodes can communicate with all pods without NAT
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有节点可以在没有NAT的情况下与所有pod通信
- en: The IP that the pod sees itself as is the same IP that others see it as
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pod看到的IP与其他人看到的IP相同
- en: 'There are several open source projects out there that can help us to reach
    this goal, and the one that suits you best will depend on your circumstances. Here
    are some of them:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源项目可以帮助我们实现这个目标，最适合您的项目将取决于您的情况。以下是其中一些：
- en: Project Calico
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Project Calico
- en: Weave Net
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave Net
- en: Flannel
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel
- en: Kube-router
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube-router
- en: Assigning IPs to pods and making them talk between them is not the only issue
    to be aware of. Kubernetes also provides DNS-based service discovery, because
    applications that talk through DNS records rather than IPs are far more efficient
    and scalable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为pod分配IP并使它们相互通信并不是唯一需要注意的问题。Kubernetes还提供基于DNS的服务发现，因为通过DNS记录而不是IP进行通信的应用程序更有效和可扩展。
- en: Kubernetes DNS-based service discovery
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Kubernetes的DNS服务发现
- en: Kubernetes has a deployment in its kube-system namespace and we will revisit
    namespaces later in this chapter. The deployment is composed of a pod with a set
    of containers that forms a DNS server that is in charge of creating all DNS records
    in the cluster and serving DNS requests for service discovery.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在其kube-system命名空间中部署了一个部署，我们将在本章后面重新讨论命名空间。该部署由一个包含一组容器的pod组成，形成一个负责在集群中创建所有DNS记录并为服务发现提供DNS请求的DNS服务器。
- en: 'Kubernetes will also create a service pointing to the mentioned deployment,
    and will tell the kubelet to configure each pod''s container to use the service''s
    IP as the DNS resolver by default. This is the default behavior, but you can overwrite
    this by setting a DNS policy on your pod''s specification. You can choose from
    the following specifications:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还将创建一个指向上述部署的服务，并告诉kubelet默认配置每个pod的容器使用服务的IP作为DNS解析器。这是默认行为，但您可以通过在pod规范上设置DNS策略来覆盖此行为。您可以从以下规范中进行选择：
- en: '**Default**: This one is counter-intuitive as it is not the default one in
    reality. With this policy, pods will inherit the name resolution from the node
    that runs that pod. For example, if a node is configured to use `8.8.8.8` as its
    DNS server, the `resolv.conf` pods will also be configured to use that same DNS
    server.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认**：这个是反直觉的，因为实际上并不是默认的。使用此策略，pod将继承运行该pod的节点的名称解析。例如，如果一个节点配置为使用`8.8.8.8`作为其DNS服务器，那么`resolv.conf`中的pod也将被配置为使用相同的DNS服务器。'
- en: '**ClusterFirst**: This is actually the default policy and, as we mentioned
    before, any pod running with ClusterFirst will have `resolv.conf` configured with
    the IP of the `kube-dns` service. Any requests that are not local to the cluster
    will be forwarded to the node''s configured DNS server.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterFirst**：这实际上是默认策略，正如我们之前提到的，任何使用ClusterFirst运行的pod都将使用`kube-dns`服务的IP配置`resolv.conf`。不是本地集群的任何请求都将转发到节点配置的DNS服务器。'
- en: 'Not all Kubernetes objects have DNS records. Only services and, in some specific
    cases, pods have records created for them. There are two types of records in the
    DNS server: **A records** and **service records** (**SRVs**). A records are created
    depending on the type of service created; and we are not referring to `spec.type` here.
    There are two types of services: **normal services**, which we revised in [Chapter
    7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml), *Understanding the Core Components
    of a Kubernetes Cluster*, and correspond to the ones under the `type` specification;
    and **headless services**. Before explaining headless services, let''s explore
    how normal services behave.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有Kubernetes对象都具有DNS记录。只有服务和在某些特定情况下，pod才会为它们创建记录。DNS服务器中有两种类型的记录：**A记录**和**服务记录**（**SRV**）。A记录是根据创建的服务类型创建的；我们这里指的不是`spec.type`。有两种类型的服务：**普通服务**，我们在[第7章](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml)中进行了修订，*理解Kubernetes集群的核心组件*，并对应于`type`规范下的服务；和**无头服务**。在解释无头服务之前，让我们探讨普通服务的行为。
- en: 'For each normal service, an A record that points to the service''s cluster
    IP address is created; these records are in the following structure:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个普通服务，将创建指向服务的集群IP地址的A记录；这些记录的结构如下：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any pod that is running on the same namespace as the service can resolve the
    service through only its `shortname: <service-name>` field. This is because any
    other pod outside of the namespace has to specify the namespace after the shortname
    instance:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '与服务运行在相同命名空间的任何pod都可以通过其`shortname: <service-name>`字段解析服务。这是因为命名空间之外的任何其他pod都必须在shortname实例之后指定命名空间：'
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For headless services, records work a little bit different. First of all, a
    headless service is a service with no cluster IP assigned to it. Therefore, an
    A record that points to the service's IP is impossible to create. To create a
    headless service, you define the `.spec.clusterIP` namespace with `none` in this
    way, so that no IP is assigned to it. Kubernetes will then create A records based
    on the endpoints of this service. Essentially, the pods are selected through the
    `selector` field, although this is not the only requirement. Because of the format
    in which the A record is created, pods require several new fields in order for
    the DNS server to create records for them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无头服务，记录的工作方式有些不同。首先，无头服务是一个没有分配集群IP的服务。因此，无法创建指向服务IP的A记录。要创建无头服务，您以这种方式定义`.spec.clusterIP`命名空间为`none`，以便不为其分配IP。然后，Kubernetes将根据此服务的端点创建A记录。基本上，通过`selector`字段选择pod，尽管这不是唯一的要求。由于A记录的创建格式，pod需要几个新字段，以便DNS服务器为它们创建记录。
- en: 'Pods will require two new specification fields: `hostname` and `subdomain`.
    The `hostname` field will be the `hostname` field of the pod, while `subdomain`
    will be the name of the headless service that you are creating for these pods.
    The A records for this will point to each pod''s IP in the following way:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Pods将需要两个新的规范字段：`hostname`和`subdomain`。`hostname`字段将是pod的`hostname`字段，而`subdomain`将是您为这些pod创建的无头服务的名称。这将指向每个pod的IP的A记录如下：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Additionally, another record will be created with only the headless service,
    as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将创建另一个仅包含无头服务的记录，如下所示：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This record will return all the IP addresses of the pods behind the service.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此记录将返回服务后面所有pod的IP地址。
- en: We now have what's necessary to start building our cluster. However, there are
    still some design features that do not only include the Kubernetes binaries and
    their configuration, Kubernetes API objects can also be tuned. We will go through
    some of the adjustments that you can perform in the next section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经有了开始构建我们的集群所需的东西。但是，还有一些设计特性不仅包括Kubernetes二进制文件及其配置，还可以调整Kubernetes API对象。我们将在下一节中介绍一些您可以执行的调整。
- en: Customizing kube objects
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义kube对象
- en: When it comes to Kubernetes objects, everything will depend on the type of workload
    or application that you are trying to build the infrastructure for. Therefore,
    rather than designing or architecting any particular customization, we will go
    through how to configure the most commonly used and helpful specifications on
    each object.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及Kubernetes对象时，一切都将取决于您尝试为其构建基础架构的工作负载或应用程序的类型。因此，与其设计或构建任何特定的自定义，我们将介绍如何在每个对象上配置最常用和有用的规范。
- en: Namespacing
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名空间
- en: Kubernetes offers namespaces as a way of segmenting your cluster into multiple
    **virtual clusters**. Think of it as a way of segmenting your cluster's resources
    and objects and putting them in logical isolation from each other.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供命名空间作为将集群分割成多个**虚拟集群**的一种方式。将其视为一种将集群资源和对象进行分割并使它们在逻辑上相互隔离的方式。
- en: 'Namespaces will only be used in very specific scenarios, but Kubernetes comes
    with some predefined namespaces:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间只会在非常特定的情况下使用，但Kubernetes带有一些预定义的命名空间：
- en: '**Default**: This is the default namespace that all objects without a namespace
    definition will be placed into.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认**：这是所有没有命名空间定义的对象将放置在其中的默认命名空间。'
- en: '**kube-system**: Any object that is created by and for the Kubernetes cluster
    will be placed on this namespace. Objects that are required for the basic functionality
    of the cluster will be placed here. For example, you will find `kube-dns`, `kubernetes-dashboard`, `kube-proxy`,
    or any additional component or agent for external applications, such as `fluentd`,
    `logstash`, `traefik`, and ingress controllers.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-system**：由Kubernetes集群创建的和为其创建的任何对象都将放置在此命名空间中。用于集群基本功能的必需对象将放置在这里。例如，您将找到`kube-dns`，`kubernetes-dashboard`，`kube-proxy`或任何外部应用程序的其他组件或代理，例如`fluentd`，`logstash`，`traefik`和入口控制器。'
- en: '**kube-public**: A namespace that is reserved for objects that can be visible
    to anyone, including non-authenticated users.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-public**：为任何人可见的对象保留的命名空间，包括非经过身份验证的用户。'
- en: 'Creating a namespace is very simple and straightforward; you can do so by running
    the following command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 创建命名空间非常简单直接；您可以通过运行以下命令来执行：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'That''s it—you now have your own namespace. To place objects in this namespace,
    you will be using the `metadata` field and adding the `namespace` key-value pair;
    for example, consider this excerpt from a YAML pod:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——现在您有了自己的命名空间。要将对象放置在此命名空间中，您将使用`metadata`字段并添加`namespace`键值对；例如，考虑来自YAML
    pod的这段摘录：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You will find yourself creating custom namespaces for clusters that are usually
    very large and have a considerable number of users or different teams that are
    consuming their resources. For these types of scenarios, namespaces are perfect.
    Namespaces will let you segregate all the objects of a team from the rest. Names
    can even be repeated on the same class objects as long as they are on different
    namespaces.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您将发现自己为通常非常庞大并且有相当数量的用户或不同团队在使用资源的集群创建自定义命名空间。对于这些类型的场景，命名空间非常完美。命名空间将允许您将一个团队的所有对象与其余对象隔离开来。名称甚至可以在相同的类对象上重复，只要它们在不同的命名空间中。
- en: Namespaces will not only provide isolation for objects, but you can also set
    resource quotas for each namespace. Let's say that you have a couple of development
    teams working on your cluster—one team is developing a very lightweight application,
    and the other one is developing a very resource-intensive app. In this scenario,
    you don't want the first development team consuming any additional compute resources
    from the resource-intensive app team—this is where resource quotas come into play.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间不仅为对象提供隔离，还可以为每个命名空间设置资源配额。假设您有几个开发团队在集群上工作——一个团队正在开发一个非常轻量级的应用程序，另一个团队正在开发一个非常占用资源的应用程序。在这种情况下，您不希望第一个开发团队从资源密集型应用程序团队那里消耗任何额外的计算资源——这就是资源配额发挥作用的地方。
- en: Limiting namespace resources
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制命名空间资源
- en: Resource quotas are also Kubernetes API objects; however, they are designed
    to work specifically on namespaces by creating limits on compute resources and
    even limiting the number of objects on each assigned space.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额也是Kubernetes API对象；但是，它们被设计为专门在命名空间上工作，通过在每个分配的空间上创建计算资源的限制，甚至限制对象的数量。
- en: The `ResourceQuota` API object is declared like any other object in Kubernetes,
    through a `YAML` file passed to the `kubectl` command.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResourceQuota` API对象与Kubernetes中的任何其他对象一样，通过传递给`kubectl`命令的`YAML`文件声明。'
- en: 'A basic resource quota definition is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基本资源配额定义如下：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There are two types of basic quotas that we can set: compute resource quotas
    and object resource quotas. As seen in the previous example, `pods` is an object
    quota and the rest are compute quotas.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设置两种基本配额：计算资源配额和对象资源配额。如前面的例子所示，“pods”是对象配额，其余是计算配额。
- en: In each of these fields, you will specify the total sum of the provided resource,
    which the namespace cannot exceed. For example, in this namespace, the total number
    of running `pods` cannot exceed `4`, and the sum of their resources can't exceed
    `1` CPU and `2Gi` of RAM memory.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些领域中，您将指定提供的资源的总和，命名空间不能超过。例如，在此命名空间中，运行的“pods”的总数不能超过“4”，它们的资源总和不能超过“1”个CPU和“2Gi”
    RAM内存。
- en: 'The maximum number of objects per namespace can be assigned to any kube API
    object that can be put in a namespace; here is a list of the objects that can
    be limited with namespaces:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为任何可以放入命名空间的kube API对象分配的每个命名空间的最大对象数；以下是可以使用命名空间限制的对象列表：
- en: '**Persistent Volume Claims** (**PVCs**)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久卷索赔**（**PVCs**）'
- en: Services
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: Secrets
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秘密
- en: ConfigMaps
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置映射
- en: Replication controllers
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制控制器
- en: Deployments
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: ReplicaSets
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本集
- en: StatefulSets
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态集
- en: Jobs
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业
- en: Cron jobs
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期作业
- en: When it comes to compute resources, it is not only memory and CPU that can be
    limited, but you can also assign quotas to storage space—these quotas will apply
    only to PVCs, however.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算资源方面，不仅可以限制内存和CPU，还可以为存储空间分配配额——但是这些配额仅适用于PVCs。
- en: In order to understand compute quotas better, we need to dive deeper and explore
    how these resources are managed and assigned on a pod basis. This will also be
    a good time to understand how to architect pods better.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解计算配额，我们需要更深入地了解这些资源是如何在pod基础上管理和分配的。这也是一个很好的时机来了解如何更好地设计pod。
- en: Customizing pods
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义pod
- en: Pods without resource limitations on non-limited namespaces can consume all
    of a node's resources without warning; however, you have a set of tools in the
    pod's specification to handle their compute allocation better.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在非受限命名空间上没有资源限制的pod可以消耗节点的所有资源而不受警告；但是，您可以在pod的规范中使用一组工具来更好地处理它们的计算分配。
- en: 'When you allocate resources to a pod, you are not actually allocating them
    to the pod. Instead, you are doing it on a container basis. Therefore, a pod with
    multiple containers will have multiple resource constraints for each of its containers;
    let''s consider the following example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当您为pod分配资源时，实际上并不是将它们分配给pod。相反，您是在容器基础上进行分配。因此，具有多个容器的pod将为其每个容器设置多个资源约束；让我们考虑以下示例：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this pod declaration, under the `containers` definition, we have two new
    fields that we haven''t covered: `env` and `resources`. The `resources` field
    contains the compute resource limitations and requirements for our `containers`.
    By setting `limits`, you are telling the container the maximum number of resources
    that it can ask of that resource type. If a container exceeds the limit, it will
    be restarted or terminated.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在此pod声明中，在`containers`定义下，我们有两个尚未涵盖的新字段：`env`和`resources`。`resources`字段包含我们的`containers`的计算资源限制和要求。通过设置`limits`，您告诉容器它可以要求该资源类型的最大资源数量。如果容器超过限制，将重新启动或终止。
- en: The `request` field refers to how much of that resource Kubernetes will guarantee
    to that container. In order for the container to be able to run, the host node
    must have enough free resources to satisfy the request.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`request`字段指的是Kubernetes将向该容器保证的资源量。为了使容器能够运行，主机节点必须有足够的空闲资源来满足请求。'
- en: 'CPU and memory are measured in different ways. For instance, when we assign
    or limit CPU, we talk in CPU units. There are several ways of setting the CPU units;
    first, you can either specify round or fractional numbers such as 1, 2, 3, 0.1,
    and 1.5, which will correspond to the number of virtual cores that you want to
    assign to that container. Another way of assigning is to use the **milicore**
    expression. One milicore (1m), which is the minimum CPU quantity that you can
    assign, is equivalent to 0.001 CPU cores; for example, you could do the following
    assignment:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和内存以不同的方式进行测量。例如，当我们分配或限制CPU时，我们使用CPU单位进行讨论。设置CPU单位有几种方法；首先，您可以指定圆整或分数，例如1、2、3、0.1和1.5，这将对应于您要分配给该容器的虚拟核心数。另一种分配的方法是使用**milicore**表达式。一个milicore（1m），是您可以分配的最小CPU数量，相当于0.001
    CPU核心；例如，您可以进行以下分配：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That would be the same as writing the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将与编写以下内容相同：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preferred way of assigning CPU is through Millicores, as the API will convert
    whole numbers into Millicores either way.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 分配CPU的首选方式是通过Millicores，因为API会将整数转换为Millicores。
- en: For memory allocation, you can use normal memory units such as kilobytes or
    kibibytes; the same goes for any other memory unit, such as E, P, T, G, and M.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存分配，您可以使用普通的内存单位，如千字节或基字节；其他内存单位也是如此，如E、P、T、G和M。
- en: Going back to resource quotas, we can see how individual container resource
    management will play together with resource quotas on namespaces. This is because
    the resource quotas will tell us how many limits and requests we can set per namespace
    in our containers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 回到资源配额，我们可以看到单个容器资源管理将如何与命名空间中的资源配额一起发挥作用。这是因为资源配额将告诉我们在容器中每个命名空间中可以设置多少限制和请求。
- en: 'The second field that we haven''t revised is the `env` field. With `env`, we
    configure the environmental variables for our containers. With variable declarations,
    we can pass settings, parameters, passwords, and more configurations to our containers.
    The simplest way to declare a variable in a pod is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有修改的第二个字段是`env`字段。通过`env`，我们为容器配置环境变量。通过变量声明，我们可以将设置、参数、密码和更多配置传递给我们的容器。在pod中声明变量的最简单方式如下：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now the container will have access to the `VAR` variable content in its shell,
    referred to as `$VAR`. As we mentioned previously, this is the easiest way to
    declare a variable and provide a value to it. However, this is not the most efficient
    one—when you declare a value in this way, the value will only live in the pod
    declaration.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在容器将可以在其shell中访问`VAR`变量内容，称为`$VAR`。正如我们之前提到的，这是声明变量并为其提供值的最简单方式。然而，这并不是最有效的方式——当您以这种方式声明值时，该值将仅存在于pod声明中。
- en: 'If we need to edit the value or pass this value to multiple pods, it becomes
    a hassle as you need to type the same value on every pod that requires it. This
    is where we will introduce two more Kubernetes API objects: `Secrets` and `ConfigMaps`.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要编辑值或将该值传递给多个pod，这将变得很麻烦，因为您需要在每个需要它的pod上键入相同的值。这就是我们将介绍另外两个Kubernetes
    API对象：`Secrets`和`ConfigMaps`的地方。
- en: With `ConfigMaps` and `Secrets`, we can store values for our variables in a
    persistent and more modular form. In essence, `ConfigMaps` and `Secrets` are the
    same, but secrets contain their values encoded in `base64`. Secrets are used to
    store sensitive information such as passwords or private keys—essentially, any
    type of confidential data. All the rest of the data that you don't need to be
    hidden can be passed through `ConfigMap`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`ConfigMaps`和`Secrets`，我们可以以持久且更模块化的形式存储变量的值。实质上，`ConfigMaps`和`Secrets`是相同的，但是secrets中包含的值是以`base64`编码的。Secrets用于存储诸如密码或私钥等敏感信息，基本上是任何类型的机密数据。您不需要隐藏的所有其他数据都可以通过`ConfigMap`传递。
- en: 'The way that you create these two types of objects is the same way as with
    any other object in Kubernetes—through `YAML`. You can create a `ConfigMap` object
    as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这两种类型的对象的方式与Kubernetes中的任何其他对象相同——通过`YAML`。您可以按以下方式创建`ConfigMap`对象：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The only difference on this definition, form all the other definitions in this
    chapter is that we are missing the specification field. Instead, we have data
    where we will be placing the key-value pairs that contain the data that we want
    to store.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个定义上唯一的区别，与本章中的所有其他定义相比，就是我们缺少了`specification`字段。相反，我们有`data`，在其中我们将放置包含我们想要存储的数据的键值对。
- en: 'With `Secrets`, this works a little bit differently. This is because the value
    for the key that we need to store has to be encoded. In order to store a value
    in a secret''s key, we pass the value to `base64`, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`Secrets`，这个过程有点不同。这是因为我们需要存储的密钥的值必须进行编码。为了将值存储在秘密的键中，我们将值传递给`base64`，如下所示：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When we have the `base64` hash of our string, we are ready to create our secret.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有字符串的`base64`哈希时，我们就可以创建我们的秘密了。
- en: 'The following code block shows a `YAML` file configured with the secret''s
    value in `base64`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了一个使用`base64`配置了秘密值的`YAML`文件：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To use our `ConfigMaps` and `Secrets` objects in pods, we use the `valueFrom`
    field in the `env` array:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`ConfigMaps`和`Secrets`对象在pod中，我们在`env`数组中使用`valueFrom`字段：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the name under `secretKeyRef` corresponds to the `Secret` API object name,
    and the `key` is the `key` from the `data` field in `Secret`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`secretKeyRef`下的名称对应于`Secret` API对象的名称，而`key`是`Secret`中`data`字段中的`key`。
- en: With `ConfigMaps`, it will look almost the same; however, in the `valueFrom`
    field, we will use `configMapKeyRef` instead of `secretKeyRef`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`ConfigMaps`，它看起来几乎一样；但是，在`valueFrom`字段中，我们将使用`configMapKeyRef`而不是`secretKeyRef`。
- en: 'The `ConfigMap` declaration is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConfigMap`声明如下：'
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that you understand the basics of customizing pods, you can take a look
    at a real-life example at [https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/](https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了定制pod的基础知识，您可以查看一个真实的示例，网址为[https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/](https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/)。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to determine the compute and network requirements
    of a Kubernetes cluster. We also touched upon the software requirements that come
    along with it, such as `etcd`, and how odd-numbered clusters are preferred (due
    to the census algorithm) as the cluster needs to achieve more than 50% of votes
    for consensus.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何确定Kubernetes集群的计算和网络要求。我们还涉及了随之而来的软件要求，比如`etcd`，以及为什么奇数编号的集群更受青睐（由于人口普查算法），因为集群需要获得超过50%的选票才能达成共识。
- en: The `etcd` cluster can either run on the kube-apiserver or have a separate set
    of clusters dedicated just for `etcd`. When it comes to resources, 2 CPUs and
    8 GB of RAM should be enough. When deciding on the storage system for `etcd`,
    opt for lower latency and higher IOPS storage such as SSD. We then jumped into
    sizing the kube-apiserver, which can be run alongside `etcd`. Given that both
    components can coexist, resources should be bumped to anything between 8 and 16
    GB of RAM and between 2 and 4 CPUs per node.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd`集群可以在kube-apiserver上运行，也可以有一个专门用于`etcd`的独立集群。在资源方面，2个CPU和8GB的RAM应该足够了。在决定`etcd`的存储系统时，选择延迟较低、IOPS较高的存储，如SSD。然后我们开始调整kube-apiserver的大小，它可以与`etcd`一起运行。鉴于这两个组件可以共存，每个节点的资源应该在8到16GB的RAM和2到4个CPU之间。'
- en: In order to properly size the worker nodes, we have to keep in mind that this
    is where the actual application workloads will be running. These nodes should
    be sized for application requirements, and additional resources should be considered
    for periods where more than the planned number of pods could be running, such
    as during rolling updates. Continuing with the requirements for the cluster, we
    touched on how a load balancer can help with the master node's communication by
    balancing requests among the cluster.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确调整工作节点的大小，我们必须记住这是实际应用工作负载将要运行的地方。这些节点应根据应用程序要求进行调整，并且在可能会运行超过计划数量的pod的时期，例如在滚动更新期间，应考虑额外的资源。继续讨论集群的要求，我们提到负载均衡器如何通过在集群中平衡请求来帮助主节点的通信。
- en: 'Storage needs for Kubernetes can be quite overwhelming as many factors can
    affect the overall setup, and leaning toward a storage system that benefits reads
    over writes is preferable. Additionally, some of the most common storage providers
    for Kubernetes are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的存储需求可能会非常庞大，因为许多因素可能会影响整体设置，倾向于选择有利于读取而不是写入的存储系统更可取。此外，Kubernetes的一些最常见的存储提供者如下：
- en: Ceph
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceph
- en: GlusterFS (covered in [Chapter 2](7a05974e-0554-45d0-a505-921d484942ef.xhtml), *Defining
    GlusterFS Storage* to [Chapter 5](b140a44b-3594-4c0d-ad7c-03de29a31815.xhtml),
    *Analyzing Performance in a Gluster System*)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS（在[第2章](7a05974e-0554-45d0-a505-921d484942ef.xhtml)中涵盖，*定义GlusterFS存储*到[第5章](b140a44b-3594-4c0d-ad7c-03de29a31815.xhtml)，*分析Gluster系统的性能*）
- en: OpenStack Cinder
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack Cinder
- en: NFS
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS
- en: We then moved on to the networking side of things and learned how Kubernetes
    provides services such as DNS-based service discovery, which is in charge of creating
    all DNS records in the cluster and serving DNS requests for service discovery. Objects
    in Kubernetes can be customized to accommodate the different needs of each workload,
    and things such as namespaces are used as a way of segmenting your cluster into
    multiple virtual clusters. Resource limits can be done through resource quotas.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们转向网络方面，了解了Kubernetes如何提供诸如基于DNS的服务发现之类的服务，负责在集群中创建所有DNS记录并为服务发现提供DNS请求。Kubernetes中的对象可以定制以适应每个工作负载的不同需求，而诸如命名空间之类的东西被用作将您的集群分成多个虚拟集群的一种方式。资源限制可以通过资源配额来实现。
- en: Finally, pods can be customized to allow an absolute maximum of resources to
    be allocated and to avoid a single pod from consuming all of the worker node's
    resources. We discussed the various storage considerations and requirements in
    detail, including how to customize kube objects and pods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以定制pod以允许分配绝对最大的资源，并避免单个pod消耗所有工作节点的资源。我们详细讨论了各种存储考虑和要求，包括如何定制kube对象和pod。
- en: In the next chapter, we'll jump into deploying a Kubernetes cluster and learn
    how to configure it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍如何部署Kubernetes集群并学习如何配置它。
- en: Questions
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why are odd-numbered `etcd` clusters preferred?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么奇数编号的`etcd`集群更受青睐？
- en: Can `etcd` run alongside kube-apiserver?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`etcd`可以与kube-apiserver一起运行吗？'
- en: Why is lower latency recommended for `etcd`?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么建议`etcd`的延迟较低？
- en: What are the worker nodes?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是工作节点？
- en: What should be considered when sizing worker nodes?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在调整工作节点大小时应考虑什么？
- en: What are some of the storage providers for Kubernetes?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes的一些存储提供者是什么？
- en: Why is a load balancer needed?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么需要负载均衡器？
- en: How can namespaces be used?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命名空间如何使用？
- en: Further reading
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Mastering Kubernetes* by Gigi Sayfan: [https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*掌握Kubernetes* 作者Gigi Sayfan：[https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
- en: '*Kubernetes for Developers* by Joseph Heck: [https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面向开发人员的Kubernetes* 作者Joseph Heck：[https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)'
- en: '*Hands-On Microservices with Kubernetes* by Gigi Sayfan: [https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes](https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Kubernetes进行微服务实践* 作者Gigi Sayfan：[https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes](https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes)'
- en: '*Getting Started with Kubernetes – Third Edition* by Jonathan Baier, Jesse
    White: [https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开始使用Kubernetes-第三版* 作者Jonathan Baier，Jesse White：[https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
- en: '*Mastering Docker – Second Edition* by Russ McKendrick, Scott Gallagher: [https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition](https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*掌握Docker-第二版* 作者Russ McKendrick，Scott Gallagher：[https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition](https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition)'
- en: '*Docker Bootcamp* by Russ McKendrick et al.: [https://www.packtpub.com/virtualization-and-cloud/docker-bootcamp](https://www.packtpub.com/virtualization-and-cloud/docker-bootcamp)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

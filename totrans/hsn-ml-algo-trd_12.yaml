- en: Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *Machine Learning
    Process*, we discussed how unsupervised learning adds value by uncovering structures
    in the data without an outcome variable, such as a teacher, to guide the search
    process. This task contrasts with the setting for supervised learning that we
    focused on in the last several chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml)，*机器学习过程*中，我们讨论了无监督学习如何在没有结果变量（如教师）指导搜索过程的情况下，发现数据中的结构。这个任务与我们在最近几章中专注的监督学习的设置形成对比。
- en: Unsupervised learning algorithms can be useful when a dataset contains only
    features and no measurement of the outcome, or when we want to extract information
    independent of the outcome. Instead of predicting future outcomes, the goal is
    to study an informative representation of the data that is useful for solving
    another task, including the exploration of a dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集仅包含特征而没有结果的测量时，或者当我们希望独立于结果提取信息时，无监督学习算法可能是有用的。目标不是预测未来结果，而是研究对解决另一个任务有用的数据的信息表示，包括数据集的探索。
- en: Examples include identifying topics to summarize documents (see [Chapter 14](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml),
    *Topic Modeling*), reducing the number of features to reduce the risk of overfitting
    and the computational cost for supervised learning, or grouping similar observations,
    as illustrated by the use of clustering for asset allocation at the end of this
    chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括识别主题以总结文档（参见[第14章](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml)，*主题建模*），减少特征数量以减少过度拟合的风险和监督学习的计算成本，或者将相似的观察结果分组，正如本章末尾使用聚类进行资产配置所示。
- en: 'Dimensionality reduction and clustering are the main tasks for unsupervised
    learning:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 降维和聚类是无监督学习的主要任务：
- en: '**Dimensionality reduction** transforms the existing features into a new, smaller
    set, while minimizing the loss of information. A broad range of algorithms exists
    that differ only in how they measure the loss of information, whether they apply
    linear or non-linear transformations, or the constraints they impose on the new
    feature set.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**将现有特征转换为一个新的、更小的集合，同时最小化信息的损失。存在广泛的算法，它们仅在衡量信息损失的方式、应用线性或非线性转换或对新特征集施加的约束方面存在差异。'
- en: '**Clustering algorithms** identify and group similar observations or features
    instead of identifying new features. Algorithms differ in how they define the
    similarity of observations and their assumptions about the resulting groups.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**识别和分组相似的观察结果或特征，而不是识别新特征。算法在定义观察结果的相似性以及它们对生成的群组的假设方面存在差异。'
- en: 'More specifically, this chapter covers the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，本章涵盖以下内容：
- en: How **Principal Component Analysis** (**PCA**) and **Independent Component Analysis** (**ICA**)
    perform linear dimensionality reduction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）和**独立成分分析**（**ICA**）如何执行线性降维。'
- en: How to apply PCA to identify risk factors and eigen portfolios from asset returns
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用PCA从资产收益中识别风险因素和特征组合
- en: How to use non-linear manifold learning to summarize high-dimensional data for
    effective visualization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用非线性流形学习来总结高维数据以实现有效的可视化
- en: How to use t-SNE and UMAP to explore high-dimensional alternative image data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用t-SNE和UMAP来探索高维替代图像数据
- en: How k-Means, hierarchical, and density-based clustering algorithms work
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值、分层和基于密度的聚类算法的工作原理
- en: How to use agglomerative clustering to build robust portfolios according to
    hierarchical risk parity
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用凝聚聚类来根据分层风险平价构建稳健的投资组合
- en: The code samples for each section are in the directory of the online GitHub
    repository for this chapter at [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分的代码示例都在本章的在线GitHub存储库目录中，网址为[https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading)。
- en: Dimensionality reduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: In linear algebra terms, the features of a dataset create a **vector space**
    whose dimensionality corresponds to the number of linearly independent columns
    (assuming there are more observations than features). Two columns are linearly
    dependent when they are perfectly correlated so that one can be computed from
    the other using the linear operations of addition and multiplication.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数术语中，数据集的特征创建了一个**向量空间**，其维度对应于线性独立列的数量（假设观察结果多于特征）。当两列线性相关时，它们是线性相关的，因此可以使用加法和乘法的线性运算从一个列计算另一个列。
- en: In other words, they are parallel vectors that represent the same rather than
    different directions or axes and only constitute a single dimension. Similarly,
    if one variable is a linear combination of several others, then it is an element
    of the vector space created by those columns, rather than adding a new dimension
    of its own.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它们是代表相同而不是不同方向或轴的平行向量，只构成一个单一维度。同样，如果一个变量是其他几个变量的线性组合，则它是由这些列创建的向量空间的一个元素，而不是添加自己的新维度。
- en: 'The number of dimensions of a dataset matter because each new dimension can
    add a signal concerning an outcome. However, there is also a downside known as
    the **curse of dimensionality**: as the number of independent features grows while
    the number of observations remains constant, the average distance between data
    points also grows, and the density of the feature space drops exponentially.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的维度数量很重要，因为每个新维度都可能添加有关结果的信号。然而，也存在一个被称为**维度灾难**的缺点：随着独立特征的数量增加，而观察结果的数量保持不变，数据点之间的平均距离也增加，特征空间的密度呈指数级下降。
- en: The implications for machine learning are dramatic because prediction becomes
    much harder when observations are more distant; that is, different to each other.
    The next section addresses the resulting challenges.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction seeks to represent the information in the data more
    efficiently by using fewer features. To this end, algorithms project the data
    to a lower-dimensional space while discarding variation in the data that is not
    informative, or by identifying a lower-dimensional subspace or manifold on or
    near which the data lives.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: A manifold is a space that locally resembles Euclidean space. One-dimensional
    manifolds include lines and circles (but not screenshots of eight, due to the
    crossing point). The manifold hypothesis maintains that high-dimensional data
    often resides in a lower-dimensional space that, if identified, permits a faithful
    representation of the data in this subspace.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction thus compresses the data by finding a different, smaller
    set of variables that capture what matters most in the original features to minimize
    the loss of information. Compression helps counter the curse of dimensionality,
    economizes on memory, and permits the visualization of salient aspects of higher-dimensional
    data that is otherwise very difficult to explore.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Linear and non-linear algorithms
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dimensionality reduction algorithms differ in the constraints they impose on
    the new variables and how they aim to minimize the loss of information:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear algorithms** such as PCA and ICA constrain the new variables to be
    linear combinations of the original features; that is, hyperplanes in a lower-dimensional
    space. Whereas PCA requires the new features to be uncorrelated, ICA goes further
    and imposes statistical independence—the absence of both linear and non-linear
    relationships. The following screenshot illustrates how PCA projects three-dimensional
    features into a two-dimensional space:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/07053c5f-7c75-45e0-be31-03b59be1d355.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'Non-linear algorithms are not restricted to hyperplanes and can capture more
    complex structure in the data. However, given the infinite number of options,
    the algorithms still need to make assumptions to arrive at a solution. In this
    section, we show how **t-distributed Stochastic Neighbor Embedding** (**t-SNE**)
    and **Uniform Manifold Approximation and Projection** (**UMAP**) are very useful
    for visualizing higher-dimensional data. The following screenshot illustrates
    how manifold learning identifies a two-dimensional sub-space in the three-dimensional
    feature space (the `manifold_learning` notebook illustrates the use of additional
    algorithms, including local linear embedding):'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/34edbd7d-ac2c-47e3-9e30-353957b438cc.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: The curse of dimensionality
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An increase in the number of dimensions of a dataset means there are more entries
    in the vector of features that represents each observation in the corresponding
    Euclidean space. We measure the distance in a vector space using Euclidean distance,
    also known as the **L2 norm**, which we applied to the vector of linear regression
    coefficients to train a regularized Ridge Regression model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'The Euclidean distance between two *n*-dimensional vectors with Cartesian coordinates
    *p = (p[1], p[2], ..., p[n])* and *q = (q[1], q[2], ..., q[n])* is computed using
    the familiar formula developed by Pythagoras:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bdfbf33-f078-47cb-98a7-36996940182d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Hence, each new dimension adds a non-negative term to the sum, so that the distance
    increases with the number of dimensions for distinct vectors. In other words,
    as the number of features grows for a given number of observations, the feature
    space becomes increasingly sparse; that is, less dense or emptier. On the flip
    side, the lower data density requires more observations to keep the average distance
    between data points the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chart shows how many data points we need to maintain the average
    distance of 10 observations uniformly distributed on a line. It increases exponentially
    from 10¹ in a single dimension to 10² in two and 10³ in three dimensions, as the
    data needs to expand by a factor of 10 each time we add a new dimension:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc8d524c-d176-474d-a115-5a48dac9902f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'The `curse_of_dimensionality` notebook in the GitHub repo folder for this section
    simulates how the average and minimum distances between data points increase as
    the number of dimensions grows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf927dcd-7464-4663-903d-2d3ef684379f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: The simulation draws features in the range [0, 1] from uncorrelated uniform
    or correlated normal distributions, and gradually increases the number of features
    to 2,500\. The average distance between data points increases to over 11 times
    the feature range for features drawn from the normal distribution, and to over
    20 times in the (extreme) case of uncorrelated uniform distribution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: When the distance between observations grows, supervised machine learning becomes
    more difficult because predictions for new samples are less likely to be based
    on learning from similar training features. Put differently, the number of possible
    unique rows grows exponentially as the number of features increases, which makes
    it so much harder to efficiently sample the space.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the complexity of the functions learned by flexible algorithms that
    make fewer assumptions about the actual relationship grows exponentially with
    the number of dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Flexible algorithms include the tree-based models we saw in [Chapter 10](7d7aa662-362a-4c3a-acda-a18fb1bad6e7.xhtml), *Decision
    Trees and Random Forests*, and [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml),
    *Gradient Boosting Machines*, and the deep neural networks that we will cover
    from Chapter 17, *Deep Learning* onward. The variance of these algorithms increases
    as they get more opportunity to overfit to noise in more dimensions, resulting
    in poor generalization performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: In practice, features are correlated, often substantially so, or do not exhibit
    much variation. For these reasons, dimensionality reduction helps to compress
    the data without losing much of the signal, and combat the curse while also economizing
    on memory. In these cases, it complements the use of regularization to manage
    prediction error due to variance and model complexity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The critical question that we take on in the following section then becomes:
    what are the best ways to find a lower-dimensional representation of the data
    that loses as little information as possible?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Linear dimensionality reduction
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear dimensionality reduction algorithms compute linear combinations that
    translate, rotate, and rescale the original features to capture significant variation
    in the data, subject to constraints on the characteristics of the new features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**), invented in 1901 by Karl Pearson,
    finds new features that reflect directions of maximal variance in the data while
    being mutually uncorrelated, or orthogonal.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**), in contrast, originated in signal
    processing in the 1980s, with the goal of separating different signals while imposing
    the stronger constraint of statistical independence.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This section introduces these two algorithms and then illustrates how to apply
    PCA to asset returns to learn risk factors from the data, and to build so-called
    eigen portfolios for systematic trading strategies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA finds principal components as linear combinations of the existing features
    and uses these components to represent the original data. The number of components
    is a hyperparameter that determines the target dimensionality and needs to be
    equal to or smaller than the number of observations or columns, whichever is smaller.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: PCA aims to capture most of the variance in the data, to make it easy to recover
    the original features and so that each component adds information. It reduces
    dimensionality by projecting the original data into the principal component space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PCA旨在捕获数据中的大部分方差，以便轻松恢复原始特征，使每个成分都添加信息。它通过将原始数据投影到主成分空间来降低维度。
- en: The PCA algorithm works by identifying a sequence of principal components, each
    of which aligns with the direction of maximum variance in the data after accounting
    for variation captured by previously-computed components. The sequential optimization
    also ensures that new components are not correlated with existing components so
    that the resulting set constitutes an orthogonal basis for a vector space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法通过识别一系列主成分来工作，每个主成分都与数据中方差的最大方向对齐，同时考虑先前计算的成分捕获的变化。顺序优化还确保新成分与现有成分不相关，从而确保生成的集合构成向量空间的正交基础。
- en: This new basis corresponds to a rotated version of the original basis so that
    the new axis point in the direction of successively decreasing variance. The decline
    in the amount of variance of the original data explained by each principal component
    reflects the extent of correlation among the original features.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的基础对应于原始基础的旋转版本，使得新轴指向逐渐减小的方差的方向。每个主成分解释的原始数据方差量的下降反映了原始特征之间相关程度的程度。
- en: The number of components that capture, for example, 95% of the original variation
    relative to the total number of features provides an insight into the linearly-independent
    information in the original data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，捕获原始变化95%的成分数量相对于总特征数量提供了对原始数据中线性独立信息的见解。
- en: Visualizing PCA in 2D
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在2D中可视化PCA
- en: 'The following screenshot illustrates several aspects of PCA for a two-dimensional
    random dataset (see the `pca_key_ideas` notebook):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图说明了二维随机数据集的PCA的几个方面（请参阅`pca_key_ideas`笔记本）：
- en: '![](img/f122db9e-99db-4235-b0d3-2f84dbaac1c1.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f122db9e-99db-4235-b0d3-2f84dbaac1c1.png)'
- en: The left panel shows how the first and second principal components align with
    the directions of maximum variance while being orthogonal.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左面板显示了第一和第二主成分如何与最大方差的方向对齐，同时是正交的。
- en: The central panel shows how the first principal component minimizes the reconstruction
    error, measured as the sum of the distances between the data points and the new
    axis.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央面板显示了第一个主成分如何最小化重构误差，重构误差是指数据点与新轴之间的距离之和。
- en: Finally, the right panel illustrates supervised OLS, which approximates the
    outcome variable (here we choose x[2]) by a (one-dimensional) hyperplane computed
    from the (single) feature. The vertical lines highlight how OLS minimizes the
    distance along the outcome axis, in contrast with PCA, which minimizes the distances
    orthogonal to the hyperplane.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，右面板说明了监督OLS，它通过从（单个）特征计算出的（一维）超平面来近似结果变量（这里我们选择x[2]）。垂直线突出显示了OLS如何最小化沿结果轴的距离，与PCA相反，PCA最小化了与超平面正交的距离。
- en: The assumptions made by PCA
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA所做的假设
- en: 'PCA makes several assumptions that are important to keep in mind. These include
    the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA做出了几个重要的假设，这些假设需要牢记。其中包括以下内容：
- en: High variance implies a high signal-to-noise ratio
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高方差意味着高信噪比
- en: The data is standardized so that the variance is comparable across features
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被标准化，以便各个特征的方差是可比较的。
- en: Linear transformations capture the relevant aspects of the data
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换捕获了数据的相关方面
- en: Higher-order statistics beyond the first and second moment do not matter, which
    implies that the data has a normal distribution
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一和第二时刻之外的高阶统计不重要，这意味着数据具有正态分布
- en: The emphasis on the first and second moments aligns with standard risk/return
    metrics, but the normality assumption may conflict with the characteristics of
    market data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对第一和第二时刻的强调与标准的风险/回报指标一致，但正态性假设可能与市场数据的特征相冲突。
- en: How the PCA algorithm works
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA算法的工作原理
- en: The algorithm finds vectors to create a hyperplane of target dimensionality
    that minimizes the reconstruction error, measured as the sum of the squared distances
    of the data points to the plane. As illustrated above, this goal corresponds to
    finding a sequence of vectors that align with directions of maximum retained variance
    given the other components while ensuring all principal components are mutually
    orthogonal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到向量来创建目标维度的超平面，以最小化重构误差，重构误差是指数据点到平面的距离的平方和。如上所述，这个目标对应于找到一系列向量，这些向量与最大保留方差的方向对齐，同时确保所有主成分彼此正交。
- en: In practice, the algorithm solves the problem either by computing the eigenvectors
    of the covariance matrix or using the singular value decomposition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，该算法通过计算协方差矩阵的特征向量或使用奇异值分解来解决问题。
- en: 'We illustrate the computation using a randomly generated three-dimensional
    ellipse with 100 data points, shown in the left panel of the following screenshot,
    including the two-dimensional hyperplane defined by the first two principal components
    (see the `the_math_behind_pca` notebook for the following code samples):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个随机生成的三维椭圆来说明计算，包括100个数据点，显示在以下截图的左面板中，包括由前两个主成分定义的二维超平面（请参阅`the_math_behind_pca`笔记本中的以下代码示例）：
- en: '![](img/0622eac2-a2c5-49dc-8d25-6e178b44e0ae.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0622eac2-a2c5-49dc-8d25-6e178b44e0ae.png)'
- en: Three-dimensional ellipse and two-dimensional hyperplane
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 三维椭圆和二维超平面
- en: PCA based on the covariance matrix
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于协方差矩阵的PCA
- en: 'We first compute the principal components using the square covariance matrix
    with the pairwise sample covariances for the features *x[i], x[j],* *i*, *j* =
    1, ..., *n* as entries in row *i* and column *j*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用成对样本协方差计算特征*x[i], x[j],* *i*, *j* = 1, ..., *n*作为第*i*行和第*j*列的条目来计算主成分：
- en: '![](img/33024c77-1334-4fa2-a548-3a2738854670.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33024c77-1334-4fa2-a548-3a2738854670.png)'
- en: 'For a square matrix *M* of *n* dimensions, we define the eigenvectors *ω[i]*
    and eigenvalues *λ[i]*, *i*=1, ..., *n* as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18c48888-f82f-4ec6-8b71-e22db24b4c18.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Hence, we can represent the matrix *M* using eigenvectors and eigenvalues,
    where *W* is a matrix that contains the eigenvectors as column vectors, and *L*
    is a matrix that contains the λ[i] as diagonal entries (and 0s otherwise). We
    define the eigendecomposition as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5efee6d1-23ff-4b2d-861a-4689996908ce.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Using NumPy, we implement this as follows, where the pandas DataFrame contains
    the 100 data points of the ellipse:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we calculate the eigenvectors and eigenvalues of the covariance matrix.
    The eigenvectors contain the principal components (where the sign is arbitrary):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can compare the result with the result obtained from sklearn, and find that
    they match in absolute terms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also verify the eigendecomposition, starting with the diagonal matrix
    *L* that contains the eigenvalues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We find that the result does indeed hold:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PCA using Singular Value Decomposition
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we'll look at the alternative computation using **Singular Value Decomposition**
    (**SVD**). This algorithm is slower when the number of observations is greater
    than the number of features (the typical case), but yields better numerical stability,
    especially when some of the features are strongly correlated (often the reason
    to use PCA in the first place).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'SVD generalizes the eigendecomposition that we just applied to the square and
    symmetric covariance matrix to the more general case of *m* x *n* rectangular
    matrices. It has the form shown at the center of the following diagram. The diagonal
    values of *Σ* are the singular values, and the transpose of *V** contains the
    principal components as column vectors:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a28fb8b0-14c4-4a60-a026-247d71cd6af9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we need to make sure our data is centered with mean zero (the
    computation of the covariance preceding took care of this):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We find that the decomposition does indeed reproduce the standardized data:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Lastly, we confirm that the columns of the transpose of *V** contain the principal
    components:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the next section, we show how sklearn implements PCA.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: PCA with sklearn
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn.decomposition.PCA` implementation follows the standard API based
    on the `fit()` and `transform()` methods, which compute the desired number of
    principal components and project the data into the component space, respectively.
    The convenience method `fit_transform()` accomplishes this in a single step.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA offers three different algorithms that can be specified using the `svd_solver`
    parameter:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '`Full` computes the exact SVD using the LAPACK solver provided by SciPy'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Arpack` runs a truncated version suitable for computing less than the full
    number of components'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Randomized` uses a sampling-based algorithm that is more efficient when the
    dataset has more than 500 observations and features, and the goal is to compute
    less than 80% of the components'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Auto` uses randomized where most efficient, otherwise, it uses the full SVD'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See references on GitHub for algorithmic implementation details.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Other key configuration parameters of the PCA object are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components`: These compute all principal components by passing `None` (the
    default), or limit the number to `int`. For `svd_solver=full`, there are two additional
    options: a float in the interval [0, 1] computes the number of components required
    to retain the corresponding share of the variance in the data, and the `mle` option
    estimates the number of dimensions using maximum likelihood.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`whiten`: If `True`, it standardizes the component vectors to unit variance
    that, in some cases, can be useful for use in a predictive model (the default
    is `False`).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To compute the first two principal components of the three-dimensional ellipsis
    and project the data into the new space, use `fit_transform()` as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The explained variance of the first two components is very close to 100%:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The screenshot at the beginning of this section shows the projection of the
    data into the new two-dimensional space.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Independent Component Analysis
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**) is another linear algorithm that
    identifies a new basis on which to represent the original data, but pursues a
    different objective to PCA.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: ICA emerged in signal processing, and the problem it aims to solve is called
    **blind source separation**. It is typically framed as the cocktail party problem,
    in which a given number of guests are speaking at the same time so that a single
    microphone would record overlapping signals. ICA assumes there are as many different
    microphones as there are speakers, each placed at different locations so as to
    record a different mix of the signals. ICA then aims to recover the individual
    signals from the different recordings.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, there are *n* original signals and an unknown square mixing
    matrix *A* that produces an *n*-dimensional set of *m* observations, so that:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e242918a-364d-4f7b-8061-8faaf7bb772e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: The goal is to find the matrix *W=A^(-1)* that untangles the mixed signals to
    recover the sources.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The ability to uniquely determine the matrix *W* hinges on the non-Gaussian
    distribution of the data. Otherwise, *W* could be rotated arbitrarily given the
    multivariate normal distribution's symmetry under rotation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, ICA assumes the mixed signal is the sum of its components and is
    unable to identify Gaussian components because their sum is also normally distributed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: ICA assumptions
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ICA makes the following critical assumptions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The sources of the signals are statistically independent
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear transformations are sufficient to capture the relevant information
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The independent components do not have a normal distribution
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mixing matrix A can be inverted
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICA also requires the data to be centered and whitened; that is, to be mutually
    uncorrelated with unit variance. Preprocessing the data using PCA as outlined
    above achieves the required transformations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The ICA algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FastICA, used by sklearn, is a fixed-point algorithm that uses higher-order
    statistics to recover the independent sources. In particular, it maximizes the
    distance to a normal distribution for each component as a proxy for independence.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: An alternative algorithm called **InfoMax** minimizes the mutual information
    between components as a measure of statistical independence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: ICA with sklearn
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ICA implementation by sklearn uses the same interface as PCA, so there is
    little to add. Note that there is no measure of explained variance because ICA
    does not compute components successively. Instead, each component aims to capture
    independent aspects of the data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: PCA for algorithmic trading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is useful for algorithmic trading in several respects. These include the
    data-driven derivation of risk factors by applying PCA to asset returns, and the
    construction of uncorrelated portfolios based on the principal components of the
    correlation matrix of asset returns.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Data-driven risk factors
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear Models*,
    we explored risk factor models used in quantitative finance to capture the main
    drivers of returns. These models explain differences in returns on assets based
    on their exposure to systematic risk factors and the rewards associated with these
    factors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we explored the Fama-French approach, which specifies factors
    based on prior knowledge about the empirical behavior of average returns, treats
    these factors as observable, and then estimates risk model coefficients using
    linear regression. An alternative approach treats risk factors as latent variables
    and uses factor analytic techniques such as PCA to simultaneously estimate the
    factors and how they drive returns from historical returns.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will review how this method derives factors in a purely
    statistical or data-driven way, with the advantage of not requiring ex-ante knowledge
    of the behavior of asset returns (see the `pca` and `risk_factor` notebook models
    for details).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the Quandl stock price data and select the daily adjusted close
    prices of the 500 stocks with the largest market capitalization and data for the
    2010-18 period. We then compute the daily returns as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We obtain `351` stocks and returns for over 2,000 trading days:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'PCA is sensitive to outliers, so we winsorize the data at the 2.5% and 97.5%
    quantiles:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'PCA does not permit missing data, so we will remove stocks that do not have
    data for at least 95% of the time period, and in a second step, remove trading
    days that do not have observations on at least 95% of the remaining stocks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are left with `314` equity return series covering a similar period:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We impute any remaining missing values using the average return for any given
    trading day:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we are ready to fit the principal components model to the asset returns
    using default parameters to compute all components using the full SVD algorithm:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We find that the most important factor explains around 40% of the daily return
    variation. The dominant factor is usually interpreted as the market, whereas the
    remaining factors can be interpreted as industry or style factors, in line with
    our discussion in [Chapter 5](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml), *Strategy
    Evaluation*, and [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear
    Models*, depending on the results of closer inspection (see the next example).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot on the right shows the cumulative explained variance, and indicates
    that around 10 factors explain 60% of the returns of this large cross-section
    of stocks:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2d2b23c-65a0-4a9f-a1ea-c48d9c225b8b.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: The notebook contains a simulation for a broader cross-section of stocks and
    the longer 2000-18 time period. It finds that, on average, the first three components
    explained 25%, 10%, and 5% of 500 randomly selected stocks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The cumulative plot shows a typical elbow pattern that can help identify a suitable
    target dimensionality because it indicates that additional components add less
    explanatory value.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'We can select the top two principal components to verify that they are indeed
    uncorrelated:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Moreover, we can plot the time series to highlight how each factor captures
    different volatility patterns:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a88573bd-0658-466e-8330-5af9cdf17d86.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: A risk factor model would employ a subset of the principal components as features
    to predict future returns, similar to our approach in [Chapter 7](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=572&action=edit#post_28), *Linear
    Models – Regression and Classification*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Eigen portfolios
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another application of PCA involves the covariance matrix of the normalized
    returns. The principal components of the correlation matrix capture most of the
    covariation among assets in descending order and are mutually uncorrelated. Moreover,
    we can use standardized principal components as portfolio weights.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the 30 largest stocks with data for the 2010-2018 period to facilitate
    the exposition:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We again winsorize and also normalize the returns:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After dropping assets and trading days as in the previous example, we are left
    with 23 assets and over 2,000 trading days. We estimate all principal components,
    and find that the two largest explain 55.9% and 15.5% of the covariation, respectively:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we select and normalize the four largest components so that they sum
    to `1` and we can use them as weights for portfolios that we can compare to an
    equal-weighted portfolio formed from all stocks:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The weights show distinct emphasis—for example, **Portfolio 3** puts large
    weights on Mastercard and Visa, the two payment processors in the sample, whereas
    **Portfolio 2** has more exposure to technology companies:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a89efa65-3d1b-4201-848c-18967d6e4a53.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'When comparing the performance of each portfolio over the sample period to
    The Market consisting of our small sample, we find that portfolio 1 performs very
    similarly, whereas the other portfolios capture different return patterns:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5475635-cc5e-4952-8b51-69e94163a966.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Comparing performances of each portfolio
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Manifold learning
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear dimensionality reduction projects the original data onto a lower-dimensional
    hyperplane that aligns with informative directions in the data. The focus on linear
    transformations simplifies the computation and echoes common financial metrics,
    such as PCA's goal to capture the maximum variance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: However, linear approaches will naturally ignore signal reflected in non-linear
    relationships in the data. Such relationships are very important in alternative
    datasets containing, for example, image or text data. Detecting such relationships
    during exploratory analysis can provide important clues about the data's potential
    signal content.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the manifold hypothesis emphasizes that high-dimensional data often
    lies on or near a lower-dimensional non-linear manifold that is embedded in the
    higher dimensional space. The two-dimensional swiss roll displayed in the screenshot
    at the beginning of this section illustrates such a topological structure.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Manifold learning aims to find the manifold of intrinsic dimensionality and
    then represent the data in this subspace. A simplified example uses a road as
    one-dimensional manifolds in a three-dimensional space and identifies data points
    using house numbers as local coordinates.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Several techniques approximate a lower dimensional manifold. One example is
    **locally-linear embedding** (**LLE**), which was developed in 2000 by Sam Roweis
    and Lawrence Saul and used to unroll the swiss roll in the previous screenshot
    (see examples in the `manifold_learning_lle` notebook).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: For each data point, LLE identifies a given number of nearest neighbors and
    computes weights that represent each point as a linear combination of its neighbors.
    It finds a lower-dimensional embedding by linearly projecting each neighborhood
    onto global internal coordinates on the lower-dimensional manifold, and can be
    thought of as a sequence of PCA applications.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Visualization requires the reduction to at least three dimensions, possibly
    below the intrinsic dimensionality, and poses the challenge of faithfully representing
    local and global structure. This challenge relates to the increasing distance
    associated with the curse of dimensionality. While the volume of a sphere expands
    exponentially with the number of dimensions, the space in lower dimensions available
    to represent high-dimensional data is much more limited.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in 12 dimensions, there can be 13 equidistant points, but in two
    dimensions there can only be three that form a triangle with sides of equal length.
    Hence, accurately reflecting the distance of one point to its high-dimensional
    neighbors in lower dimensions risks distorting the relations among all other points.
    The result is the crowding problem: to maintain global distances, local points
    may need to be placed too closely together, and vice versa.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two sections cover techniques that have made progress in addressing
    the crowding problem for the visualization of complex datasets. We will use the
    fashion `MNIST` dataset, a more sophisticated alternative to the classic handwritten
    digit MNIST benchmark data used for computer vision. It contains 60,000 train
    and 10,000 test images of fashion objects in 10 classes (see following samples):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaba9ade-5f81-43d1-abad-f3d1034a6f1d.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: The goal of a manifold learning algorithm for this data is to detect whether
    the classes lie on distinct manifolds, to facilitate their recognition and differentiation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The t-distributed stochastic neighbor embedding is an award-winning algorithm
    developed in 2010 by Laurens van der Maaten and Geoff Hinton to detect patterns
    in high-dimensional data. It takes a probabilistic, non-linear approach to locating
    data on several different but related low-dimensional manifolds.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm emphasizes keeping similar points together in low dimensions,
    as opposed to maintaining the distance between points that are apart in high dimensions,
    which results from algorithms such as PCA that minimize squared distances.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm proceeds by converting high-dimensional distances to (conditional)
    probabilities, where high probabilities imply low distance and reflect the likelihood
    of sampling two points based on similarity. It accomplishes this by positioning
    a normal distribution over each point and computing the density for a point and
    each neighbor, where the perplexity parameter controls the effective number of
    neighbors.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: In a second step, it arranges points in low dimensions and uses similarly computed
    low-dimensional probabilities to match the high-dimensional distribution. It measures
    the difference between the distributions using the Kullback-Leibler divergence,
    which puts a high penalty on misplacing similar points in low dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The low-dimensional probabilities use a Student's t-distribution with one degree
    of freedom, as it has fatter tails that reduce the penalty of misplacing points
    that are more distant in high dimensions, to manage the crowding problem.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The upper panels of the following chart show how t-SNE is able to differentiate
    between the image classes. A higher perplexity value increases the number of neighbors
    used to compute local structure, and gradually results in more emphasis on global
    relationships:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/644b48d6-0a84-43cc-9e00-5f6a0d374c1f.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: t-SNE is currently the state-of-the-art in high-dimensional data visualization.
    Weaknesses include the computational complexity that scales quadratically in the
    number *n* of points because it evaluates all pairwise distances, but a subsequent
    tree-based implementation has reduced the cost to *n log n*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE does not facilitate the projection of new data points into the low-dimensional
    space. The compressed output is not a very useful input for distance-based or
    density-based cluster algorithms, because t-SNE treats small and large distances
    differently.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: UMAP
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Uniform Manifold Approximation and Projection is a more recent algorithm for
    visualization and general dimensionality reduction. It assumes the data is uniformly
    distributed on a locally-connected manifold and looks for the closest low-dimensional
    equivalent using fuzzy topology. It uses a neighbors parameter that impacts the
    result similarly as perplexity above.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: It is faster, and hence scales better to large datasets than t-SNE, and sometimes
    preserves global structure than better than t-SNE. It can also work with different
    distance functions, including, for example, cosine similarity, which is used to
    measure the distance between word count vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The four charts in the bottom row of the previous figure illustrates how UMAP
    does indeed move the different clusters further apart, whereas t-SNE provides
    more granular insight into the local structure.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The notebook also contains interactive Plotly visualizations for each algorithm,
    which permit the exploration of the labels and identify which objects are placed
    close to each other.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both clustering and dimensionality reduction summarize the data. As just discussed
    in detail, dimensionality reduction compresses the data by representing it using
    new, fewer features that capture the most relevant information. Clustering algorithms,
    by contrast, assign existing observations to subgroups that consist of similar
    data points.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Clustering can serve to better understand the data through the lens of categories
    learned from continuous variables. It also permits automatically categorizing
    new objects according to the learned criteria. Examples of related applications
    include hierarchical taxonomies, medical diagnostics, and customer segmentation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, clusters can be used to represent groups as prototypes, using
    (for example) the midpoint of a cluster as the best representative of learned
    grouping. An example application includes image compression.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering algorithms differ with respect to their strategies for identifying
    groupings:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Combinatorial algorithms select the most coherent of different groupings of
    observations
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic modeling estimates distributions that most likely generated the
    clusters
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering finds a sequence of nested clusters that optimizes coherence
    at any given stage
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Algorithms also differ in their notion of what constitutes a useful collection
    of objects, which needs to match the data characteristics, domain, and the goal
    of the applications. Types of groupings include the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Clearly separated groups of various shapes
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prototype-based or center-based compact clusters
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based clusters of arbitrary shape
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectivity-based or graph-based clusters
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Important additional aspects of a clustering algorithm include the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Whether it requires exclusive cluster membership
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether it makes hard (binary) or soft (probabilistic) assignment
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether it is complete and assigns all data points to clusters
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following sections introduce key algorithms, including k-Means, hierarchical,
    and density-based clustering, as well as Gaussian mixture models. The `clustering_algos`
    notebook compares the performance of these algorithms on different, labeled datasets
    to highlight their strengths and weaknesses. It uses mutual information (see [Chapter
    6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *The* *Machine Learning Process*)
    to measure the congruence of cluster assignments and labels.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: k-Means clustering
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: k-Means is the most well-known clustering algorithm and was first proposed by
    Stuart Lloyd at Bell Labs in 1957.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm finds *K* centroids and assigns each data point to exactly one
    cluster with the goal of minimizing the within-cluster variance (called inertia).
    It typically uses Euclidean distance, but other metrics can also be used. k-Means
    assumes that clusters are spherical and of equal size, and ignores the covariance
    among features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is computationally difficult (np-hard) because there are *K^N*
    ways to partition the *N* observations into *K* clusters. The standard iterative
    algorithm delivers a local optimum for a given *K* and proceeds as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Randomly define *K* cluster centers and assign points to nearest centroid.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each cluster, compute the centroid as the average of the features
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign each observation to the closest centroid
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Convergence: assignments (or within-cluster variation) don''t change.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `kmeans_implementation` notebook shows how to code the algorithm using
    Python, and visualizes the algorithm''s iterative optimization. The following
    screenshot highlights how the resulting centroids partition the feature space
    into areas called **Voronoi** which delineate the clusters:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d174126-9251-4803-970b-0d1369da0989.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: The result is optimal for the given initialization, but alternative starting
    positions will produce different results. Hence, we compute multiple clusterings
    from different initial values and select the solution that minimizes within-cluster
    variance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: k-Means requires continuous or one-hot encoded categorical variables. Distance
    metrics are typically sensitive to scale so that standardizing features is necessary
    to make sure they have equal weight.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The strengths of k-Means include its wide range of applicability, fast convergence,
    and linear scalability to large data while producing clusters of even size.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'The weaknesses include:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The need to tune the hyperparameter *k*
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lack of a guarantee to find a global optimum
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restrictive assumption that clusters are spheres and features are not correlated
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitivity to outliers
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating cluster quality
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cluster quality metrics help select among alternative clustering results. The `kmeans_evaluation`
    notebook illustrates the following options:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The k-Means objective function suggests we compare the evolution of the inertia
    or within-cluster variance.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initially, additional centroids decrease the inertia sharply because new clusters
    improve the overall fit.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once an appropriate number of clusters has been found (assuming it exists),
    new centroids reduce the within-cluster variance by much less as they tend to
    split natural groupings.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hence, when k-Means finds a good cluster representation of the data, the inertia
    tends to follow an elbow-shaped path similar to the explained variance ratio for
    PCA, as shown in the following screenshot (see notebook for implementation details):'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c58c7f92-24fd-4c80-84e9-47374cf599f0.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'The silhouette coefficient provides a more detailed picture of cluster quality.
    It answers the question: how far are the points in the nearest cluster, relative
    to the points in the assigned cluster?'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, it compares the mean intra-cluster distance (*a*) to the mean
    distance of the nearest cluster (*b*) and computes the following score *s*:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de29d1c9-9959-45c3-b16b-3397a9f7e7ce.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: The score can vary from between *-1* and *1*, but negative values are unlikely
    in practice because they imply that the majority of points are assigned to the
    wrong cluster. A useful visualization of the silhouette score compares the values
    for each data point to the global average because it highlights the coherence
    of each cluster relative to the global configuration. The rule of thumb is to
    avoid clusters with mean scores below the average for all samples.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an excerpt from the silhouette plot for three
    and four clusters, where the former highlights the poor fit of cluster *1* by
    sub-par contributions to the global silhouette score, whereas all of the four
    clusters have some values that exhibit above average scores:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5293c0c-9b6d-4722-bff9-aeeb1d7fc14b.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: In sum, given the usually unsupervised nature, it is necessary to vary the hyperparameters
    of the cluster algorithms and evaluate the different results. It is also important
    to calibrate the scale of the features, in particular when some should be given
    a higher weight and should thus be measured on a larger scale.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to validate the robustness of the results, use subsets of data to identify
    whether particular patterns emerge consistently.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical clustering avoids the need to specify a target number of clusters
    because it assumes that data can successively be merged into increasingly dissimilar
    clusters. It does not pursue a global objective but decides incrementally how
    to produce a sequence of nested clusters that range from a single cluster to clusters
    consisting of the individual data points.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '**Agglomerative clustering** proceeds bottom-up, sequentially merging two of
    the remaining groups based on similarity'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Divisive clustering** works top-down and sequentially splits the remaining
    clusters to produce the most distinct subgroups'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both groups produce *N*-1 hierarchical levels and facilitate the selection of
    a clustering at the level that best partitions data into homogenous groups. We
    will focus on the more common agglomerative clustering approach.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The agglomerative clustering algorithm departs from the individual data points
    and computes a similarity matrix containing all mutual distances. It then takes
    *N*-1 steps until there are no more distinct clusters, and each time updates the
    similarity matrix to substitute elements that have been merged by the new cluster
    so that the matrix progressively shrinks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'While hierarchical clustering does not have hyperparameters like k-Means, the
    measure of dissimilarity between clusters (as opposed to individual data points)
    has an important impact on the clustering result. The options differ as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-link**: the distance between nearest neighbors of two clusters'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete link**: the maximum distance between respective cluster members'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group average:** the distance between averages for each group'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward''s method**: minimizes within-cluster variance'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization – dendrograms
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical clustering provides insight into degrees of similarity among observations
    as it continues to merge data. A significant change in the similarity metric from
    one merge to the next suggests a natural clustering existed prior to this point.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The dendrogram visualizes the successive merges as a binary tree, displaying
    the individual data points as leaves and the final merge as the root of the tree.
    It also shows how the similarity monotonically decreases from bottom to top. Hence,
    it is natural to select a clustering by cutting the dendrogram.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot (see the `hierarchical_clustering` notebook for implementation
    details) illustrates the dendrogram for the classic Iris dataset with four classes
    and three features, using the four different distance metrics introduced precedingly:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/000bcf25-596d-4af8-b8e8-02cab8bab63d.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: It evaluates the fit of the hierarchical clustering using the cophenetic correlation
    coefficient, which compares the pairwise distances among points and the cluster
    similarity metric at which a pairwise merge occurred. A coefficient of 1 implies
    that closer points always merge earlier.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Different linkage methods produce dendrograms of different appearance, so we
    cannot use this visualization to compare results across methods. In addition,
    Ward's method, which minimizes within-cluster variance, may not properly reflect
    the change in variance, but rather the total variance, which may be misleading.
    Instead, other quality metrics such as cophenetic correlation, or measures such
    as inertia (if aligned with the overall goal), may be more appropriate.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'The strengths of clustering include:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: You do not need to specify the number of clusters
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It offers insight about potential clustering by means of an intuitive visualization
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It produces a hierarchy of clusters that can serve as taxonomy
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be combined with k-Means to reduce the number of items at the start of
    the agglomerative process
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses of hierarchical clustering include:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: The high cost in terms of computation and memory because of the numerous similarity
    matrix updates
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not achieve the global optimum because all merges are final
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The curse of dimensionality leads to difficulties with noisy, high-dimensional
    data
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based clustering
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Density-based clustering algorithms assign cluster membership based on proximity
    to other cluster members. They pursue the goal of identifying dense regions of
    arbitrary shapes and sizes. They do not require the specification of a certain
    number of clusters but instead rely on parameters that define the size of a neighborhood
    and a density threshold (see the `density_based_clustering` notebook for relevant
    code samples).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Density-based spatial clustering of applications with noise** (**DBSCAN**)
    was developed in 1996, and received the *Test of Time* award at the 2014 KDD conference
    because of the attention it has received in both theory and practice.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: It aims to identify core and non-core samples, where the former extend a cluster
    and the latter are part of a cluster but do not have sufficient nearby neighbors
    to further grow the cluster. Other samples are outliers and not assigned to any
    cluster.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: It uses an `eps` parameter for the radius of the neighborhood and `min_samples`
    for the number of members required for core samples. It is deterministic and exclusive
    and has difficulties with clusters of different density and high-dimensional data.
    It can be challenging to tune the parameters to the requisite density, especially
    as it is often not constant.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical DBSCAN
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical DBSCAN is a more recent development that assumes clusters are islands
    of potentially differing density, to overcome the DBSCAN challenges just mentioned.
    It also aims to identify the core and non-core samples. It uses the `min_cluster_
    size` and `min_samples` parameters to select a neighborhood and extend a cluster.
    The algorithm iterates over multiple `eps` values and chooses the most stable
    clustering.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: In addition to identifying clusters of varying density, it provides insight
    into the density and hierarchical structure of the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshots show how DBSCAN and HDBSCAN are able to identify
    very differently shaped clusters:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8667993d-ea61-4372-b107-a000c8036cdf.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: Gaussian mixture models
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Gaussian mixture model** (**GMM**) is a generative model that assumes the
    data has been generated by a mix of various multivariate normal distributions.
    The algorithm aims to estimate the mean and covariance matrices of these distributions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'It generalizes the k-Means algorithm: it adds covariance among features so
    that clusters can be ellipsoids rather than spheres, while the centroids are represented
    by the means of each distribution. The GMM algorithm performs soft assignments
    because each point has the probability to be a member of any cluster.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The expectation-maximization algorithm
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GMM uses the expectation-maximization algorithm to identify the components of
    the mixture of Gaussian distributions. The goal is to learn the probability distribution
    parameters from unlabeled data.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm proceeds iteratively as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Initialization—Assume random centroids (for example, using k-Means)
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the following steps until convergence (that is, changes in assignments
    drop below the threshold):'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expectation step**: Soft assignment—compute probabilities for each point
    from each distribution'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximization step**: Adjust normal-distribution parameters to make data points
    most likely'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the GMM cluster membership probabilities for
    the Iris dataset as contour lines:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffc6ce3d-4da8-4d41-8a8a-a9d68111943c.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: Hierarchical risk parity
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key idea of hierarchical risk parity is to use hierarchical clustering on
    the covariance matrix in order to be able to group assets with similar correlations
    together, and reduce the number of degrees of freedom by only considering similar
    assets as substitutes when constructing the portfolio (see notebook and Python
    files in the `hierarchical_risk_parity` subfolder for details).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to compute a distance matrix that represents proximity for
    correlated assets and meets distance metric requirements. The resulting matrix
    becomes an input to the SciPy hierarchical clustering function which computes
    the successive clusters using one of several available methods discussed so far:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `linkage_matrix` can be used as input to the `seaborn.clustermap` function
    to visualize the resulting hierarchical clustering. The dendrogram displayed by
    `seaborn` shows how individual assets and clusters of assets are merged based
    on their relative distances:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/d243557e-7c8a-4cbc-83c5-222cd87119ad.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: Heatmap
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Compared to a `seaborn.heatmap` of the original correlation matrix, there is
    now significantly more structure in the sorted data (right panel).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the tickers sorted according to the hierarchy induced by the clustering
    algorithm, HRP now proceeds to compute a top-down inverse-variance allocation
    that successively adjusts weights depending on the variance of the subclusters
    further down the tree:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To this end, the algorithm uses bisectional search to allocate the variance
    of a cluster to its elements based on their relative riskiness:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The resulting portfolio allocation produces weights that sum to `1` and reflect
    the structure present in the correlation matrix (see notebook for details).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored unsupervised learning methods that allow us to
    extract valuable signal from our data, without relying on the help of outcome
    information provided by labels.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: We saw how we can use linear dimensionality reduction methods, such as PCA and
    ICA, to extract uncorrelated or independent components from the data that can
    serve as risk factors or portfolio weights. We also covered advanced non-linear
    manifold learning techniques that produce state-of-the-art visualizations of complex
    alternative datasets.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we covered several clustering methods that produce data-driven
    groupings under various assumptions. These groupings can be useful, for example,
    to construct portfolios that apply risk-parity principles to assets that have
    been clustered hierarchically.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: In the next three chapters, we will learn about various ML techniques for a
    key source of alternative data, namely, natural language processing for text documents.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL

- en: Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *Machine Learning
    Process*, we discussed how unsupervised learning adds value by uncovering structures
    in the data without an outcome variable, such as a teacher, to guide the search
    process. This task contrasts with the setting for supervised learning that we
    focused on in the last several chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml)，*机器学习过程*中，我们讨论了无监督学习如何在没有结果变量（如教师）指导搜索过程的情况下，发现数据中的结构。这个任务与我们在最近几章中专注的监督学习的设置形成对比。
- en: Unsupervised learning algorithms can be useful when a dataset contains only
    features and no measurement of the outcome, or when we want to extract information
    independent of the outcome. Instead of predicting future outcomes, the goal is
    to study an informative representation of the data that is useful for solving
    another task, including the exploration of a dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集仅包含特征而没有结果的测量时，或者当我们希望独立于结果提取信息时，无监督学习算法可能是有用的。目标不是预测未来结果，而是研究对解决另一个任务有用的数据的信息表示，包括数据集的探索。
- en: Examples include identifying topics to summarize documents (see [Chapter 14](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml),
    *Topic Modeling*), reducing the number of features to reduce the risk of overfitting
    and the computational cost for supervised learning, or grouping similar observations,
    as illustrated by the use of clustering for asset allocation at the end of this
    chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括识别主题以总结文档（参见[第14章](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml)，*主题建模*），减少特征数量以减少过度拟合的风险和监督学习的计算成本，或者将相似的观察结果分组，正如本章末尾使用聚类进行资产配置所示。
- en: 'Dimensionality reduction and clustering are the main tasks for unsupervised
    learning:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 降维和聚类是无监督学习的主要任务：
- en: '**Dimensionality reduction** transforms the existing features into a new, smaller
    set, while minimizing the loss of information. A broad range of algorithms exists
    that differ only in how they measure the loss of information, whether they apply
    linear or non-linear transformations, or the constraints they impose on the new
    feature set.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**将现有特征转换为一个新的、更小的集合，同时最小化信息的损失。存在广泛的算法，它们仅在衡量信息损失的方式、应用线性或非线性转换或对新特征集施加的约束方面存在差异。'
- en: '**Clustering algorithms** identify and group similar observations or features
    instead of identifying new features. Algorithms differ in how they define the
    similarity of observations and their assumptions about the resulting groups.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**识别和分组相似的观察结果或特征，而不是识别新特征。算法在定义观察结果的相似性以及它们对生成的群组的假设方面存在差异。'
- en: 'More specifically, this chapter covers the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，本章涵盖以下内容：
- en: How **Principal Component Analysis** (**PCA**) and **Independent Component Analysis** (**ICA**)
    perform linear dimensionality reduction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）和**独立成分分析**（**ICA**）如何执行线性降维。'
- en: How to apply PCA to identify risk factors and eigen portfolios from asset returns
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用PCA从资产收益中识别风险因素和特征组合
- en: How to use non-linear manifold learning to summarize high-dimensional data for
    effective visualization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用非线性流形学习来总结高维数据以实现有效的可视化
- en: How to use t-SNE and UMAP to explore high-dimensional alternative image data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用t-SNE和UMAP来探索高维替代图像数据
- en: How k-Means, hierarchical, and density-based clustering algorithms work
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值、分层和基于密度的聚类算法的工作原理
- en: How to use agglomerative clustering to build robust portfolios according to
    hierarchical risk parity
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用凝聚聚类来根据分层风险平价构建稳健的投资组合
- en: The code samples for each section are in the directory of the online GitHub
    repository for this chapter at [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分的代码示例都在本章的在线GitHub存储库目录中，网址为[https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading)。
- en: Dimensionality reduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: In linear algebra terms, the features of a dataset create a **vector space**
    whose dimensionality corresponds to the number of linearly independent columns
    (assuming there are more observations than features). Two columns are linearly
    dependent when they are perfectly correlated so that one can be computed from
    the other using the linear operations of addition and multiplication.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数术语中，数据集的特征创建了一个**向量空间**，其维度对应于线性独立列的数量（假设观察结果多于特征）。当两列线性相关时，它们是线性相关的，因此可以使用加法和乘法的线性运算从一个列计算另一个列。
- en: In other words, they are parallel vectors that represent the same rather than
    different directions or axes and only constitute a single dimension. Similarly,
    if one variable is a linear combination of several others, then it is an element
    of the vector space created by those columns, rather than adding a new dimension
    of its own.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它们是代表相同而不是不同方向或轴的平行向量，只构成一个单一维度。同样，如果一个变量是其他几个变量的线性组合，则它是由这些列创建的向量空间的一个元素，而不是添加自己的新维度。
- en: 'The number of dimensions of a dataset matter because each new dimension can
    add a signal concerning an outcome. However, there is also a downside known as
    the **curse of dimensionality**: as the number of independent features grows while
    the number of observations remains constant, the average distance between data
    points also grows, and the density of the feature space drops exponentially.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的维度数量很重要，因为每个新维度都可能添加有关结果的信号。然而，也存在一个被称为**维度灾难**的缺点：随着独立特征的数量增加，而观察结果的数量保持不变，数据点之间的平均距离也增加，特征空间的密度呈指数级下降。
- en: The implications for machine learning are dramatic because prediction becomes
    much harder when observations are more distant; that is, different to each other.
    The next section addresses the resulting challenges.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习的影响是巨大的，因为当观测值彼此之间距离更远时，预测变得更加困难。接下来的部分将解决由此产生的挑战。
- en: Dimensionality reduction seeks to represent the information in the data more
    efficiently by using fewer features. To this end, algorithms project the data
    to a lower-dimensional space while discarding variation in the data that is not
    informative, or by identifying a lower-dimensional subspace or manifold on or
    near which the data lives.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 降维旨在通过使用更少的特征更有效地表示数据中的信息。为此，算法将数据投影到较低维空间，同时丢弃数据中不具信息量的变化，或者通过识别数据所在的较低维子空间或流形来实现。
- en: A manifold is a space that locally resembles Euclidean space. One-dimensional
    manifolds include lines and circles (but not screenshots of eight, due to the
    crossing point). The manifold hypothesis maintains that high-dimensional data
    often resides in a lower-dimensional space that, if identified, permits a faithful
    representation of the data in this subspace.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 流形是在局部上类似于欧几里得空间的空间。一维流形包括线和圆（但不包括八的截图，因为交叉点）。流形假设认为高维数据通常存在于低维空间中，如果识别出来，可以在这个子空间中忠实地表示数据。
- en: Dimensionality reduction thus compresses the data by finding a different, smaller
    set of variables that capture what matters most in the original features to minimize
    the loss of information. Compression helps counter the curse of dimensionality,
    economizes on memory, and permits the visualization of salient aspects of higher-dimensional
    data that is otherwise very difficult to explore.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维通过找到一个不同的、更小的变量集来压缩数据，以捕捉原始特征中最重要的内容，以最小化信息的丢失。压缩有助于对抗维度的诅咒，节约内存，并允许可视化高维数据的显著方面，否则这是非常难以探索的。
- en: Linear and non-linear algorithms
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性和非线性算法
- en: 'Dimensionality reduction algorithms differ in the constraints they impose on
    the new variables and how they aim to minimize the loss of information:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法在它们对新变量施加的约束和它们如何旨在最小化信息丢失方面有所不同：
- en: '**Linear algorithms** such as PCA and ICA constrain the new variables to be
    linear combinations of the original features; that is, hyperplanes in a lower-dimensional
    space. Whereas PCA requires the new features to be uncorrelated, ICA goes further
    and imposes statistical independence—the absence of both linear and non-linear
    relationships. The following screenshot illustrates how PCA projects three-dimensional
    features into a two-dimensional space:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）和独立成分分析（ICA）等线性算法约束新变量为原始特征的线性组合；也就是说，在较低维空间中的超平面。而PCA要求新特征不相关，ICA则进一步要求统计独立性——即不存在线性和非线性关系。以下截图说明了PCA如何将三维特征投影到二维空间：
- en: '![](img/07053c5f-7c75-45e0-be31-03b59be1d355.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07053c5f-7c75-45e0-be31-03b59be1d355.png)'
- en: 'Non-linear algorithms are not restricted to hyperplanes and can capture more
    complex structure in the data. However, given the infinite number of options,
    the algorithms still need to make assumptions to arrive at a solution. In this
    section, we show how **t-distributed Stochastic Neighbor Embedding** (**t-SNE**)
    and **Uniform Manifold Approximation and Projection** (**UMAP**) are very useful
    for visualizing higher-dimensional data. The following screenshot illustrates
    how manifold learning identifies a two-dimensional sub-space in the three-dimensional
    feature space (the `manifold_learning` notebook illustrates the use of additional
    algorithms, including local linear embedding):'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性算法不受超平面的限制，可以捕捉数据中更复杂的结构。然而，由于选项的数量是无限的，算法仍然需要做出假设来得出解决方案。在本节中，我们展示了t-分布随机邻居嵌入（t-SNE）和均匀流形逼近和投影（UMAP）对于可视化高维数据非常有用。以下截图说明了流形学习如何在三维特征空间中识别二维子空间（manifold_learning笔记本展示了使用其他算法，包括局部线性嵌入）：
- en: '![](img/34edbd7d-ac2c-47e3-9e30-353957b438cc.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34edbd7d-ac2c-47e3-9e30-353957b438cc.png)'
- en: The curse of dimensionality
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度的诅咒
- en: An increase in the number of dimensions of a dataset means there are more entries
    in the vector of features that represents each observation in the corresponding
    Euclidean space. We measure the distance in a vector space using Euclidean distance,
    also known as the **L2 norm**, which we applied to the vector of linear regression
    coefficients to train a regularized Ridge Regression model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集维度的增加意味着在相应的欧几里得空间中，表示每个观测的特征向量中有更多的条目。我们使用欧几里得距离来测量向量空间中的距离，也称为L2范数，我们将其应用于线性回归系数的向量，以训练正则化的岭回归模型。
- en: 'The Euclidean distance between two *n*-dimensional vectors with Cartesian coordinates
    *p = (p[1], p[2], ..., p[n])* and *q = (q[1], q[2], ..., q[n])* is computed using
    the familiar formula developed by Pythagoras:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 两个具有笛卡尔坐标*p = (p[1], p[2], ..., p[n])*和*q = (q[1], q[2], ..., q[n])*的*n*维向量之间的欧几里得距离是使用毕达哥拉斯开发的熟悉公式计算的：
- en: '![](img/1bdfbf33-f078-47cb-98a7-36996940182d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bdfbf33-f078-47cb-98a7-36996940182d.png)'
- en: Hence, each new dimension adds a non-negative term to the sum, so that the distance
    increases with the number of dimensions for distinct vectors. In other words,
    as the number of features grows for a given number of observations, the feature
    space becomes increasingly sparse; that is, less dense or emptier. On the flip
    side, the lower data density requires more observations to keep the average distance
    between data points the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个新的维度都会向总和中添加一个非负项，因此对于不同的向量，随着维度的增加，距离也会增加。换句话说，随着观测特征数量的增加，特征空间变得越来越稀疏；也就是说，更少的密度或更空旷。另一方面，较低的数据密度需要更多的观测来保持数据点之间的平均距离不变。
- en: 'The following chart shows how many data points we need to maintain the average
    distance of 10 observations uniformly distributed on a line. It increases exponentially
    from 10¹ in a single dimension to 10² in two and 10³ in three dimensions, as the
    data needs to expand by a factor of 10 each time we add a new dimension:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我们需要多少数据点来维持在一条线上均匀分布的10个观察的平均距离。随着数据需要每次增加一个新维度时呈指数级增长，从单个维度的10¹增加到两个维度的10²，再增加到三个维度的10³：
- en: '![](img/fc8d524c-d176-474d-a115-5a48dac9902f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc8d524c-d176-474d-a115-5a48dac9902f.png)'
- en: 'The `curse_of_dimensionality` notebook in the GitHub repo folder for this section
    simulates how the average and minimum distances between data points increase as
    the number of dimensions grows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本节GitHub存储库文件夹中的`curse_of_dimensionality`笔记本模拟了数据点之间的平均距离和最小距离随着维度数量的增加而增加的情况：
- en: '![](img/cf927dcd-7464-4663-903d-2d3ef684379f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf927dcd-7464-4663-903d-2d3ef684379f.png)'
- en: The simulation draws features in the range [0, 1] from uncorrelated uniform
    or correlated normal distributions, and gradually increases the number of features
    to 2,500\. The average distance between data points increases to over 11 times
    the feature range for features drawn from the normal distribution, and to over
    20 times in the (extreme) case of uncorrelated uniform distribution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟从不相关均匀分布或相关正态分布中绘制范围为[0, 1]的特征，并逐渐增加特征数量到2500。从正态分布中绘制的特征之间的平均距离增加到特征范围的11倍以上，在不相关均匀分布的（极端）情况下增加到20倍以上。
- en: When the distance between observations grows, supervised machine learning becomes
    more difficult because predictions for new samples are less likely to be based
    on learning from similar training features. Put differently, the number of possible
    unique rows grows exponentially as the number of features increases, which makes
    it so much harder to efficiently sample the space.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察之间的距离增加时，监督机器学习变得更加困难，因为对新样本的预测不太可能是基于对类似训练特征的学习。换句话说，随着特征数量的增加，可能的唯一行数呈指数级增长，这使得有效地对空间进行采样变得更加困难。
- en: Similarly, the complexity of the functions learned by flexible algorithms that
    make fewer assumptions about the actual relationship grows exponentially with
    the number of dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，灵活算法学习的函数复杂性随着维度数量的增加呈指数级增长，这些算法对实际关系的假设越少。
- en: Flexible algorithms include the tree-based models we saw in [Chapter 10](7d7aa662-362a-4c3a-acda-a18fb1bad6e7.xhtml), *Decision
    Trees and Random Forests*, and [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml),
    *Gradient Boosting Machines*, and the deep neural networks that we will cover
    from Chapter 17, *Deep Learning* onward. The variance of these algorithms increases
    as they get more opportunity to overfit to noise in more dimensions, resulting
    in poor generalization performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活的算法包括我们在[第10章](7d7aa662-362a-4c3a-acda-a18fb1bad6e7.xhtml)中看到的基于树的模型，*决策树和随机森林*，以及[第11章](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml)中的*梯度提升机*，以及我们将从第17章*深度学习*开始涵盖的深度神经网络。随着它们在更多维度上过度拟合噪音的机会增加，这些算法的方差也会增加，导致泛化性能较差。
- en: In practice, features are correlated, often substantially so, or do not exhibit
    much variation. For these reasons, dimensionality reduction helps to compress
    the data without losing much of the signal, and combat the curse while also economizing
    on memory. In these cases, it complements the use of regularization to manage
    prediction error due to variance and model complexity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，特征通常是相关的，甚至很大程度上是相关的，或者没有展现出太多的变化。出于这些原因，降维有助于在不丢失太多信号的情况下压缩数据，并解决诅咒，同时也节省内存。在这些情况下，它可以辅助使用正则化来管理由于方差和模型复杂性而产生的预测误差。
- en: 'The critical question that we take on in the following section then becomes:
    what are the best ways to find a lower-dimensional representation of the data
    that loses as little information as possible?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的关键问题是：如何找到数据的低维表示的最佳方法，以尽量少地丢失信息？
- en: Linear dimensionality reduction
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性降维
- en: Linear dimensionality reduction algorithms compute linear combinations that
    translate, rotate, and rescale the original features to capture significant variation
    in the data, subject to constraints on the characteristics of the new features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维算法计算线性组合，将原始特征进行平移、旋转和重新缩放，以捕捉数据中的显著变化，同时受到新特征特性的约束。
- en: '**Principal Component Analysis** (**PCA**), invented in 1901 by Karl Pearson,
    finds new features that reflect directions of maximal variance in the data while
    being mutually uncorrelated, or orthogonal.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是由Karl Pearson于1901年发明的，它找到了反映数据中最大方差方向的新特征，同时这些特征是相互不相关或正交的。'
- en: '**Independent Component Analysis** (**ICA**), in contrast, originated in signal
    processing in the 1980s, with the goal of separating different signals while imposing
    the stronger constraint of statistical independence.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**独立成分分析**（**ICA**）起源于20世纪80年代的信号处理，其目标是在施加更强的统计独立约束的同时分离不同的信号。
- en: This section introduces these two algorithms and then illustrates how to apply
    PCA to asset returns to learn risk factors from the data, and to build so-called
    eigen portfolios for systematic trading strategies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了这两种算法，然后说明了如何将PCA应用于资产回报，以从数据中学习风险因素，并构建所谓的特征组合以进行系统交易策略。
- en: Principal Component Analysis
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: PCA finds principal components as linear combinations of the existing features
    and uses these components to represent the original data. The number of components
    is a hyperparameter that determines the target dimensionality and needs to be
    equal to or smaller than the number of observations or columns, whichever is smaller.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PCA找到现有特征的主成分作为线性组合，并使用这些成分来表示原始数据。成分的数量是一个超参数，它决定了目标维度，并且需要等于或小于观察值或列的数量，以较小者为准。
- en: PCA aims to capture most of the variance in the data, to make it easy to recover
    the original features and so that each component adds information. It reduces
    dimensionality by projecting the original data into the principal component space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PCA旨在捕获数据中的大部分方差，以便轻松恢复原始特征，使每个成分都添加信息。它通过将原始数据投影到主成分空间来降低维度。
- en: The PCA algorithm works by identifying a sequence of principal components, each
    of which aligns with the direction of maximum variance in the data after accounting
    for variation captured by previously-computed components. The sequential optimization
    also ensures that new components are not correlated with existing components so
    that the resulting set constitutes an orthogonal basis for a vector space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法通过识别一系列主成分来工作，每个主成分都与数据中方差的最大方向对齐，同时考虑先前计算的成分捕获的变化。顺序优化还确保新成分与现有成分不相关，从而确保生成的集合构成向量空间的正交基础。
- en: This new basis corresponds to a rotated version of the original basis so that
    the new axis point in the direction of successively decreasing variance. The decline
    in the amount of variance of the original data explained by each principal component
    reflects the extent of correlation among the original features.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的基础对应于原始基础的旋转版本，使得新轴指向逐渐减小的方差的方向。每个主成分解释的原始数据方差量的下降反映了原始特征之间相关程度的程度。
- en: The number of components that capture, for example, 95% of the original variation
    relative to the total number of features provides an insight into the linearly-independent
    information in the original data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，捕获原始变化95%的成分数量相对于总特征数量提供了对原始数据中线性独立信息的见解。
- en: Visualizing PCA in 2D
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在2D中可视化PCA
- en: 'The following screenshot illustrates several aspects of PCA for a two-dimensional
    random dataset (see the `pca_key_ideas` notebook):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图说明了二维随机数据集的PCA的几个方面（请参阅`pca_key_ideas`笔记本）：
- en: '![](img/f122db9e-99db-4235-b0d3-2f84dbaac1c1.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f122db9e-99db-4235-b0d3-2f84dbaac1c1.png)'
- en: The left panel shows how the first and second principal components align with
    the directions of maximum variance while being orthogonal.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左面板显示了第一和第二主成分如何与最大方差的方向对齐，同时是正交的。
- en: The central panel shows how the first principal component minimizes the reconstruction
    error, measured as the sum of the distances between the data points and the new
    axis.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央面板显示了第一个主成分如何最小化重构误差，重构误差是指数据点与新轴之间的距离之和。
- en: Finally, the right panel illustrates supervised OLS, which approximates the
    outcome variable (here we choose x[2]) by a (one-dimensional) hyperplane computed
    from the (single) feature. The vertical lines highlight how OLS minimizes the
    distance along the outcome axis, in contrast with PCA, which minimizes the distances
    orthogonal to the hyperplane.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，右面板说明了监督OLS，它通过从（单个）特征计算出的（一维）超平面来近似结果变量（这里我们选择x[2]）。垂直线突出显示了OLS如何最小化沿结果轴的距离，与PCA相反，PCA最小化了与超平面正交的距离。
- en: The assumptions made by PCA
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA所做的假设
- en: 'PCA makes several assumptions that are important to keep in mind. These include
    the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA做出了几个重要的假设，这些假设需要牢记。其中包括以下内容：
- en: High variance implies a high signal-to-noise ratio
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高方差意味着高信噪比
- en: The data is standardized so that the variance is comparable across features
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被标准化，以便各个特征的方差是可比较的。
- en: Linear transformations capture the relevant aspects of the data
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换捕获了数据的相关方面
- en: Higher-order statistics beyond the first and second moment do not matter, which
    implies that the data has a normal distribution
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一和第二时刻之外的高阶统计不重要，这意味着数据具有正态分布
- en: The emphasis on the first and second moments aligns with standard risk/return
    metrics, but the normality assumption may conflict with the characteristics of
    market data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对第一和第二时刻的强调与标准的风险/回报指标一致，但正态性假设可能与市场数据的特征相冲突。
- en: How the PCA algorithm works
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA算法的工作原理
- en: The algorithm finds vectors to create a hyperplane of target dimensionality
    that minimizes the reconstruction error, measured as the sum of the squared distances
    of the data points to the plane. As illustrated above, this goal corresponds to
    finding a sequence of vectors that align with directions of maximum retained variance
    given the other components while ensuring all principal components are mutually
    orthogonal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到向量来创建目标维度的超平面，以最小化重构误差，重构误差是指数据点到平面的距离的平方和。如上所述，这个目标对应于找到一系列向量，这些向量与最大保留方差的方向对齐，同时确保所有主成分彼此正交。
- en: In practice, the algorithm solves the problem either by computing the eigenvectors
    of the covariance matrix or using the singular value decomposition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，该算法通过计算协方差矩阵的特征向量或使用奇异值分解来解决问题。
- en: 'We illustrate the computation using a randomly generated three-dimensional
    ellipse with 100 data points, shown in the left panel of the following screenshot,
    including the two-dimensional hyperplane defined by the first two principal components
    (see the `the_math_behind_pca` notebook for the following code samples):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个随机生成的三维椭圆来说明计算，包括100个数据点，显示在以下截图的左面板中，包括由前两个主成分定义的二维超平面（请参阅`the_math_behind_pca`笔记本中的以下代码示例）：
- en: '![](img/0622eac2-a2c5-49dc-8d25-6e178b44e0ae.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0622eac2-a2c5-49dc-8d25-6e178b44e0ae.png)'
- en: Three-dimensional ellipse and two-dimensional hyperplane
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 三维椭圆和二维超平面
- en: PCA based on the covariance matrix
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于协方差矩阵的PCA
- en: 'We first compute the principal components using the square covariance matrix
    with the pairwise sample covariances for the features *x[i], x[j],* *i*, *j* =
    1, ..., *n* as entries in row *i* and column *j*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用成对样本协方差计算特征*x[i], x[j],* *i*, *j* = 1, ..., *n*作为第*i*行和第*j*列的条目来计算主成分：
- en: '![](img/33024c77-1334-4fa2-a548-3a2738854670.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33024c77-1334-4fa2-a548-3a2738854670.png)'
- en: 'For a square matrix *M* of *n* dimensions, we define the eigenvectors *ω[i]*
    and eigenvalues *λ[i]*, *i*=1, ..., *n* as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*n*维的方阵*M*，我们定义特征向量*ω[i]*和特征值*λ[i]*，*i*=1，...，*n*如下：
- en: '![](img/18c48888-f82f-4ec6-8b71-e22db24b4c18.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18c48888-f82f-4ec6-8b71-e22db24b4c18.png)'
- en: 'Hence, we can represent the matrix *M* using eigenvectors and eigenvalues,
    where *W* is a matrix that contains the eigenvectors as column vectors, and *L*
    is a matrix that contains the λ[i] as diagonal entries (and 0s otherwise). We
    define the eigendecomposition as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用特征向量和特征值表示矩阵*M*，其中*W*是包含特征向量作为列向量的矩阵，*L*是包含λ[i]作为对角线条目（其他情况为0）的矩阵。我们定义特征分解如下：
- en: '![](img/5efee6d1-23ff-4b2d-861a-4689996908ce.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5efee6d1-23ff-4b2d-861a-4689996908ce.png)'
- en: 'Using NumPy, we implement this as follows, where the pandas DataFrame contains
    the 100 data points of the ellipse:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy，我们实现如下，其中pandas DataFrame包含椭圆的100个数据点：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we calculate the eigenvectors and eigenvalues of the covariance matrix.
    The eigenvectors contain the principal components (where the sign is arbitrary):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算协方差矩阵的特征向量和特征值。特征向量包含主成分（符号是任意的）：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can compare the result with the result obtained from sklearn, and find that
    they match in absolute terms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将结果与从sklearn获得的结果进行比较，并发现它们在绝对值上匹配：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also verify the eigendecomposition, starting with the diagonal matrix
    *L* that contains the eigenvalues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以验证特征分解，从包含特征值的对角矩阵*L*开始：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We find that the result does indeed hold:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现结果确实成立：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PCA using Singular Value Decomposition
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用奇异值分解的PCA
- en: Next, we'll look at the alternative computation using **Singular Value Decomposition**
    (**SVD**). This algorithm is slower when the number of observations is greater
    than the number of features (the typical case), but yields better numerical stability,
    especially when some of the features are strongly correlated (often the reason
    to use PCA in the first place).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下使用**奇异值分解**（**SVD**）的替代计算。当观测数大于特征数（典型情况）时，该算法速度较慢，但在一些特征强相关的情况下（通常是使用PCA的原因），它具有更好的数值稳定性。
- en: 'SVD generalizes the eigendecomposition that we just applied to the square and
    symmetric covariance matrix to the more general case of *m* x *n* rectangular
    matrices. It has the form shown at the center of the following diagram. The diagonal
    values of *Σ* are the singular values, and the transpose of *V** contains the
    principal components as column vectors:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: SVD将我们刚刚应用于方阵和对称协方差矩阵的特征分解推广到更一般的*m* x *n*矩形矩阵。它的形式如下图中心所示。*Σ*的对角线值是奇异值，*V*的转置包含主成分作为列向量：
- en: '![](img/a28fb8b0-14c4-4a60-a026-247d71cd6af9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a28fb8b0-14c4-4a60-a026-247d71cd6af9.png)'
- en: 'In this case, we need to make sure our data is centered with mean zero (the
    computation of the covariance preceding took care of this):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要确保我们的数据以零均值为中心（先前的协方差计算已经处理了这个问题）：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We find that the decomposition does indeed reproduce the standardized data:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现分解确实能够再现标准化数据：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Lastly, we confirm that the columns of the transpose of *V** contain the principal
    components:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们确认*V*的转置的列包含主成分：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the next section, we show how sklearn implements PCA.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示sklearn如何实现PCA。
- en: PCA with sklearn
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: sklearn中的PCA
- en: The `sklearn.decomposition.PCA` implementation follows the standard API based
    on the `fit()` and `transform()` methods, which compute the desired number of
    principal components and project the data into the component space, respectively.
    The convenience method `fit_transform()` accomplishes this in a single step.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.decomposition.PCA`实现遵循基于`fit()`和`transform()`方法的标准API，这些方法分别计算所需数量的主成分并将数据投影到组件空间。方便的方法`fit_transform()`可以在一步中完成这个过程。'
- en: 'PCA offers three different algorithms that can be specified using the `svd_solver`
    parameter:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: PCA提供了三种不同的算法，可以使用`svd_solver`参数进行指定：
- en: '`Full` computes the exact SVD using the LAPACK solver provided by SciPy'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Full`使用由SciPy提供的LAPACK求解器来计算精确的SVD'
- en: '`Arpack` runs a truncated version suitable for computing less than the full
    number of components'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Arpack`运行一个适合计算少于完整组件数量的截断版本'
- en: '`Randomized` uses a sampling-based algorithm that is more efficient when the
    dataset has more than 500 observations and features, and the goal is to compute
    less than 80% of the components'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Randomized`使用基于抽样的算法，当数据集具有超过500个观测值和特征，并且目标是计算少于80%的组件时，它更有效率'
- en: '`Auto` uses randomized where most efficient, otherwise, it uses the full SVD'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Auto`在大多数有效时使用随机化，否则使用完整的SVD'
- en: See references on GitHub for algorithmic implementation details.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有关算法实现细节的参考，请参阅GitHub上的参考资料。
- en: 'Other key configuration parameters of the PCA object are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对象的其他关键配置参数如下：
- en: '`n_components`: These compute all principal components by passing `None` (the
    default), or limit the number to `int`. For `svd_solver=full`, there are two additional
    options: a float in the interval [0, 1] computes the number of components required
    to retain the corresponding share of the variance in the data, and the `mle` option
    estimates the number of dimensions using maximum likelihood.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components`：通过传递`None`（默认值）来计算所有主成分，或将数量限制为`int`。对于`svd_solver=full`，还有两个额外的选项：在区间[0,1]中的浮点数计算需要保留数据方差相应份额的组件数量，`mle`选项使用最大似然估计来估计维度的数量。'
- en: '`whiten`: If `True`, it standardizes the component vectors to unit variance
    that, in some cases, can be useful for use in a predictive model (the default
    is `False`).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`whiten`：如果为`True`，它会将组件向量标准化为单位方差，在某些情况下，这对于在预测模型中使用可能是有用的（默认为`False`）。'
- en: 'To compute the first two principal components of the three-dimensional ellipsis
    and project the data into the new space, use `fit_transform()` as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算三维椭圆的前两个主成分，并将数据投影到新空间中，可以使用`fit_transform()`如下：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The explained variance of the first two components is very close to 100%:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个成分的解释方差非常接近100%：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The screenshot at the beginning of this section shows the projection of the
    data into the new two-dimensional space.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Independent Component Analysis
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**) is another linear algorithm that
    identifies a new basis on which to represent the original data, but pursues a
    different objective to PCA.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: ICA emerged in signal processing, and the problem it aims to solve is called
    **blind source separation**. It is typically framed as the cocktail party problem,
    in which a given number of guests are speaking at the same time so that a single
    microphone would record overlapping signals. ICA assumes there are as many different
    microphones as there are speakers, each placed at different locations so as to
    record a different mix of the signals. ICA then aims to recover the individual
    signals from the different recordings.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, there are *n* original signals and an unknown square mixing
    matrix *A* that produces an *n*-dimensional set of *m* observations, so that:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e242918a-364d-4f7b-8061-8faaf7bb772e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: The goal is to find the matrix *W=A^(-1)* that untangles the mixed signals to
    recover the sources.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The ability to uniquely determine the matrix *W* hinges on the non-Gaussian
    distribution of the data. Otherwise, *W* could be rotated arbitrarily given the
    multivariate normal distribution's symmetry under rotation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, ICA assumes the mixed signal is the sum of its components and is
    unable to identify Gaussian components because their sum is also normally distributed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: ICA assumptions
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ICA makes the following critical assumptions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The sources of the signals are statistically independent
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear transformations are sufficient to capture the relevant information
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The independent components do not have a normal distribution
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mixing matrix A can be inverted
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICA also requires the data to be centered and whitened; that is, to be mutually
    uncorrelated with unit variance. Preprocessing the data using PCA as outlined
    above achieves the required transformations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The ICA algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FastICA, used by sklearn, is a fixed-point algorithm that uses higher-order
    statistics to recover the independent sources. In particular, it maximizes the
    distance to a normal distribution for each component as a proxy for independence.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: An alternative algorithm called **InfoMax** minimizes the mutual information
    between components as a measure of statistical independence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: ICA with sklearn
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ICA implementation by sklearn uses the same interface as PCA, so there is
    little to add. Note that there is no measure of explained variance because ICA
    does not compute components successively. Instead, each component aims to capture
    independent aspects of the data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: PCA for algorithmic trading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is useful for algorithmic trading in several respects. These include the
    data-driven derivation of risk factors by applying PCA to asset returns, and the
    construction of uncorrelated portfolios based on the principal components of the
    correlation matrix of asset returns.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Data-driven risk factors
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear Models*,
    we explored risk factor models used in quantitative finance to capture the main
    drivers of returns. These models explain differences in returns on assets based
    on their exposure to systematic risk factors and the rewards associated with these
    factors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we explored the Fama-French approach, which specifies factors
    based on prior knowledge about the empirical behavior of average returns, treats
    these factors as observable, and then estimates risk model coefficients using
    linear regression. An alternative approach treats risk factors as latent variables
    and uses factor analytic techniques such as PCA to simultaneously estimate the
    factors and how they drive returns from historical returns.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will review how this method derives factors in a purely
    statistical or data-driven way, with the advantage of not requiring ex-ante knowledge
    of the behavior of asset returns (see the `pca` and `risk_factor` notebook models
    for details).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾这种方法如何以纯粹的统计或数据驱动方式推导因子，其优势在于不需要对资产回报行为的先验知识（有关详细信息，请参见`pca`和`risk_factor`笔记本模型）。
- en: 'We will use the Quandl stock price data and select the daily adjusted close
    prices of the 500 stocks with the largest market capitalization and data for the
    2010-18 period. We then compute the daily returns as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Quandl股价数据，并选择市值最大的500支股票的每日调整收盘价和2010-18年期间的数据。然后我们计算每日回报如下：
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We obtain `351` stocks and returns for over 2,000 trading days:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了`351`支股票和超过2,000个交易日的回报：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'PCA is sensitive to outliers, so we winsorize the data at the 2.5% and 97.5%
    quantiles:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对异常值敏感，因此我们在2.5%和97.5%的分位数处对数据进行了截尾处理：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'PCA does not permit missing data, so we will remove stocks that do not have
    data for at least 95% of the time period, and in a second step, remove trading
    days that do not have observations on at least 95% of the remaining stocks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: PCA不允许缺失数据，因此我们将删除至少在95%的时间段内没有数据的股票，并在第二步中删除至少在剩余股票中95%的交易日没有观察到的交易日：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are left with `314` equity return series covering a similar period:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下了`314`个股票回报系列，覆盖了类似的时期：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We impute any remaining missing values using the average return for any given
    trading day:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用任何给定交易日的平均回报来填补任何剩余的缺失值：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we are ready to fit the principal components model to the asset returns
    using default parameters to compute all components using the full SVD algorithm:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用默认参数将主成分模型拟合到资产回报上，使用完整的SVD算法计算所有成分：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We find that the most important factor explains around 40% of the daily return
    variation. The dominant factor is usually interpreted as the market, whereas the
    remaining factors can be interpreted as industry or style factors, in line with
    our discussion in [Chapter 5](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml), *Strategy
    Evaluation*, and [Chapter 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml), *Linear
    Models*, depending on the results of closer inspection (see the next example).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现最重要的因子解释了大约40%的日回报变化。主导因子通常被解释为市场，而其余因子可以被解释为行业或风格因子，与我们在[第5章](1de6a332-69f8-4530-8d18-1007d0a3eb7e.xhtml)中的讨论一致，*策略评估*，以及[第7章](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml)，*线性模型*，具体取决于更详细的检查结果（请参见下一个示例）。
- en: 'The plot on the right shows the cumulative explained variance, and indicates
    that around 10 factors explain 60% of the returns of this large cross-section
    of stocks:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的图表显示了累积解释方差，并表明大约有10个因子解释了这个大横截面股票回报的60%：
- en: '![](img/a2d2b23c-65a0-4a9f-a1ea-c48d9c225b8b.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2d2b23c-65a0-4a9f-a1ea-c48d9c225b8b.png)'
- en: The notebook contains a simulation for a broader cross-section of stocks and
    the longer 2000-18 time period. It finds that, on average, the first three components
    explained 25%, 10%, and 5% of 500 randomly selected stocks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含了更广泛的股票横截面和更长的2000-18年时间段的模拟。结果发现，平均而言，前三个成分解释了500支随机选定股票的25%、10%和5%。
- en: The cumulative plot shows a typical elbow pattern that can help identify a suitable
    target dimensionality because it indicates that additional components add less
    explanatory value.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 累积图显示了一个典型的肘部模式，可以帮助确定合适的目标维度，因为它表明额外的成分增加的解释价值较少。
- en: 'We can select the top two principal components to verify that they are indeed
    uncorrelated:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择前两个主成分来验证它们是否确实不相关：
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Moreover, we can plot the time series to highlight how each factor captures
    different volatility patterns:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以绘制时间序列，以突出每个因子捕捉不同的波动模式：
- en: '![](img/a88573bd-0658-466e-8330-5af9cdf17d86.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a88573bd-0658-466e-8330-5af9cdf17d86.png)'
- en: A risk factor model would employ a subset of the principal components as features
    to predict future returns, similar to our approach in [Chapter 7](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=572&action=edit#post_28), *Linear
    Models – Regression and Classification*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 风险因子模型将使用主成分的子集作为特征来预测未来的回报，类似于我们在[第7章](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=572&action=edit#post_28)中的方法，*线性模型-回归和分类*。
- en: Eigen portfolios
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征组合
- en: Another application of PCA involves the covariance matrix of the normalized
    returns. The principal components of the correlation matrix capture most of the
    covariation among assets in descending order and are mutually uncorrelated. Moreover,
    we can use standardized principal components as portfolio weights.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的另一个应用涉及标准化回报的协方差矩阵。相关矩阵的主成分按降序捕捉了资产之间的大部分协变化，并且彼此不相关。此外，我们可以使用标准化的主成分作为投资组合权重。
- en: 'Let''s use the 30 largest stocks with data for the 2010-2018 period to facilitate
    the exposition:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用2010-2018年期间有数据的30家最大的股票来便于阐述：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We again winsorize and also normalize the returns:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次对回报进行截尾处理和标准化：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After dropping assets and trading days as in the previous example, we are left
    with 23 assets and over 2,000 trading days. We estimate all principal components,
    and find that the two largest explain 55.9% and 15.5% of the covariation, respectively:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除资产和交易日的情况下，我们剩下了23个资产和超过2,000个交易日。我们估计了所有的主成分，并发现前两个解释了55.9%和15.5%的协变化：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we select and normalize the four largest components so that they sum
    to `1` and we can use them as weights for portfolios that we can compare to an
    equal-weighted portfolio formed from all stocks:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择和标准化四个最大的成分，使它们总和为`1`，我们可以将它们用作投资组合的权重，以便与由所有股票组成的等权投资组合进行比较：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The weights show distinct emphasis—for example, **Portfolio 3** puts large
    weights on Mastercard and Visa, the two payment processors in the sample, whereas
    **Portfolio 2** has more exposure to technology companies:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 权重显示出明显的重点，例如**Portfolio 3**对样本中的两家支付处理公司Mastercard和Visa有很大的权重，而**Portfolio
    2**更多地暴露于科技公司：
- en: '![](img/a89efa65-3d1b-4201-848c-18967d6e4a53.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a89efa65-3d1b-4201-848c-18967d6e4a53.png)'
- en: 'When comparing the performance of each portfolio over the sample period to
    The Market consisting of our small sample, we find that portfolio 1 performs very
    similarly, whereas the other portfolios capture different return patterns:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当将每个投资组合在样本期间与我们小样本构成的市场进行比较时，我们发现投资组合1的表现非常相似，而其他投资组合捕捉到不同的回报模式：
- en: '![](img/e5475635-cc5e-4952-8b51-69e94163a966.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5475635-cc5e-4952-8b51-69e94163a966.png)'
- en: Comparing performances of each portfolio
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个投资组合的表现进行比较
- en: Manifold learning
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形学
- en: Linear dimensionality reduction projects the original data onto a lower-dimensional
    hyperplane that aligns with informative directions in the data. The focus on linear
    transformations simplifies the computation and echoes common financial metrics,
    such as PCA's goal to capture the maximum variance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 线性维度降低将原始数据投影到一个与数据中信息方向对齐的低维超平面上。专注于线性变换简化了计算，并呼应了常见的金融指标，比如PCA的目标是捕捉最大方差。
- en: However, linear approaches will naturally ignore signal reflected in non-linear
    relationships in the data. Such relationships are very important in alternative
    datasets containing, for example, image or text data. Detecting such relationships
    during exploratory analysis can provide important clues about the data's potential
    signal content.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，线性方法自然会忽略数据中非线性关系反映的信号。这样的关系在包含图像或文本数据的替代数据集中非常重要。在探索性分析中检测这样的关系可以提供有关数据潜在信号内容的重要线索。
- en: In contrast, the manifold hypothesis emphasizes that high-dimensional data often
    lies on or near a lower-dimensional non-linear manifold that is embedded in the
    higher dimensional space. The two-dimensional swiss roll displayed in the screenshot
    at the beginning of this section illustrates such a topological structure.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，流形假设强调高维数据通常位于或接近嵌入在高维空间中的较低维度非线性流形上。本节开始时显示的二维瑞士卷就说明了这样的拓扑结构。
- en: Manifold learning aims to find the manifold of intrinsic dimensionality and
    then represent the data in this subspace. A simplified example uses a road as
    one-dimensional manifolds in a three-dimensional space and identifies data points
    using house numbers as local coordinates.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学旨在找到固有维度的流形，然后在这个子空间中表示数据。一个简化的例子是在三维空间中将道路作为一维流形，并使用房屋编号作为局部坐标来识别数据点。
- en: Several techniques approximate a lower dimensional manifold. One example is
    **locally-linear embedding** (**LLE**), which was developed in 2000 by Sam Roweis
    and Lawrence Saul and used to unroll the swiss roll in the previous screenshot
    (see examples in the `manifold_learning_lle` notebook).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 几种技术近似于较低维度的流形。一个例子是2000年由Sam Roweis和Lawrence Saul开发的**局部线性嵌入**（**LLE**），用于展开前一个截图中的瑞士卷（请参见`manifold_learning_lle`笔记本中的示例）。
- en: For each data point, LLE identifies a given number of nearest neighbors and
    computes weights that represent each point as a linear combination of its neighbors.
    It finds a lower-dimensional embedding by linearly projecting each neighborhood
    onto global internal coordinates on the lower-dimensional manifold, and can be
    thought of as a sequence of PCA applications.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，LLE识别一定数量的最近邻，并计算权重，表示每个点是其邻居的线性组合。它通过将每个邻域线性投影到较低维度流形上的全局内部坐标来找到一个较低维度的嵌入，并可以被视为一系列PCA应用。
- en: Visualization requires the reduction to at least three dimensions, possibly
    below the intrinsic dimensionality, and poses the challenge of faithfully representing
    local and global structure. This challenge relates to the increasing distance
    associated with the curse of dimensionality. While the volume of a sphere expands
    exponentially with the number of dimensions, the space in lower dimensions available
    to represent high-dimensional data is much more limited.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '可视化需要至少降至三维，可能低于固有维度，并且面临着忠实地表示局部和全局结构的挑战。这个挑战与与维度诅咒相关的增加距离有关。虽然球体的体积随着维度的增加呈指数增长，但用于表示高维数据的较低维度空间要有限得多。 '
- en: 'For example, in 12 dimensions, there can be 13 equidistant points, but in two
    dimensions there can only be three that form a triangle with sides of equal length.
    Hence, accurately reflecting the distance of one point to its high-dimensional
    neighbors in lower dimensions risks distorting the relations among all other points.
    The result is the crowding problem: to maintain global distances, local points
    may need to be placed too closely together, and vice versa.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在12维空间中可能有13个等距点，但在二维空间中可能只有三个点形成等边三角形。因此，在较低维度准确反映一个点到其高维邻居的距离可能会扭曲所有其他点之间的关系。结果就是拥挤问题：为了保持全局距离，局部点可能需要放置得太近，反之亦然。
- en: 'The following two sections cover techniques that have made progress in addressing
    the crowding problem for the visualization of complex datasets. We will use the
    fashion `MNIST` dataset, a more sophisticated alternative to the classic handwritten
    digit MNIST benchmark data used for computer vision. It contains 60,000 train
    and 10,000 test images of fashion objects in 10 classes (see following samples):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个部分涵盖了在解决复杂数据集可视化中的拥挤问题方面取得进展的技术。我们将使用时尚`MNIST`数据集，这是经典手写数字MNIST基准数据的更复杂的替代品，用于计算机视觉。它包含了10个类别中时尚物品的60,000个训练和10,000个测试图像（见下面的样本）：
- en: '![](img/aaba9ade-5f81-43d1-abad-f3d1034a6f1d.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaba9ade-5f81-43d1-abad-f3d1034a6f1d.png)'
- en: The goal of a manifold learning algorithm for this data is to detect whether
    the classes lie on distinct manifolds, to facilitate their recognition and differentiation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据的流形学算法的目标是检测类别是否位于不同的流形上，以便促进它们的识别和区分。
- en: t-SNE
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE
- en: The t-distributed stochastic neighbor embedding is an award-winning algorithm
    developed in 2010 by Laurens van der Maaten and Geoff Hinton to detect patterns
    in high-dimensional data. It takes a probabilistic, non-linear approach to locating
    data on several different but related low-dimensional manifolds.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: t-分布随机邻居嵌入是由Laurens van der Maaten和Geoff Hinton于2010年开发的一种获奖算法，用于检测高维数据中的模式。它采用概率非线性方法来定位数据在几个不同但相关的低维流形上。
- en: The algorithm emphasizes keeping similar points together in low dimensions,
    as opposed to maintaining the distance between points that are apart in high dimensions,
    which results from algorithms such as PCA that minimize squared distances.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法强调将相似的点保持在低维度中，而不是保持高维度中相距较远的点之间的距离，这是由于诸如PCA之类的算法最小化了平方距离。
- en: The algorithm proceeds by converting high-dimensional distances to (conditional)
    probabilities, where high probabilities imply low distance and reflect the likelihood
    of sampling two points based on similarity. It accomplishes this by positioning
    a normal distribution over each point and computing the density for a point and
    each neighbor, where the perplexity parameter controls the effective number of
    neighbors.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过将高维距离转换为（条件）概率来进行，其中高概率意味着低距离，并反映了基于相似性对两个点进行采样的可能性。它通过在每个点上放置正态分布并计算点和每个邻居的密度来实现这一点，其中困惑参数控制有效邻居的数量。
- en: In a second step, it arranges points in low dimensions and uses similarly computed
    low-dimensional probabilities to match the high-dimensional distribution. It measures
    the difference between the distributions using the Kullback-Leibler divergence,
    which puts a high penalty on misplacing similar points in low dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，它将点排列在低维度，并使用类似计算的低维概率来匹配高维度分布。它使用Kullback-Leibler散度来衡量分布之间的差异，这对于在低维度中放置相似点有很高的惩罚。
- en: The low-dimensional probabilities use a Student's t-distribution with one degree
    of freedom, as it has fatter tails that reduce the penalty of misplacing points
    that are more distant in high dimensions, to manage the crowding problem.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 低维概率使用具有一个自由度的学生t分布，因为它具有更厚的尾部，可以减少在高维度中放置更远点的惩罚，以解决拥挤问题。
- en: 'The upper panels of the following chart show how t-SNE is able to differentiate
    between the image classes. A higher perplexity value increases the number of neighbors
    used to compute local structure, and gradually results in more emphasis on global
    relationships:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表的上部显示了t-SNE如何能够区分图像类别。更高的困惑值增加了用于计算局部结构的邻居数量，并逐渐导致更多强调全局关系：
- en: '![](img/644b48d6-0a84-43cc-9e00-5f6a0d374c1f.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/644b48d6-0a84-43cc-9e00-5f6a0d374c1f.png)
- en: t-SNE is currently the state-of-the-art in high-dimensional data visualization.
    Weaknesses include the computational complexity that scales quadratically in the
    number *n* of points because it evaluates all pairwise distances, but a subsequent
    tree-based implementation has reduced the cost to *n log n*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE目前是高维数据可视化的最新技术。其缺点包括计算复杂度，因为它对点数*n*进行二次评估所有成对距离，但随后的基于树的实现已将成本降低到*n log
    n*。
- en: t-SNE does not facilitate the projection of new data points into the low-dimensional
    space. The compressed output is not a very useful input for distance-based or
    density-based cluster algorithms, because t-SNE treats small and large distances
    differently.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE不便于将新数据点投影到低维空间中。压缩输出对于基于距离或基于密度的聚类算法来说并不是一个非常有用的输入，因为t-SNE对待小距离和大距离的方式不同。
- en: UMAP
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UMAP
- en: Uniform Manifold Approximation and Projection is a more recent algorithm for
    visualization and general dimensionality reduction. It assumes the data is uniformly
    distributed on a locally-connected manifold and looks for the closest low-dimensional
    equivalent using fuzzy topology. It uses a neighbors parameter that impacts the
    result similarly as perplexity above.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀流形逼近和投影是一种更近期的用于可视化和一般降维的算法。它假设数据在局部连接的流形上均匀分布，并寻找最接近的低维等价物，使用模糊拓扑。它使用一个影响结果的邻居参数，类似于上面的困惑。
- en: It is faster, and hence scales better to large datasets than t-SNE, and sometimes
    preserves global structure than better than t-SNE. It can also work with different
    distance functions, including, for example, cosine similarity, which is used to
    measure the distance between word count vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 它比t-SNE更快，因此在大型数据集上的规模更好，并且有时比t-SNE更好地保留全局结构。它还可以使用不同的距离函数，包括例如余弦相似度，用于衡量词数向量之间的距离。
- en: The four charts in the bottom row of the previous figure illustrates how UMAP
    does indeed move the different clusters further apart, whereas t-SNE provides
    more granular insight into the local structure.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前图的底部行中的四个图表说明了UMAP确实将不同的聚类移得更远，而t-SNE提供了更详细的局部结构洞察。
- en: The notebook also contains interactive Plotly visualizations for each algorithm,
    which permit the exploration of the labels and identify which objects are placed
    close to each other.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本还包含每个算法的交互式Plotly可视化，允许探索标签并确定哪些对象彼此靠近。
- en: Clustering
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Both clustering and dimensionality reduction summarize the data. As just discussed
    in detail, dimensionality reduction compresses the data by representing it using
    new, fewer features that capture the most relevant information. Clustering algorithms,
    by contrast, assign existing observations to subgroups that consist of similar
    data points.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维都总结了数据。正如刚刚详细讨论的那样，降维通过使用捕获最相关信息的新的更少的特征来压缩数据。相比之下，聚类算法将现有观察结果分配给由相似数据点组成的子组。
- en: Clustering can serve to better understand the data through the lens of categories
    learned from continuous variables. It also permits automatically categorizing
    new objects according to the learned criteria. Examples of related applications
    include hierarchical taxonomies, medical diagnostics, and customer segmentation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以通过从连续变量学习的类别的角度更好地理解数据。它还允许根据学习的标准自动对新对象进行分类。相关应用示例包括分层分类法、医学诊断和客户细分。
- en: Alternatively, clusters can be used to represent groups as prototypes, using
    (for example) the midpoint of a cluster as the best representative of learned
    grouping. An example application includes image compression.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用群集来表示原型组，例如使用群集的中点作为学习分组的最佳代表。一个示例应用包括图像压缩。
- en: 'Clustering algorithms differ with respect to their strategies for identifying
    groupings:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在识别分组策略方面有所不同：
- en: Combinatorial algorithms select the most coherent of different groupings of
    observations
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合算法选择观察结果的不同分组中最连贯的一个
- en: Probabilistic modeling estimates distributions that most likely generated the
    clusters
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率建模估计最有可能生成群集的分布
- en: Hierarchical clustering finds a sequence of nested clusters that optimizes coherence
    at any given stage
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类找到一系列嵌套群集，优化任何给定阶段的连贯性
- en: 'Algorithms also differ in their notion of what constitutes a useful collection
    of objects, which needs to match the data characteristics, domain, and the goal
    of the applications. Types of groupings include the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在其对构成有用对象集合的概念上也有所不同，这需要与数据特征、领域和应用目标相匹配。分组类型包括以下内容：
- en: Clearly separated groups of various shapes
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种形状的清晰分离的群组
- en: Prototype-based or center-based compact clusters
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于原型或基于中心的紧凑群集
- en: Density-based clusters of arbitrary shape
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意形状的基于密度的群集
- en: Connectivity-based or graph-based clusters
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于连接性或基于图的群组
- en: 'Important additional aspects of a clustering algorithm include the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的重要附加方面包括以下内容：
- en: Whether it requires exclusive cluster membership
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否需要独占的群集成员资格
- en: Whether it makes hard (binary) or soft (probabilistic) assignment
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否进行硬（二进制）或软（概率）分配
- en: Whether it is complete and assigns all data points to clusters
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否完整并将所有数据点分配到群集
- en: The following sections introduce key algorithms, including k-Means, hierarchical,
    and density-based clustering, as well as Gaussian mixture models. The `clustering_algos`
    notebook compares the performance of these algorithms on different, labeled datasets
    to highlight their strengths and weaknesses. It uses mutual information (see [Chapter
    6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *The* *Machine Learning Process*)
    to measure the congruence of cluster assignments and labels.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分介绍了关键算法，包括k-Means、层次和基于密度的聚类，以及高斯混合模型。`clustering_algos`笔记本比较了这些算法在不同的标记数据集上的性能，以突出它们的优势和劣势。它使用互信息（见[第6章](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml)，*机器学习过程*）来衡量聚类分配和标签的一致性。
- en: k-Means clustering
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-Means聚类
- en: k-Means is the most well-known clustering algorithm and was first proposed by
    Stuart Lloyd at Bell Labs in 1957.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means是最著名的聚类算法，最早由贝尔实验室的斯图尔特·劳埃德在1957年提出。
- en: The algorithm finds *K* centroids and assigns each data point to exactly one
    cluster with the goal of minimizing the within-cluster variance (called inertia).
    It typically uses Euclidean distance, but other metrics can also be used. k-Means
    assumes that clusters are spherical and of equal size, and ignores the covariance
    among features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到*K*个质心，并将每个数据点分配给恰好一个群集，目标是最小化群内方差（称为惯性）。它通常使用欧氏距离，但也可以使用其他度量。k-Means假设群集是球形的且大小相等，并忽略特征之间的协方差。
- en: 'The problem is computationally difficult (np-hard) because there are *K^N*
    ways to partition the *N* observations into *K* clusters. The standard iterative
    algorithm delivers a local optimum for a given *K* and proceeds as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在计算上很困难（np-hard），因为有*K^N*种方式将*N*观察分成*K*个群集。标准的迭代算法为给定的*K*提供了局部最优解，并按以下方式进行：
- en: Randomly define *K* cluster centers and assign points to nearest centroid.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机定义*K*群集中心并将点分配给最近的质心。
- en: 'Repeat as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复如下：
- en: For each cluster, compute the centroid as the average of the features
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个集群，计算质心作为特征的平均值
- en: Assign each observation to the closest centroid
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个观察分配给最近的质心
- en: 'Convergence: assignments (or within-cluster variation) don''t change.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛：分配（或群内变化）不变。
- en: 'The `kmeans_implementation` notebook shows how to code the algorithm using
    Python, and visualizes the algorithm''s iterative optimization. The following
    screenshot highlights how the resulting centroids partition the feature space
    into areas called **Voronoi** which delineate the clusters:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmeans_implementation`笔记本展示了如何使用Python编写算法，并可视化算法的迭代优化。以下屏幕截图突出显示了生成的质心如何将特征空间划分为称为**Voronoi**的区域，这些区域划定了群集：'
- en: '![](img/5d174126-9251-4803-970b-0d1369da0989.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d174126-9251-4803-970b-0d1369da0989.png)'
- en: The result is optimal for the given initialization, but alternative starting
    positions will produce different results. Hence, we compute multiple clusterings
    from different initial values and select the solution that minimizes within-cluster
    variance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对于给定的初始化是最佳的，但是替代的起始位置将产生不同的结果。因此，我们从不同的初始值计算多个聚类，并选择最小化群内方差的解决方案。
- en: k-Means requires continuous or one-hot encoded categorical variables. Distance
    metrics are typically sensitive to scale so that standardizing features is necessary
    to make sure they have equal weight.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means需要连续或独热编码的分类变量。距离度量通常对比例敏感，因此需要对特征进行标准化，以确保它们具有相同的权重。
- en: The strengths of k-Means include its wide range of applicability, fast convergence,
    and linear scalability to large data while producing clusters of even size.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means的优势包括其广泛的适用性、快速收敛和对大数据的线性可扩展性，同时产生大小均匀的群集。
- en: 'The weaknesses include:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点包括：
- en: The need to tune the hyperparameter *k*
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整超参数*k*
- en: The lack of a guarantee to find a global optimum
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有找到全局最优解的保证
- en: Restrictive assumption that clusters are spheres and features are not correlated
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制性假设，即群组是球体，特征不相关
- en: Sensitivity to outliers
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对离群值的敏感性
- en: Evaluating cluster quality
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类质量
- en: 'Cluster quality metrics help select among alternative clustering results. The `kmeans_evaluation`
    notebook illustrates the following options:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 群组质量指标有助于在不同的聚类结果中进行选择。`kmeans_evaluation`笔记本说明了以下选项：
- en: The k-Means objective function suggests we compare the evolution of the inertia
    or within-cluster variance.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k均值目标函数建议我们比较惯性或群内方差的演变。
- en: Initially, additional centroids decrease the inertia sharply because new clusters
    improve the overall fit.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，额外的质心会急剧减少惯性，因为新的群组会改善整体拟合。
- en: Once an appropriate number of clusters has been found (assuming it exists),
    new centroids reduce the within-cluster variance by much less as they tend to
    split natural groupings.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到了适当数量的群组（假设存在），新的质心减少了群内方差，因为它们倾向于分割自然的分组。
- en: 'Hence, when k-Means finds a good cluster representation of the data, the inertia
    tends to follow an elbow-shaped path similar to the explained variance ratio for
    PCA, as shown in the following screenshot (see notebook for implementation details):'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，当k均值找到数据的良好群组表示时，惯性往往会遵循类似于PCA的解释方差比例的拐点路径，如下面的屏幕截图所示（有关实施细节，请参见笔记本）：
- en: '![](img/c58c7f92-24fd-4c80-84e9-47374cf599f0.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c58c7f92-24fd-4c80-84e9-47374cf599f0.png)'
- en: 'The silhouette coefficient provides a more detailed picture of cluster quality.
    It answers the question: how far are the points in the nearest cluster, relative
    to the points in the assigned cluster?'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数提供了对群组质量的更详细的描述。它回答了一个问题：最近群组中的点与分配的群组中的点有多远？
- en: 'To this end, it compares the mean intra-cluster distance (*a*) to the mean
    distance of the nearest cluster (*b*) and computes the following score *s*:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，它将平均群内距离(*a*)与最近群组的平均距离(*b*)进行比较，并计算以下分数*s*：
- en: '![](img/de29d1c9-9959-45c3-b16b-3397a9f7e7ce.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de29d1c9-9959-45c3-b16b-3397a9f7e7ce.png)'
- en: The score can vary from between *-1* and *1*, but negative values are unlikely
    in practice because they imply that the majority of points are assigned to the
    wrong cluster. A useful visualization of the silhouette score compares the values
    for each data point to the global average because it highlights the coherence
    of each cluster relative to the global configuration. The rule of thumb is to
    avoid clusters with mean scores below the average for all samples.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 得分可以在*-1*和*1*之间变化，但在实践中负值不太可能出现，因为它意味着大多数点被分配到错误的群组。轮廓分数的有用可视化将每个数据点的值与全局平均值进行比较，因为它突出了每个群组相对于全局配置的一致性。经验法则是避免平均分数低于所有样本的平均值的群组。
- en: 'The following screenshot shows an excerpt from the silhouette plot for three
    and four clusters, where the former highlights the poor fit of cluster *1* by
    sub-par contributions to the global silhouette score, whereas all of the four
    clusters have some values that exhibit above average scores:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了三个和四个群组的轮廓图摘录，前者突出了群组*1*的拙劣拟合，因为对全局轮廓分数的贡献不佳，而所有四个群组都有一些值表现出高于平均分数：
- en: '![](img/b5293c0c-9b6d-4722-bff9-aeeb1d7fc14b.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5293c0c-9b6d-4722-bff9-aeeb1d7fc14b.png)'
- en: In sum, given the usually unsupervised nature, it is necessary to vary the hyperparameters
    of the cluster algorithms and evaluate the different results. It is also important
    to calibrate the scale of the features, in particular when some should be given
    a higher weight and should thus be measured on a larger scale.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，鉴于通常是无监督的性质，有必要改变聚类算法的超参数并评估不同的结果。调整特征的比例也很重要，特别是当一些特征应该被赋予更高的权重并且因此应该在更大的比例上进行测量时。
- en: Finally, to validate the robustness of the results, use subsets of data to identify
    whether particular patterns emerge consistently.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了验证结果的稳健性，使用数据子集来确定是否会一致出现特定模式。
- en: Hierarchical clustering
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Hierarchical clustering avoids the need to specify a target number of clusters
    because it assumes that data can successively be merged into increasingly dissimilar
    clusters. It does not pursue a global objective but decides incrementally how
    to produce a sequence of nested clusters that range from a single cluster to clusters
    consisting of the individual data points.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类避免了需要指定目标群组数量的需要，因为它假设数据可以逐渐合并成越来越不相似的群组。它不追求全局目标，而是逐步决定如何产生一系列嵌套的群组，从单一群组到由个体数据点组成的群组。
- en: 'There are two approaches:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法：
- en: '**Agglomerative clustering** proceeds bottom-up, sequentially merging two of
    the remaining groups based on similarity'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**凝聚聚类**是自下而上进行的，根据相似性依次合并剩余的两个群组'
- en: '**Divisive clustering** works top-down and sequentially splits the remaining
    clusters to produce the most distinct subgroups'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分裂聚类**自上而下工作，依次分裂剩余的群组以产生最不同的子群组'
- en: Both groups produce *N*-1 hierarchical levels and facilitate the selection of
    a clustering at the level that best partitions data into homogenous groups. We
    will focus on the more common agglomerative clustering approach.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 两个群组都产生了*N*-1个层次水平，并且有助于选择最佳将数据分区为同质群组的层次的聚类。我们将重点放在更常见的凝聚聚类方法上。
- en: The agglomerative clustering algorithm departs from the individual data points
    and computes a similarity matrix containing all mutual distances. It then takes
    *N*-1 steps until there are no more distinct clusters, and each time updates the
    similarity matrix to substitute elements that have been merged by the new cluster
    so that the matrix progressively shrinks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 凝聚聚类算法不同于个体数据点，并计算包含所有相互距离的相似性矩阵。然后，它经过*N*-1步，直到没有更多不同的群组，每次更新相似性矩阵以替换已被新群组合并的元素，使矩阵逐渐缩小。
- en: 'While hierarchical clustering does not have hyperparameters like k-Means, the
    measure of dissimilarity between clusters (as opposed to individual data points)
    has an important impact on the clustering result. The options differ as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-link**: the distance between nearest neighbors of two clusters'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete link**: the maximum distance between respective cluster members'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group average:** the distance between averages for each group'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward''s method**: minimizes within-cluster variance'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization – dendrograms
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical clustering provides insight into degrees of similarity among observations
    as it continues to merge data. A significant change in the similarity metric from
    one merge to the next suggests a natural clustering existed prior to this point.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The dendrogram visualizes the successive merges as a binary tree, displaying
    the individual data points as leaves and the final merge as the root of the tree.
    It also shows how the similarity monotonically decreases from bottom to top. Hence,
    it is natural to select a clustering by cutting the dendrogram.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot (see the `hierarchical_clustering` notebook for implementation
    details) illustrates the dendrogram for the classic Iris dataset with four classes
    and three features, using the four different distance metrics introduced precedingly:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/000bcf25-596d-4af8-b8e8-02cab8bab63d.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: It evaluates the fit of the hierarchical clustering using the cophenetic correlation
    coefficient, which compares the pairwise distances among points and the cluster
    similarity metric at which a pairwise merge occurred. A coefficient of 1 implies
    that closer points always merge earlier.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Different linkage methods produce dendrograms of different appearance, so we
    cannot use this visualization to compare results across methods. In addition,
    Ward's method, which minimizes within-cluster variance, may not properly reflect
    the change in variance, but rather the total variance, which may be misleading.
    Instead, other quality metrics such as cophenetic correlation, or measures such
    as inertia (if aligned with the overall goal), may be more appropriate.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'The strengths of clustering include:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: You do not need to specify the number of clusters
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It offers insight about potential clustering by means of an intuitive visualization
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It produces a hierarchy of clusters that can serve as taxonomy
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be combined with k-Means to reduce the number of items at the start of
    the agglomerative process
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses of hierarchical clustering include:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: The high cost in terms of computation and memory because of the numerous similarity
    matrix updates
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not achieve the global optimum because all merges are final
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The curse of dimensionality leads to difficulties with noisy, high-dimensional
    data
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based clustering
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Density-based clustering algorithms assign cluster membership based on proximity
    to other cluster members. They pursue the goal of identifying dense regions of
    arbitrary shapes and sizes. They do not require the specification of a certain
    number of clusters but instead rely on parameters that define the size of a neighborhood
    and a density threshold (see the `density_based_clustering` notebook for relevant
    code samples).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Density-based spatial clustering of applications with noise** (**DBSCAN**)
    was developed in 1996, and received the *Test of Time* award at the 2014 KDD conference
    because of the attention it has received in both theory and practice.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: It aims to identify core and non-core samples, where the former extend a cluster
    and the latter are part of a cluster but do not have sufficient nearby neighbors
    to further grow the cluster. Other samples are outliers and not assigned to any
    cluster.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: It uses an `eps` parameter for the radius of the neighborhood and `min_samples`
    for the number of members required for core samples. It is deterministic and exclusive
    and has difficulties with clusters of different density and high-dimensional data.
    It can be challenging to tune the parameters to the requisite density, especially
    as it is often not constant.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`eps`参数作为邻域的半径和`min_samples`作为核心样本所需的成员数量。它是确定性的和排他的，并且在不同密度和高维数据的聚类中存在困难。调整参数以满足必要密度可能具有挑战性，尤其是因为它通常不是恒定的。
- en: Hierarchical DBSCAN
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次DBSCAN
- en: Hierarchical DBSCAN is a more recent development that assumes clusters are islands
    of potentially differing density, to overcome the DBSCAN challenges just mentioned.
    It also aims to identify the core and non-core samples. It uses the `min_cluster_
    size` and `min_samples` parameters to select a neighborhood and extend a cluster.
    The algorithm iterates over multiple `eps` values and chooses the most stable
    clustering.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 分层DBSCAN是一个更近期的发展，它假设聚类是潜在密度不同的岛屿，以克服刚才提到的DBSCAN挑战。它还旨在识别核心和非核心样本。它使用`min_cluster_size`和`min_samples`参数来选择邻域并扩展聚类。该算法在多个`eps`值上进行迭代，并选择最稳定的聚类。
- en: In addition to identifying clusters of varying density, it provides insight
    into the density and hierarchical structure of the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 除了识别不同密度的聚类，它还提供了关于数据的密度和层次结构的见解。
- en: 'The following screenshots show how DBSCAN and HDBSCAN are able to identify
    very differently shaped clusters:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了DBSCAN和HDBSCAN如何能够识别形状非常不同的聚类：
- en: '![](img/8667993d-ea61-4372-b107-a000c8036cdf.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8667993d-ea61-4372-b107-a000c8036cdf.png)'
- en: Gaussian mixture models
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: A **Gaussian mixture model** (**GMM**) is a generative model that assumes the
    data has been generated by a mix of various multivariate normal distributions.
    The algorithm aims to estimate the mean and covariance matrices of these distributions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（**GMM**）是一种生成模型，假设数据是由各种多元正态分布的混合生成的。该算法旨在估计这些分布的均值和协方差矩阵。'
- en: 'It generalizes the k-Means algorithm: it adds covariance among features so
    that clusters can be ellipsoids rather than spheres, while the centroids are represented
    by the means of each distribution. The GMM algorithm performs soft assignments
    because each point has the probability to be a member of any cluster.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 它推广了k-Means算法：它在特征之间添加了协方差，使得聚类可以是椭球体而不是球体，而质心由每个分布的均值表示。GMM算法执行软分配，因为每个点都有可能成为任何聚类的成员。
- en: The expectation-maximization algorithm
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望最大化算法
- en: GMM uses the expectation-maximization algorithm to identify the components of
    the mixture of Gaussian distributions. The goal is to learn the probability distribution
    parameters from unlabeled data.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: GMM使用期望最大化算法来识别高斯分布混合的组件。其目标是从未标记的数据中学习概率分布参数。
- en: 'The algorithm proceeds iteratively as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 算法按以下方式进行迭代：
- en: Initialization—Assume random centroids (for example, using k-Means)
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化——假设随机质心（例如，使用k-Means）
- en: 'Repeat the following steps until convergence (that is, changes in assignments
    drop below the threshold):'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤直到收敛（即，分配的变化低于阈值）：
- en: '**Expectation step**: Soft assignment—compute probabilities for each point
    from each distribution'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**期望步骤**：软分配——计算每个点来自每个分布的概率'
- en: '**Maximization step**: Adjust normal-distribution parameters to make data points
    most likely'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大化步骤**：调整正态分布参数以使数据点最有可能'
- en: 'The following screenshot shows the GMM cluster membership probabilities for
    the Iris dataset as contour lines:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了Iris数据集的GMM聚类成员概率作为等高线：
- en: '![](img/ffc6ce3d-4da8-4d41-8a8a-a9d68111943c.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffc6ce3d-4da8-4d41-8a8a-a9d68111943c.png)'
- en: Hierarchical risk parity
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层风险平价
- en: The key idea of hierarchical risk parity is to use hierarchical clustering on
    the covariance matrix in order to be able to group assets with similar correlations
    together, and reduce the number of degrees of freedom by only considering similar
    assets as substitutes when constructing the portfolio (see notebook and Python
    files in the `hierarchical_risk_parity` subfolder for details).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 分层风险平价的关键思想是在协方差矩阵上使用分层聚类，以便能够将具有相似相关性的资产分组在一起，并通过仅在构建投资组合时考虑相似资产作为替代品来减少自由度的数量（有关详细信息，请参见`hierarchical_risk_parity`子文件夹中的笔记本和Python文件）。
- en: 'The first step is to compute a distance matrix that represents proximity for
    correlated assets and meets distance metric requirements. The resulting matrix
    becomes an input to the SciPy hierarchical clustering function which computes
    the successive clusters using one of several available methods discussed so far:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算一个距离矩阵，表示相关资产的接近度并满足距离度量要求。结果矩阵成为SciPy分层聚类函数的输入，该函数使用迄今为止讨论的几种可用方法计算连续的聚类：
- en: '[PRE22]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `linkage_matrix` can be used as input to the `seaborn.clustermap` function
    to visualize the resulting hierarchical clustering. The dendrogram displayed by
    `seaborn` shows how individual assets and clusters of assets are merged based
    on their relative distances:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`linkage_matrix`可以作为输入传递给`seaborn.clustermap`函数，以可视化生成的分层聚类。`seaborn`显示的树状图显示了基于它们的相对距离如何合并个别资产和资产集群：'
- en: '[PRE23]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/d243557e-7c8a-4cbc-83c5-222cd87119ad.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d243557e-7c8a-4cbc-83c5-222cd87119ad.png)'
- en: Heatmap
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 热图
- en: Compared to a `seaborn.heatmap` of the original correlation matrix, there is
    now significantly more structure in the sorted data (right panel).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始相关矩阵的`seaborn.heatmap`相比，排序数据（右侧面板）中现在有明显更多的结构。
- en: 'Using the tickers sorted according to the hierarchy induced by the clustering
    algorithm, HRP now proceeds to compute a top-down inverse-variance allocation
    that successively adjusts weights depending on the variance of the subclusters
    further down the tree:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用根据聚类算法引起的层次排序的股票代码，HRP现在继续计算自上而下的逆方差分配，根据树下面的子集的方差依次调整权重：
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To this end, the algorithm uses bisectional search to allocate the variance
    of a cluster to its elements based on their relative riskiness:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，该算法使用二分搜索来根据相对风险将集群的方差分配给其元素：
- en: '[PRE25]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The resulting portfolio allocation produces weights that sum to `1` and reflect
    the structure present in the correlation matrix (see notebook for details).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生的投资组合分配产生的权重总和为`1`，并反映了相关矩阵中存在的结构（详细信息请参见笔记本）。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored unsupervised learning methods that allow us to
    extract valuable signal from our data, without relying on the help of outcome
    information provided by labels.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了无监督学习方法，这些方法使我们能够从数据中提取有价值的信号，而无需依赖标签提供的结果信息。
- en: We saw how we can use linear dimensionality reduction methods, such as PCA and
    ICA, to extract uncorrelated or independent components from the data that can
    serve as risk factors or portfolio weights. We also covered advanced non-linear
    manifold learning techniques that produce state-of-the-art visualizations of complex
    alternative datasets.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用线性降维方法，比如PCA和ICA，从数据中提取不相关或独立的成分，这些成分可以作为风险因素或投资组合权重。我们还介绍了先进的非线性流形学习技术，可以产生复杂替代数据集的最新可视化效果。
- en: In the second part, we covered several clustering methods that produce data-driven
    groupings under various assumptions. These groupings can be useful, for example,
    to construct portfolios that apply risk-parity principles to assets that have
    been clustered hierarchically.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们介绍了几种聚类方法，这些方法根据不同的假设产生了数据驱动的分组。例如，这些分组可以用于构建将风险平价原则应用于经过分层聚类的资产的投资组合。
- en: In the next three chapters, we will learn about various ML techniques for a
    key source of alternative data, namely, natural language processing for text documents.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将学习关于另类数据的一个关键来源，即自然语言处理文本文档的各种机器学习技术。

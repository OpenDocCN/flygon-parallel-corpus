- en: Word Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: In the two previous chapters, we applied the bag-of-words model to convert text
    data into a numerical format. The results were sparse, fixed-length vectors that
    represent documents in a high-dimensional word space. This allows evaluating the
    similarity of documents and creates features to train a machine learning algorithm
    and classify a document's content or rate the sentiment expressed in it. However,
    these vectors ignore the context in which a term is used so that, for example,
    a different sentence containing the same words would be encoded by the same vector.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们应用了词袋模型将文本数据转换为数字格式。结果是稀疏的、固定长度的向量，代表了高维词空间中的文档。这允许评估文档的相似性，并创建特征来训练机器学习算法，并对文档的内容进行分类或评估其中表达的情感。然而，这些向量忽略了术语的使用上下文，因此，例如，包含相同单词的不同句子将由相同的向量编码。
- en: In this chapter, we will introduce an alternative class of algorithms that use
    neural networks to learn a vector representation of individual semantic units such
    as a word or a paragraph. These vectors are dense rather than sparse, and have
    a few hundred real-valued rather than tens of thousands of binary or discrete
    entries. They are called **embeddings** because they assign each semantic unit
    a location in a continuous vector space.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一类使用神经网络学习单个语义单元（如单词或段落）的向量表示的替代算法。这些向量是密集的，而不是稀疏的，具有几百个实值，而不是数万个二进制或离散条目。它们被称为**嵌入**，因为它们为每个语义单元分配了一个在连续向量空间中的位置。
- en: Embeddings result from training a model to relate tokens to their context with
    the benefit that similar usage implies a similar vector. Moreover, we will see
    how the embeddings encode semantic aspects, such as relationships among words
    by means of their relative location. As a result, they are powerful features for
    use in the deep learning models that we will introduce in the following chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是通过训练模型将标记与它们的上下文相关联而产生的，这样的好处是相似的使用意味着相似的向量。此外，我们将看到嵌入如何编码语义方面，例如通过它们的相对位置来表示单词之间的关系。因此，它们是在我们将在接下来的章节中介绍的深度学习模型中使用的强大特征。
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，在本章中，我们将涵盖以下主题：
- en: What word embeddings are and how they work and capture semantic information
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入是什么，它们是如何工作和捕捉语义信息的
- en: How to use trained word vectors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用训练好的词向量
- en: Which network architectures are useful to train Word2vec models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些网络架构对训练Word2vec模型有用
- en: How to train a Word2vec model using Keras, gensim, and TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Keras、gensim和TensorFlow训练Word2vec模型
- en: How to visualize and evaluate the quality of word vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化和评估词向量的质量
- en: How to train a Word2vec model using SEC filings
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用SEC文件训练Word2vec模型
- en: How `Doc2vec` extends Word2vec
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Doc2vec`如何扩展Word2vec'
- en: How word embeddings encode semantics
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入如何编码语义
- en: The bag-of-words model represents documents as vectors that reflect the tokens
    they contain. Word embeddings represent tokens as lower dimensional vectors so
    that their relative location reflects their relationship in terms of how they
    are used in context. They embody the distributional hypothesis from linguistics
    that claims words are best defined by the company they keep.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型将文档表示为反映它们包含的标记的向量。词嵌入将标记表示为较低维度的向量，以便它们的相对位置反映它们在上下文中的使用关系。它们体现了语言学中的分布假设，即单词最好由它们所保持的公司来定义。
- en: Word vectors are capable of capturing numerous semantic aspects; not only are
    synonyms close to each other, but words can have multiple degrees of similarity,
    for example, the word driver could be similar to motorist or to cause. Furthermore,
    embeddings reflect relationships among pairs of words such as analogies (Tokyo
    is to Japan what Paris is to France, or went is to go what saw is to see) as we
    will illustrate later in this section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量能够捕捉许多语义方面；不仅同义词彼此接近，而且单词之间可以有多个相似度，例如，单词"driver"可能与"motorist"或"cause"相似。此外，嵌入反映了词对之间的关系，比如类比（东京对日本就像巴黎对法国，或者went对go就像saw对see），我们将在本节后面进行说明。
- en: Embeddings result from training a machine learning model to predict words from
    their context or vice versa. In the following section, we will introduce how these
    neural language models work and present successful approaches including Word2vec,
    `Doc2vec`, and fastText.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是通过训练机器学习模型来预测单词或其上下文的结果。在接下来的部分中，我们将介绍这些神经语言模型的工作原理，并介绍成功的方法，包括Word2vec、`Doc2vec`和fastText。
- en: How neural language models learn usage in context
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经语言模型如何学习上下文中的使用
- en: Word embeddings result from training a shallow neural network to predict a word
    given its context. Whereas traditional language models define context as the words
    preceding the target, word-embedding models use the words contained in a symmetric
    window surrounding the target. In contrast, the bag-of-words model uses the entirety
    of documents as context and uses (weighted) counts to capture the cooccurrence
    of words rather than predictive vectors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是通过训练一个浅层神经网络来预测给定上下文的单词而产生的。而传统的语言模型将上下文定义为目标之前的单词，词嵌入模型使用包围目标的对称窗口中包含的单词。相比之下，词袋模型使用整个文档作为上下文，并使用（加权）计数来捕捉单词的共现而不是预测向量。
- en: Earlier neural language models that were used included nonlinear hidden layers
    that increased the computational complexity. Word2vec and its extensions simplified
    the architecture to enable training on large datasets (Wikipedia, for example,
    contains over two billion tokens; see [Chapter 17](https://www.packtpub.com/sites/default/files/downloads/Deep_Learning.pdf),
    *Deep Learning*, for additional details on feed-forward networks).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的神经语言模型包括非线性隐藏层，增加了计算复杂性。Word2vec及其扩展简化了架构，使其能够在大型数据集上进行训练（例如，维基百科包含超过20亿个标记；有关前馈网络的更多细节，请参见[第17章](https://www.packtpub.com/sites/default/files/downloads/Deep_Learning.pdf)，*深度学习*）。
- en: The Word2vec model – learn embeddings at scale
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec模型-规模学习嵌入
- en: 'A Word2vec model is a two-layer neural net that takes a text corpus as input
    and outputs a set of embedding vectors for words in that corpus. There are two
    different architectures to learn word vectors efficiently using shallow neural
    networks depicted in the following figure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The **Continuous-Bag-Of-Words** (**CBOW**) model predicts the target word using
    the average of the context word vectors as input so that their order does not
    matter. A CBOW model trains faster and tends to be slightly more accurate for
    frequent terms, but pays less attention to infrequent words.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Skip-Gram** (**SG**) model, by contrast, uses the target word to predict
    words sampled from the context. It works well with small datasets and finds good
    representations even for rare words or phrases:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/86b7aa9d-1435-458d-b352-da6d4097cea6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Hence, the Word2vec model receives an embedding vector as input and computes
    the dot product with another embedding vector. Note that, assuming normed vectors,
    the dot product is maximized (in absolute terms) when vectors are equal, and minimized
    when they are orthogonal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: It then uses backpropagation to adjust the embedding weights in response to
    the loss computed by an objective function due to any classification errors. We
    will see in the next section how Word2vec computes the loss.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Training proceeds by sliding the context window over the documents, typically
    segmented into sentences. Each complete iteration over the corpus is called an
    **epoch**. Depending on the data, several dozen epochs may be necessary for vector
    quality to converge.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Technically, the SG model has been shown to factorize a word-context matrix
    that contains the pointwise mutual information of the respective word and context
    pairs implicitly (see references on GitHub).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Model objective – simplifying the softmax
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2vec models aim to predict a single word out of the potentially very large
    vocabulary. Neural networks often use the softmax function that maps any number
    of real values to an equal number of probabilities to implement the corresponding
    multiclass objective, where *h* refers to the embedding and *v* to the input vectors,
    and *c* is the context of word *w*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35620ce6-e09a-4043-a8ff-b5f2b6630334.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'However, the softmax complexity scales with the number of classes, as the denominator
    requires the computation of the dot product for all words in the vocabulary to
    standardize the probabilities. Word2vec models gain efficiency by using a simplified
    version of the softmax or sampling-based approaches (see references for details):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The **hierarchical softmax** organizes the vocabulary as a binary tree with
    words as leaf nodes. The unique path to each node can be used to compute the word
    probability.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise-contrastive estimation** (**NCE**) samples out-of-context "noise words"
    and approximates the multiclass task by a binary classification problem. The NCE
    derivative approaches the softmax gradient as the number of samples increases,
    but as few as 25 samples can yield convergence similar to the softmax, at a rate
    that is 45 times faster.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Negative sampling** (**NEG**) omits the noise word samples to approximate
    NCE and directly maximizes the probability of the target word. Hence, NEG optimizes
    the semantic quality of embedding vectors (similar vectors for similar usage)
    rather than the accuracy on a test set. It may, however, produce poorer representations
    for infrequent words than the hierarchical softmax objective.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic phrase detection
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preprocessing typically involves phrase detection, that is, the identification
    of tokens that are commonly used together and should receive a single vector representation
    (for example, New York City, see the discussion of n-grams in [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml),
    *Working with Text Data*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Word2vec authors use a simple lift scoring method that identifies
    two words *w[i]*, *w[j]* as a bigram if their joint occurrence exceeds a given
    threshold relative to each word''s individual appearance, corrected by a discount
    factor *δ*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Word2vec作者使用简单的提升评分方法，如果两个单词*w[i]*，*w[j]*的联合出现超过给定阈值相对于每个单词的个体出现，通过折扣因子*δ*进行校正，则将它们识别为二元组。
- en: '![](img/21e54b2d-361b-4c00-9c9f-5f06a332317c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21e54b2d-361b-4c00-9c9f-5f06a332317c.png)'
- en: The scorer can be applied repeatedly to identify successively longer phrases.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 评分器可以重复应用，以识别逐渐更长的短语。
- en: 'An alternative is the normalized point-wise mutual information score that is
    more accurate, but also more costly to compute. It uses the relative word frequency
    *P(w)* and varies between +1 and -1:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是归一化的点间互信息分数，这更准确，但计算成本更高。它使用相对词频*P(w)*，在+1和-1之间变化：
- en: '![](img/84b9a2c6-2ef4-4777-9dc3-1891145ae39c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84b9a2c6-2ef4-4777-9dc3-1891145ae39c.png)'
- en: How to evaluate embeddings – vector arithmetic and analogies
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估嵌入 - 向量算术和类比
- en: The bag-of-words model creates document vectors that reflect the presence and
    relevance of tokens to the document. **Latent semantic analysis** reduces the
    dimensionality of these vectors and identifies what can be interpreted as latent
    concepts in the process. **Latent Dirichlet allocation** represents both documents
    and terms as vectors that contain the weights of latent topics.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型创建反映标记对文档的存在和相关性的文档向量。潜在语义分析降低了这些向量的维度，并在过程中确定了可以被解释为潜在概念的内容。潜在狄利克雷分配表示文档和术语，这些向量包含潜在主题的权重。
- en: The dimensions of the word and phrase vectors do not have an explicit meaning.
    However, the embeddings encode similar usage as proximity in the latent space
    in a way that carries over to semantic relationships. This results in the interesting
    properties that analogies can be expressed by adding and subtracting word vectors.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 单词和短语向量的维度没有明确的含义。然而，嵌入编码了潜在空间中的相似用法，以一种能够延续到语义关系的方式。这导致了类比可以通过添加和减去单词向量来表达的有趣属性。
- en: 'The following figure shows how the vector connecting Paris and France (that
    is, the difference of their embeddings) reflects the capital of relationship.
    The analogous relationship, London: UK, corresponds to the same vector, that is,
    the UK is very close to the location obtained by adding the capital of vector
    to London:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了连接巴黎和法国的向量（即它们的嵌入的差异）如何反映了首都的关系。类似的关系，伦敦：英国，对应于相同的向量，即英国非常接近通过将首都向量添加到伦敦获得的位置：
- en: '![](img/896d4b2a-33bd-4eeb-98eb-34b7653ae59e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/896d4b2a-33bd-4eeb-98eb-34b7653ae59e.png)'
- en: Just as words can be used in different contexts, they can be related to other
    words in different ways, and these relationships correspond to different directions
    in the latent space. Accordingly, there are several types of analogies that the
    embeddings should reflect if the training data permits.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就像单词可以在不同的上下文中使用一样，它们可以以不同的方式与其他单词相关联，这些关系对应于潜在空间中的不同方向。因此，如果训练数据允许，嵌入应该反映出几种类型的类比。
- en: 'The Word2vec authors provide a list of several thousand relationships spanning
    aspects of geography, grammar and syntax, and family relationships to evaluate
    the quality of embedding vectors. As illustrated above, the test validates that
    the target word (UK) is closest to the result of adding the vector that represents
    an analogous relationship (Paris: France) to the target''s complement (London).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec的作者提供了数千个关系的列表，涵盖地理、语法和句法以及家庭关系的各个方面，以评估嵌入向量的质量。如上所示，测试验证了目标词（英国）最接近于添加代表类似关系（巴黎：法国）的向量到目标的补充（伦敦）的结果。
- en: 'The following figure projects the 300-dimensional embeddings of the most closely
    related analogies for a Word2vec model trained on the Wikipedia corpus, with over
    2 billion tokens, into two dimensions using **principal component analysis** (**PCA**).
    A test of over 24,400 analogies from the following categories achieved an accuracy
    of over 73.5% (see notebook):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下图将在维基百科语料库上训练的Word2vec模型的300维嵌入投影到两个维度，使用主成分分析（PCA）找到最相关类比的嵌入。来自以下类别的超过24,400个类比的测试达到了超过73.5％的准确率（请参见笔记本）：
- en: '![](img/2871a658-5c0d-4506-a052-876002c88b8a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2871a658-5c0d-4506-a052-876002c88b8a.png)'
- en: Working with embedding models
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入模型
- en: Similar to other unsupervised learning techniques, the goal of learning embedding
    vectors is to generate features for other tasks such as text classification or
    sentiment analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他无监督学习技术类似，学习嵌入向量的目标是为其他任务生成特征，如文本分类或情感分析。
- en: 'There are several options to obtain embedding vectors for a given corpus of
    documents:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种选项可以为给定的文档语料库获取嵌入向量：
- en: Use embeddings learned from a generic large corpus such as Wikipedia or Google
    News
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从维基百科或谷歌新闻等通用大语料库中学习的嵌入
- en: Train your own model using documents that reflect a domain of interest
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反映感兴趣领域的文档来训练自己的模型
- en: The less generic and more specialized the content of the subsequent text modeling
    task is, the more preferable is the second approach. However, quality word embeddings
    are data-hungry and require informative documents containing hundreds of millions
    of words.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 后续文本建模任务的内容越专业化，第二种方法就越可取。然而，高质量的词嵌入需要数据丰富，需要包含数亿字的信息性文档。
- en: How to use pre-trained word vectors
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用预训练的词向量
- en: There are several sources for pretrained word embeddings. Popular options include
    Stanford's GloVE and spaCy's built-in vectors (see the notebook `using_trained_vectors`
    for details).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练词嵌入有几个来源。流行的选择包括斯坦福的GloVE和spaCy的内置向量（有关详细信息，请参见笔记本“using_trained_vectors”）。
- en: GloVe – global vectors for word representation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GloVe - 用于单词表示的全局向量
- en: 'GloVe is an unsupervised algorithm developed at the Stanford NLP lab that learns
    vector representations for words from aggregated global word-word co-occurrence
    statistics (see references). Vectors pretrained on the following web-scale sources
    are available:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe是在斯坦福NLP实验室开发的一种无监督算法，它从聚合的全局词-词共现统计中学习单词的向量表示（见参考文献）。在以下网络规模的预训练向量可用：
- en: Common Crawl with 42B or 840B tokens and a vocabulary or 1.9M or 2.2M tokens
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见爬网有420亿或840亿标记和190万或220万标记的词汇表
- en: Wikipedia 2014 + Gigaword 5 with 6B tokens and a vocabulary of 400K tokens
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科2014 + Gigaword 5，有60亿标记和40万标记的词汇表
- en: Twitter using 2B tweets, 27B tokens and a vocabulary of 1.2M tokens
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter使用了20亿推文，270亿标记和120万标记的词汇表
- en: 'We can use gensim to convert and load the vector text files into the `KeyedVector`
    object:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用gensim将向量文本文件转换并加载到`KeyedVector`对象中：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Word2vec authors provide text files containing over 24,000 analogy tests
    that gensim uses to evaluate word vectors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec的作者提供了包含超过24000个类比测试的文本文件，gensim用这些文件来评估词向量。
- en: 'The word vectors trained on the Wikipedia corpus cover all analogies and achieve
    an overall accuracy of 75.5% with some variation across categories:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在维基百科语料库上训练的词向量涵盖了所有类比，并在各个类别上达到了总体准确率为75.5%的水平，但在各个类别上有所不同：
- en: '| **Category** | **Samples** | **Accuracy** | **Category** | **Samples** |
    **Accuracy** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **样本** | **准确率** | **类别** | **样本** | **准确率** |'
- en: '| capital-common-countries | 506 | 94.86% | comparative | 1,332 | 88.21% |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: 常见国家首都 | 506 | 94.86% | 比较级 | 1,332 | 88.21%
- en: '| capital-world | 8,372 | 96.46% | superlative | 1,056 | 74.62% |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: 首都世界 | 8,372 | 96.46% | 最高级 | 1,056 | 74.62%
- en: '| city-in-state | 4,242 | 60.00% | present-participle | 1,056 | 69.98% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: 州内城市 | 4,242 | 60.00% | 现在分词 | 1,056 | 69.98%
- en: '| currency | 752 | 17.42% | nationality-adjective | 1,640 | 92.50% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: 货币 | 752 | 17.42% | 国籍形容词 | 1,640 | 92.50%
- en: '| family | 506 | 88.14% | past-tense | 1,560 | 61.15% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: 家庭 | 506 | 88.14% | 过去式 | 1,560 | 61.15%
- en: '| adjective-to-adverb | 992 | 22.58% | plural | 1,332 | 78.08% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: 形容词到副词 | 992 | 22.58% | 复数 | 1,332 | 78.08%
- en: '| opposite | 756 | 28.57% | plural-verbs | 870 | 58.51% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: 相反 | 756 | 28.57% | 复数动词 | 870 | 58.51%
- en: The Common Crawl vectors for the 100,000 most common tokens cover about 80%
    of the analogies and achieve slightly higher accuracy at 78%, whereas the Twitter
    vectors cover only 25% with 62% accuracy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 常见爬网向量覆盖了10万个最常见标记的约80%的类比，并在78%的准确率上略高，而Twitter向量只覆盖了25%，准确率为62%。
- en: How to train your own word vector embeddings
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练自己的词向量嵌入
- en: Many tasks require embeddings or domain-specific vocabulary that pretrained
    models based on a generic corpus may not represent well or at all. Standard Word2vec
    models are not able to assign vectors to out-of-vocabulary words and instead use
    a default vector that reduces their predictive value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务需要嵌入或特定领域的词汇表，预训练模型基于通用语料库可能无法很好地或根本无法表示。标准的Word2vec模型无法为词汇表外的词分配向量，而是使用默认向量，降低了它们的预测价值。
- en: For example, when working with industry-specific documents, the vocabulary or
    its usage may change over time as new technologies or products emerge. As a result,
    the embeddings need to evolve as well. In addition, corporate earnings releases
    use nuanced language not fully reflected in GloVe vectors pretrained on Wikipedia
    articles.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在处理特定行业的文件时，词汇表或其使用可能会随着时间的推移而发生变化，因为新技术或产品出现。因此，嵌入也需要相应地发展。此外，公司盈利发布使用的语言不完全反映在维基百科文章上预训练的GloVe向量中。
- en: We will illustrate the Word2vec architecture using the Keras library that we
    will introduce in more detail in the next chapter and the more performant gensim
    adaptation of the code provided by the Word2vec authors. The notebook Word2vec
    contains additional implementation detail, including a reference of a TensorFlow
    implementation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras库来说明Word2vec架构，我们将在下一章中更详细地介绍，并使用Word2vec作者提供的更高性能的gensim代码适配。笔记本Word2vec包含了额外的实现细节，包括TensorFlow实现的参考。
- en: The Skip-Gram architecture in Keras
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的Skip-Gram架构
- en: To illustrate the Word2vec network architecture, we use the TED Talk dataset
    with aligned English and Spanish subtitles that we first introduced in [Chapter
    13](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=682&action=edit#post_584), *Working
    with Text Data*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Word2vec网络架构，我们使用TED Talk数据集，该数据集具有对齐的英语和西班牙语字幕，我们首次在[第13章](https://cdp.packtpub.com/hands_on_machine_learning_for_algorithmic_trading/wp-admin/post.php?post=682&action=edit#post_584)中介绍了*处理文本数据*。
- en: The notebook contains the code to tokenize the documents and assign a unique
    ID to each item in the vocabulary. We require at least five occurrences in the
    corpus and keep a vocabulary of 31,300 tokens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含了对文档进行标记和为词汇表中的每个项目分配唯一ID的代码。我们需要语料库中至少出现五次，并保留31300个标记的词汇表。
- en: Noise-contrastive estimation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声对比估计
- en: Keras includes a `make_sampling_table` method that allows us to create a training
    set as pairs of context and noise words with corresponding labels, sampled according
    to their corpus frequencies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包括一个`make_sampling_table`方法，允许我们创建一个训练集，其中上下文和噪声词成对出现，并根据它们在语料库中的频率进行采样。
- en: The result is 27 million positive and negative examples of context and target
    pairs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是2700万个上下文和目标对的正负例。
- en: The model components
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型组件
- en: The *Skip-Gram* model contains a 200-dimensional embedding vector for each vocabulary
    item, resulting in 31,300 x 200 trainable parameters, plus two for the sigmoid
    output.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*Skip-Gram*模型包含每个词汇项的200维嵌入向量，导致31300 x 200可训练参数，加上两个用于sigmoid输出的参数。'
- en: In each iteration, the model computes the dot product of the context and the
    target-embedding vectors, passes the result through the sigmoid to produce a probability
    and adjusts the embedding based on the gradient of the loss.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，模型计算上下文和目标嵌入向量的点积，通过sigmoid产生概率，并根据损失的梯度调整嵌入。
- en: Visualizing embeddings using TensorBoard
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化嵌入
- en: TensorBoard is a visualization tool that permits the projection of the embedding
    vectors into three dimensions to explore the word and phrase locations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一种可视化工具，允许将嵌入向量投影到三维空间中，以探索单词和短语的位置。
- en: Word vectors from SEC filings using gensim
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gensim从SEC备案中提取的词向量
- en: In this section, we will learn word and phrase vectors from annual US **Securities
    and Exchange Commission** (**SEC**) filings using gensim to illustrate the potential
    value of word embeddings for algorithmic trading. In the following sections, we
    will combine these vectors as features with price returns to train neural networks
    to predict equity prices from the content of security filings.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用gensim从年度美国证券交易委员会（SEC）备案中学习词和短语向量，以说明词嵌入对算法交易的潜在价值。在接下来的部分中，我们将将这些向量与价格回报结合为特征，训练神经网络，以预测证券备案内容中的股票价格。
- en: In particular, we use a dataset containing over 22,000 10-K annual reports from
    the period 2013-2016 that are filed by listed companies and contain both financial
    information and management commentary (see [Chapter 3](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml),
    *Alternative Data for Finance*). For about half of the 11-K filings for companies,
    we have stock prices to label the data for predictive modeling (see references
    about data sources and the notebooks in the `sec-filings` folder for details).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们使用的数据集包含2013-2016年期间上市公司提交的超过22,000份10-K年度报告，其中包含财务信息和管理评论（参见[第3章](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml)，*金融的替代数据*）。对于大约一半的公司的11-K备案，我们有股价数据来标记预测建模的数据（有关数据来源和`sec-filings`文件夹中的笔记本的参考详细信息）。
- en: Preprocessing
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: 'Each filing is a separate text file and a master index contains filing metadata.
    We extract the most informative sections, namely, the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个备案都是一个单独的文本文件，主索引包含备案元数据。我们提取最具信息量的部分，即以下部分：
- en: '**Items 1 and 1A**: Business and Risk Factors'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目1和1A**：业务和风险因素'
- en: '**Items 7 and 7A**: Management''s Discussion and Disclosures about Market Risks'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目7和7A**：管理层讨论和关于市场风险的披露'
- en: The notebook preprocessing shows how to parse and tokenize the text using spaCy,
    similar to the approach taken in [Chapter 14](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml),
    *Topic Modeling*. We do not lemmatize the tokens to preserve the nuances of word
    usage.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理笔记本展示了如何使用spaCy解析和标记文本，类似于[第14章](beb6fa08-c790-47d5-82ef-f48a81dcf3d1.xhtml)，*主题建模*中采取的方法。我们不对标记进行词形还原，以保留词语使用的细微差别。
- en: Automatic phrase detection
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动短语检测
- en: 'We use `gensim` to detect phrases as previously introduced. The `Phrases` module
    scores the tokens and the `Phraser` class transforms the text data accordingly.
    The notebook shows how to repeat the process to create longer phrases:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`gensim`来检测先前介绍的短语。`Phrases`模块对标记进行评分，`Phraser`类相应地转换文本数据。笔记本展示了如何重复这个过程来创建更长的短语：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The most frequent bigrams include `common_stock`, `united_states`, `cash_flows`,
    `real_estate`, and `interest_rates`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的二元组包括`common_stock`，`united_states`，`cash_flows`，`real_estate`和`interest_rates`。
- en: Model training
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: The `gensim.models.Word2vec` class implements the SG and CBOW architectures
    introduced previously. The Word2vec notebook contains additional implementation
    detail.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.Word2vec`类实现了先前介绍的SG和CBOW架构。Word2vec笔记本包含了额外的实现细节。'
- en: 'To facilitate memory-efficient text ingestion, the `LineSentence` class creates
    a generator from individual sentences contained in the provided text file:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于高效地处理文本，`LineSentence`类从提供的文本文件中包含的单独句子创建一个生成器：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `Word2vec` class offers the configuration options previously introduced:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2vec`类提供了先前介绍的配置选项：'
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The notebook shows how to persist and reload models to continue training, or
    how to store the embedding vectors separately, for example, for use in ML models.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本展示了如何持久化和重新加载模型以继续训练，或者如何单独存储嵌入向量，例如用于ML模型。
- en: Model evaluation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Basic functionality includes identifying similar words:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基本功能包括识别相似的词：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also validate individual analogies using positive and negative contributions
    accordingly:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用正面和负面的贡献来验证单个类比：
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Performance impact of parameter settings
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数设置的性能影响
- en: 'We can use the analogies to evaluate the impact of different parameter settings.
    The following results stand out (see detailed results in the `models` folder):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类比来评估不同参数设置的影响。以下结果突出显示（在`models`文件夹中查看详细结果）：
- en: Negative sampling outperforms the hierarchical softmax, while also training
    faster
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负采样优于分层softmax，同时训练速度更快
- en: The Skip-Gram architecture outperforms CBOW given the objective function
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-Gram架构优于CBOW，给定目标函数
- en: Different `min_count` settings have a smaller impact, with the midpoint of 50
    performing best
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的`min_count`设置影响较小，50的中点效果最好
- en: 'Further experiments with the best performing SG model, using negative sampling
    and a `min_count` of 50, show the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用负采样和`min_count`为50的最佳性能SG模型进行进一步实验，结果如下：
- en: Smaller context windows than five lower the performance
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比五更小的上下文窗口会降低性能
- en: A higher negative sampling rate improves performance at the expense of slower
    training
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的负采样率提高了性能，但训练速度较慢
- en: Larger vectors improve performance, with a size of 600 yielding the best accuracy
    at 38.5%
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的向量提高了性能，大小为600时的准确率最高，为38.5%
- en: Sentiment analysis with Doc2vec
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Doc2vec进行情感分析
- en: Text classification requires combining multiple word embeddings. A common approach
    is to average the embedding vectors for each word in the document. This uses information
    from all embeddings and effectively uses vector addition to arrive at a different
    location point in the embedding space. However, relevant information about the
    order of words is lost.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类需要结合多个词向量。一种常见的方法是对文档中每个词的嵌入向量进行平均。这使用了所有嵌入的信息，并有效地使用向量加法来到达嵌入空间中的不同位置点。然而，有关单词顺序的相关信息会丢失。
- en: By contrast, the state-of-the-art generation of embeddings for pieces of text
    such as a paragraph or a product review is to use the document-embedding model
    `Doc2vec`. This model was developed by the Word2vec authors shortly after publishing
    their original contribution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，对于文本片段如段落或产品评论的最新嵌入式生成，使用文档嵌入模型`Doc2vec`。这个模型是在Word2vec作者发布原始贡献后不久开发的。
- en: 'Similar to Word2vec, there are also two flavors of `Doc2vec`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与Word2vec类似，`Doc2vec`也有两种风格：
- en: The **distributed bag of words** (**DBOW**) model corresponds to the Word2vec
    CBOW model. The document vectors result from training a network in the synthetic
    task of predicting a target word based on both the context word vectors and the
    document's doc vector.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式词袋**（**DBOW**）模型对应于Word2vec CBOW模型。文档向量来自于在预测目标词的合成任务中训练网络，基于上下文词向量和文档的文档向量。'
- en: The **distributed memory** (**DM**) model corresponds to the Word2vec Skip-Gram
    architecture. The doc vectors result from training a neural net to predict a target
    word using the full document's doc vector.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式记忆**（**DM**）模型对应于Word2vec Skip-Gram架构。文档向量来自于训练神经网络，以预测目标词使用完整文档的文档向量。'
- en: Gensim's `Doc2vec` class implements this algorithm.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim的`Doc2vec`类实现了这个算法。
- en: Training Doc2vec on yelp sentiment data
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Yelp情感数据上训练Doc2vec
- en: 'We use a random sample of 500,000 Yelp (see [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml), *Working
    with Text Data*) reviews with their associated star ratings (see notebook `yelp_sentiment`):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用50万个Yelp（参见[第13章](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml)，*处理文本数据*）评论的随机样本及其相关的星级评分（参见笔记本`yelp_sentiment`）：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We apply use simple pre-processing to remove stopwords and punctuation using
    `NLTK`''s tokenizer and drop reviews with fewer than 10 tokens:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用简单的预处理来删除停用词和标点，使用`NLTK`的分词器，并且删除少于10个标记的评论：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create input data
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建输入数据
- en: 'The `gensim.models.doc2vec` class processes documents in the `TaggedDocument`
    format that contains the tokenized documents alongside a unique tag that permits
    accessing the document vectors after training:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.doc2vec`类以包含标记化文档的`TaggedDocument`格式处理文档，该格式允许在训练后访问文档向量。'
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The training interface works similar to `word2vec` with additional parameters
    to specify the Doc2vec algorithm:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 训练接口与`word2vec`类似，还有额外的参数来指定Doc2vec算法：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can also use the `train()` method to continue the learning process and,
    for example, iteratively reduce the learning rate:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`train()`方法来继续学习过程，并且例如迭代减少学习率：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As a result, we can access the document vectors as features to train a sentiment
    classifier:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以访问文档向量作为特征来训练情感分类器：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will train a `lightgbm` gradient boosting machine as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个`lightgbm`梯度提升机，如下所示：
- en: 'Create `lightgbm` `Dataset` objects from the train and test sets:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练和测试集创建`lightgbm` `Dataset`对象：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the training parameters for a multiclass model with five classes (using
    defaults otherwise):'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为具有五个类别的多类模型定义训练参数（否则使用默认值）：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Train the model for 250 iterations and monitor the validation set error:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型进行250次迭代训练，并监视验证集错误：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Lightgbm predicts probabilities for all five classes. We obtain class predictions
    using `np.argmax()` to obtain the column index with the highest predicted probability:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lightgbm预测所有五个类别的概率。我们使用`np.argmax()`来获得具有最高预测概率的列索引来获得类别预测：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We compute the accuracy score to evaluate the result and see an improvement
    of more than 100% over the baseline of 20% for five balanced classes:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算准确度分数来评估结果，并看到与五个平衡类别的20%基线相比，有超过100%的改进：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we take a closer look at predictions for each class using the confusion
    matrix:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们仔细研究了使用混淆矩阵对每个类别的预测：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And visualize the result as a `seaborn` heatmap:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并将结果可视化为`seaborn`热图：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/c99b5339-66b3-4736-87ac-9fdc6ece26d3.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c99b5339-66b3-4736-87ac-9fdc6ece26d3.png)'
- en: In sum, the `doc2vec` method allowed us to achieve a very substantial improvement
    in test accuracy over a naive benchmark without much tuning. If we only select
    top and bottom reviews (with five and one stars, respectively) and train a binary
    classifier, the AUC score achieves over 0.86 using 250,000 samples from each class.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`doc2vec`方法使我们能够在没有太多调整的情况下，显著提高测试准确度。如果我们只选择顶部和底部评论（分别为五星和一星），并训练一个二元分类器，AUC分数在每个类别中使用25万个样本时可以达到0.86以上。
- en: Bonus – Word2vec for translation
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励 - 用于翻译的Word2vec
- en: The notebook translation demonstrates that the relationships encoded in one
    language often correspond to similar relationships in another language.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本翻译表明，一个语言中编码的关系通常对应于另一种语言中的类似关系。
- en: It illustrates how word vectors can be used to translate words and phrases by
    projecting word vectors from the embedding space of one language into the space
    of another language using a translation matrix.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 它说明了如何使用单词向量来通过将一个语言的嵌入空间中的单词向量投影到另一种语言的空间中使用翻译矩阵来翻译单词和短语。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter started with how word embeddings encode semantics for individual
    tokens more effectively than the bag-of-words model that we used in [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml),
    *Working with Text Data.* We also saw how to evaluated embedding by validating
    if semantic relationships among words are properly represented using linear vector
    arithmetic.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从单词嵌入如何更有效地编码单个标记的语义开始，比我们在[第13章](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml)中使用的词袋模型。我们还看到了如何通过验证单词之间的语义关系是否正确表示来评估嵌入。
- en: To learn word embeddings, we use shallow neural networks that used to be slow
    to train at the scale of web data containing billions of tokens. The `word2vec`
    model combines several algorithmic innovations to dramatically speed up training
    and has established a new standard for text feature generation. We saw how to
    use pretrained word vectors using `spaCy` and `gensim`, and learned to train our
    own word vector embeddings. We then applied a `word2vec` model to SEC filings.
    Finally, we covered the `doc2vec` extension that learns vector representations
    for documents in a similar fashion as word vectors and applied it to Yelp business
    reviews.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 学习词嵌入时，我们使用浅层神经网络，以前在包含数十亿标记的网络数据规模上训练速度很慢。`word2vec`模型结合了几种算法创新，大大加快了训练速度，并建立了文本特征生成的新标准。我们看到了如何使用`spaCy`和`gensim`来使用预训练的词向量，并学会了训练自己的词向量嵌入。然后，我们将`word2vec`模型应用于SEC提交的文件。最后，我们介绍了`doc2vec`扩展，它以与词向量类似的方式学习文档的向量表示，并将其应用于Yelp商业评论。
- en: Now, we will begin part 4 on deep learning (available online as mentioned in
    the Preface), starting with an introduction to feed-forward networks, popular
    deep learning frameworks and techniques for efficient training at scale.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始第4部分关于深度学习（如前言中提到的在线可用），首先介绍前馈网络、流行的深度学习框架和大规模高效训练的技术。

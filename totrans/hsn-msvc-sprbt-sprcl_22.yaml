- en: Centralized Logging with the EFK Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to collect and store log records from microservice
    instances, as well as how to search and analyze log records. As we mentioned in
    [Chapter 1](282e7b49-42b8-4649-af81-b4b6830d391d.xhtml), *Introduction to Microservices*
    (refer to the *Centralized log analysis* section), it is difficult to get an overview
    of what is going on in a system landscape of microservices when each microservice
    instance writes log records to its local filesystem. We need a component that
    can collect the log records from the microservice''s local filesystem and store
    them in a central database for analysis, search, and visualization. A popular
    open source-based solution for this builds on the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticsearch***,* a distributed database with great capabilities for the
    search and analysis of large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fluentd**, a data collector that can be used to collect log records from
    various sources, filter and transform the collected information, and finally send
    it to various consumers, for example, Elasticsearch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**, a graphical frontend to Elasticsearch that can be used to visualize
    search results and run analyses of the collected log records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these tools are called the **EFK stack**, named after the initials
    of each tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Fluentd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the EFK stack on Kubernetes for development and test usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trying out the EFK stack by:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the collected log records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering log records from the microservices and finding related log records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing root cause analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the commands that are described in this book have been run on a MacBook
    Pro using macOS Mojave but should be straightforward to modify so that they can
    be run on another platform, such as Linux or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: No new tools need to be installed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The source code for this chapter can be found in this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19).
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to run the commands that are described in this book, you need to
    download the source code to a folder and set up an environment variable, `$BOOK_HOME`,
    which points to that folder. Some sample commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: All of the source code examples in this chapter come from the source code in `$BOOK_HOME/Chapter19` and have
    been tested using Kubernetes 1.15.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take a look at the changes we applied to the source code in this
    chapter, that is, look at the changes we made so that we can use the EFK stack
    for centralized log analyses, you can compare it with the source code for [Chapter
    18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service Mesh to Improve
    Observability and Management*. You can use your favorite `diff` tool and compare
    the two folders, `$BOOK_HOME/Chapter18` and `$BOOK_HOME/Chapter19`.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Fluentd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we will learn the basics for how to configure Fluentd. Before
    we do that, let's learn a bit about the background of Fluentd and how it works
    on a high level.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Fluentd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historically, one of the most popular open source stacks for handling log records
    has been the ELK stack from Elastic ([https://www.elastic.co](https://www.elastic.co)),
    based on Elasticsearch, Logstash (used for log collection and transformation),
    and Kibana. Since Logstash runs on a Java VM, it requires a relatively large amount
    of memory. Over the years, a number of open source alternatives have been developed
    that require significantly less memory than Logstash, one of them being Fluentd ([https://www.fluentd.org](https://www.fluentd.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd is managed by the **Cloud Native Computing Foundation** (**CNCF**) ([https://www.cncf.io](https://www.cncf.io)),
    that is, the same organization that manages the Kubernetes project. Therefore,
    Fluentd has become a natural choice as an open source-based log collector that
    runs in Kubernetes. Together with Elastic and Kibana, it forms the EFK stack.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd is written in a mix of C and Ruby, using C for the performance-critical
    parts and Ruby where flexibility is of more importance, for example, allowing
    the simple installation of third-party plugins using Ruby's `gem install` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'A log record is processed as an event in Fluentd and consists of the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: A `time` field describing when the log record was created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `tag` field that identifies what type of log record it is—the tag is used
    by Fluentd's routing engine to determine how a log record shall be processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **record **that contains the actual log information, which is stored as a
    JSON object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Fluentd configuration file is used to tell Fluentd how to collect, process,
    and finally send log records to various targets, such as Elasticsearch. A configuration
    file consists of the following types of core elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<source>`: Source elements describe where Fluentd will collect log records.
    For example, tailing log files that have been written to by Docker containers.
    Source elements typically tag the log records, describing the type of log record.
    It could, for example, be used to tag log records to state that they come from
    containers running in Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<filter>`: Filter elements are used to process the log records, for example,
    a filter element can parse log records that come from Spring Boot-based microservices
    and extract interesting parts of the log message into separate fields in the log
    record. Extracting information into separate fields in the log record makes the
    information searchable by Elasticsearch. A filter element selects what log records
    to process based on their tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<match>`: Output elements are used to perform two main tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send processed log records to targets such as Elasticsearch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Routing is to decide how to process log records. A routing rule can rewrite
    the tag and reemit the log record into the Fluentd routing engine for further
    processing. A routing rule is expressed as an embedded `<rule>` element inside
    the `<match>` element. Output elements decide what log records to process, in
    the same way as a filter: based on the tag of the log records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluentd comes with a number of built-in and external third-party plugins that
    are used by the source, filter, and output elements. We will see some of them
    in action when we walk through the configuration file in the next section. For
    more information on the available plugins, see Fluentd's documentation, which
    is available at [https://docs.fluentd.org](https://docs.fluentd.org).
  prefs: []
  type: TYPE_NORMAL
- en: With this introduction to Fluentd out of the way, we are ready to see how Fluentd can
    be configured to process the log records from our microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Fluentd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The configuration of Fluentd is based on the configuration files from a Fluentd
    project on GitHub, `fluentd-kubernetes-daemonset`. The project contains Fluentd
    configuration files for how to collect log records from containers that run in
    Kubernetes and how to send them to Elasticsearch once they have been processed.
    We can reuse this configuration without changes and it will simplify our own configuration
    to a great extent. The Fluentd configuration files can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration files that provide this functionality are `kubernetes.conf`
    and `fluent.conf`. The `kubernetes.conf` configuration file contains the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: Source elements that tail container log files and log files from processes that
    run outside of Kubernetes, for example, the `kubelet` and the Docker daemon. The
    source elements also tag the log records from Kubernetes with the full name of
    the log file with `/` replaced by `.` and prefixed with `kubernetes`. Since the
    tag is based on the full filename, the name contains the name of the namespace,
    pod, and container, among other things. So, the tag is very useful for finding
    log records of interest by matching the tag. For example, the tag from the `product-composite` microservice
    could be something like `kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`,
    while the tag for the corresponding `istio-proxy` in the same pod could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A filter element that enriches the log records that come from containers running
    inside Kubernetes, along with Kubernetes-specific fields that contain information
    such as the names of the containers and the namespace they run in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main configuration file, `fluent.conf`, contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@include` statements for other configuration files, for example, the `kubernetes.conf` file
    we described previously. It also includes custom configuration files that are
    placed in a specific folder, making it very easy for us to reuse these configuration
    files without any changes and provide our own configuration file that only handles
    processing related to our own log records. We simply need to place our own configuration
    file in the folder specified by the `fluent.conf` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output element that sends log records to Elasticsearch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we described in the *Deploying Fluentd* section, these two configuration
    files will be packaged into the Docker image we will build for Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s left to cover in our own configuration file is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Detect and parse Spring Boot formatted log records from our microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling multiline stack traces. Stack traces, for example, are written to log
    files using multiple lines. This makes it hard for Fluentd to handle a stack trace
    as a single log record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating log records from the `istio-proxy` sidecars from the log records
    that were created by the microservices running in the same pod. The log records
    that are created by `istio-proxy` don't follow the same pattern as the log patterns
    that are created by our Spring Boot-based microservices. Therefore, they must
    be handled separately so that Fluentd doesn't try to parse them as Spring Boot
    formatted log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve this, the configuration is, to a large extent, based on using the
    `rewrite_tag_flter` plugin. This plugin can be used for routing log records based
    on the concept of changing the name of a tag and then reemitting the log record
    to the Fluentd routing engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This processing is summarized by the following UML activity diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4aa9c779-4a07-422d-af6c-00539062d30a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At a high level, the design of the configuration file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The tags of all of the log records from Istio, including `istio-proxy`, are
    prefixed with `istio` so that they can be separated from the Spring Boot-based
    log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tags of all of the log records from the `hands-on` namespace (except for
    the log records from `istio-proxy`) are prefixed with `spring-boot`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log records from Spring Boot are checked for the presence of multiline stack
    traces. If the log record is part of a multiline stack trace, it is processed
    by the third-party `detect-exceptions` plugin to recreate the stack trace. Otherwise,
    it is parsed using a regular expression to extract information of interest. See
    the *Deploying Fluentd* section for details on this third-party plugin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `fluentd-hands-on.conf` configuration file follows this activity diagram
    closely. The configuration file is placed inside a Kubernetes config map (see
    `kubernetes/efk/fluentd-hands-on-configmap.yml`). Let''s go through this step
    by step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First comes the definition of the config map and the filename of the configuration
    file, `fluentd-hands-on.conf`. It looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding source code, we understand that the `data` element will contain
    the configuration of Fluentd. It starts with the filename and uses a vertical
    bar `|` to mark the beginning of the embedded configuration file for Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `<match>` element matches the log records from Istio, that is, tags
    that are prefixed with `kubernetes` and contain either `istio` as part of its
    namespace or part of its container name. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The `<match>` element matches any tags that follow the `kubernetes.**istio**` pattern,
    that is, it starts with `kubernetes` and then contains the word `istio` somewhere
    in the tag name. `istio` can either come from the name of the namespace or the
    container; both are part of the tag.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<match>` element contains only one `<rule>` element, which prefixes the
    tag with `istio`. The `${tag}` variable holds the current value of the tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since this is the only `<rule>` element in the `<match>` element, it is configured
    to match all of the log records like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since all of the log records that come from Kubernetes have a `log` field, the
    `key` field is set to `log`, that is, the rule looks for a `log` field in the
    log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To match any string in the `log` field, the `pattern` field is set to the `^(.*)$` regular
    expression. `^` marks the beginning of a string, while `$` marks the end of a
    string. `(.*)` matches any number of characters, except for line breaks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log records are reemitted to the Fluentd routing engine. Since no other
    elements in the configuration file match tags starting with `istio`, they will
    be sent directly to the output element for Elasticsearch, which is defined in
    the `fluent.conf` file we described previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second `<match>` element matches all of the log records from the `hands-on`
    namespace, that is, the log records that are emitted by our microservices. It
    looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding source code we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The log records emitted by our microservices use formatting rules for the log
    message defined by Spring Boot so their tags are prefixed with `spring-boot`.
    Then, they are re-emitted for further processing.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<match>` element is configured in the same way as the `<match kubernetes.**istio**>`
    element we looked at previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third `<match>` element matches `spring-boot` log records and determines
    whether they are ordinary Spring Boot log records or are part of a multiline stack
    trace. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the preceding source code, this is determined by using two `<rule>` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The first uses a regular expression to check whether the `log` field in the
    log element starts with a timestamp or not.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: If the `log` field starts with a timestamp, the log record is treated as an
    ordinary Spring Boot log record and its tag is prefixed with `parse`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the second `<rule>` element will match and the log record is handled
    as a multiline log record. Its tag is prefixed with `check.exception`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log record is re-emitted in either case and its tag will either start with
    `check.exception.spring-boot.kubernetes` or `parse.spring-boot.kubernetes` after
    this process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the fourth `<match>` element, the selected log records have a tag that starts
    with `check.exception.spring-boot`, that is, log records that are part of a multiline
    stack trace. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The source code for the `detect_exceptions` plugin works like:'
  prefs: []
  type: TYPE_NORMAL
- en: The `detect_exceptions` plugin is used to combine multiple one-line log records
    into a single log record that contains a complete stack trace.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Before a multiline log record is reemitted into the routing engine, the `check` prefix
    is removed from the tag to prevent a never-ending processing loop of the log record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the configuration file consists of a filter element that parses Spring
    Boot log messages using a regular expression, extracting information of interest.
    It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that filter elements don't re-emit log records; instead, they just pass
    them on to the next element in the configuration file that matches the log record's
    tag.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following fields are extracted from the Spring Boot log message that''s
    stored in the `log` field in the log record:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<time>`: The timestamp for when the log record was created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.level>`: The log level of the log record, for example, `FATAL`, `ERROR`,
    `WARN`, `INFO`, `DEBUG`, or `TRACE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.service>`: The name of the microservice'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.trace>`: The trace ID used to perform distributed tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.span>`: The span ID, the ID of the part of the distributed processing
    that this microservice executed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.pid>`: The process ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.thread>`: The thread ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.class>`: The name of the Java class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<log>`: The actual log message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The names of Spring Boot-based microservices are specified using the `spring.application.name` property.
    This property has been added to each microservice-specific property file in the
    config repository, in the `config-repo` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Getting regular expressions right can be challenging, to say the least. Thankfully,
    there are several websites that can help. When it comes to using regular expressions
    together with Fluentd, I recommend using the following site: [https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've been introduced to how Fluentd works and how the configuration
    file is constructed, we are ready to deploy the EKF stack.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the EFK stack on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deploying the EFK stack on Kubernetes will be done in the same way as we have
    deployed our own microservices: using Kubernetes definition files for objects
    such as deployments, services, and configuration maps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment of the EFK stack is divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: One part where we deploy Elasticsearch and Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One part where we deploy Fluentd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But first, we need to build and deploy our own microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying our microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building, deploying, and verifying the deployment using the `test-em-all.bash` test
    script is done in the same way as it was done in [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, in the *Running commands
    to create the service mesh* section. Run the following commands to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, build the Docker images from the source with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Recreate the namespace, `hands-on`, and set it as the default namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the deployment by running the `deploy-dev-env.bash` script with the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Minikube tunnel, if it''s not already running (see [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, the *Setting up access
    to Istio services* section for a recap, if required):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Remember that this command requires that your user has `sudo` privileges and
    that you enter your password during startup and shutdown. It takes a couple of
    seconds before the command asks for the password, so it is easy to miss!
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the normal tests to verify the deployment with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect the output to be similar to what we have seen from the previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/042de643-01de-4776-8b29-ab05be9fe519.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also try out the APIs manually by running the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Expect the requested product ID, `2`, in the response.
  prefs: []
  type: TYPE_NORMAL
- en: With the microservices deployed, we can move on and deploy Elasticsearch and
    Kibana!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Elasticsearch and Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will deploy Elasticsearch and Kibana to its own namespace, `logging`. Both
    Elasticsearch and Kibana will be deployed for development and test usage using
    a Kubernetes deployment object. This will be done with a single pod and a Kubernetes
    node port service. The services will expose the standard ports for Elasticsearch
    and Kibana internally in the Kubernetes cluster, that is, port `9200` for Elasticserach
    and port `5601` for Kibana. Thanks to the `minikube tunnel` command, we will be
    able to access these services locally using the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`elasticsearch.logging.svc.cluster.local:9200` for Elasticserch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kibana.logging.svc.cluster.local:5601` for Kibana'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the recommended deployment in a production environment on Kubernetes, see [https://www.elastic.co/elasticsearch-kubernetes](https://www.elastic.co/elasticsearch-kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the versions that were available when this chapter was written:'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch version 7.3.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana version 7.3.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we perform the deployments, let's look at the most interesting parts
    of the definition files.
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the definition files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The definition file for Elasticsearch, `kubernetes/efk/elasticsearch.yml`,
    contains a standard Kubernetes deployment and service object that we have seen
    multiples times before, for example, in [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml),
    *Introduction to Kubernetes*, in the *Trying out a sample deployment* section.
    The most interesting part, as we explained previously, of the definition file
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: We use an official Docker image from Elastic that's available at `docker.elastic.co`
    with a package that only contains open source components. This is ensured by using
    the `-oss` suffix on the name of the Docker image, `elasticsearch-oss`. The version
    is set to `7.3.0`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Elasticsearch container is allowed to allocate a relatively large amount
    of memory—2 GB—to be able to perform queries with good performance. The more memory,
    the better the performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The definition file for Kibana, `kubernetes/efk/kibana.yml`, also contains
    a standard Kubernetes deployment and service object. The most interesting parts
    in the definition file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: For Kibana, we also use an official Docker image from Elastic that's available
    at `docker.elastic.co`, along with a package that only contains open source components, `kibana-oss`. The
    version is set to `7.3.0`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To connect Kibana with the Elasticsearch pod, an environment variable, `ELASTICSEARCH_URL`,
    is defined to specify the address to the Elasticsearch service, `http://elasticsearch:9200`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these insights, we are ready to perform the deployment of Elasticsearch
    and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Running the deploy commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deploy Elasticsearch and Kibana by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace for Elasticsearch and Kibana with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the deploy steps run faster, prefetch the Docker images for Elasticsearch
    and Kibana with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy Elasticsearch and wait for its pod to be ready with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that Elasticsearch is up and running with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Expect `You Know, for Search` as a response.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your hardware, you might need to wait for a minute or two before
    Elasticsearch responds with this message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy Kibana and wait for its pod to be ready with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that Kibana is up and running with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Expect `200` as the response.
  prefs: []
  type: TYPE_NORMAL
- en: With Elasticsearch and Kibana deployed, we can start to deploy Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Fluentd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Fluentd is a bit more complex compared to deploying Elasticsearch
    and Kibana. To deploy Fluentd, we will use a Docker image that's been published
    by the Fluentd project on Docker Hub, `fluent/fluentd-kubernetes-daemonset`, and
    sample the Kubernetes definition files from a Fluentd project on GitHub, `fluentd-kubernetes-daemonset`.
    It is located at [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
    As it's implied by the name of the project, Fluentd will be deployed as a daemon
    set, running one pod per node in the Kubernetes cluster. Each Fluentd pod is responsible
    for collecting log output from processes and containers that run on the same node
    as the pod. Since we are using Minikube, that is, a single node cluster, we will
    only have one Fluentd pod.
  prefs: []
  type: TYPE_NORMAL
- en: To handle multiline log records that contain stack traces from exceptions, we
    will use a third-party Fluentd plugin provided by Google, `fluent-plugin-detect-exceptions`,
    which is available at [https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions). To
    be able to use this plugin, we will build our own Docker image where the `fluent-plugin-detect-exceptions` plugin
    will be installed. Fluentd's Docker image, `fluentd-kubernetes-daemonset`, will
    be used as the base image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the versions that were available when this chapter was written:'
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd version 1.4.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fluent-plugin-detect-exceptions version 0.0.12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we perform the deployments, let's look at the most interesting parts
    of the definition files.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the log records from microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to utilize one of the main features of centralized
    logging, that is, finding log records from our microservices. We will also learn
    how to use the trace ID in the log records to find log records from other microservices
    that belong to one and the same process, for example, a request to the API.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by creating some log records that we can look up with the help of
    Kibana. We will use the API to create a product with a unique product ID and then
    retrieve information about the product. After that, we can try to find the log
    records that were created when retrieving the product information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creation of log records in the microservices has updated a bit from the
    previous chapter so that the product composite and the three core microservices,
    `product`, `recommendation`, and `review`, all write a log record with the log
    level set to `INFO` when they begin processing a get request. Let''s go over the
    source code that''s been added to each microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Product composite microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Product microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Recommendation microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Review microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: For more details, see the source code in the `microservices` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to use the API to create log records and then use
    Kibana to look up the log records:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get an access token with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the introduction to this section we will start by creating
    a product with a unique product ID. Create a minimalistic product (without recommendations
    and reviews) for `"productId" :1234` by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the product with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/258fd470-f7ae-432b-933e-0098656e7199.png)'
  prefs: []
  type: TYPE_IMG
- en: Hopefully, we got some log records created by these API calls. Let's jump over
    to Kibana and find out!
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Kibana web page, click on the `Discover` menu on the left. You will
    see something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75f6dd32-2743-454f-a301-007444bb2cfd.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left-top corner, we can see that Kibaba has found 326,642 log records.
    The time picker shows that they are from the last 7 days. In the histogram, we
    can see how the log records are spread out over time. Following that is a table showing
    the most recent log events that were found by the query.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to change the time interval, you can use the time picker. Click
    on its calendar icon to adjust the time interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get a better view of the content in the log records, add some fields from
    the log records to the table under the histogram. Select the fields from the list
    of available fields to the left. Scroll down until the field is found. Hold the
    cursor over the field and an add button will appear; click on it to add the field
    as a column in the table. Select the following fields, in order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: spring.level, the log level
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: kubernetes.container_name, the name of the container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: spring.trace, the trace ID used for distributed tracing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'log, the actual log message. The web page should look something similar to
    the following:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png)'
  prefs: []
  type: TYPE_IMG
- en: The table now contains information that is of interest regarding the log records!
  prefs: []
  type: TYPE_NORMAL
- en: 'To find log records from the call to the `GET` API, we can ask Kibana to find
    log records where the log field contains the text product.id=1234\. This matches
    the log output from the product composite microservice that was shown previously.This
    can be done by entering `log:"product.id=1234"` in the Search field and clicking
    on the Update button (this button can also be labeled Refresh). Expect one log
    record to be found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c5370730-05a8-4dc0-a414-3020057d3bb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Verify that the timestamp is from when you called the `GET` API and verify that
    the name of the container that created the log record is comp, that is, verify
    that the log record was sent by the product composite microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we want to see the related log records from the other microservices that
    participated in the process of returning information about the product with productId
    1234, that is, finding log records with the same trace ID as that of the log record
    we found. To do that, place the cursor over the `spring.trace` field for the log
    record. Two small magnifying glasses will be shown to the right of the field,
    one with a `+` sign and one with a `-` sign. Click on the magnifying glass with
    the `+` sign to filter on the trace ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clean the Search field so that the only search criteria is the filter of the
    trace field. Then, click on the Update button to see the result. Expect a response
    similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/909125b4-7868-4ed9-b33c-3a5197408304.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see a lot of detailed debug and trace messages that clutter the view;
    let's get rid of them!
  prefs: []
  type: TYPE_NORMAL
- en: Place the cursor over a TRACE value and click on the magnifying glass with the
    - sign to filter out log records with the log level set to TRACE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding step for the DEBUG log record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should now be able to see the four expected log records, one for each microservice
    involved in the lookup of product information for the product with product ID
    1234:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, note the filters that were applied included the trace ID but excluded
    log records with the log level set to DEBUG or TRACE.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to find the expected log records, we are ready to take
    the next step. This will be to learn how to find unexpected log records, that
    is, error messages, and how to perform root cause analysis, that is, find the
    reason for these error messages.
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the definition files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Dockerfile that''s used to build the Docker image, `kubernetes/efk/Dockerfile`,
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The base image is Fluentd's Docker image, `fluentd-kubernetes-daemonset`. The `v1.4.2-debian-elasticsearch-1.1` tag
    specifies that version v1.4.2 shall be used with a package that contains built-in
    support for sending log records to Elasticsearch. The base Docker image contains
    the Fluentd configuration files that were mentioned in the *Configuring Fluentd* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Google plugin, `fluent-plugin-detect-exceptions`, is installed using Ruby's
    package manager, `gem`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The definition file of the daemon set, `kubernetes/efk/fluentd-ds.yml`, is
    based on a sample definition file in the `fluentd-kubernetes-daemonset` project,
    which can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml).
    This file is a bit complex, so let''s go through the most interesting parts separately:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, here''s the declaration of the daemon set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The `kind` key specifies that this is a daemon set.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: The `namespace` key specifies that the daemon set shall be created in the `kube-system`
    namespace and not in the `logging` namespace where Elasticseach and Kibana are
    deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next part specifies the template for the pods that are created by the daemon
    set. The most interesting parts are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker image that's used for the pods is `hands-on/fluentd:v1`. We will
    build this Docker image after walking through the definition files using the Dockerfile
    we described previously.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A number of environment variables are supported by the Docker image and are
    used to customize it. The two most important ones are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLUENT_ELASTICSEARCH_HOST`, which specifies the hostname of the Elasticsearch
    service, that is, `elasticsearch.logging`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLUENT_ELASTICSEARCH_PORT`, which specifies the port that''s used to communicate
    with Elasticsearch, that is, `9200`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the Fluentd pod runs in another namespace than Elasticsearch, the hostname
    cannot be specified using its short name, that is, `elasticsearch`. Instead, the
    namespace part of the DNS name must also be specified, that is, `elasticsearch.logging`. As
    an alternative, the **fully qualified domain name** (**FQDN**), `elasticsearch.logging.svc.cluster.local`,
    can also be used. But since the last part of the DNS name, `svc.cluster.local`,
    is shared by all DNS names inside a Kubernetes cluster, it does not need to be
    specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a number of volumes, that is, filesystems, are mapped into the pod,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Three folders on the host (that is, the node) are mapped into the Fluentd pod.
    These folders contain the log files that Fluentd will tail and collect log records
    from. The folders are: `/var/log`, `/var/lib/docker/containers` and `/run/log/journal`.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Our own configuration file that specifies how Fluentd shall process log records
    from our microservices is mapped using a config map called `fluentd-hands-on-config`
    to the `/fluentd/etc/conf.d` folder. The base Docker image that's used for preceding
    Fluentd, `fluentd-kubernetes-daemonset`, configures Fluentd to include any configuration
    file that's found in the `/fluentd/etc/conf.d` folder. See the *Configuring Fluentd* section
    for details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code of the definition file for the daemon set, see the
    `kubernetes/efk/fluentd-ds.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've walked through everything, we are ready to perform the deployment
    of Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: Running the deploy commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To deploy Fluentd, we have to build the Docker image, create the config map,
    and finally deploy the daemon set. Run the following commands to perform these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the Docker image and tag it with `hands-on/fluentd:v1` using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the config map, deploy Fluentd''s daemon set, and wait for the pod to
    be ready with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the Fluentd pod is healthy with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect a response of `2019-08-16 15:11:33 +0000 [info]: #0 fluentd worker is
    now running worker=0`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fluentd will start to collect a considerable amount of log records from the
    various processes and containers in the Minkube instance. After a minute or so,
    you can ask Elasticsearch how many log records have been collected with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The command can be a bit slow the first time it is executed, but should return
    a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14a27b89-c4e5-4d29-86c2-4652b7fbe575.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, Elasticsearch contains `144750` log records.
  prefs: []
  type: TYPE_NORMAL
- en: This completes the deployment of the EFK stack. Now, it's time to try it out
    and find out what all of the collected log records are about!
  prefs: []
  type: TYPE_NORMAL
- en: Trying out the EFK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need to do before we can try out the EFK stack is initialize
    Kibana so it knows what search indices to use in Elasticsearch. Once that is done,
    we will try out the following, in my experience, common tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: We will start by analyzing of what types of log records Fluentd has collected
    and stored in Elasticsearch. Kibana has a very useful visualization capability
    that can be used for this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will learn how to discover log records from different microservices
    that belong to one and the same processing of an external request to the API. We
    will use the **trace ID** in the log records as a correlation ID to find related
    log records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thirdly, we will learn how to use Kibana to perform **root cause analysis**, that
    is, find the actual reason for an error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start to use Kibana, we must specify what search indices to use in
    Elasticsearch and what field in the indices holds the timestamps for the log records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to initialize Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Kibana's web UI using the `http://kibana.logging.svc.cluster.local:5601` URL
    in a web browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the welcome page, Welcome to Kibana, click on the Explore on my own button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Expand button in the lower-left corner to view the names of the
    menu choices. These will be shown on the left-hand side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Discover in the menu to the left. You will be asked to define a pattern
    that's used by Kibana to identify what Elasticsearch indices it shall retrieve
    log records from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the `logstash-*` index pattern and click on Next Step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, you will be asked to specify the name of the field that contains
    the timestamp for the log records. Click on the drop-down list for the Time Filter
    field name and select the only available field, @timestamp.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Create index pattern button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kibana will show a page that summarizes the fields that are available in the
    selected indices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indices are, by default, named `logstash` for historical reasons, even though
    it is Flutentd that is used for log collection.
  prefs: []
  type: TYPE_NORMAL
- en: With Kibana initialized, we are ready to examine the log records we have collected.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the log records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the deployment of Fluentd, we know that it immediately started to collect
    a significant number of log records. So, the first thing we need to do is get
    an understanding of what types of log records Fluentd has collected and stored
    in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Kibana''s visualization feature to divide the log records per Kubernetes
    namespace and then ask Kibana to show us how the log records are divided per type
    of container within each namespace. A pie chart is a suitable chart type for this
    type of analysis. Perform the following steps to create a pie chart:'
  prefs: []
  type: TYPE_NORMAL
- en: In Kibana's web UI, click on Visualize in the menu to the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Create new visualization button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Pie as the visualization type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select logstash-* as the source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the time picker (a date interval selector) above the pie chart, set a date
    interval of your choice (set to the last 7 days in the following screenshot).
    Click on its calendar icon to adjust the time interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on Add to create the first bucket, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the bucket type, that is, Split slices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the aggregation type, select Terms from the drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the field, select kubernetes.namespace_name.keyword.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the size, select 10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Group other values in separate bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Show missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press the Apply changes button (the blue play icon above the Bucket definition).
    Expect a pie chart that looks similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1d79e158-3af1-495b-8415-e2b769b59454.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the log records are divided over the namespaces we have been
    working with in the previous chapters: `kube-system`, `istio-system`, `logging`, `cert-manager`,
    and our own `hands-on` namespace. To see what containers have created the log
    records divided per namespace, we need to create a second bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Add again to create a second bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the bucket type, that is, Split slices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the sub-aggregation type, select Terms from the drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the field, select kubernetes.container_name.keyword.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the size, select 10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Group other values in separate bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Show missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press the Apply changes button again. Expect a pie chart that looks similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/660ceb82-205e-401d-8595-5a23cc328e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can find the log records from our microservices. Most of the log records
    come from the `product-composite` microservice.
  prefs: []
  type: TYPE_NORMAL
- en: At the top of the pie chart, we have a group of log records labeled `missing`,
    that is, they neither have a Kubernetes namespace nor a container name specified.
    What's behind these missing log records? These log records come from processes
    running outside of the Kubernetes cluster in the Minikube instance and they are
    stored using Syslog. They can be analyzed using Syslog-specific fields, specifically
    the *identifier field.* Let's create a third bucket that divides log records based
    on their Syslog identifier field, if any.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on `Add` again to create a third bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the bucket type, that is, Split slices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the sub-aggregation type, select Terms from the drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the field, select SYSLOG_IDENTIFIER.keyword.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Group other values in separate bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Show missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press the Apply changes button and expect a pie chart that looks similar to
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/56a1d5be-61bf-49e6-87c2-a18290f65e38.png)'
  prefs: []
  type: TYPE_IMG
- en: The `missing` log records turn out to come from the `kubelet` process, which
    manages the node from a Kubernetes perspective, and `dockerd`, the Docker daemon
    that manages all of the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have found out where the log records come from, we can start to
    locate the actual log records from our microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Performing root cause analyses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important features of centralized logging is that it makes it
    possible to analyze errors using log records from many sources and, based on that,
    perform root cause analysis, that is, find the actual reason for the error message.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will simulate an error and see how we can find information
    about it, all of the way down to the line of source code that caused the error
    in one of the microservices in the system landscape. To simulate an error, we
    will reuse the fault parameter we introduced in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml),
    *Improving Resilience Using Resilience4j*, in the *Adding programmable delays
    and random errors* section. We can use this to force the product microservice
    to throw an exception. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to generate a fault in the product microservice while
    searching for product information on the product with product ID `666`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect the following error in response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d0a558f-7fc3-4f83-bb92-320d419859ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have to pretend that we have no clue about the reason for this error!
    Otherwise, the root cause analysis wouldn't be very exciting, right? Let's assume
    that we work in a support organization and have been asked to investigate some
    problems that just occurred while an end user tried to look up information regarding
    a product with product ID `666`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start to analyze the problem, let''s delete the previous search filters
    in the Kibana web UI so that we can start from scratch. For each filter we defined
    in the previous section, click on their close icon (an x) to remove them. After
    all of the filters have been removed, the web page should look similar to the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/82e6c766-b5d4-4bf8-900d-081dfff40319.png)'
  prefs: []
  type: TYPE_IMG
- en: Start by selecting a time interval that includes the point in time when the
    problem occurred using the time picker. For example, search the last seven days
    if you know that the problem occurred within the last seven days.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, search for log records with the log level set to ERROR within this timeframe.
    This can be done by clicking on the spring.level field in the list of selected
    fields. When you click on this field, its most commonly used values will be displayed
    under it. Filter on the ERROR value by clicking on its magnifier, shown with the
    + sign. Kibana will now show log records within the selected time frame with its
    log level set to ERROR, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/70b40e75-4168-4207-90fb-e528fad127f7.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see a number of error messages related to product ID `666`. The top four
    have the same trace ID, so this seems like a trace ID of interest to use for further
    investigation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also see more error messages below the top four that seem to be related
    to the same error but with different trace IDs. Those are caused by the retry
    mechanism in the product composite microservice, that is, it retries the request
    a couple of times before giving up and returning an error message to the caller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter on the trace ID of the first log record in the same way we did in the
    previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the filter of the ERROR log level to be able to see all of the records
    belonging to this trace ID. Expect Kibana to respond with a lot of log records.
    Look to the oldest log record, that is, the one that occurred first, that looks
    suspicious. For example, it may have a WARN or ERROR log level or a strange log
    message. The default sort order is showing the latest log record at the top, so
    scroll down to the end and search backward (you can also change the sort order
    to show the oldest log record first by clicking on the small up/down arrow next
    to the `Time` column header). The WARN log message that says `Bad luck, and error
    occurred` looks like it could be the root cause of the problem. Let''s investigate
    it further:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/47ac8d1a-707a-410f-869f-d0923b4392b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Once a log record has been found that might be the root cause of the problem,
    it is of great interest to be able to find the nearby stack trace describing where
    exceptions were thrown in the source code**. **Unfortunately, the Fluentd plugin
    we use for collecting multiline exceptions, `fluent-plugin-detect-exceptions`,
    is unable to relate stack traces to the trace ID that was used. Therefore, stack
    traces will not show up in Kibana when we filter on a trace ID. Instead, we can
    use a feature in Kibana for finding surrounding log records that show log records
    that have occurred in near time to a specific log record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expand the log record that says bad luck using the arrow to the left of the
    log record. Detailed information about this specific log record will be revealed.
    There is also a link named View surrounding documents; click on it to see nearby
    log records. Expect a web page similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d533fe87-485e-4fe5-95c6-8de6c2ae04c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The log record above the bad luck log record with the stack trace for the error
    message Something went wrong... looks interesting and was logged by the product
    microservice just two milliseconds after it logged the *bad luck* log record.
    They seem to be related! The stack trace in that log record points to line 96
    in `ProductServiceImpl.java`. Looking in the source code (see `microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java`),
    line 96 looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is the root cause of the error. We did know this in advance, but now we
    have seen how we can navigate to it as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the problem is quite simple to resolve: simply omit the `faultPercent`
    parameter in the request to the API. In other cases, the resolution of the root
    cause can be much harder to figure out!'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes this chapter on using the EFK stack for centralized logging.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the importance of collecting log records from
    microservices in a system landscape into a common centralized database where analysis
    and searches among the stored log records can be performed. We used the EFK stack,
    that is, Elasticsearch, Fluentd, and Kibana, to collect, process, store, analyze,
    and search for log records.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd was used to collect log records not only from our microservices but
    also from the various supporting containers and processes in the Kubernetes cluster.
    Elasticsearch was used as a text search engine. Together with Kibana, we saw how
    easy it is to get an understanding of what types of log records we have collected.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to use Kibana to perform important tasks such as finding
    related log records from cooperating microservices and how to perform root cause
    analysis, that is, finding the real problem for an error message. Finally, we
    learned how to update the configuration of Fluentd and how to get the change reflected
    by the executing Fluentd pod.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to collect and analyze log records in this way is an important capability
    in a production environment, but these types of activities are always done afterward,
    once the log record has been collected. Another important capability is to be
    able to monitor the current health of the microservices, that is, collect and
    visualize runtime metrics in terms of the use of hardware resources, response
    times, and so on. We touched on this subject in the previous chapter, [Chapter
    18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service Mesh to Improve
    Observability and Management*, and in the next chapter, [Chapter 20](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml),
    *Monitoring Microservices*, we will learn more about monitoring microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A user searched for ERROR log messages in the `hands-on` namespace for the last
    30 days using the search criteria shown in the following screenshot, but none
    were found. Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6d818708-6286-4a4c-b9f5-1ad9d67581cd.png)'
  prefs: []
  type: TYPE_IMG
- en: A user has found a log record of interest. How can the user find related log
    records from this and other microservices, for example, that come from processing
    an external API request?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/292c8785-1180-40d6-8ad2-2b7e564cc8ef.png)'
  prefs: []
  type: TYPE_IMG
- en: A user has found a log record that seems to indicate the root cause of a problem
    that was reported by an end user. How can the user find the stack trace that shows
    wherein the source code the error occurred?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6e935782-d0e3-46aa-8d97-141af6289a9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Why doesn't the following Fluentd configuration element work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How can you determine whether Elasticsearch is up and running?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suddenly, you lose connection to Kibana from your web browser. What caused this
    problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

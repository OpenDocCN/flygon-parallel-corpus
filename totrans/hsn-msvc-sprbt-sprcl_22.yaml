- en: Centralized Logging with the EFK Stack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EFK堆栈进行集中日志记录
- en: 'In this chapter, we will learn how to collect and store log records from microservice
    instances, as well as how to search and analyze log records. As we mentioned in
    [Chapter 1](282e7b49-42b8-4649-af81-b4b6830d391d.xhtml), *Introduction to Microservices*
    (refer to the *Centralized log analysis* section), it is difficult to get an overview
    of what is going on in a system landscape of microservices when each microservice
    instance writes log records to its local filesystem. We need a component that
    can collect the log records from the microservice''s local filesystem and store
    them in a central database for analysis, search, and visualization. A popular
    open source-based solution for this builds on the following tools:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何收集和存储来自微服务实例的日志记录，以及如何搜索和分析日志记录。正如我们在[第1章](282e7b49-42b8-4649-af81-b4b6830d391d.xhtml)中提到的那样，*微服务简介*（参考*集中式日志分析*部分），当每个微服务实例将日志记录写入其本地文件系统时，很难获得对微服务系统景观的概述。我们需要一个组件，可以从微服务的本地文件系统收集日志记录，并将其存储在中央数据库中，以进行分析、搜索和可视化。基于以下工具的流行的基于开源的解决方案：
- en: '**Elasticsearch***,* a distributed database with great capabilities for the
    search and analysis of large datasets'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**，一个具有强大能力的分布式数据库，用于搜索和分析大型数据集'
- en: '**Fluentd**, a data collector that can be used to collect log records from
    various sources, filter and transform the collected information, and finally send
    it to various consumers, for example, Elasticsearch'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fluentd**，一个数据收集器，可用于从各种来源收集日志记录，过滤和转换收集的信息，最终将其发送给各种消费者，例如Elasticsearch'
- en: '**Kibana**, a graphical frontend to Elasticsearch that can be used to visualize
    search results and run analyses of the collected log records'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kibana**，Elasticsearch的图形前端，可用于可视化搜索结果并对收集的日志记录进行分析'
- en: Together, these tools are called the **EFK stack**, named after the initials
    of each tool.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具一起被称为**EFK堆栈**，以每个工具的首字母命名。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Configuring Fluentd
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Fluentd
- en: Deploying the EFK stack on Kubernetes for development and test usage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈以进行开发和测试使用
- en: 'Trying out the EFK stack by:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以下方式尝试EFK堆栈：
- en: Analyzing the collected log records
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析收集的日志记录
- en: Discovering log records from the microservices and finding related log records
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现来自微服务的日志记录并找到相关的日志记录
- en: Performing root cause analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行根本原因分析
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All of the commands that are described in this book have been run on a MacBook
    Pro using macOS Mojave but should be straightforward to modify so that they can
    be run on another platform, such as Linux or Windows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中描述的所有命令都在使用macOS Mojave的MacBook Pro上运行，但应该很容易修改，以便在其他平台上运行，例如Linux或Windows。
- en: No new tools need to be installed in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不需要安装新工具。
- en: The source code for this chapter can be found in this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在本书的GitHub存储库中找到：[https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19)。
- en: 'To be able to run the commands that are described in this book, you need to
    download the source code to a folder and set up an environment variable, `$BOOK_HOME`,
    which points to that folder. Some sample commands are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够运行本书中描述的命令，您需要将源代码下载到一个文件夹中，并设置一个环境变量`$BOOK_HOME`，该变量指向该文件夹。一些示例命令如下：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All of the source code examples in this chapter come from the source code in `$BOOK_HOME/Chapter19` and have
    been tested using Kubernetes 1.15.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有源代码示例都来自`$BOOK_HOME/Chapter19`中的源代码，并已经使用Kubernetes 1.15进行了测试。
- en: If you want to take a look at the changes we applied to the source code in this
    chapter, that is, look at the changes we made so that we can use the EFK stack
    for centralized log analyses, you can compare it with the source code for [Chapter
    18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service Mesh to Improve
    Observability and Management*. You can use your favorite `diff` tool and compare
    the two folders, `$BOOK_HOME/Chapter18` and `$BOOK_HOME/Chapter19`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想查看本章中应用于源代码的更改，即查看我们所做的更改，以便我们可以使用EFK堆栈进行集中日志分析，您可以将其与[第18章](422649a4-94bc-48ae-b92b-e3894c014962.xhtml)的源代码进行比较，*使用服务网格改进可观察性和管理*。您可以使用您喜欢的`diff`工具并比较两个文件夹`$BOOK_HOME/Chapter18`和`$BOOK_HOME/Chapter19`。
- en: Configuring Fluentd
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Fluentd
- en: In this section we will learn the basics for how to configure Fluentd. Before
    we do that, let's learn a bit about the background of Fluentd and how it works
    on a high level.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何配置Fluentd的基础知识。在此之前，让我们简单了解一下Fluentd的背景以及其在高层次上的工作原理。
- en: Introducing Fluentd
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Fluentd
- en: Historically, one of the most popular open source stacks for handling log records
    has been the ELK stack from Elastic ([https://www.elastic.co](https://www.elastic.co)),
    based on Elasticsearch, Logstash (used for log collection and transformation),
    and Kibana. Since Logstash runs on a Java VM, it requires a relatively large amount
    of memory. Over the years, a number of open source alternatives have been developed
    that require significantly less memory than Logstash, one of them being Fluentd ([https://www.fluentd.org](https://www.fluentd.org)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，处理日志记录的最受欢迎的开源堆栈之一是Elastic的ELK堆栈（[https://www.elastic.co](https://www.elastic.co)），基于Elasticsearch、Logstash（用于日志收集和转换）和Kibana。由于Logstash在Java虚拟机上运行，因此需要相对较大的内存。多年来，已经开发了许多开源替代方案，这些替代方案需要的内存比Logstash少得多，其中之一就是Fluentd（[https://www.fluentd.org](https://www.fluentd.org)）。
- en: Fluentd is managed by the **Cloud Native Computing Foundation** (**CNCF**) ([https://www.cncf.io](https://www.cncf.io)),
    that is, the same organization that manages the Kubernetes project. Therefore,
    Fluentd has become a natural choice as an open source-based log collector that
    runs in Kubernetes. Together with Elastic and Kibana, it forms the EFK stack.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd由**Cloud Native Computing Foundation**（**CNCF**）（[https://www.cncf.io](https://www.cncf.io)）管理，即与管理Kubernetes项目的同一组织。因此，Fluentd已成为在Kubernetes中运行的基于开源的日志收集器的自然选择。它与Elastic和Kibana一起形成EFK堆栈。
- en: Fluentd is written in a mix of C and Ruby, using C for the performance-critical
    parts and Ruby where flexibility is of more importance, for example, allowing
    the simple installation of third-party plugins using Ruby's `gem install` command.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd是用C和Ruby混合编写的，使用C处理性能关键部分，使用Ruby处理更为重要的灵活性，例如，使用Ruby的`gem install`命令简单安装第三方插件。
- en: 'A log record is processed as an event in Fluentd and consists of the following
    information:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录在Fluentd中被处理为事件，并包括以下信息：
- en: A `time` field describing when the log record was created
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述日志记录创建时间的`time`字段
- en: A `tag` field that identifies what type of log record it is—the tag is used
    by Fluentd's routing engine to determine how a log record shall be processed
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标识日志记录类型的`tag`字段——标签由Fluentd的路由引擎使用，以确定如何处理日志记录
- en: A **record **that contains the actual log information, which is stored as a
    JSON object
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含实际日志信息的**记录**，存储为JSON对象
- en: 'A Fluentd configuration file is used to tell Fluentd how to collect, process,
    and finally send log records to various targets, such as Elasticsearch. A configuration
    file consists of the following types of core elements:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd配置文件用于告诉Fluentd如何收集、处理和最终发送日志记录到各种目标，例如Elasticsearch。配置文件包括以下类型的核心元素：
- en: '`<source>`: Source elements describe where Fluentd will collect log records.
    For example, tailing log files that have been written to by Docker containers.
    Source elements typically tag the log records, describing the type of log record.
    It could, for example, be used to tag log records to state that they come from
    containers running in Kubernetes.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<source>`：源元素描述Fluentd将从何处收集日志记录。例如，追踪由Docker容器写入的日志文件。源元素通常会对日志记录进行标记，描述日志记录的类型。例如，可以用来标记来自在Kubernetes中运行的容器的日志记录。'
- en: '`<filter>`: Filter elements are used to process the log records, for example,
    a filter element can parse log records that come from Spring Boot-based microservices
    and extract interesting parts of the log message into separate fields in the log
    record. Extracting information into separate fields in the log record makes the
    information searchable by Elasticsearch. A filter element selects what log records
    to process based on their tags.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<filter>`：过滤器元素用于处理日志记录，例如，过滤器元素可以解析来自基于Spring Boot的微服务的日志记录，并将日志消息的有趣部分提取到日志记录中的单独字段中。将信息提取到日志记录的单独字段中使得Elasticsearch可以搜索到这些信息。过滤器元素根据它们的标签选择要处理的日志记录。'
- en: '`<match>`: Output elements are used to perform two main tasks:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`：输出元素用于执行两项主要任务：'
- en: Send processed log records to targets such as Elasticsearch.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将处理后的日志记录发送到诸如Elasticsearch之类的目标。
- en: 'Routing is to decide how to process log records. A routing rule can rewrite
    the tag and reemit the log record into the Fluentd routing engine for further
    processing. A routing rule is expressed as an embedded `<rule>` element inside
    the `<match>` element. Output elements decide what log records to process, in
    the same way as a filter: based on the tag of the log records.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由用于决定如何处理日志记录。路由规则可以重写标签，并将日志记录重新发射到Fluentd路由引擎中进行进一步处理。路由规则表达为嵌入在`<match>`元素内的`<rule>`元素。输出元素决定要处理的日志记录，方式与过滤器相同：根据日志记录的标签。
- en: Fluentd comes with a number of built-in and external third-party plugins that
    are used by the source, filter, and output elements. We will see some of them
    in action when we walk through the configuration file in the next section. For
    more information on the available plugins, see Fluentd's documentation, which
    is available at [https://docs.fluentd.org](https://docs.fluentd.org).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd带有许多内置和外部第三方插件，这些插件被源、过滤器和输出元素使用。当我们在下一节中浏览配置文件时，我们将看到其中一些插件的作用。有关可用插件的更多信息，请参阅Fluentd的文档，网址为[https://docs.fluentd.org](https://docs.fluentd.org)。
- en: With this introduction to Fluentd out of the way, we are ready to see how Fluentd can
    be configured to process the log records from our microservices.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个Fluentd的介绍，我们准备好看看如何配置Fluentd来处理我们微服务的日志记录。
- en: Configuring Fluentd
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Fluentd
- en: The configuration of Fluentd is based on the configuration files from a Fluentd
    project on GitHub, `fluentd-kubernetes-daemonset`. The project contains Fluentd
    configuration files for how to collect log records from containers that run in
    Kubernetes and how to send them to Elasticsearch once they have been processed.
    We can reuse this configuration without changes and it will simplify our own configuration
    to a great extent. The Fluentd configuration files can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd的配置基于GitHub上Fluentd项目`fluentd-kubernetes-daemonset`的配置文件。该项目包含了如何从在Kubernetes中运行的容器收集日志记录以及在处理后如何将它们发送到Elasticsearch的Fluentd配置文件。我们可以在不进行更改的情况下重用此配置，并且这将极大地简化我们自己的配置。Fluentd配置文件可以在[https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf)找到。
- en: 'The configuration files that provide this functionality are `kubernetes.conf`
    and `fluent.conf`. The `kubernetes.conf` configuration file contains the following
    information:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提供此功能的配置文件是`kubernetes.conf`和`fluent.conf`。`kubernetes.conf`配置文件包含以下信息：
- en: Source elements that tail container log files and log files from processes that
    run outside of Kubernetes, for example, the `kubelet` and the Docker daemon. The
    source elements also tag the log records from Kubernetes with the full name of
    the log file with `/` replaced by `.` and prefixed with `kubernetes`. Since the
    tag is based on the full filename, the name contains the name of the namespace,
    pod, and container, among other things. So, the tag is very useful for finding
    log records of interest by matching the tag. For example, the tag from the `product-composite` microservice
    could be something like `kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`,
    while the tag for the corresponding `istio-proxy` in the same pod could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A filter element that enriches the log records that come from containers running
    inside Kubernetes, along with Kubernetes-specific fields that contain information
    such as the names of the containers and the namespace they run in.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main configuration file, `fluent.conf`, contains the following information:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '`@include` statements for other configuration files, for example, the `kubernetes.conf` file
    we described previously. It also includes custom configuration files that are
    placed in a specific folder, making it very easy for us to reuse these configuration
    files without any changes and provide our own configuration file that only handles
    processing related to our own log records. We simply need to place our own configuration
    file in the folder specified by the `fluent.conf` file.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output element that sends log records to Elasticsearch.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we described in the *Deploying Fluentd* section, these two configuration
    files will be packaged into the Docker image we will build for Fluentd.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s left to cover in our own configuration file is the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Detect and parse Spring Boot formatted log records from our microservices.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling multiline stack traces. Stack traces, for example, are written to log
    files using multiple lines. This makes it hard for Fluentd to handle a stack trace
    as a single log record.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating log records from the `istio-proxy` sidecars from the log records
    that were created by the microservices running in the same pod. The log records
    that are created by `istio-proxy` don't follow the same pattern as the log patterns
    that are created by our Spring Boot-based microservices. Therefore, they must
    be handled separately so that Fluentd doesn't try to parse them as Spring Boot
    formatted log records.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve this, the configuration is, to a large extent, based on using the
    `rewrite_tag_flter` plugin. This plugin can be used for routing log records based
    on the concept of changing the name of a tag and then reemitting the log record
    to the Fluentd routing engine.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'This processing is summarized by the following UML activity diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4aa9c779-4a07-422d-af6c-00539062d30a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'At a high level, the design of the configuration file looks as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The tags of all of the log records from Istio, including `istio-proxy`, are
    prefixed with `istio` so that they can be separated from the Spring Boot-based
    log records.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tags of all of the log records from the `hands-on` namespace (except for
    the log records from `istio-proxy`) are prefixed with `spring-boot`.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log records from Spring Boot are checked for the presence of multiline stack
    traces. If the log record is part of a multiline stack trace, it is processed
    by the third-party `detect-exceptions` plugin to recreate the stack trace. Otherwise,
    it is parsed using a regular expression to extract information of interest. See
    the *Deploying Fluentd* section for details on this third-party plugin.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `fluentd-hands-on.conf` configuration file follows this activity diagram
    closely. The configuration file is placed inside a Kubernetes config map (see
    `kubernetes/efk/fluentd-hands-on-configmap.yml`). Let''s go through this step
    by step, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`fluentd-hands-on.conf`配置文件紧密遵循这个活动图。配置文件放置在Kubernetes配置映射中（参见`kubernetes/efk/fluentd-hands-on-configmap.yml`）。让我们逐步进行，如下所示：'
- en: 'First comes the definition of the config map and the filename of the configuration
    file, `fluentd-hands-on.conf`. It looks as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是配置映射的定义和配置文件的文件名`fluentd-hands-on.conf`。它看起来像这样：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From the preceding source code, we understand that the `data` element will contain
    the configuration of Fluentd. It starts with the filename and uses a vertical
    bar `|` to mark the beginning of the embedded configuration file for Fluentd.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的源代码中，我们了解到`data`元素将包含Fluentd的配置。它以文件名开头，并使用竖线`|`标记Fluentd的嵌入式配置文件的开始。
- en: 'The first `<match>` element matches the log records from Istio, that is, tags
    that are prefixed with `kubernetes` and contain either `istio` as part of its
    namespace or part of its container name. It looks like this:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个`<match>`元素匹配来自Istio的日志记录，即以`kubernetes`为前缀并包含`istio`作为其命名空间或容器名称的标签。它看起来像这样：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s explain the preceding source code in more detail:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地解释前面的源代码：
- en: The `<match>` element matches any tags that follow the `kubernetes.**istio**` pattern,
    that is, it starts with `kubernetes` and then contains the word `istio` somewhere
    in the tag name. `istio` can either come from the name of the namespace or the
    container; both are part of the tag.
  id: totrans-66
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`元素匹配任何遵循`kubernetes.**istio**`模式的标签，即以`kubernetes`开头，然后在标签名称中的某个位置包含单词`istio`。`istio`可以来自命名空间或容器的名称；两者都是标签的一部分。'
- en: The `<match>` element contains only one `<rule>` element, which prefixes the
    tag with `istio`. The `${tag}` variable holds the current value of the tag.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`元素只包含一个`<rule>`元素，它将标签前缀设置为`istio`。`${tag}`变量保存标签的当前值。'
- en: 'Since this is the only `<rule>` element in the `<match>` element, it is configured
    to match all of the log records like so:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这是`<match>`元素中唯一的`<rule>`元素，因此它被配置为匹配所有日志记录，如下所示：
- en: Since all of the log records that come from Kubernetes have a `log` field, the
    `key` field is set to `log`, that is, the rule looks for a `log` field in the
    log records.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于所有来自Kubernetes的日志记录都有一个`log`字段，因此`key`字段设置为`log`，即规则在日志记录中查找`log`字段。
- en: To match any string in the `log` field, the `pattern` field is set to the `^(.*)$` regular
    expression. `^` marks the beginning of a string, while `$` marks the end of a
    string. `(.*)` matches any number of characters, except for line breaks.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了匹配`log`字段中的任何字符串，`pattern`字段设置为`^(.*)$`正则表达式。`^`标记字符串的开头，而`$`标记字符串的结尾。`(.*)`匹配任意数量的字符，除了换行符。
- en: The log records are reemitted to the Fluentd routing engine. Since no other
    elements in the configuration file match tags starting with `istio`, they will
    be sent directly to the output element for Elasticsearch, which is defined in
    the `fluent.conf` file we described previously.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录被重新发送到Fluentd路由引擎。由于配置文件中没有其他元素匹配以`istio`开头的标签，它们将直接发送到Elasticsearch的输出元素，该输出元素在我们之前描述的`fluent.conf`文件中定义。
- en: 'The second `<match>` element matches all of the log records from the `hands-on`
    namespace, that is, the log records that are emitted by our microservices. It
    looks like this:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个`<match>`元素匹配来自`hands-on`命名空间的所有日志记录，即我们的微服务发出的日志记录。它看起来像这样：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the preceding source code we can see that:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的源代码中我们可以看到：
- en: The log records emitted by our microservices use formatting rules for the log
    message defined by Spring Boot so their tags are prefixed with `spring-boot`.
    Then, they are re-emitted for further processing.
  id: totrans-75
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的微服务发出的日志记录使用Spring Boot定义的日志消息格式规则，因此它们的标签前缀为`spring-boot`。然后，它们被重新发送进行进一步处理。
- en: The `<match>` element is configured in the same way as the `<match kubernetes.**istio**>`
    element we looked at previously.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`元素的配置方式与我们之前查看的`<match kubernetes.**istio**>`元素相同。'
- en: 'The third `<match>` element matches `spring-boot` log records and determines
    whether they are ordinary Spring Boot log records or are part of a multiline stack
    trace. It looks like this:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个`<match>`元素匹配`spring-boot`日志记录，并确定它们是普通的Spring Boot日志记录还是多行堆栈跟踪的一部分。它看起来像这样：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As seen in the preceding source code, this is determined by using two `<rule>` elements:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的源代码所示，这是通过使用两个`<rule>`元素来确定的：
- en: The first uses a regular expression to check whether the `log` field in the
    log element starts with a timestamp or not.
  id: totrans-80
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个使用正则表达式来检查日志元素中的`log`字段是否以时间戳开头。
- en: If the `log` field starts with a timestamp, the log record is treated as an
    ordinary Spring Boot log record and its tag is prefixed with `parse`.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`log`字段以时间戳开头，则将日志记录视为普通的Spring Boot日志记录，并且其标签前缀为`parse`。
- en: Otherwise, the second `<rule>` element will match and the log record is handled
    as a multiline log record. Its tag is prefixed with `check.exception`.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，第二个`<rule>`元素将匹配，并且日志记录将被处理为多行日志记录。其标签前缀为`check.exception`。
- en: The log record is re-emitted in either case and its tag will either start with
    `check.exception.spring-boot.kubernetes` or `parse.spring-boot.kubernetes` after
    this process.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论哪种情况下，日志记录都将被重新发送，并且其标签将在此过程之后以`check.exception.spring-boot.kubernetes`或`parse.spring-boot.kubernetes`开头。
- en: 'In the fourth `<match>` element, the selected log records have a tag that starts
    with `check.exception.spring-boot`, that is, log records that are part of a multiline
    stack trace. It looks like this:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第四个`<match>`元素中，所选的日志记录具有以`check.exception.spring-boot`开头的标签，即多行堆栈跟踪的日志记录。它看起来像这样：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The source code for the `detect_exceptions` plugin works like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`detect_exceptions`插件的源代码工作原理如下：'
- en: The `detect_exceptions` plugin is used to combine multiple one-line log records
    into a single log record that contains a complete stack trace.
  id: totrans-87
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`detect_exceptions`插件将多个单行日志记录组合成包含完整堆栈跟踪的单个日志记录。
- en: Before a multiline log record is reemitted into the routing engine, the `check` prefix
    is removed from the tag to prevent a never-ending processing loop of the log record.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将多行日志记录重新发送到路由引擎之前，将标签中的`check`前缀删除，以防止日志记录的无限处理循环。
- en: 'Finally, the configuration file consists of a filter element that parses Spring
    Boot log messages using a regular expression, extracting information of interest.
    It looks like this:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，配置文件包括一个filter元素，使用正则表达式解析Spring Boot日志消息，提取感兴趣的信息。它看起来像这样：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s explain the preceding source code in more detail:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地解释前面的源代码：
- en: Note that filter elements don't re-emit log records; instead, they just pass
    them on to the next element in the configuration file that matches the log record's
    tag.
  id: totrans-92
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，filter元素不会重新发送日志记录；相反，它们只是将其传递给配置文件中与日志记录标签匹配的下一个元素。
- en: 'The following fields are extracted from the Spring Boot log message that''s
    stored in the `log` field in the log record:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Spring Boot日志消息中提取以下字段，这些字段存储在日志记录的`log`字段中：
- en: '`<time>`: The timestamp for when the log record was created'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<时间>`：记录日志创建的时间戳'
- en: '`<spring.level>`: The log level of the log record, for example, `FATAL`, `ERROR`,
    `WARN`, `INFO`, `DEBUG`, or `TRACE`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.level>`：日志记录的日志级别，例如`FATAL`、`ERROR`、`WARN`、`INFO`、`DEBUG`或`TRACE`'
- en: '`<spring.service>`: The name of the microservice'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.service>`：微服务的名称'
- en: '`<spring.trace>`: The trace ID used to perform distributed tracing'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.trace>`：用于执行分布式跟踪的跟踪ID'
- en: '`<spring.span>`: The span ID, the ID of the part of the distributed processing
    that this microservice executed'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.span>`：跨度ID，这个微服务执行的分布式处理的一部分的ID'
- en: '`<spring.pid>`: The process ID'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.pid>`：进程ID'
- en: '`<spring.thread>`: The thread ID'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.thread>`：线程ID'
- en: '`<spring.class>`: The name of the Java class'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.class>`：Java类的名称'
- en: '`<log>`: The actual log message'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<log>`：实际的日志消息'
- en: The names of Spring Boot-based microservices are specified using the `spring.application.name` property.
    This property has been added to each microservice-specific property file in the
    config repository, in the `config-repo` folder.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Boot基础微服务的名称是使用`spring.application.name`属性指定的。该属性已添加到配置存储库`config-repo`文件夹中的每个微服务特定属性文件中。
- en: Getting regular expressions right can be challenging, to say the least. Thankfully,
    there are several websites that can help. When it comes to using regular expressions
    together with Fluentd, I recommend using the following site: [https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确使用正则表达式可能会有挑战，至少可以这么说。幸运的是，有几个网站可以提供帮助。在使用Fluentd与正则表达式时，我建议使用以下网站：[https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/)。
- en: Now that you've been introduced to how Fluentd works and how the configuration
    file is constructed, we are ready to deploy the EKF stack.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了Fluentd的工作原理和配置文件的构造方式，我们准备部署EKF堆栈。
- en: Deploying the EFK stack on Kubernetes
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈
- en: 'Deploying the EFK stack on Kubernetes will be done in the same way as we have
    deployed our own microservices: using Kubernetes definition files for objects
    such as deployments, services, and configuration maps.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈将与我们部署自己的微服务的方式相同：使用Kubernetes定义文件来定义对象，如部署、服务和配置映射。
- en: 'The deployment of the EFK stack is divided into two parts:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: EFK堆栈的部署分为两部分：
- en: One part where we deploy Elasticsearch and Kibana
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Elasticsearch和Kibana的一个部分
- en: One part where we deploy Fluentd
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Fluentd的一个部分
- en: But first, we need to build and deploy our own microservices.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们需要构建和部署我们自己的微服务。
- en: Building and deploying our microservices
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和部署我们的微服务
- en: 'Building, deploying, and verifying the deployment using the `test-em-all.bash` test
    script is done in the same way as it was done in [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, in the *Running commands
    to create the service mesh* section. Run the following commands to get started:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 构建、部署和使用`test-em-all.bash`测试脚本验证部署的方式与[第18章](422649a4-94bc-48ae-b92b-e3894c014962.xhtml)中的*使用服务网格改进可观察性和管理*，*运行命令创建服务网格*部分中所做的方式相同。运行以下命令开始：
- en: 'First, build the Docker images from the source with the following commands:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令从源代码构建Docker镜像：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Recreate the namespace, `hands-on`, and set it as the default namespace:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建命名空间`hands-on`，并将其设置为默认命名空间：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Execute the deployment by running the `deploy-dev-env.bash` script with the
    following command:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下命令运行`deploy-dev-env.bash`脚本来执行部署：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Start the Minikube tunnel, if it''s not already running (see [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, the *Setting up access
    to Istio services* section for a recap, if required):'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Minikube隧道尚未运行，请启动它（请参阅[第18章](422649a4-94bc-48ae-b92b-e3894c014962.xhtml)，*使用服务网格改进可观察性和管理*，*设置访问Istio服务*部分进行回顾，如果需要）：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remember that this command requires that your user has `sudo` privileges and
    that you enter your password during startup and shutdown. It takes a couple of
    seconds before the command asks for the password, so it is easy to miss!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，此命令要求您的用户具有`sudo`权限，并且在启动和关闭过程中输入密码。在命令要求输入密码之前会有几秒钟的延迟，所以很容易错过！
- en: 'Run the normal tests to verify the deployment with the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行正常测试以验证以下命令的部署：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Expect the output to be similar to what we have seen from the previous chapters:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出与我们从前几章看到的类似：
- en: '![](img/042de643-01de-4776-8b29-ab05be9fe519.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/042de643-01de-4776-8b29-ab05be9fe519.png)'
- en: 'You can also try out the APIs manually by running the following commands:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以通过运行以下命令手动尝试API：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Expect the requested product ID, `2`, in the response.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 预期响应中包含请求的产品ID`2`。
- en: With the microservices deployed, we can move on and deploy Elasticsearch and
    Kibana!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务的部署，我们可以继续部署Elasticsearch和Kibana！
- en: Deploying Elasticsearch and Kibana
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Elasticsearch和Kibana
- en: 'We will deploy Elasticsearch and Kibana to its own namespace, `logging`. Both
    Elasticsearch and Kibana will be deployed for development and test usage using
    a Kubernetes deployment object. This will be done with a single pod and a Kubernetes
    node port service. The services will expose the standard ports for Elasticsearch
    and Kibana internally in the Kubernetes cluster, that is, port `9200` for Elasticserach
    and port `5601` for Kibana. Thanks to the `minikube tunnel` command, we will be
    able to access these services locally using the following URLs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署Elasticsearch和Kibana到它自己的命名空间`logging`中。Elasticsearch和Kibana将使用Kubernetes部署对象进行开发和测试。这将通过一个单独的pod和一个Kubernetes节点端口服务来完成。服务将在Kubernetes集群内部公开Elasticsearch和Kibana的标准端口，即Elasticserach的端口`9200`和Kibana的端口`5601`。借助`minikube
    tunnel`命令，我们将能够使用以下URL在本地访问这些服务：
- en: '`elasticsearch.logging.svc.cluster.local:9200` for Elasticserch'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elasticsearch.logging.svc.cluster.local:9200` 用于Elasticserch'
- en: '`kibana.logging.svc.cluster.local:5601` for Kibana'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kibana.logging.svc.cluster.local:5601` 用于Kibana'
- en: For the recommended deployment in a production environment on Kubernetes, see [https://www.elastic.co/elasticsearch-kubernetes](https://www.elastic.co/elasticsearch-kubernetes).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Kubernetes上的生产环境中推荐部署，请参阅[https://www.elastic.co/elasticsearch-kubernetes](https://www.elastic.co/elasticsearch-kubernetes)。
- en: 'We will use the versions that were available when this chapter was written:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在撰写本章时可用的版本：
- en: Elasticsearch version 7.3.0
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch版本7.3.0
- en: Kibana version 7.3.0
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana版本7.3.0
- en: Before we perform the deployments, let's look at the most interesting parts
    of the definition files.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行部署之前，让我们看看定义文件中最有趣的部分。
- en: A walkthrough of the definition files
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义文件的详细步骤
- en: 'The definition file for Elasticsearch, `kubernetes/efk/elasticsearch.yml`,
    contains a standard Kubernetes deployment and service object that we have seen
    multiples times before, for example, in [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml),
    *Introduction to Kubernetes*, in the *Trying out a sample deployment* section.
    The most interesting part, as we explained previously, of the definition file
    is the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch的定义文件`kubernetes/efk/elasticsearch.yml`包含了我们之前多次见过的标准Kubernetes部署和服务对象，例如在[第15章](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml)中，*Kubernetes简介*，*尝试示例部署*部分。如前所述，定义文件中最有趣的部分如下：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: We use an official Docker image from Elastic that's available at `docker.elastic.co`
    with a package that only contains open source components. This is ensured by using
    the `-oss` suffix on the name of the Docker image, `elasticsearch-oss`. The version
    is set to `7.3.0`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Elastic提供的官方Docker镜像，该镜像位于`docker.elastic.co`，其中只包含开源组件的软件包。通过在Docker镜像的名称上使用`-oss`后缀来确保这一点，即`elasticsearch-oss`。版本设置为`7.3.0`。
- en: The Elasticsearch container is allowed to allocate a relatively large amount
    of memory—2 GB—to be able to perform queries with good performance. The more memory,
    the better the performance.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch容器被允许分配相对较大的内存——2GB——以便能够执行具有良好性能的查询。内存越多，性能越好。
- en: 'The definition file for Kibana, `kubernetes/efk/kibana.yml`, also contains
    a standard Kubernetes deployment and service object. The most interesting parts
    in the definition file are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana的定义文件`kubernetes/efk/kibana.yml`中还包含了标准的Kubernetes部署和服务对象。定义文件中最有趣的部分如下：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: For Kibana, we also use an official Docker image from Elastic that's available
    at `docker.elastic.co`, along with a package that only contains open source components, `kibana-oss`. The
    version is set to `7.3.0`.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Kibana，我们还使用了来自Elastic的官方Docker镜像，该镜像位于`docker.elastic.co`，并且只包含开源组件`kibana-oss`。版本设置为`7.3.0`。
- en: To connect Kibana with the Elasticsearch pod, an environment variable, `ELASTICSEARCH_URL`,
    is defined to specify the address to the Elasticsearch service, `http://elasticsearch:9200`.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将Kibana与Elasticsearch pod连接，定义了一个环境变量`ELASTICSEARCH_URL`，用于指定Elasticsearch服务的地址，即`http://elasticsearch:9200`。
- en: With these insights, we are ready to perform the deployment of Elasticsearch
    and Kibana.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些见解，我们准备部署Elasticsearch和Kibana。
- en: Running the deploy commands
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行部署命令
- en: 'Deploy Elasticsearch and Kibana by performing the following steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下步骤部署Elasticsearch和Kibana：
- en: 'Create a namespace for Elasticsearch and Kibana with the following command:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令为Elasticsearch和Kibana创建一个命名空间：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To make the deploy steps run faster, prefetch the Docker images for Elasticsearch
    and Kibana with the following commands:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使部署步骤运行更快，使用以下命令预取Elasticsearch和Kibana的Docker镜像：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Deploy Elasticsearch and wait for its pod to be ready with the following commands:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署Elasticsearch并等待其pod准备就绪：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Verify that Elasticsearch is up and running with the following command:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证Elasticsearch是否正在运行：
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Expect `You Know, for Search` as a response.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 期望作为响应收到`You Know, for Search`。
- en: Depending on your hardware, you might need to wait for a minute or two before
    Elasticsearch responds with this message.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的硬件配置，您可能需要等待一两分钟，然后Elasticsearch才会响应此消息。
- en: 'Deploy Kibana and wait for its pod to be ready with the following commands:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署Kibana并等待其pod准备就绪：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Verify that Kibana is up and running with the following command:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证Kibana是否正在运行：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Expect `200` as the response.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 期望收到`200`作为响应。
- en: With Elasticsearch and Kibana deployed, we can start to deploy Fluentd.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了Elasticsearch和Kibana后，我们可以开始部署Fluentd。
- en: Deploying Fluentd
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Fluentd
- en: Deploying Fluentd is a bit more complex compared to deploying Elasticsearch
    and Kibana. To deploy Fluentd, we will use a Docker image that's been published
    by the Fluentd project on Docker Hub, `fluent/fluentd-kubernetes-daemonset`, and
    sample the Kubernetes definition files from a Fluentd project on GitHub, `fluentd-kubernetes-daemonset`.
    It is located at [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
    As it's implied by the name of the project, Fluentd will be deployed as a daemon
    set, running one pod per node in the Kubernetes cluster. Each Fluentd pod is responsible
    for collecting log output from processes and containers that run on the same node
    as the pod. Since we are using Minikube, that is, a single node cluster, we will
    only have one Fluentd pod.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Fluentd相对于部署Elasticsearch和Kibana来说要复杂一些。为了部署Fluentd，我们将使用Fluentd项目在Docker
    Hub上发布的Docker镜像“fluent/fluentd-kubernetes-daemonset”，并从GitHub上的Fluentd项目“fluentd-kubernetes-daemonset”中获取Kubernetes定义文件。它位于[https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset)。由于项目名称暗示，Fluentd将作为一个守护进程集部署，每个节点在Kubernetes集群中运行一个pod。每个Fluentd
    pod负责收集在同一节点上运行的进程和容器的日志输出。由于我们使用的是Minikube，即单节点集群，我们只会有一个Fluentd pod。
- en: To handle multiline log records that contain stack traces from exceptions, we
    will use a third-party Fluentd plugin provided by Google, `fluent-plugin-detect-exceptions`,
    which is available at [https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions). To
    be able to use this plugin, we will build our own Docker image where the `fluent-plugin-detect-exceptions` plugin
    will be installed. Fluentd's Docker image, `fluentd-kubernetes-daemonset`, will
    be used as the base image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理包含异常堆栈的多行日志记录，我们将使用Google提供的第三方Fluentd插件“fluent-plugin-detect-exceptions”，该插件可在[https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions)上获得。为了能够使用这个插件，我们将构建自己的Docker镜像，其中将安装“fluent-plugin-detect-exceptions”插件。Fluentd的Docker镜像“fluentd-kubernetes-daemonset”将被用作基础镜像。
- en: 'We will use the versions that were available when this chapter was written:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在编写本章时可用的版本：
- en: Fluentd version 1.4.2
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd版本1.4.2
- en: fluent-plugin-detect-exceptions version 0.0.12
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fluent-plugin-detect-exceptions版本0.0.12
- en: Before we perform the deployments, let's look at the most interesting parts
    of the definition files.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行部署之前，让我们看一下定义文件中最有趣的部分。
- en: Discovering the log records from microservices
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从微服务中发现日志记录
- en: In this section, we will learn how to utilize one of the main features of centralized
    logging, that is, finding log records from our microservices. We will also learn
    how to use the trace ID in the log records to find log records from other microservices
    that belong to one and the same process, for example, a request to the API.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何利用集中式日志的主要功能之一，即查找来自我们微服务的日志记录。我们还将学习如何使用日志记录中的跟踪ID来查找属于同一进程的其他微服务的日志记录，例如对API的请求。
- en: Let's start by creating some log records that we can look up with the help of
    Kibana. We will use the API to create a product with a unique product ID and then
    retrieve information about the product. After that, we can try to find the log
    records that were created when retrieving the product information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一些日志记录，以便在Kibana的帮助下查找。我们将使用API创建具有唯一产品ID的产品，然后检索有关该产品的信息。之后，我们可以尝试查找在检索产品信息时创建的日志记录。
- en: 'The creation of log records in the microservices has updated a bit from the
    previous chapter so that the product composite and the three core microservices,
    `product`, `recommendation`, and `review`, all write a log record with the log
    level set to `INFO` when they begin processing a get request. Let''s go over the
    source code that''s been added to each microservice:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务中的日志记录创建与上一章有所更新，因此产品组合和三个核心微服务“product”、“recommendation”和“review”在处理获取请求时都会写入一个日志记录，日志级别设置为“INFO”。让我们看一下已添加到每个微服务中的源代码：
- en: 'Product composite microservice log creation:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品组合微服务日志创建：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Product microservice log creation:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品微服务日志创建：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Recommendation microservice log creation:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐微服务日志创建：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Review microservice log creation:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Review微服务日志创建：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For more details, see the source code in the `microservices` folder.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅“microservices”文件夹中的源代码。
- en: 'Perform the following steps to use the API to create log records and then use
    Kibana to look up the log records:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤使用API创建日志记录，然后使用Kibana查找日志记录：
- en: 'Get an access token with the following command:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取访问令牌：
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As mentioned in the introduction to this section we will start by creating
    a product with a unique product ID. Create a minimalistic product (without recommendations
    and reviews) for `"productId" :1234` by executing the following command:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本节介绍，我们将首先创建一个具有唯一产品ID的产品。通过执行以下命令为“productId”：1234创建一个最简产品（没有推荐和评论）：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Read the product with the following command:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令读取产品：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Expect a response similar to the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 期望类似以下的响应：
- en: '![](img/258fd470-f7ae-432b-933e-0098656e7199.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/258fd470-f7ae-432b-933e-0098656e7199.png)'
- en: Hopefully, we got some log records created by these API calls. Let's jump over
    to Kibana and find out!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们通过这些API调用创建了一些日志记录。让我们跳转到Kibana并找出！
- en: 'On the Kibana web page, click on the `Discover` menu on the left. You will
    see something like the following:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kibana网页上，点击左侧的“Discover”菜单。您将看到类似以下的内容：
- en: '![](img/75f6dd32-2743-454f-a301-007444bb2cfd.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75f6dd32-2743-454f-a301-007444bb2cfd.png)'
- en: On the left-top corner, we can see that Kibaba has found 326,642 log records.
    The time picker shows that they are from the last 7 days. In the histogram, we
    can see how the log records are spread out over time. Following that is a table showing
    the most recent log events that were found by the query.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在左上角，我们可以看到Kibaba找到了326,642条日志记录。时间选择器显示它们来自过去7天。在直方图中，我们可以看到日志记录如何随时间分布。接下来是一个表，显示了查询找到的最近的日志事件。
- en: If you want to change the time interval, you can use the time picker. Click
    on its calendar icon to adjust the time interval.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想更改时间间隔，可以使用时间选择器。单击其日历图标以调整时间间隔。
- en: 'To get a better view of the content in the log records, add some fields from
    the log records to the table under the histogram. Select the fields from the list
    of available fields to the left. Scroll down until the field is found. Hold the
    cursor over the field and an add button will appear; click on it to add the field
    as a column in the table. Select the following fields, in order:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地查看日志记录中的内容，将一些字段从日志记录添加到直方图下的表中。从左侧的可用字段列表中选择字段。向下滚动直到找到该字段。将光标悬停在字段上，将出现一个添加按钮；单击它将该字段添加为表中的列。按顺序选择以下字段：
- en: spring.level, the log level
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spring.level，日志级别
- en: kubernetes.container_name, the name of the container
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kubernetes.container_name，容器的名称
- en: spring.trace, the trace ID used for distributed tracing
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spring.trace，用于分布式跟踪的跟踪ID
- en: 'log, the actual log message. The web page should look something similar to
    the following:'
  id: totrans-208
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: log，实际的日志消息。网页应该看起来类似于以下内容：
- en: '![](img/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png)'
- en: The table now contains information that is of interest regarding the log records!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表现在包含了关于日志记录的感兴趣的信息！
- en: 'To find log records from the call to the `GET` API, we can ask Kibana to find
    log records where the log field contains the text product.id=1234\. This matches
    the log output from the product composite microservice that was shown previously.This
    can be done by entering `log:"product.id=1234"` in the Search field and clicking
    on the Update button (this button can also be labeled Refresh). Expect one log
    record to be found:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查找调用`GET` API的日志记录，我们可以要求Kibana查找日志字段包含文本product.id=1234的日志记录。这与先前显示的产品组合微服务的日志输出匹配。这可以通过在搜索字段中输入`log:"product.id=1234"`并单击“更新”按钮（此按钮也可以标记为刷新）来完成。预期会找到一条日志记录：
- en: '![](img/c5370730-05a8-4dc0-a414-3020057d3bb6.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5370730-05a8-4dc0-a414-3020057d3bb6.png)'
- en: Verify that the timestamp is from when you called the `GET` API and verify that
    the name of the container that created the log record is comp, that is, verify
    that the log record was sent by the product composite microservice.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证时间戳是否来自您调用`GET` API的时间，并验证创建日志记录的容器的名称是comp，也就是验证日志记录是由产品组合微服务发送的。
- en: Now, we want to see the related log records from the other microservices that
    participated in the process of returning information about the product with productId
    1234, that is, finding log records with the same trace ID as that of the log record
    we found. To do that, place the cursor over the `spring.trace` field for the log
    record. Two small magnifying glasses will be shown to the right of the field,
    one with a `+` sign and one with a `-` sign. Click on the magnifying glass with
    the `+` sign to filter on the trace ID.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们想要查看其他微服务相关的日志记录，这些微服务参与了返回有关产品ID为1234的产品信息的过程，也就是说，查找具有与我们找到的日志记录相同的跟踪ID的日志记录。为此，将光标放在日志记录的`spring.trace`字段上。该字段右侧将显示两个小放大镜，一个带有“+”符号，一个带有“-”符号。单击带有“+”符号的放大镜以根据跟踪ID进行过滤。
- en: 'Clean the Search field so that the only search criteria is the filter of the
    trace field. Then, click on the Update button to see the result. Expect a response
    similar to the following:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清除搜索字段，以便唯一的搜索条件是跟踪字段的过滤器。然后，单击“更新”按钮查看结果。期望得到类似以下的响应：
- en: '![](img/909125b4-7868-4ed9-b33c-3a5197408304.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/909125b4-7868-4ed9-b33c-3a5197408304.png)'
- en: We can see a lot of detailed debug and trace messages that clutter the view;
    let's get rid of them!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到很多详细的调试和跟踪消息，这些消息混乱了视图；让我们摆脱它们！
- en: Place the cursor over a TRACE value and click on the magnifying glass with the
    - sign to filter out log records with the log level set to TRACE.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将光标放在TRACE值上，然后单击带有“-”符号的放大镜，以过滤掉日志级别设置为TRACE的日志记录。
- en: Repeat the preceding step for the DEBUG log record.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上一步，为DEBUG日志记录执行相同的操作。
- en: 'We should now be able to see the four expected log records, one for each microservice
    involved in the lookup of product information for the product with product ID
    1234:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们应该能够看到四条预期的日志记录，每个微服务都涉及查找产品ID为1234的产品信息：
- en: '![](img/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png)'
- en: Also, note the filters that were applied included the trace ID but excluded
    log records with the log level set to DEBUG or TRACE.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意应用的过滤器包括跟踪ID，但排除了日志级别设置为DEBUG或TRACE的日志记录。
- en: Now that we know how to find the expected log records, we are ready to take
    the next step. This will be to learn how to find unexpected log records, that
    is, error messages, and how to perform root cause analysis, that is, find the
    reason for these error messages.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何找到预期的日志记录，我们准备迈出下一步。这将是学习如何找到意外的日志记录，也就是错误消息，以及如何执行根本原因分析，也就是找到这些错误消息的原因。
- en: A walkthrough of the definition files
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义文件的漫游
- en: 'The Dockerfile that''s used to build the Docker image, `kubernetes/efk/Dockerfile`,
    looks as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建Docker镜像的Dockerfile，`kubernetes/efk/Dockerfile`，如下所示：
- en: '[PRE28]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: The base image is Fluentd's Docker image, `fluentd-kubernetes-daemonset`. The `v1.4.2-debian-elasticsearch-1.1` tag
    specifies that version v1.4.2 shall be used with a package that contains built-in
    support for sending log records to Elasticsearch. The base Docker image contains
    the Fluentd configuration files that were mentioned in the *Configuring Fluentd* section.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础镜像是Fluentd的Docker镜像，`fluentd-kubernetes-daemonset`。`v1.4.2-debian-elasticsearch-1.1`标签指定将使用v1.4.2版本的包含内置支持将日志记录发送到Elasticsearch的软件包。基础Docker镜像包含了在*配置Fluentd*部分中提到的Fluentd配置文件。
- en: The Google plugin, `fluent-plugin-detect-exceptions`, is installed using Ruby's
    package manager, `gem`.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ruby的软件包管理器`gem`安装Google插件`fluent-plugin-detect-exceptions`。
- en: 'The definition file of the daemon set, `kubernetes/efk/fluentd-ds.yml`, is
    based on a sample definition file in the `fluentd-kubernetes-daemonset` project,
    which can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml).
    This file is a bit complex, so let''s go through the most interesting parts separately:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 守护进程集的定义文件`kubernetes/efk/fluentd-ds.yml`基于`fluentd-kubernetes-daemonset`项目中的示例定义文件，该项目可以在[https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml)找到。这个文件有点复杂，所以让我们分别讨论最有趣的部分：
- en: 'First, here''s the declaration of the daemon set:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，这是守护进程集的声明：
- en: '[PRE29]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: The `kind` key specifies that this is a daemon set.
  id: totrans-234
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind`键指定这是一个守护进程集。'
- en: The `namespace` key specifies that the daemon set shall be created in the `kube-system`
    namespace and not in the `logging` namespace where Elasticseach and Kibana are
    deployed.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespace`键指定了守护进程集应该在`kube-system`命名空间中创建，而不是在部署Elasticseach和Kibana的`logging`命名空间中。'
- en: 'The next part specifies the template for the pods that are created by the daemon
    set. The most interesting parts are as follows:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的部分指定了由守护进程集创建的pod的模板。最有趣的部分如下：
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: The Docker image that's used for the pods is `hands-on/fluentd:v1`. We will
    build this Docker image after walking through the definition files using the Dockerfile
    we described previously.
  id: totrans-239
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于pod的Docker镜像是`hands-on/fluentd:v1`。我们将在使用我们之前描述的Dockerfile的定义文件后构建这个Docker镜像。
- en: 'A number of environment variables are supported by the Docker image and are
    used to customize it. The two most important ones are as follows:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker镜像支持许多环境变量，并用于自定义。最重要的两个是：
- en: '`FLUENT_ELASTICSEARCH_HOST`, which specifies the hostname of the Elasticsearch
    service, that is, `elasticsearch.logging`'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FLUENT_ELASTICSEARCH_HOST`，指定Elasticsearch服务的主机名，即`elasticsearch.logging`'
- en: '`FLUENT_ELASTICSEARCH_PORT`, which specifies the port that''s used to communicate
    with Elasticsearch, that is, `9200`'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FLUENT_ELASTICSEARCH_PORT`，指定用于与Elasticsearch通信的端口，即`9200`'
- en: Since the Fluentd pod runs in another namespace than Elasticsearch, the hostname
    cannot be specified using its short name, that is, `elasticsearch`. Instead, the
    namespace part of the DNS name must also be specified, that is, `elasticsearch.logging`. As
    an alternative, the **fully qualified domain name** (**FQDN**), `elasticsearch.logging.svc.cluster.local`,
    can also be used. But since the last part of the DNS name, `svc.cluster.local`,
    is shared by all DNS names inside a Kubernetes cluster, it does not need to be
    specified.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Fluentd pod运行在与Elasticsearch不同的命名空间中，因此不能使用其简称`elasticsearch`来指定主机名。相反，必须同时指定DNS名称的命名空间部分，即`elasticsearch.logging`。作为替代，也可以使用**完全限定域名**（**FQDN**）`elasticsearch.logging.svc.cluster.local`。但是，由于DNS名称的最后一部分`svc.cluster.local`被Kubernetes集群中所有DNS名称共享，因此不需要指定。
- en: 'Finally, a number of volumes, that is, filesystems, are mapped into the pod,
    as follows:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，许多卷，即文件系统，被映射到pod中，如下所示：
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: Three folders on the host (that is, the node) are mapped into the Fluentd pod.
    These folders contain the log files that Fluentd will tail and collect log records
    from. The folders are: `/var/log`, `/var/lib/docker/containers` and `/run/log/journal`.
  id: totrans-247
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机（即节点）上的三个文件夹被映射到Fluentd pod中。这些文件夹包含Fluentd将要追踪并收集日志记录的日志文件。这些文件夹是：`/var/log`，`/var/lib/docker/containers`和`/run/log/journal`。
- en: Our own configuration file that specifies how Fluentd shall process log records
    from our microservices is mapped using a config map called `fluentd-hands-on-config`
    to the `/fluentd/etc/conf.d` folder. The base Docker image that's used for preceding
    Fluentd, `fluentd-kubernetes-daemonset`, configures Fluentd to include any configuration
    file that's found in the `/fluentd/etc/conf.d` folder. See the *Configuring Fluentd* section
    for details.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们自己的配置文件指定了Fluentd如何处理来自我们微服务的日志记录，使用名为`fluentd-hands-on-config`的配置映射到`/fluentd/etc/conf.d`文件夹。用于前面的Fluentd的基础Docker镜像`fluentd-kubernetes-daemonset`配置了Fluentd以包括在`/fluentd/etc/conf.d`文件夹中找到的任何配置文件。有关详细信息，请参阅*配置Fluentd*部分。
- en: For the full source code of the definition file for the daemon set, see the
    `kubernetes/efk/fluentd-ds.yml` file.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有关守护进程集定义文件的完整源代码，请参阅`kubernetes/efk/fluentd-ds.yml`文件。
- en: Now that we've walked through everything, we are ready to perform the deployment
    of Fluentd.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了所有内容，我们准备部署Fluentd。
- en: Running the deploy commands
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行部署命令
- en: 'To deploy Fluentd, we have to build the Docker image, create the config map,
    and finally deploy the daemon set. Run the following commands to perform these
    steps:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署Fluentd，我们必须构建Docker镜像，创建配置映射，最后部署守护进程集。运行以下命令执行这些步骤：
- en: 'Build the Docker image and tag it with `hands-on/fluentd:v1` using the following
    command:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令构建Docker镜像并标记为`hands-on/fluentd:v1`：
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create the config map, deploy Fluentd''s daemon set, and wait for the pod to
    be ready with the following commands:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建配置映射，部署Fluentd的守护进程集，并使用以下命令等待pod准备就绪：
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Verify that the Fluentd pod is healthy with the following command:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Expect a response of `2019-08-16 15:11:33 +0000 [info]: #0 fluentd worker is
    now running worker=0`.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Fluentd will start to collect a considerable amount of log records from the
    various processes and containers in the Minkube instance. After a minute or so,
    you can ask Elasticsearch how many log records have been collected with the following
    command:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The command can be a bit slow the first time it is executed, but should return
    a response similar to the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14a27b89-c4e5-4d29-86c2-4652b7fbe575.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: In this example, Elasticsearch contains `144750` log records.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: This completes the deployment of the EFK stack. Now, it's time to try it out
    and find out what all of the collected log records are about!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Trying out the EFK stack
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need to do before we can try out the EFK stack is initialize
    Kibana so it knows what search indices to use in Elasticsearch. Once that is done,
    we will try out the following, in my experience, common tasks:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: We will start by analyzing of what types of log records Fluentd has collected
    and stored in Elasticsearch. Kibana has a very useful visualization capability
    that can be used for this.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will learn how to discover log records from different microservices
    that belong to one and the same processing of an external request to the API. We
    will use the **trace ID** in the log records as a correlation ID to find related
    log records.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thirdly, we will learn how to use Kibana to perform **root cause analysis**, that
    is, find the actual reason for an error.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing Kibana
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start to use Kibana, we must specify what search indices to use in
    Elasticsearch and what field in the indices holds the timestamps for the log records.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to initialize Kibana:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Open Kibana's web UI using the `http://kibana.logging.svc.cluster.local:5601` URL
    in a web browser.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the welcome page, Welcome to Kibana, click on the Explore on my own button.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Expand button in the lower-left corner to view the names of the
    menu choices. These will be shown on the left-hand side.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Discover in the menu to the left. You will be asked to define a pattern
    that's used by Kibana to identify what Elasticsearch indices it shall retrieve
    log records from.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the `logstash-*` index pattern and click on Next Step.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, you will be asked to specify the name of the field that contains
    the timestamp for the log records. Click on the drop-down list for the Time Filter
    field name and select the only available field, @timestamp.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Create index pattern button.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kibana will show a page that summarizes the fields that are available in the
    selected indices.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indices are, by default, named `logstash` for historical reasons, even though
    it is Flutentd that is used for log collection.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: With Kibana initialized, we are ready to examine the log records we have collected.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the log records
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the deployment of Fluentd, we know that it immediately started to collect
    a significant number of log records. So, the first thing we need to do is get
    an understanding of what types of log records Fluentd has collected and stored
    in Elasticsearch.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Kibana''s visualization feature to divide the log records per Kubernetes
    namespace and then ask Kibana to show us how the log records are divided per type
    of container within each namespace. A pie chart is a suitable chart type for this
    type of analysis. Perform the following steps to create a pie chart:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In Kibana's web UI, click on Visualize in the menu to the left.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Create new visualization button.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Pie as the visualization type.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select logstash-* as the source.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the time picker (a date interval selector) above the pie chart, set a date
    interval of your choice (set to the last 7 days in the following screenshot).
    Click on its calendar icon to adjust the time interval.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on Add to create the first bucket, as follows:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the bucket type, that is, Split slices.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择桶类型，即“拆分切片”。
- en: For the aggregation type, select Terms from the drop-down list.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于聚合类型，从下拉列表中选择“Terms”。
- en: As the field, select kubernetes.namespace_name.keyword.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为字段，选择kubernetes.namespace_name.keyword。
- en: For the size, select 10.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大小，选择10。
- en: Enable Group other values in separate bucket.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用将其他值分组到单独的桶中。
- en: Enable Show missing values.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用显示缺失值。
- en: 'Press the Apply changes button (the blue play icon above the Bucket definition).
    Expect a pie chart that looks similar to the following:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下“应用更改”按钮（桶定义上方的蓝色播放图标）。期望看到一个类似以下的饼图：
- en: '![](img/1d79e158-3af1-495b-8415-e2b769b59454.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d79e158-3af1-495b-8415-e2b769b59454.png)'
- en: We can see that the log records are divided over the namespaces we have been
    working with in the previous chapters: `kube-system`, `istio-system`, `logging`, `cert-manager`,
    and our own `hands-on` namespace. To see what containers have created the log
    records divided per namespace, we need to create a second bucket.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到日志记录分布在我们在前几章中使用的命名空间上：“kube-system”，“istio-system”，“logging”，“cert-manager”和我们自己的“hands-on”命名空间。要查看每个命名空间创建了哪些容器的日志记录，我们需要创建第二个桶。
- en: 'Click on Add again to create a second bucket:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“添加”再次创建第二个桶：
- en: Select the bucket type, that is, Split slices.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择桶类型，即“拆分切片”。
- en: As the sub-aggregation type, select Terms from the drop-down list.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为子聚合类型，从下拉列表中选择“Terms”。
- en: As the field, select kubernetes.container_name.keyword.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为字段，选择kubernetes.container_name.keyword。
- en: For the size, select 10.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大小，选择10。
- en: Enable Group other values in separate bucket.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用将其他值分组到单独的桶中。
- en: Enable Show missing values.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用显示缺失值。
- en: 'Press the Apply changes button again. Expect a pie chart that looks similar
    to the following:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次按下“应用更改”按钮。期望看到一个类似以下的饼图：
- en: '![](img/660ceb82-205e-401d-8595-5a23cc328e26.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/660ceb82-205e-401d-8595-5a23cc328e26.png)'
- en: Here, we can find the log records from our microservices. Most of the log records
    come from the `product-composite` microservice.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以找到来自我们微服务的日志记录。大多数日志记录来自“product-composite”微服务。
- en: At the top of the pie chart, we have a group of log records labeled `missing`,
    that is, they neither have a Kubernetes namespace nor a container name specified.
    What's behind these missing log records? These log records come from processes
    running outside of the Kubernetes cluster in the Minikube instance and they are
    stored using Syslog. They can be analyzed using Syslog-specific fields, specifically
    the *identifier field.* Let's create a third bucket that divides log records based
    on their Syslog identifier field, if any.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在饼图的顶部，我们有一组标记为“缺失”的日志记录，即它们既没有Kubernetes命名空间也没有指定容器名称。这些缺失的日志记录背后是什么？这些日志记录来自Kubernetes集群外部在Minikube实例中运行的进程，并且它们使用Syslog进行存储。它们可以使用Syslog特定字段进行分析，特别是*标识符字段*。如果有的话，让我们创建一个第三个桶，根据它们的Syslog标识符字段来划分日志记录。
- en: 'Click on `Add` again to create a third bucket:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“添加”再次创建第三个桶：
- en: Select the bucket type, that is, Split slices.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择桶类型，即“拆分切片”。
- en: As the sub-aggregation type, select Terms from the drop-down list.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为子聚合类型，从下拉列表中选择“Terms”。
- en: As the field, select SYSLOG_IDENTIFIER.keyword.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为字段，选择SYSLOG_IDENTIFIER.keyword。
- en: Enable Group other values in separate bucket.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用将其他值分组到单独的桶中。
- en: Enable Show missing values.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用显示缺失值。
- en: 'Press the Apply changes button and expect a pie chart that looks similar to
    the following:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下“应用更改”按钮，并期望看到一个类似以下的饼图：
- en: '![](img/56a1d5be-61bf-49e6-87c2-a18290f65e38.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56a1d5be-61bf-49e6-87c2-a18290f65e38.png)'
- en: The `missing` log records turn out to come from the `kubelet` process, which
    manages the node from a Kubernetes perspective, and `dockerd`, the Docker daemon
    that manages all of the containers.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: “缺失”日志记录原来来自“kubelet”进程，它从Kubernetes的角度管理节点，以及“dockerd”，管理所有容器的Docker守护程序。
- en: Now that we have found out where the log records come from, we can start to
    locate the actual log records from our microservices.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了日志记录的来源，我们可以开始定位来自我们微服务的实际日志记录。
- en: Performing root cause analyses
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行根本原因分析
- en: One of the most important features of centralized logging is that it makes it
    possible to analyze errors using log records from many sources and, based on that,
    perform root cause analysis, that is, find the actual reason for the error message.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 集中日志的最重要特性之一是，它使得可以使用来自许多来源的日志记录来分析错误，并且基于此进行根本原因分析，即找到错误消息的实际原因。
- en: 'In this section, we will simulate an error and see how we can find information
    about it, all of the way down to the line of source code that caused the error
    in one of the microservices in the system landscape. To simulate an error, we
    will reuse the fault parameter we introduced in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml),
    *Improving Resilience Using Resilience4j*, in the *Adding programmable delays
    and random errors* section. We can use this to force the product microservice
    to throw an exception. Perform the following steps:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟一个错误，并看看如何找到有关它的信息，一直到系统架构中的一个微服务中导致错误的源代码行。为了模拟错误，我们将重用我们在[第13章](23795d34-4068-4961-842d-989cde26b642.xhtml)中介绍的故障参数，“使用Resilience4j改进韧性”，在“添加可编程延迟和随机错误”部分。我们可以使用这个来强制产品微服务抛出异常。执行以下步骤：
- en: 'Run the following command to generate a fault in the product microservice while
    searching for product information on the product with product ID `666`:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令，在搜索产品ID为“666”的产品信息时，在产品微服务中生成一个故障：
- en: '[PRE36]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Expect the following error in response:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 期望得到以下错误响应：
- en: '![](img/8d0a558f-7fc3-4f83-bb92-320d419859ab.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d0a558f-7fc3-4f83-bb92-320d419859ab.png)'
- en: Now, we have to pretend that we have no clue about the reason for this error!
    Otherwise, the root cause analysis wouldn't be very exciting, right? Let's assume
    that we work in a support organization and have been asked to investigate some
    problems that just occurred while an end user tried to look up information regarding
    a product with product ID `666`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start to analyze the problem, let''s delete the previous search filters
    in the Kibana web UI so that we can start from scratch. For each filter we defined
    in the previous section, click on their close icon (an x) to remove them. After
    all of the filters have been removed, the web page should look similar to the
    following:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/82e6c766-b5d4-4bf8-900d-081dfff40319.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: Start by selecting a time interval that includes the point in time when the
    problem occurred using the time picker. For example, search the last seven days
    if you know that the problem occurred within the last seven days.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, search for log records with the log level set to ERROR within this timeframe.
    This can be done by clicking on the spring.level field in the list of selected
    fields. When you click on this field, its most commonly used values will be displayed
    under it. Filter on the ERROR value by clicking on its magnifier, shown with the
    + sign. Kibana will now show log records within the selected time frame with its
    log level set to ERROR, like so:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/70b40e75-4168-4207-90fb-e528fad127f7.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: We can see a number of error messages related to product ID `666`. The top four
    have the same trace ID, so this seems like a trace ID of interest to use for further
    investigation.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also see more error messages below the top four that seem to be related
    to the same error but with different trace IDs. Those are caused by the retry
    mechanism in the product composite microservice, that is, it retries the request
    a couple of times before giving up and returning an error message to the caller.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter on the trace ID of the first log record in the same way we did in the
    previous section.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the filter of the ERROR log level to be able to see all of the records
    belonging to this trace ID. Expect Kibana to respond with a lot of log records.
    Look to the oldest log record, that is, the one that occurred first, that looks
    suspicious. For example, it may have a WARN or ERROR log level or a strange log
    message. The default sort order is showing the latest log record at the top, so
    scroll down to the end and search backward (you can also change the sort order
    to show the oldest log record first by clicking on the small up/down arrow next
    to the `Time` column header). The WARN log message that says `Bad luck, and error
    occurred` looks like it could be the root cause of the problem. Let''s investigate
    it further:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/47ac8d1a-707a-410f-869f-d0923b4392b2.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: Once a log record has been found that might be the root cause of the problem,
    it is of great interest to be able to find the nearby stack trace describing where
    exceptions were thrown in the source code**. **Unfortunately, the Fluentd plugin
    we use for collecting multiline exceptions, `fluent-plugin-detect-exceptions`,
    is unable to relate stack traces to the trace ID that was used. Therefore, stack
    traces will not show up in Kibana when we filter on a trace ID. Instead, we can
    use a feature in Kibana for finding surrounding log records that show log records
    that have occurred in near time to a specific log record.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expand the log record that says bad luck using the arrow to the left of the
    log record. Detailed information about this specific log record will be revealed.
    There is also a link named View surrounding documents; click on it to see nearby
    log records. Expect a web page similar to the following:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d533fe87-485e-4fe5-95c6-8de6c2ae04c6.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: 'The log record above the bad luck log record with the stack trace for the error
    message Something went wrong... looks interesting and was logged by the product
    microservice just two milliseconds after it logged the *bad luck* log record.
    They seem to be related! The stack trace in that log record points to line 96
    in `ProductServiceImpl.java`. Looking in the source code (see `microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java`),
    line 96 looks as follows:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is the root cause of the error. We did know this in advance, but now we
    have seen how we can navigate to it as well.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the problem is quite simple to resolve: simply omit the `faultPercent`
    parameter in the request to the API. In other cases, the resolution of the root
    cause can be much harder to figure out!'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: This concludes this chapter on using the EFK stack for centralized logging.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the importance of collecting log records from
    microservices in a system landscape into a common centralized database where analysis
    and searches among the stored log records can be performed. We used the EFK stack,
    that is, Elasticsearch, Fluentd, and Kibana, to collect, process, store, analyze,
    and search for log records.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd was used to collect log records not only from our microservices but
    also from the various supporting containers and processes in the Kubernetes cluster.
    Elasticsearch was used as a text search engine. Together with Kibana, we saw how
    easy it is to get an understanding of what types of log records we have collected.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to use Kibana to perform important tasks such as finding
    related log records from cooperating microservices and how to perform root cause
    analysis, that is, finding the real problem for an error message. Finally, we
    learned how to update the configuration of Fluentd and how to get the change reflected
    by the executing Fluentd pod.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Being able to collect and analyze log records in this way is an important capability
    in a production environment, but these types of activities are always done afterward,
    once the log record has been collected. Another important capability is to be
    able to monitor the current health of the microservices, that is, collect and
    visualize runtime metrics in terms of the use of hardware resources, response
    times, and so on. We touched on this subject in the previous chapter, [Chapter
    18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service Mesh to Improve
    Observability and Management*, and in the next chapter, [Chapter 20](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml),
    *Monitoring Microservices*, we will learn more about monitoring microservices.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A user searched for ERROR log messages in the `hands-on` namespace for the last
    30 days using the search criteria shown in the following screenshot, but none
    were found. Why?
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6d818708-6286-4a4c-b9f5-1ad9d67581cd.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: A user has found a log record of interest. How can the user find related log
    records from this and other microservices, for example, that come from processing
    an external API request?
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/292c8785-1180-40d6-8ad2-2b7e564cc8ef.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: A user has found a log record that seems to indicate the root cause of a problem
    that was reported by an end user. How can the user find the stack trace that shows
    wherein the source code the error occurred?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6e935782-d0e3-46aa-8d97-141af6289a9c.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: Why doesn't the following Fluentd configuration element work?
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How can you determine whether Elasticsearch is up and running?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suddenly, you lose connection to Kibana from your web browser. What caused this
    problem?
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

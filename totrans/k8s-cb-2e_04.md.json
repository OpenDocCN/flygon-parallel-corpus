["```\n// etcd installation script\n$ cat install-etcd.sh\nETCD_VER=v3.3.0\n\n# ${DOWNLOAD_URL} could be ${GOOGLE_URL} or ${GITHUB_URL}\nGOOGLE_URL=https://storage.googleapis.com/etcd\nGITHUB_URL=https://github.com/coreos/etcd/releases/download\nDOWNLOAD_URL=${GOOGLE_URL}\n\n# delete tmp files\nrm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\nrm -rf /tmp/etcd && rm -rf /etc/etcd && mkdir -p /etc/etcd\n\ncurl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\ntar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /etc/etcd --strip-components=1\nrm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\n\n# check etcd version\n/etc/etcd/etcd --version\n```", "```\n// install etcd on linux\n# sudo sh install-etcd.sh\n\u2026\netcd Version: 3.3.0\nGit SHA: c23606781\nGo Version: go1.9.3\nGo OS/Arch: linux/amd64\n```", "```\n$ export PATH=/etc/etcd:$PATH\n$ export ETCDCTL_API=3\n```", "```\n// sample config, put under $repo/config/req-csr.json\n$ cat config/req-csr.json\n{\n  \"CN\": \"etcd\",\n  \"hosts\": [\n    \"172.31.3.80\",\n    \"172.31.14.133\",\n    \"172.31.13.239\"\n  ],\n  \"key\": {\n    \"algo\": \"ecdsa\",\n    \"size\": 384\n  },\n  \"names\": [\n    {\n      \"O\": \"autogenerated\",\n      \"OU\": \"etcd cluster\",\n      \"L\": \"the internet\"\n    }\n  ]\n}\n```", "```\n$ export GOPATH=$HOME/go\n$ make\n```", "```\n// set as environment variables, or alternatively, passing by \u2013-initial-cluster and \u2013-initial-cluster-state parameters inside launch command.\n# ETCD_INITIAL_CLUSTER=\"etcd0=http://172.31.3.80:2380,etcd1=http://172.31.14.133:2380,etcd2=http://172.31.13.239:2380\"\nETCD_INITIAL_CLUSTER_STATE=new\n```", "```\n// first node: 172.31.3.80\n# etcd --name etcd0 --initial-advertise-peer-urls https://172.31.3.80:2380 \\\n  --listen-peer-urls https://172.31.3.80:2380 \\\n  --listen-client-urls https://172.31.3.80:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://172.31.3.80:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n\n```", "```\n2018-02-06 22:15:20.508687 I | etcdmain: etcd Version: 3.3.0\n2018-02-06 22:15:20.508726 I | etcdmain: Git SHA: c23606781\n2018-02-06 22:15:20.508794 I | etcdmain: Go Version: go1.9.3\n2018-02-06 22:15:20.508824 I | etcdmain: Go OS/Arch: linux/amd64\n\u2026\n2018-02-06 22:15:21.439067 N | etcdserver/membership: set the initial cluster version to 3.0\n2018-02-06 22:15:21.439134 I | etcdserver/api: enabled capabilities for version 3.0\n\n```", "```\n// second node: 172.31.14.133\n$ etcd --name etcd1 --initial-advertise-peer-urls https://172.31.14.133:2380 \\\n  --listen-peer-urls https://172.31.14.133:2380 \\\n  --listen-client-urls https://172.31.14.133:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://172.31.14.133:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```", "```\n2018-02-06 22:15:20.646320 I | etcdserver: starting member ce7c9e3024722f01 in cluster a7e82f7083dba2c1\n2018-02-06 22:15:20.646384 I | raft: ce7c9e3024722f01 became follower at term 0\n2018-02-06 22:15:20.646397 I | raft: newRaft ce7c9e3024722f01 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]\n2018-02-06 22:15:20.646403 I | raft: ce7c9e3024722f01 became follower at term 1\n\u2026\n2018-02-06 22:15:20.675928 I | rafthttp: starting peer 25654e0e7ea045f8...\n2018-02-06 22:15:20.676024 I | rafthttp: started HTTP pipelining with peer 25654e0e7ea045f8\n2018-02-06 22:15:20.678515 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (writer)\n2018-02-06 22:15:20.678717 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (writer)\n```", "```\n// third node: 172.31.13.239\n$ etcd --name etcd2 --initial-advertise-peer-urls https://172.31.13.239:2380 \\\n  --listen-peer-urls https://172.31.13.239:2380 \\\n  --listen-client-urls https://172.31.13.239:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://172.31.13.239:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n\n// in node2 console, it listens and receives new member (4834416c2c1e751e) added.\n2018-02-06 22:15:20.679548 I | rafthttp: starting peer 4834416c2c1e751e...\n2018-02-06 22:15:20.679642 I | rafthttp: started HTTP pipelining with peer 4834416c2c1e751e\n2018-02-06 22:15:20.679923 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (stream Message reader)\n2018-02-06 22:15:20.680190 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (stream MsgApp v2 reader)\n2018-02-06 22:15:20.680364 I | rafthttp: started streaming with peer 4834416c2c1e751e (writer)\n2018-02-06 22:15:20.681880 I | rafthttp: started peer 4834416c2c1e751e\n2018-02-06 22:15:20.681909 I | rafthttp: added peer 4834416c2c1e751e\nAfter all nodes are in, it'll start to elect the leader inside the cluster, we could find it in the logs:\n2018-02-06 22:15:21.334985 I | raft: raft.node: ce7c9e3024722f01 elected leader 4834416c2c1e751e at term 27\n...\n2018-02-06 22:17:21.510271 N | etcdserver/membership: updated the cluster version from 3.0 to 3.3\n2018-02-06 22:17:21.510343 I | etcdserver/api: enabled capabilities for version 3.3\n```", "```\n$ etcdctl cluster-health\nmember 25654e0e7ea045f8is healthy: got healthy result from http://172.31.3.80:2379\nmember ce7c9e3024722f01 is healthy: got healthy result from http://172.31.14.133:2379\nmember 4834416c2c1e751e is healthy: got healthy result from http://172.31.13.239:2379\n```", "```\n// get a request URL\n# curl -w \"n\" 'https://discovery.etcd.io/new?size=3'\nhttps://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8\n```", "```\n// in node1, 127.0.0.1 is used for internal client listeneretcd -name ip-172-31-3-80 -initial-advertise-peer-urls http://172.31.3.80:2380  -listen-peer-urls http://172.31.3.80:2380  -listen-client-urls http://172.31.3.80:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.3.80:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8\n\n// in node2, 127.0.0.1 is used for internal client listener\netcd -name ip-172-31-14-133 -initial-advertise-peer-urls http://172.31.14.133:2380  -listen-peer-urls http://172.31.14.133:2380  -listen-client-urls http://172.31.14.133:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.14.133:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8\n\n// in node3, 127.0.0.1 is used for internal client listener\netcd -name ip-172-31-13-239 -initial-advertise-peer-urls http://172.31.13.239:2380  -listen-peer-urls http://172.31.13.239:2380  -listen-client-urls http://172.31.13.239:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.13.239:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8\n```", "```\n2018-02-10 04:58:03.819963 I | etcdmain: etcd Version: 3.3.0\n...\n2018-02-10 04:58:03.820400 I | embed: listening for peers on http://172.31.3.80:2380\n2018-02-10 04:58:03.820427 I | embed: listening for client requests on\n127.0.0.1:2379\n2018-02-10 04:58:03.820444 I | embed: listening for client requests on 172.31.3.80:2379\n2018-02-10 04:58:03.947753 N | discovery: found self f60c98e749d41d1b in the cluster\n2018-02-10 04:58:03.947771 N | discovery: found 1 peer(s), waiting for 2 more\n2018-02-10 04:58:22.289571 N | discovery: found peer 6645fe871c820573 in the cluster\n2018-02-10 04:58:22.289628 N | discovery: found 2 peer(s), waiting for 1 more\n2018-02-10 04:58:36.907165 N | discovery: found peer 1ce61c15bdbb20b2 in the cluster\n2018-02-10 04:58:36.907192 N | discovery: found 3 needed peer(s)\n...\n2018-02-10 04:58:36.931319 I | etcdserver/membership: added member 1ce61c15bdbb20b2 [http://172.31.13.239:2380] to cluster 29c0e2579c2f9563\n2018-02-10 04:58:36.931422 I | etcdserver/membership: added member 6645fe871c820573 [http://172.31.14.133:2380] to cluster 29c0e2579c2f9563\n2018-02-10 04:58:36.931494 I | etcdserver/membership: added member f60c98e749d41d1b [http://172.31.3.80:2380] to cluster 29c0e2579c2f9563\n2018-02-10 04:58:37.116189 I | raft: f60c98e749d41d1b became leader at term 2\n```", "```\n// in node 2\n2018-02-10 04:58:37.118601 I | raft: raft.node: 6645fe871c820573 elected leader f60c98e749d41d1b at term 2\n```", "```\n# etcdctl member list\n1ce61c15bdbb20b2: name=ip-172-31-13-239 peerURLs=http://172.31.13.239:2380 clientURLs=http://172.31.13.239:2379 isLeader=false\n6645fe871c820573: name=ip-172-31-14-133 peerURLs=http://172.31.14.133:2380 clientURLs=http://172.31.14.133:2379 isLeader=false\nf60c98e749d41d1b: name=ip-172-31-3-80 peerURLs=http://172.31.3.80:2380 clientURLs=http://172.31.3.80:2379 isLeader=true\n```", "```\n# etcdctl cluster-health\nmember 1ce61c15bdbb20b2 is healthy: got healthy result from http://172.31.13.239:2379\nmember 6645fe871c820573 is healthy: got healthy result from http://172.31.14.133:2379\nmember f60c98e749d41d1b is healthy: got healthy result from http://172.31.3.80:2379\ncluster is healthy\n```", "```\n# etcdctl member remove f60c98e749d41d1b\n```", "```\n# etcdctl member list\n1ce61c15bdbb20b2: name=ip-172-31-13-239 peerURLs=http://172.31.13.239:2380 clientURLs=http://172.31.13.239:2379 isLeader=false\n6645fe871c820573: name=ip-172-31-14-133 peerURLs=http://172.31.14.133:2380 clientURLs=http://172.31.14.133:2379 isLeader=true\n```", "```\n# curl http://127.0.0.1:2379/v2/stats/leader\n{\"leader\":\"6645fe871c820573\",\"followers\":{\"1ce61c15bdbb20b2\":{\"latency\":{\"current\":0.002463,\"average\":0.0038775,\"standardDeviation\":0.0014144999999999997,\"minimum\":0.002463,\"maximum\":0.005292},\"counts\":{\"fail\":0,\"success\":2}}}}\n```", "```\n# cat inventory/inventory.cfg\nmy-master-1 ansible_ssh_host=<master_ip>\nmy-node-1 ansible_ssh_host=<node_ip>\nmy-etcd-1 ansible_ssh_host=<etcd1_ip>\nmy-etcd-2 ansible_ssh_host=<etcd2_ip>\nmy-etcd-3 ansible_ssh_host=<etcd3_ip>\n\n[kube-master]\nmy-master-1\n\n[etcd]\nmy-etcd-1\nmy-etcd-2\nmy-etcd-3\n\n[kube-node]\nmy-master-1\nmy-node-1\n```", "```\n// provision a cluster \n$ ansible-playbook -b -i inventory/inventory.cfg cluster.yml\n```", "```\netcdClusters:\n  - etcdMembers:\n    - instanceGroup: my-master-us-east-1a\n      name: my-etcd-1\n    - instanceGroup: my-master-us-east-1b\n      name: my-etcd-2\n    - instanceGroup: my-master-us-east-1c\n      name: my-etcd-3\n```", "```\n// you are now in the terminal of host for first master\n$ sudo systemctl enable kubelet && sudo systemctl start kubelet\n```", "```\n$ cat custom-init-1st.conf\napiVersion: kubeadm.k8s.io/v1alpha1\nkind: MasterConfiguration\napi:\n  advertiseAddress: \"<FIRST_MASTER_IP>\"\netcd:\n  endpoints:\n  - \"<ETCD_CLUSTER_ENDPOINT>\"\napiServerCertSANs:\n- \"<FIRST_MASTER_IP>\"\n- \"<SECOND_MASTER_IP>\"\n- \"<LOAD_BALANCER_IP>\"\n- \"127.0.0.1\"\ntoken: \"<CUSTOM_TOKEN: [a-z0-9]{6}.[a-z0-9]{16}>\"\ntokenTTL: \"0\"\napiServerExtraArgs:\n  endpoint-reconciler-type: \"lease\"\n```", "```\n$ sudo kubeadm init --config=custom-init-1st.conf\n```", "```\n$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```", "```\n$ kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml\n```", "```\n// now you're in the second master\n$ sudo systemctl enable kubelet && sudo systemctl start kubelet\n```", "```\n$ sudo scp -r root@$FIRST_MASTER_IP:/etc/kubernetes/pki/* /etc/kubernetes/pki/\n```", "```\n$ sudo rm /etc/kubernetes/pki/apiserver.*\n```", "```\n// Please modify the change by your case\n$ cat custom-init-2nd.conf\napiVersion: kubeadm.k8s.io/v1alpha1\nkind: MasterConfiguration\napi:\n  advertiseAddress: \"<SECOND_MASTER_IP>\"\n...\n```", "```\n$ sudo kubeadm init --config custom-init-2nd.conf\n// copy the \"kubeadm join\" command showing in the output\n$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```", "```\n$ kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nmaster01   Ready     master    8m        v1.10.2\nmaster02   Ready     master    1m        v1.10.2\n```", "```\n// now you're in the second master\n$ sudo systemctl enable kubelet && sudo systemctl start kubelet\n```", "```\n// your join command should look similar to following one\n$ sudo kubeadm join --token <CUSTOM_TOKEN> <LOAD_BALANCER_IP>:6443 --discovery-token-ca-cert-hash sha256:<HEX_STRING>\n```", "```\n// you can see the node is added\n$ kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nmaster01   Ready     master    4h        v1.10.2\nmaster02   Ready     master    3h        v1.10.2\nnode01     Ready     <none>    22s       v1.10.2\n```", "```\n$ kubectl get pod -n kube-system\nNAME                                      READY     STATUS    RESTARTS   AGE\ncalico-etcd-6bnrk                         1/1       Running   0          1d\ncalico-etcd-p7lpv                         1/1       Running   0          1d\ncalico-kube-controllers-d554689d5-qjht2   1/1       Running   0          1d\ncalico-node-2r2zs                         2/2       Running   0          1d\ncalico-node-97fjk                         2/2       Running   0          1d\ncalico-node-t55l8                         2/2       Running   0          1d\nkube-apiserver-master01                   1/1       Running   0          1d\nkube-apiserver-master02                   1/1       Running   0          1d\nkube-controller-manager-master01          1/1       Running   0          1d\nkube-controller-manager-master02          1/1       Running   0          1d\nkube-dns-6f4fd4bdf-xbfvp                  3/3       Running   0          1d\nkube-proxy-8jk69                          1/1       Running   0          1d\nkube-proxy-qbt7q                          1/1       Running   0          1d\nkube-proxy-rkxwp                          1/1       Running   0          1d\nkube-scheduler-master01                   1/1       Running   0          1d\nkube-scheduler-master02                   1/1       Running   0          1d\n```", "```\n// check flag leader-elect on master node\n$ sudo cat /etc/kubernetes/manifests/kube-controller-manager.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: \"\"\n  creationTimestamp: null\n  labels:\n    component: kube-controller-manager\n    tier: control-plane\n  name: kube-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-controller-manager\n...\n    - --leader-elect=true...\n```", "```\n// ep is the abbreviation of resource type \"endpoints\"\n$ kubectl get ep -n kube-system\nNAME                      ENDPOINTS                                   AGE\ncalico-etcd               192.168.122.201:6666,192.168.122.202:6666   1d\nkube-controller-manager   <none>                                      1d\nkube-dns                  192.168.241.67:53,192.168.241.67:53         1d\nkube-scheduler            <none>                                      1d\n\n// check endpoint of controller-manager with YAML output format\n$ kubectl get ep kube-controller-manager -n kube-system -o yaml\napiVersion: v1\nkind: Endpoints\nmetadata:\n  annotations:\n    control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\"master01_bf4e22f7-4f56-11e8-aee3-52540048ed9b\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2018-05-04T04:51:11Z\",\"renewTime\":\"2018-05-04T05:28:34Z\",\"leaderTransitions\":0}'\n  creationTimestamp: 2018-05-04T04:51:11Z\n  name: kube-controller-manager\n  namespace: kube-system\n  resourceVersion: \"3717\"\n  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager\n  uid: 5e2717b0-0609-11e8-b36f-52540048ed9b\n```", "```\n// your pod should be named as kube-controller-manager-<HOSTNAME OF MASTER>\n$ kubectl logs kube-controller-manager-master01 -n kube-system | grep \"leader\"\nI0504 04:51:03.015151 1 leaderelection.go:175] attempting to acquire leader lease kube-system/kube-controller-manager...\n...\nI0504 04:51:11.627737 1 event.go:218] Event(v1.ObjectReference{Kind:\"Endpoints\", Namespace:\"kube-system\", Name:\"kube-controller-manager\", UID:\"5e2717b0-0609-11e8-b36f-52540048ed9b\", APIVersion:\"v1\", ResourceVersion:\"187\", FieldPath:\"\"}): type: 'Normal' reason: 'LeaderElection' master01_bf4e22f7-4f56-11e8-aee3-52540048ed9b became leader\n```", "```\n// jump into the master node of leader\n// temporary move the configuration file out of kubeadm's control\n$ sudo mv /etc/kubernetes/manifests/kube-controller-manager.yaml ./\n// check the endpoint\n$ kubectl get ep kube-controller-manager -n kube-system -o yaml\napiVersion: v1\nkind: Endpoints\nmetadata:\n  annotations:\n    control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\"master02_4faf95c7-4f5b-11e8-bda3-525400b06612\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2018-05-04T05:37:03Z\",\"renewTime\":\"2018-05-04T05:37:47Z\",\"leaderTransitions\":1}'\n  creationTimestamp: 2018-05-04T04:51:11Z\n  name: kube-controller-manager\n  namespace: kube-system\n  resourceVersion: \"4485\"\n  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager\n  uid: 5e2717b0-0609-11e8-b36f-52540048ed9b\nsubsets: null\n```", "```\n$ kubectl logs kube-controller-manager-master01 -n kube-system\nI0504 05:40:10.218946 1 controllermanager.go:116] Version: v1.10.2\nW0504 05:40:10.219688 1 authentication.go:55] Authentication is disabled\nI0504 05:40:10.219702 1 insecure_serving.go:44] Serving insecurely on 127.0.0.1:10252\nI0504 05:40:10.219965 1 leaderelection.go:175] attempting to acquire leader lease kube-system/kube-controller-manager...\n```"]
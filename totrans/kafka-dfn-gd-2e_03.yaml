- en: Chapter 1\. Meet Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every enterprise is powered by data. We take information in, analyze it, manipulate
    it, and create more as output. Every application creates data, whether it is log
    messages, metrics, user activity, outgoing messages, or something else. Every
    byte of data has a story to tell, something of importance that will inform the
    next thing to be done. In order to know what that is, we need to get the data
    from where it is created to where it can be analyzed. We see this every day on
    websites like Amazon, where our clicks on items of interest to us are turned into
    recommendations that are shown to us a little later.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The faster we can do this, the more agile and responsive our organizations can
    be. The less effort we spend on moving data around, the more we can focus on the
    core business at hand. This is why the pipeline is a critical component in the
    data-driven enterprise. How we move the data becomes nearly as important as the
    data itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Any time scientists disagree, it’s because we have insufficient data. Then we
    can agree on what kind of data to get; we get the data; and the data solves the
    problem. Either I’m right, or you’re right, or we’re both wrong. And we move on.
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '>'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Neil deGrasse Tyson
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Publish/Subscribe Messaging
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before discussing the specifics of Apache Kafka, it is important for us to understand
    the concept of publish/subscribe messaging and why it is a critical component
    of data-driven applications. *Publish/subscribe (pub/sub) messaging* is a pattern
    that is characterized by the sender (publisher) of a piece of data (message) not
    specifically directing it to a receiver. Instead, the publisher classifies the
    message somehow, and that receiver (subscriber) subscribes to receive certain
    classes of messages. Pub/sub systems often have a broker, a central point where
    messages are published, to facilitate this pattern.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: How It Starts
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many use cases for publish/subscribe start out the same way: with a simple
    message queue or interprocess communication channel. For example, you create an
    application that needs to send monitoring information somewhere, so you open a
    direct connection from your application to an app that displays your metrics on
    a dashboard, and push metrics over that connection, as seen in [Figure 1-1](#fig-1-singleconn).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0101](assets/kdg2_0101.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. A single, direct metrics publisher
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a simple solution to a simple problem that works when you are getting
    started with monitoring. Before long, you decide you would like to analyze your
    metrics over a longer term, and that doesn’t work well in the dashboard. You start
    a new service that can receive metrics, store them, and analyze them. In order
    to support this, you modify your application to write metrics to both systems.
    By now you have three more applications that are generating metrics, and they
    all make the same connections to these two services. Your coworker thinks it would
    be a good idea to do active polling of the services for alerting as well, so you
    add a server on each of the applications to provide metrics on request. After
    a while, you have more applications that are using those servers to get individual
    metrics and use them for various purposes. This architecture can look much like
    [Figure 1-2](#fig-2-multiconn), with connections that are even harder to trace.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0102](assets/kdg2_0102.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Many metrics publishers, using direct connections
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The technical debt built up here is obvious, so you decide to pay some of it
    back. You set up a single application that receives metrics from all the applications
    out there, and provide a server to query those metrics for any system that needs
    them. This reduces the complexity of the architecture to something similar to
    [Figure 1-3](#fig-3-single-pubsub). Congratulations, you have built a publish/subscribe
    messaging system!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0103](assets/kdg2_0103.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. A metrics publish/subscribe system
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Individual Queue Systems
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the same time that you have been waging this war with metrics, one of your
    coworkers has been doing similar work with log messages. Another has been working
    on tracking user behavior on the frontend website and providing that information
    to developers who are working on machine learning, as well as creating some reports
    for management. You have all followed a similar path of building out systems that
    decouple the publishers of the information from the subscribers to that information.
    [Figure 1-4](#fig-4-multi-pubsub) shows such an infrastructure, with three separate
    pub/sub systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0104](assets/kdg2_0104.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Multiple publish/subscribe systems
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is certainly a lot better than utilizing point-to-point connections (as
    in [Figure 1-2](#fig-2-multiconn)), but there is a lot of duplication. Your company
    is maintaining multiple systems for queuing data, all of which have their own
    individual bugs and limitations. You also know that there will be more use cases
    for messaging coming soon. What you would like to have is a single centralized
    system that allows for publishing generic types of data, which will grow as your
    business grows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Enter Kafka
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka was developed as a publish/subscribe messaging system designed
    to solve this problem. It is often described as a “distributed commit log” or
    more recently as a “distributing streaming platform.” A filesystem or database
    commit log is designed to provide a durable record of all transactions so that
    they can be replayed to consistently build the state of a system. Similarly, data
    within Kafka is stored durably, in order, and can be read deterministically. In
    addition, the data can be distributed within the system to provide additional
    protections against failures, as well as significant opportunities for scaling
    performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Messages and Batches
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The unit of data within Kafka is called a *message*. If you are approaching
    Kafka from a database background, you can think of this as similar to a *row*
    or a *record*. A message is simply an array of bytes as far as Kafka is concerned,
    so the data contained within it does not have a specific format or meaning to
    Kafka. A message can have an optional piece of metadata, which is referred to
    as a *key*. The key is also a byte array and, as with the message, has no specific
    meaning to Kafka. Keys are used when messages are to be written to partitions
    in a more controlled manner. The simplest such scheme is to generate a consistent
    hash of the key and then select the partition number for that message by taking
    the result of the hash modulo the total number of partitions in the topic. This
    ensures that messages with the same key are always written to the same partition
    (provided that the partition count does not change).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'For efficiency, messages are written into Kafka in batches. A *batch* is just
    a collection of messages, all of which are being produced to the same topic and
    partition. An individual round trip across the network for each message would
    result in excessive overhead, and collecting messages together into a batch reduces
    this. Of course, this is a trade-off between latency and throughput: the larger
    the batches, the more messages that can be handled per unit of time, but the longer
    it takes an individual message to propagate. Batches are also typically compressed,
    providing more efficient data transfer and storage at the cost of some processing
    power. Both keys and batches are discussed in more detail in [Chapter 3](ch03.html#writing_messages_to_kafka).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Schemas
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While messages are opaque byte arrays to Kafka itself, it is recommended that
    additional structure, or schema, be imposed on the message content so that it
    can be easily understood. There are many options available for message *schema*,
    depending on your application’s individual needs. Simplistic systems, such as
    JavaScript Object Notation (JSON) and Extensible Markup Language (XML), are easy
    to use and human readable. However, they lack features such as robust type handling
    and compatibility between schema versions. Many Kafka developers favor the use
    of Apache Avro, which is a serialization framework originally developed for Hadoop.
    Avro provides a compact serialization format, schemas that are separate from the
    message payloads and that do not require code to be generated when they change,
    and strong data typing and schema evolution, with both backward and forward compatibility.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: A consistent data format is important in Kafka, as it allows writing and reading
    messages to be decoupled. When these tasks are tightly coupled, applications that
    subscribe to messages must be updated to handle the new data format, in parallel
    with the old format. Only then can the applications that publish the messages
    be updated to utilize the new format. By using well-defined schemas and storing
    them in a common repository, the messages in Kafka can be understood without coordination.
    Schemas and serialization are covered in more detail in [Chapter 3](ch03.html#writing_messages_to_kafka).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Topics and Partitions
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Messages in Kafka are categorized into *topics*. The closest analogies for a
    topic are a database table or a folder in a filesystem. Topics are additionally
    broken down into a number of *partitions*. Going back to the “commit log” description,
    a partition is a single log. Messages are written to it in an append-only fashion
    and are read in order from beginning to end. Note that as a topic typically has
    multiple partitions, there is no guarantee of message ordering across the entire
    topic, just within a single partition. [Figure 1-5](#fig-5-partitions) shows a
    topic with four partitions, with writes being appended to the end of each one.
    Partitions are also the way that Kafka provides redundancy and scalability. Each
    partition can be hosted on a different server, which means that a single topic
    can be scaled horizontally across multiple servers to provide performance far
    beyond the ability of a single server. Additionally, partitions can be replicated,
    such that different servers will store a copy of the same partition in case one
    server fails.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0105](assets/kdg2_0105.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Representation of a topic with multiple partitions
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term *stream* is often used when discussing data within systems like Kafka.
    Most often, a stream is considered to be a single topic of data, regardless of
    the number of partitions. This represents a single stream of data moving from
    the producers to the consumers. This way of referring to messages is most common
    when discussing stream processing, which is when frameworks—some of which are
    Kafka Streams, Apache Samza, and Storm—operate on the messages in real time. This
    method of operation can be compared to the way offline frameworks, namely Hadoop,
    are designed to work on bulk data at a later time. An overview of stream processing
    is provided in [Chapter 14](ch14.html#stream_processing).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Producers and Consumers
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kafka clients are users of the system, and there are two basic types: producers
    and consumers. There are also advanced client APIs—Kafka Connect API for data
    integration and Kafka Streams for stream processing. The advanced clients use
    producers and consumers as building blocks and provide higher-level functionality
    on top.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '*Producers* create new messages. In other publish/subscribe systems, these
    may be called *publishers* or *writers*. A message will be produced to a specific
    topic. By default, the producer will balance messages over all partitions of a
    topic evenly. In some cases, the producer will direct messages to specific partitions.
    This is typically done using the message key and a partitioner that will generate
    a hash of the key and map it to a specific partition. This ensures that all messages
    produced with a given key will get written to the same partition. The producer
    could also use a custom partitioner that follows other business rules for mapping
    messages to partitions. Producers are covered in more detail in [Chapter 3](ch03.html#writing_messages_to_kafka).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '*Consumers* read messages. In other publish/subscribe systems, these clients
    may be called *subscribers* or *readers*. The consumer subscribes to one or more
    topics and reads the messages in the order in which they were produced to each
    partition. The consumer keeps track of which messages it has already consumed
    by keeping track of the offset of messages. The *offset*—an integer value that
    continually increases—is another piece of metadata that Kafka adds to each message
    as it is produced. Each message in a given partition has a unique offset, and
    the following message has a greater offset (though not necessarily monotonically
    greater). By storing the next possible offset for each partition, typically in
    Kafka itself, a consumer can stop and restart without losing its place.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Consumers work as part of a *consumer group*, which is one or more consumers
    that work together to consume a topic. The group ensures that each partition is
    only consumed by one member. In [Figure 1-6](#fig-6-consumer), there are three
    consumers in a single group consuming a topic. Two of the consumers are working
    from one partition each, while the third consumer is working from two partitions.
    The mapping of a consumer to a partition is often called *ownership* of the partition
    by the consumer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In this way, consumers can horizontally scale to consume topics with a large
    number of messages. Additionally, if a single consumer fails, the remaining members
    of the group will reassign the partitions being consumed to take over for the
    missing member. Consumers and consumer groups are discussed in more detail in
    [Chapter 4](ch04.html#reading_data_from_kafka).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0106](assets/kdg2_0106.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. A consumer group reading from a topic
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Brokers and Clusters
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A single Kafka server is called a *broker*. The broker receives messages from
    producers, assigns offsets to them, and writes the messages to storage on disk.
    It also services consumers, responding to fetch requests for partitions and responding
    with the messages that have been published. Depending on the specific hardware
    and its performance characteristics, a single broker can easily handle thousands
    of partitions and millions of messages per second.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Kafka brokers are designed to operate as part of a *cluster*. Within a cluster
    of brokers, one broker will also function as the cluster *controller* (elected
    automatically from the live members of the cluster). The controller is responsible
    for administrative operations, including assigning partitions to brokers and monitoring
    for broker failures. A partition is owned by a single broker in the cluster, and
    that broker is called the *leader* of the partition. A replicated partition (as
    seen in [Figure 1-7](#fig-7-replication)) is assigned to additional brokers, called
    *followers* of the partition. Replication provides redundancy of messages in the
    partition, such that one of the followers can take over leadership if there is
    a broker failure. All producers must connect to the leader in order to publish
    messages, but consumers may fetch from either the leader or one of the followers.
    Cluster operations, including partition replication, are covered in detail in
    [Chapter 7](ch07.html#reliable_data_delivery).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0107](assets/kdg2_0107.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. Replication of partitions in a cluster
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A key feature of Apache Kafka is that of *retention*, which is the durable storage
    of messages for some period of time. Kafka brokers are configured with a default
    retention setting for topics, either retaining messages for some period of time
    (e.g., 7 days) or until the partition reaches a certain size in bytes (e.g., 1
    GB). Once these limits are reached, messages are expired and deleted. In this
    way, the retention configuration defines a minimum amount of data available at
    any time. Individual topics can also be configured with their own retention settings
    so that messages are stored for only as long as they are useful. For example,
    a tracking topic might be retained for several days, whereas application metrics
    might be retained for only a few hours. Topics can also be configured as *log
    compacted*, which means that Kafka will retain only the last message produced
    with a specific key. This can be useful for changelog-type data, where only the
    last update is interesting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Clusters
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As Kafka deployments grow, it is often advantageous to have multiple clusters.
    There are several reasons why this can be useful:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Segregation of types of data
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolation for security requirements
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple datacenters (disaster recovery)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When working with multiple datacenters in particular, it is often required that
    messages be copied between them. In this way, online applications can have access
    to user activity at both sites. For example, if a user changes public information
    in their profile, that change will need to be visible regardless of the datacenter
    in which search results are displayed. Or, monitoring data can be collected from
    many sites into a single central location where the analysis and alerting systems
    are hosted. The replication mechanisms within the Kafka clusters are designed
    only to work within a single cluster, not between multiple clusters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka project includes a tool called *MirrorMaker*, used for replicating
    data to other clusters. At its core, MirrorMaker is simply a Kafka consumer and
    producer, linked together with a queue. Messages are consumed from one Kafka cluster
    and produced to another. [Figure 1-8](#fig-8-tiers) shows an example of an architecture
    that uses MirrorMaker, aggregating messages from two local clusters into an aggregate
    cluster and then copying that cluster to other datacenters. The simple nature
    of the application belies its power in creating sophisticated data pipelines,
    which will be detailed further in [Chapter 9](ch09.html#building_data_pipelines).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0108](assets/kdg2_0108.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. Multiple datacenters architecture
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why Kafka?
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many choices for publish/subscribe messaging systems, so what makes
    Apache Kafka a good choice?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Producers
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka is able to seamlessly handle multiple producers, whether those clients
    are using many topics or the same topic. This makes the system ideal for aggregating
    data from many frontend systems and making it consistent. For example, a site
    that serves content to users via a number of microservices can have a single topic
    for page views that all services can write to using a common format. Consumer
    applications can then receive a single stream of page views for all applications
    on the site without having to coordinate consuming from multiple topics, one for
    each application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Consumers
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to multiple producers, Kafka is designed for multiple consumers
    to read any single stream of messages without interfering with each other client.
    This is in contrast to many queuing systems where once a message is consumed by
    one client, it is not available to any other. Multiple Kafka consumers can choose
    to operate as part of a group and share a stream, assuring that the entire group
    processes a given message only once.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Disk-Based Retention
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not only can Kafka handle multiple consumers, but durable message retention
    means that consumers do not always need to work in real time. Messages are written
    to disk and will be stored with configurable retention rules. These options can
    be selected on a per-topic basis, allowing for different streams of messages to
    have different amounts of retention depending on the consumer needs. Durable retention
    means that if a consumer falls behind, either due to slow processing or a burst
    in traffic, there is no danger of losing data. It also means that maintenance
    can be performed on consumers, taking applications offline for a short period
    of time, with no concern about messages backing up on the producer or getting
    lost. Consumers can be stopped, and the messages will be retained in Kafka. This
    allows them to restart and pick up processing messages where they left off with
    no data loss.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka不仅可以处理多个消费者，而且持久的消息保留意味着消费者不总是需要实时工作。消息被写入磁盘，并且将根据可配置的保留规则进行存储。这些选项可以根据主题选择，允许不同的消息流具有不同的保留量，以满足消费者的需求。持久的保留意味着如果消费者落后，无论是由于处理速度慢还是流量激增，都不会丢失数据。这也意味着可以对消费者进行维护，将应用程序离线一小段时间，而不用担心生产者上的消息积压或丢失。消费者可以停止，消息将被保留在Kafka中。这使它们可以重新启动并在离开时继续处理消息，而不会丢失数据。
- en: Scalable
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展
- en: Kafka’s flexible scalability makes it easy to handle any amount of data. Users
    can start with a single broker as a proof of concept, expand to a small development
    cluster of three brokers, and move into production with a larger cluster of tens
    or even hundreds of brokers that grows over time as the data scales up. Expansions
    can be performed while the cluster is online, with no impact on the availability
    of the system as a whole. This also means that a cluster of multiple brokers can
    handle the failure of an individual broker and continue servicing clients. Clusters
    that need to tolerate more simultaneous failures can be configured with higher
    replication factors. Replication is discussed in more detail in [Chapter 7](ch07.html#reliable_data_delivery).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的灵活可扩展性使其能够轻松处理任意数量的数据。用户可以从单个代理作为概念验证开始，扩展到由三个代理组成的小型开发集群，然后随着数据规模的扩大，进入由数十甚至数百个代理组成的生产集群。扩展可以在集群在线时进行，对整个系统的可用性没有影响。这也意味着多个代理组成的集群可以处理单个代理的故障并继续为客户提供服务。需要容忍更多同时故障的集群可以配置更高的复制因子。复制将在[第7章](ch07.html#reliable_data_delivery)中进行更详细的讨论。
- en: High Performance
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高性能
- en: All of these features come together to make Apache Kafka a publish/subscribe
    messaging system with excellent performance under high load. Producers, consumers,
    and brokers can all be scaled out to handle very large message streams with ease.
    This can be done while still providing subsecond message latency from producing
    a message to availability to consumers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能共同使Apache Kafka成为一个在高负载下具有出色性能的发布/订阅消息系统。生产者、消费者和代理都可以扩展以轻松处理非常大的消息流。这可以在仍然提供从生成消息到可供消费者使用的亚秒级消息延迟的情况下完成。
- en: Platform Features
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平台功能
- en: The core Apache Kafka project has also added some streaming platform features
    that can make it much easier for developers to perform common types of work. While
    not full platforms, which typically include a structured runtime environment like
    YARN, these features are in the form of APIs and libraries that provide a solid
    foundation to build on and flexibility as to where they can be run. Kafka Connect
    assists with the task of pulling data from a source data system and pushing it
    into Kafka, or pulling data from Kafka and pushing it into a sink data system.
    Kafka Streams provides a library for easily developing stream processing applications
    that are scalable and fault tolerant. Connect is discussed in [Chapter 9](ch09.html#building_data_pipelines),
    while Streams is covered in great detail in [Chapter 14](ch14.html#stream_processing).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka核心项目还添加了一些流平台功能，可以使开发人员更容易执行常见类型的工作。虽然不是完整的平台，通常包括像YARN这样的结构化运行时环境，但这些功能是以API和库的形式提供的，为构建和灵活性提供了坚实的基础，可以在其中运行。Kafka
    Connect帮助从源数据系统中提取数据并将其推送到Kafka，或者从Kafka中提取数据并将其推送到接收数据系统。Kafka Streams提供了一个库，用于轻松开发可扩展和容错的流处理应用程序。Connect在[第9章](ch09.html#building_data_pipelines)中讨论，而Streams在[第14章](ch14.html#stream_processing)中有详细介绍。
- en: The Data Ecosystem
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据生态系统
- en: Many applications participate in the environments we build for data processing.
    We have defined inputs in the form of applications that create data or otherwise
    introduce it to the system. We have defined outputs in the form of metrics, reports,
    and other data products. We create loops, with some components reading data from
    the system, transforming it using data from other sources, and then introducing
    it back into the data infrastructure to be used elsewhere. This is done for numerous
    types of data, with each having unique qualities of content, size, and usage.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序参与我们为数据处理构建的环境。我们已经定义了以应用程序形式的输入，这些应用程序创建数据或以其他方式将其引入系统。我们已经定义了以度量标准、报告和其他数据产品形式的输出。我们创建循环，一些组件从系统中读取数据，使用其他来源的数据进行转换，然后将其重新引入数据基础设施以供其他地方使用。这是针对多种类型的数据进行的，每种数据都具有独特的内容、大小和用途。
- en: Apache Kafka provides the circulatory system for the data ecosystem, as shown
    in [Figure 1-9](#fig-9-ecosystem). It carries messages between the various members
    of the infrastructure, providing a consistent interface for all clients. When
    coupled with a system to provide message schemas, producers and consumers no longer
    require tight coupling or direct connections of any sort. Components can be added
    and removed as business cases are created and dissolved, and producers do not
    need to be concerned about who is using the data or the number of consuming applications.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0109](assets/kdg2_0109.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. A big data ecosystem
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Use Cases
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activity tracking
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original use case for Kafka, as it was designed at LinkedIn, is that of
    user activity tracking. A website’s users interact with frontend applications,
    which generate messages regarding actions the user is taking. This can be passive
    information, such as page views and click tracking, or it can be more complex
    actions, such as information that a user adds to their profile. The messages are
    published to one or more topics, which are then consumed by applications on the
    backend. These applications may be generating reports, feeding machine learning
    systems, updating search results, or performing other operations that are necessary
    to provide a rich user experience.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Messaging
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kafka is also used for messaging, where applications need to send notifications
    (such as emails) to users. Those applications can produce messages without needing
    to be concerned about formatting or how the messages will actually be sent. A
    single application can then read all the messages to be sent and handle them consistently,
    including:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Formatting the messages (also known as *decorating*) using a common look and
    feel
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting multiple messages into a single notification to be sent
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying a user’s preferences for how they want to receive messages
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a single application for this avoids the need to duplicate functionality
    in multiple applications, as well as allows operations like aggregation that would
    not otherwise be possible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Metrics and logging
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kafka is also ideal for collecting application and system metrics and logs.
    This is a use case in which the ability to have multiple applications producing
    the same type of message shines. Applications publish metrics on a regular basis
    to a Kafka topic, and those metrics can be consumed by systems for monitoring
    and alerting. They can also be used in an offline system like Hadoop to perform
    longer-term analysis, such as growth projections. Log messages can be published
    in the same way and can be routed to dedicated log search systems like Elasticsearch
    or security analysis applications. Another added benefit of Kafka is that when
    the destination system needs to change (e.g., it’s time to update the log storage
    system), there is no need to alter the frontend applications or the means of aggregation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Commit log
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since Kafka is based on the concept of a commit log, database changes can be
    published to Kafka, and applications can easily monitor this stream to receive
    live updates as they happen. This changelog stream can also be used for replicating
    database updates to a remote system, or for consolidating changes from multiple
    applications into a single database view. Durable retention is useful here for
    providing a buffer for the changelog, meaning it can be replayed in the event
    of a failure of the consuming applications. Alternately, log-compacted topics
    can be used to provide longer retention by only retaining a single change per
    key.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Stream processing
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another area that provides numerous types of applications is stream processing.
    While almost all usage of Kafka can be thought of as stream processing, the term
    is typically used to refer to applications that provide similar functionality
    to map/reduce processing in Hadoop. Hadoop usually relies on aggregation of data
    over a long time frame, either hours or days. Stream processing operates on data
    in real time, as quickly as messages are produced. Stream frameworks allow users
    to write small applications to operate on Kafka messages, performing tasks such
    as counting metrics, partitioning messages for efficient processing by other applications,
    or transforming messages using data from multiple sources. Stream processing is
    covered in [Chapter 14](ch14.html#stream_processing).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Kafka’s Origin
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka was created to address the data pipeline problem at LinkedIn. It was designed
    to provide a high-performance messaging system that can handle many types of data
    and provide clean, structured data about user activity and system metrics in real
    time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Data really powers everything that we do.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '>'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Jeff Weiner, former CEO of LinkedIn
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LinkedIn’s Problem
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the example described at the beginning of this chapter, LinkedIn
    had a system for collecting system and application metrics that used custom collectors
    and open source tools for storing and presenting data internally. In addition
    to traditional metrics, such as CPU usage and application performance, there was
    a sophisticated request-tracing feature that used the monitoring system and could
    provide introspection into how a single user request propagated through internal
    applications. The monitoring system had many faults, however. This included metrics
    collection based on polling, large intervals between metrics, and no ability for
    application owners to manage their own metrics. The system was high-touch, requiring
    human intervention for most simple tasks, and inconsistent, with differing metric
    names for the same measurement across different systems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, there was a system created for tracking user activity information.
    This was an HTTP service that frontend servers would connect to periodically and
    publish a batch of messages (in XML format) to the HTTP service. These batches
    were then moved to offline processing platforms, which is where the files were
    parsed and collated. This system had many faults. The XML formatting was inconsistent,
    and parsing it was computationally expensive. Changing the type of user activity
    that was tracked required a significant amount of coordinated work between frontends
    and offline processing. Even then, the system would break constantly due to changing
    schemas. Tracking was built on hourly batching, so it could not be used in real
    time.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and user-activity tracking could not use the same backend service.
    The monitoring service was too clunky, the data format was not oriented for activity
    tracking, and the polling model for monitoring was not compatible with the push
    model for tracking. At the same time, the tracking service was too fragile to
    use for metrics, and the batch-oriented processing was not the right model for
    real-time monitoring and alerting. However, the monitoring and tracking data shared
    many traits, and correlation of the information (such as how specific types of
    user activity affected application performance) was highly desirable. A drop in
    specific types of user activity could indicate problems with the application that
    serviced it, but hours of delay in processing activity batches meant a slow response
    to these types of issues.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: At first, existing off-the-shelf open source solutions were thoroughly investigated
    to find a new system that would provide real-time access to the data and scale
    out to handle the amount of message traffic needed. Prototype systems were set
    up using ActiveMQ, but at the time it could not handle the scale. It was also
    a fragile solution for the way LinkedIn needed to use it, discovering many flaws
    in ActiveMQ that would cause the brokers to pause. These pauses would back up
    connections to clients and interfere with the ability of the applications to serve
    requests to users. The decision was made to move forward with a custom infrastructure
    for the data pipeline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The Birth of Kafka
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The development team at LinkedIn was led by Jay Kreps, a principal software
    engineer who was previously responsible for the development and open source release
    of Voldemort, a distributed key-value storage system. The initial team also included
    Neha Narkhede and, later, Jun Rao. Together, they set out to create a messaging
    system that could meet the needs of both the monitoring and tracking systems,
    and scale for the future. The primary goals were to:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Decouple producers and consumers by using a push-pull model
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide persistence for message data within the messaging system to allow multiple
    consumers
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize for high throughput of messages
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow for horizontal scaling of the system to grow as the data streams grew
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result was a publish/subscribe messaging system that had an interface typical
    of messaging systems but a storage layer more like a log-aggregation system. Combined
    with the adoption of Apache Avro for message serialization, Kafka was effective
    for handling both metrics and user-activity tracking at a scale of billions of
    messages per day. The scalability of Kafka has helped LinkedIn’s usage grow in
    excess of seven trillion messages produced (as of February 2020) and over five
    petabytes of data consumed daily.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Open Source
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka was released as an open source project on GitHub in late 2010\. As it
    started to gain attention in the open source community, it was proposed and accepted
    as an Apache Software Foundation incubator project in July of 2011\. Apache Kafka
    graduated from the incubator in October of 2012\. Since then, it has continuously
    been worked on and has found a robust community of contributors and committers
    outside of LinkedIn. Kafka is now used in some of the largest data pipelines in
    the world, including those at Netflix, Uber, and many other companies.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Widespread adoption of Kafka has created a healthy ecosystem around the core
    project as well. There are active meetup groups in dozens of countries around
    the world, providing local discussion and support of stream processing. There
    are also numerous open source projects related to Apache Kafka. LinkedIn continues
    to maintain several, including Cruise Control, Kafka Monitor, and Burrow. In addition
    to its commercial offerings, Confluent has released projects including ksqlDB,
    a schema registry, and a REST proxy under a community license (which is not strictly
    open source, as it includes use restrictions). Several of the most popular projects
    are listed in [Appendix B](app02.html#appendix_3rd_party_tools).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Commercial Engagement
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the fall of 2014, Jay Kreps, Neha Narkhede, and Jun Rao left LinkedIn to
    found Confluent, a company centered around providing development, enterprise support,
    and training for Apache Kafka. They also joined other companies (such as Heroku)
    in providing cloud services for Kafka. Confluent, through a partnership with Google,
    provides managed Kafka clusters on Google Cloud Platform, as well as similar services
    on Amazon Web Services and Azure. One of the other major initiatives of Confluent
    is to organize the Kafka Summit conference series. Started in 2016, with conferences
    held annually in the United States and London, Kafka Summit provides a place for
    the community to come together on a global scale and share knowledge about Apache
    Kafka and related projects.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The Name
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'People often ask how Kafka got its name and if it signifies anything specific
    about the application itself. Jay Kreps offered the following insight:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: I thought that since Kafka was a system optimized for writing, using a writer’s
    name would make sense. I had taken a lot of lit classes in college and liked Franz
    Kafka. Plus the name sounded cool for an open source project.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '>'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: So basically there is not much of a relationship.
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Getting Started with Kafka
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know all about Kafka and its history, we can set it up and build
    our own data pipeline. In the next chapter, we will explore installing and configuring
    Kafka. We will also cover selecting the right hardware to run Kafka on, and some
    things to keep in mind when moving to production operations.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL

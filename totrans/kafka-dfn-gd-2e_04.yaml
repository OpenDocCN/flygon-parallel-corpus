- en: Chapter 2\. Installing Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。安装Kafka
- en: This chapter describes how to get started with the Apache Kafka broker, including
    how to set up Apache ZooKeeper, which is used by Kafka for storing metadata for
    the brokers. The chapter will also cover basic configuration options for Kafka
    deployments, as well as some suggestions for selecting the correct hardware to
    run the brokers on. Finally, we cover how to install multiple Kafka brokers as
    part of a single cluster and things you should know when using Kafka in a production
    environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了如何开始使用Apache Kafka代理，包括如何设置Apache ZooKeeper，Kafka用于存储代理的元数据。本章还将涵盖Kafka部署的基本配置选项，以及一些关于选择正确硬件来运行代理的建议。最后，我们将介绍如何在单个集群的一部分安装多个Kafka代理，以及在生产环境中使用Kafka时应该了解的一些事项。
- en: Environment Setup
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: Before using Apache Kafka, your environment needs to be set up with a few prerequisites
    to ensure it runs properly. The following sections will guide you through that
    process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Apache Kafka之前，您的环境需要设置一些先决条件，以确保其正常运行。以下部分将指导您完成这个过程。
- en: Choosing an Operating System
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择操作系统
- en: Apache Kafka is a Java application and can run on many operating systems. While
    Kafka is capable of being run on many OSs, including Windows, macOS, Linux, and
    others, Linux is the recommended OS for the general use case. The installation
    steps in this chapter will focus on setting up and using Kafka in a Linux environment.
    For information on installing Kafka on Windows and macOS, see [Appendix A](app01.html#appendix_installing_other_os).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个Java应用程序，可以在许多操作系统上运行。虽然Kafka可以在许多操作系统上运行，包括Windows、macOS、Linux和其他操作系统，但Linux是一般用例的推荐操作系统。本章中的安装步骤将重点介绍在Linux环境中设置和使用Kafka。有关在Windows和macOS上安装Kafka的信息，请参阅[附录A](app01.html#appendix_installing_other_os)。
- en: Installing Java
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Java
- en: Prior to installing either ZooKeeper or Kafka, you will need a Java environment
    set up and functioning. Kafka and ZooKeeper work well with all OpenJDK-based Java
    implementations, including Oracle JDK. The latest versions of Kafka support both
    Java 8 and Java 11\. The exact version installed can be the version provided by
    your OS or one directly downloaded from the web—for example, [the Oracle website
    for the Oracle version](https://www.oracle.com/java). Though ZooKeeper and Kafka
    will work with a runtime edition of Java, it is recommended when developing tools
    and applications to have the full Java Development Kit (JDK). It is recommended
    to install the latest released patch version of your Java environment, as older
    versions may have security vulnerabilities. The installation steps will assume
    you have installed JDK version 11 update 10 deployed at */usr/java/jdk-11.0.10*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装ZooKeeper或Kafka之前，您需要设置并运行Java环境。Kafka和ZooKeeper与所有基于OpenJDK的Java实现（包括Oracle
    JDK）兼容。最新版本的Kafka支持Java 8和Java 11。安装的确切版本可以是操作系统提供的版本，也可以是直接从网络下载的版本，例如[Oracle版本的Oracle网站](https://www.oracle.com/java)。虽然ZooKeeper和Kafka可以与运行时版本的Java一起工作，但在开发工具和应用程序时建议使用完整的Java开发工具包（JDK）。建议安装Java环境的最新发布的补丁版本，因为旧版本可能存在安全漏洞。安装步骤将假定您已经安装了部署在*/usr/java/jdk-11.0.10*的JDK版本11更新10。
- en: Installing ZooKeeper
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装ZooKeeper
- en: Apache Kafka uses Apache ZooKeeper to store metadata about the Kafka cluster,
    as well as consumer client details, as shown in [Figure 2-1](#fig-1-kafkazk).
    ZooKeeper is a centralized service for maintaining configuration information,
    naming, providing distributed synchronization, and providing group services. This
    book won’t go into extensive detail about ZooKeeper but will limit explanations
    to only what is needed to operate Kafka. While it is possible to run a ZooKeeper
    server using scripts contained in the Kafka distribution, it is trivial to install
    a full version of ZooKeeper from the distribution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka使用Apache ZooKeeper存储有关Kafka集群的元数据，以及消费者客户端详细信息，如[图2-1](#fig-1-kafkazk)所示。ZooKeeper是一个集中式服务，用于维护配置信息、命名、提供分布式同步和提供组服务。本书不会详细介绍ZooKeeper，而是将解释限制在操作Kafka所需的内容。虽然可以使用Kafka分发中包含的脚本运行ZooKeeper服务器，但从分发中安装完整版本的ZooKeeper是微不足道的。
- en: '![kdg2 0201](assets/kdg2_0201.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0201](assets/kdg2_0201.png)'
- en: Figure 2-1\. Kafka and ZooKeeper
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。Kafka和ZooKeeper
- en: Kafka has been tested extensively with the stable 3.5 release of ZooKeeper and
    is regularly updated to include the latest release. In this book, we will be using
    ZooKeeper 3.5.9, which can be downloaded from the [ZooKeeper website](https://oreil.ly/iMZjR).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka已经广泛测试了稳定的ZooKeeper 3.5版本，并定期更新以包括最新版本。在本书中，我们将使用ZooKeeper 3.5.9，可以从[ZooKeeper网站](https://oreil.ly/iMZjR)下载。
- en: Standalone server
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立服务器
- en: 'ZooKeeper comes with a base example config file that will work well for most
    use cases in */usr/local/zookeeper/config/zoo_sample.cfg*. However, we will manually
    create ours with some basic settings for demo purposes in this book. The following
    example installs ZooKeeper with a basic configuration in */usr/local/zookeeper*,
    storing its data in */var/lib/zookeeper*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper附带一个基本示例配置文件，对于大多数用例来说都可以正常工作，位于*/usr/local/zookeeper/config/zoo_sample.cfg*。但是，为了演示目的，我们将在本书中手动创建一些基本设置的配置文件。以下示例在*/usr/local/zookeeper*中使用基本配置安装ZooKeeper，并将其数据存储在*/var/lib/zookeeper*中：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can now validate that ZooKeeper is running correctly in standalone mode
    by connecting to the client port and sending the four-letter command `srvr`. This
    will return basic ZooKeeper information from the running server:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过连接到客户端端口并发送四字命令`srvr`来验证ZooKeeper是否在独立模式下正确运行。这将从运行的服务器返回基本的ZooKeeper信息：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ZooKeeper ensemble
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZooKeeper集合
- en: ZooKeeper is designed to work as a cluster, called an *ensemble*, to ensure
    high availability. Due to the balancing algorithm used, it is recommended that
    ensembles contain an odd number of servers (e.g., 3, 5, and so on) as a majority
    of ensemble members (a *quorum*) must be working in order for ZooKeeper to respond
    to requests. This means that in a three-node ensemble, you can run with one node
    missing. With a five-node ensemble, you can run with two nodes missing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper被设计为作为一个名为*ensemble*的集群工作，以确保高可用性。由于使用的平衡算法，建议集群包含奇数个服务器（例如3、5等），因为大多数集群成员（*quorum*）必须正常工作才能响应ZooKeeper的请求。这意味着在一个三节点集群中，您可以运行一个节点缺失。在一个五节点集群中，您可以运行两个节点缺失。
- en: Sizing Your ZooKeeper Ensemble
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整ZooKeeper集群的大小
- en: Consider running ZooKeeper in a five-node ensemble. To make configuration changes
    to the ensemble, including swapping a node, you will need to reload nodes one
    at a time. If your ensemble cannot tolerate more than one node being down, doing
    maintenance work introduces additional risk. It is also not recommended to run
    more than seven nodes, as performance can start to degrade due to the nature of
    the consensus protocol.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在一个五节点集群中运行ZooKeeper。要对集群进行配置更改，包括交换节点，您需要逐个重新加载节点。如果您的集群不能容忍多于一个节点宕机，进行维护工作会增加额外的风险。此外，不建议运行超过七个节点，因为由于共识协议的性质，性能可能开始下降。
- en: Additionally, if you feel that five or seven nodes aren’t supporting the load
    due to too many client connections, consider adding additional observer nodes
    for help in balancing read-only traffic.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您觉得五个或七个节点无法支持由于太多客户端连接而产生的负载，请考虑添加额外的观察者节点来帮助平衡只读流量。
- en: 'To configure ZooKeeper servers in an ensemble, they must have a common configuration
    that lists all servers, and each server needs a *myid* file in the data directory
    that specifies the ID number of the server. If the hostnames of the servers in
    the ensemble are `zoo1.example.com`, `zoo2.example.com`, and `zoo3.example.com`,
    the configuration file might look like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要在集群中配置ZooKeeper服务器，它们必须有一个列出所有服务器的共同配置，每个服务器在数据目录中需要一个指定服务器ID号的*myid*文件。如果集群中的服务器的主机名是`zoo1.example.com`，`zoo2.example.com`和`zoo3.example.com`，配置文件可能如下所示：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this configuration, the `initLimit` is the amount of time to allow followers
    to connect with a leader. The `syncLimit` value limits how long out-of-sync followers
    can be with the leader. Both values are a number of `tickTime` units, which makes
    the `init​Li⁠mit` 20 × 2,000 ms, or 40 seconds. The configuration also lists each
    server in the ensemble. The servers are specified in the format `*server.X=hostname:peerPort:leaderPort*`,
    with the following parameters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，`initLimit`是允许跟随者与领导者连接的时间。`syncLimit`值限制了落后于领导者的跟随者可以有多久。这两个值都是`tickTime`单位的数字，这使得`init​Li⁠mit`为20×2,000毫秒，即40秒。配置还列出了集群中的每个服务器。服务器以`*server.X=hostname:peerPort:leaderPort*`的格式指定，具有以下参数：
- en: '`X`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`'
- en: The ID number of the server. This must be an integer, but it does not need to
    be zero-based or sequential.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的ID号。这必须是一个整数，但不需要基于零或连续。
- en: '`hostname`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostname`'
- en: The hostname or IP address of the server.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的主机名或IP地址。
- en: '`peerPort`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`peerPort`'
- en: The TCP port over which servers in the ensemble communicate with one another.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用于集群中的服务器相互通信的TCP端口。
- en: '`leaderPort`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`leaderPort`'
- en: The TCP port over which leader election is performed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行领导者选举的TCP端口。
- en: Clients only need to be able to connect to the ensemble over the `*clientPort*`,
    but the members of the ensemble must be able to communicate with one another over
    all three ports.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端只需要能够通过`*clientPort*`连接到集群，但集群的成员必须能够通过所有三个端口相互通信。
- en: In addition to the shared configuration file, each server must have a file in
    the *dataDir* directory with the name *myid*. This file must contain the ID number
    of the server, which must match the configuration file. Once these steps are complete,
    the servers will start up and communicate with one another in an ensemble.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了共享配置文件之外，每个服务器必须在*dataDir*目录中有一个名为*myid*的文件。该文件必须包含服务器的ID号，该号码必须与配置文件匹配。完成这些步骤后，服务器将启动并在集群中相互通信。
- en: Testing ZooKeeper Ensemble on a Single Machine
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单台机器上测试ZooKeeper集群
- en: It is possible to test and run a ZooKeeper ensemble on a single machine by specifying
    all hostnames in the config as `localhost` and have unique ports specified for
    `*peerPort*` and `*leaderPort*` for each instance. Additionally, a separate *zoo.cfg*
    would need to be created for each instance with a unique *dataDir* and `*clientPort*`
    defined for each instance. This can be useful for testing purposes only, but it
    is *not* recommended for production systems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在配置中指定所有主机名为`localhost`并为每个实例指定唯一的`*peerPort*`和`*leaderPort*`来在单台机器上测试和运行ZooKeeper集群。此外，每个实例都需要创建一个单独的*zoo.cfg*，其中为每个实例定义了唯一的*dataDir*和`*clientPort*`。这只对测试目的有用，但*不*建议用于生产系统。
- en: Installing a Kafka Broker
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kafka Broker
- en: Once Java and ZooKeeper are configured, you are ready to install Apache Kafka.
    The current release can be downloaded from the [Kafka website](https://oreil.ly/xLopS).
    At press time, that version is 2.8.0 running under Scala version 2.13.0\. The
    examples in this chapters are shown using version 2.7.0.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Java和ZooKeeper配置完成，您就可以安装Apache Kafka了。当前版本可以从[Kafka网站](https://oreil.ly/xLopS)下载。截至目前，该版本是在Scala版本2.13.0下运行的2.8.0版本。本章的示例是使用2.7.0版本显示的。
- en: 'The following example installs Kafka in */usr/local/kafka*, configured to use
    the ZooKeeper server started previously and to store the message log segments
    stored in */tmp/kafka-logs*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例在*/usr/local/kafka*中安装Kafka，配置为使用先前启动的ZooKeeper服务器，并将消息日志段存储在*/tmp/kafka-logs*中：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the Kafka broker is started, we can verify that it is working by performing
    some simple operations against the cluster: creating a test topic, producing some
    messages, and consuming the same messages.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Kafka经纪人启动，我们可以通过针对集群执行一些简单操作来验证它是否正常工作：创建一个测试主题，生成一些消息，并消费相同的消息。
- en: 'Create and verify a topic:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和验证主题：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Produce messages to a test topic (use Ctrl-C to stop the producer at any time):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 向测试主题生成消息（使用Ctrl-C随时停止生产者）：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Consume messages from a test topic:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从测试主题消费消息：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Deprecation of ZooKeeper Connections on Kafka CLI Utilities
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka CLI实用程序上的ZooKeeper连接的弃用
- en: If you are familiar with older versions of the Kafka utilities, you may be used
    to using a `--zookeeper` connection string. This has been deprecated in almost
    all cases. The current best practice is to use the newer `--bootstrap-server`
    option and connect directly to the Kafka broker. If you are running in a cluster,
    you can provide the host:port of any broker in the cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉Kafka实用程序的旧版本，您可能习惯于使用“--zookeeper”连接字符串。在几乎所有情况下，这已经被弃用。当前的最佳实践是使用更新的“--bootstrap-server”选项，并直接连接到Kafka经纪人。如果在集群中运行，可以提供集群中任何经纪人的主机:端口。
- en: Configuring the Broker
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置经纪人
- en: The example configuration provided with the Kafka distribution is sufficient
    to run a standalone server as a proof of concept, but most likely will not be
    sufficient for large installations. There are numerous configuration options for
    Kafka that control all aspects of setup and tuning. Most of the options can be
    left at the default settings, though, as they deal with tuning aspects of the
    Kafka broker that will not be applicable until you have a specific use case that
    requires adjusting these settings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka发行版提供的示例配置足以作为概念验证运行独立服务器，但很可能不足以满足大型安装的需求。Kafka有许多配置选项，可以控制设置和调整的各个方面。大多数选项可以保留默认设置，因为它们涉及Kafka经纪人的调整方面，直到您有特定的用例需要调整这些设置之前，这些设置都不适用。
- en: General Broker Parameters
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般经纪人参数
- en: There are several broker configuration parameters that should be reviewed when
    deploying Kafka for any environment other than a standalone broker on a single
    server. These parameters deal with the basic configuration of the broker, and
    most of them must be changed to run properly in a cluster with other brokers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署Kafka到除单个服务器上的独立经纪人之外的任何环境时，应该审查几个经纪人配置参数。这些参数涉及经纪人的基本配置，大多数必须更改才能在与其他经纪人的集群中正确运行。
- en: broker.id
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: broker.id
- en: Every Kafka broker must have an integer identifier, which is set using the `broker.id`
    configuration. By default, this integer is set to `0`, but it can be any value.
    It is essential that the integer must be unique for each broker within a single
    Kafka cluster. The selection of this number is technically arbitrary, and it can
    be moved between brokers if necessary for maintenance tasks. However, it is highly
    recommended to set this value to something intrinsic to the host so that when
    performing maintenance it is not onerous to map broker ID numbers to hosts. For
    example, if your hostnames contain a unique number (such as `host1.example.com`,
    `host2.example.com`, etc.), then `1` and `2` would be good choices for the `broker.id`
    values, respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Kafka经纪人必须有一个整数标识符，这是使用“broker.id”配置设置的。默认情况下，此整数设置为“0”，但可以是任何值。对于单个Kafka集群中的每个经纪人，这个整数必须是唯一的。选择这个数字在技术上是任意的，如果需要进行维护任务，它可以在经纪人之间移动。然而，强烈建议将此值设置为主机的某些固有值，以便在执行维护时，将经纪人ID号映射到主机上不是繁重的任务。例如，如果您的主机名包含唯一的数字（例如`host1.example.com`，`host2.example.com`等），那么分别为`broker.id`值选择`1`和`2`是不错的选择。
- en: listeners
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: listeners
- en: Older versions of Kafka used a simple `port` configuration. This can still be
    used as a backup for simple configurations but is a deprecated config. The example
    configuration file starts Kafka with a listener on TCP port 9092\. The new `listeners`
    config is a comma-separated list of URIs that we listen on with the listener names.
    If the listener name is not a common security protocol, then another config `listener.security.protocol.map`
    must also be configured. A listener is defined as `*<protocol>://<hostname>:<port>*`.
    An example of a legal `listener` config is `PLAINTEXT://localhost:9092,SSL://:9091`.
    Specifying the hostname as `0.0.0.0` will bind to all interfaces. Leaving the
    hostname empty will bind it to the default interface. Keep in mind that if a port
    lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root
    is not a recommended configuration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的旧版本使用简单的“端口”配置。这仍然可以作为简单配置的备份使用，但已经是一个不推荐的配置。示例配置文件在TCP端口9092上启动Kafka监听器。新的“listeners”配置是一个以逗号分隔的URI列表，我们使用监听器名称进行监听。如果监听器名称不是常见的安全协议，那么必须配置另一个配置“listener.security.protocol.map”。监听器被定义为“*<protocol>://<hostname>:<port>*”。合法的“listener”配置示例是“PLAINTEXT://localhost:9092,SSL://:9091”。将主机名指定为“0.0.0.0”将绑定到所有接口。将主机名留空将绑定到默认接口。请记住，如果选择低于1024的端口，Kafka必须以root用户身份启动。以root用户身份运行Kafka不是推荐的配置。
- en: zookeeper.connect
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: zookeeper.connect
- en: 'The location of the ZooKeeper used for storing the broker metadata is set using
    the `zookeeper.connect` configuration parameter. The example configuration uses
    a ZooKeeper running on port 2181 on the local host, which is specified as `localhost:2181`.
    The format for this parameter is a semicolon-separated list of `hostname:port/path`
    strings, which include:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 用于存储经纪人元数据的ZooKeeper的位置是使用“zookeeper.connect”配置参数设置的。示例配置使用在本地主机上端口2181上运行的ZooKeeper，指定为“localhost:2181”。此参数的格式是一个以分号分隔的“hostname:port/path”字符串列表，其中包括：
- en: '`hostname`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostname`'
- en: The hostname or IP address of the ZooKeeper server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper服务器的主机名或IP地址。
- en: '`port`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`port`'
- en: The client port number for the server.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的客户端端口号。
- en: '`/path`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`/path`'
- en: An optional ZooKeeper path to use as a chroot environment for the Kafka cluster.
    If it is omitted, the root path is used.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用作Kafka集群的chroot环境的可选ZooKeeper路径。如果省略，将使用根路径。
- en: If a chroot path (a path designated to act as the root directory for a given
    application) is specified and does not exist, it will be created by the broker
    when it starts up.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了chroot路径（指定为给定应用程序的根目录的路径）并且不存在，代理在启动时将创建它。
- en: Why Use a Chroot Path?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用Chroot路径？
- en: It is generally considered to be good practice to use a chroot path for the
    Kafka cluster. This allows the ZooKeeper ensemble to be shared with other applications,
    including other Kafka clusters, without a conflict. It is also best to specify
    multiple ZooKeeper servers (which are all part of the same ensemble) in this configuration.
    This allows the Kafka broker to connect to another member of the ZooKeeper ensemble
    in the event of server failure.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为使用Kafka集群的chroot路径是一个良好的做法。这允许ZooKeeper集合与其他应用程序共享，包括其他Kafka集群，而不会发生冲突。最好还要在此配置中指定多个ZooKeeper服务器（它们都是同一个集合的一部分）。这允许Kafka代理在服务器故障时连接到ZooKeeper集合的另一个成员。
- en: log.dirs
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: log.dirs
- en: Kafka persists all messages to disk, and these log segments are stored in the
    directory specified in the `log.dir` configuration. For multiple directories,
    the config `log.dirs` is preferable. If this value is not set, it will default
    back to `log.dir`. `log.dirs` is a comma-separated list of paths on the local
    system. If more than one path is specified, the broker will store partitions on
    them in a “least-used” fashion, with one partition’s log segments stored within
    the same path. Note that the broker will place a new partition in the path that
    has the least number of partitions currently stored in it, not the least amount
    of disk space used, so an even distribution of data across multiple directories
    is not guaranteed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka将所有消息持久化到磁盘，并将这些日志段存储在`log.dir`配置中指定的目录中。对于多个目录，首选使用`log.dirs`配置。如果未设置此值，它将默认回到`log.dir`。`log.dirs`是本地系统上路径的逗号分隔列表。如果指定了多个路径，代理将以“最少使用”的方式在其中存储分区，一个分区的日志段存储在同一路径中。请注意，代理将在当前存储的分区数量最少的路径中放置新的分区，而不是使用的磁盘空间最少，因此不能保证在多个目录中均匀分布数据。
- en: num.recovery.threads.per.data.dir
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: num.recovery.threads.per.data.dir
- en: 'Kafka uses a configurable pool of threads for handling log segments. Currently,
    this thread pool is used:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用可配置的线程池来处理日志段。目前，该线程池用于：
- en: When starting normally, to open each partition’s log segments
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常启动时，打开每个分区的日志段
- en: When starting after a failure, to check and truncate each partition’s log segments
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在故障后启动时，检查和截断每个分区的日志段
- en: When shutting down, to cleanly close log segments
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关闭时，清理关闭日志段
- en: By default, only one thread per log directory is used. As these threads are
    only used during startup and shutdown, it is reasonable to set a larger number
    of threads in order to parallelize operations. Specifically, when recovering from
    an unclean shutdown, this can mean the difference of several hours when restarting
    a broker with a large number of partitions! When setting this parameter, remember
    that the number configured is per log directory specified with `log.dirs`. This
    means that if `num.​recov⁠ery.threads.per.data.dir` is set to 8, and there are
    3 paths specified in `log.dirs`​, this is a total of 24 threads.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每个日志目录只使用一个线程。由于这些线程仅在启动和关闭期间使用，因此合理地设置更多的线程以并行化操作是合理的。特别是在从不干净的关闭中恢复时，这可能意味着在重新启动具有大量分区的代理时节省数小时的时间！设置此参数时，请记住配置的数量是指`log.dirs`指定的每个日志目录。这意味着如果`num.​recov⁠ery.threads.per.data.dir`设置为8，并且在`log.dirs`中指定了3个路径，则总共有24个线程。
- en: auto.create.topics.enable
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: auto.create.topics.enable
- en: 'The default Kafka configuration specifies that the broker should automatically
    create a topic under the following circumstances:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Kafka配置指定代理在以下情况下应自动创建主题：
- en: When a producer starts writing messages to the topic
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当生产者开始向主题写入消息时
- en: When a consumer starts reading messages from the topic
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当消费者开始从主题中读取消息时
- en: When any client requests metadata for the topic
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当任何客户端请求主题的元数据时
- en: In many situations, this can be undesirable behavior, especially as there is
    no way to validate the existence of a topic through the Kafka protocol without
    causing it to be created. If you are managing topic creation explicitly, whether
    manually or through a provisioning system, you can set the `auto.create.topics.enable`
    configuration to `false`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这可能是不希望的行为，特别是因为没有办法通过Kafka协议验证主题的存在而不导致其被创建。如果您正在显式管理主题创建，无论是手动还是通过配置系统，都可以将`auto.create.topics.enable`配置设置为`false`。
- en: auto.leader.rebalance.enable
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: auto.leader.rebalance.enable
- en: In order to ensure a Kafka cluster doesn’t become unbalanced by having all topic
    leadership on one broker, this config can be specified to ensure leadership is
    balanced as much as possible. It enables a background thread that checks the distribution
    of partitions at regular intervals (this interval is configurable via `leader.​imbal⁠ance.check.interval.seconds`).
    If leadership imbalance exceeds another config, `leader.imbalance.per.broker.percentage`,
    then a rebalance of preferred leaders for partitions is started.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保Kafka集群不会因为所有主题领导都在一个代理上而变得不平衡，可以指定此配置以尽可能平衡领导。它启用了一个后台线程，定期检查分区的分布（此间隔可通过`leader.​imbal⁠ance.check.interval.seconds`进行配置）。如果领导不平衡超过另一个配置`leader.imbalance.per.broker.percentage`，则会开始重新平衡分区的首选领导。
- en: delete.topic.enable
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: delete.topic.enable
- en: Depending on your environment and data retention guidelines, you may wish to
    lock down a cluster to prevent arbitrary deletions of topics. Disabling topic
    deletion can be set by setting this flag to `false`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的环境和数据保留指南，您可能希望锁定集群以防止任意删除主题。通过将此标志设置为`false`，可以禁用主题删除。
- en: Topic Defaults
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题默认值
- en: The Kafka server configuration specifies many default configurations for topics
    that are created. Several of these parameters, including partition counts and
    message retention, can be set per topic using the administrative tools (covered
    in [Chapter 12](ch12.html#administering_kafka)). The defaults in the server configuration
    should be set to baseline values that are appropriate for the majority of the
    topics in the cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka服务器配置指定了为创建的主题设置的许多默认配置。其中包括分区计数和消息保留等参数，可以使用管理工具（在[第12章](ch12.html#administering_kafka)中介绍）针对每个主题进行设置。服务器配置中的默认值应设置为适用于集群中大多数主题的基线值。
- en: Using Per-Topic Overrides
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用每个主题的覆盖
- en: In older versions of Kafka, it was possible to specify per-topic overrides for
    these configurations in the broker configuration using the parameters `log.retention.hours.per.topic`,
    `log.reten⁠tion.​bytes.per.topic`, and `log.segment.bytes.per.topic`. These parameters
    are no longer supported, and overrides must be specified using the administrative
    tools.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧版本的Kafka中，可以使用代理配置中的参数`log.retention.hours.per.topic`、`log.reten⁠tion.​bytes.per.topic`和`log.segment.bytes.per.topic`为这些配置指定每个主题的覆盖。这些参数不再受支持，必须使用管理工具指定覆盖。
- en: num.partitions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: num.partitions
- en: The `num.partitions` parameter determines how many partitions a new topic is
    created with, primarily when automatic topic creation is enabled (which is the
    default setting). This parameter defaults to one partition. Keep in mind that
    the number of partitions for a topic can only be increased, never decreased. This
    means that if a topic needs to have fewer partitions than `num.partitions`, care
    will need to be taken to manually create the topic (discussed in [Chapter 12](ch12.html#administering_kafka)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`num.partitions`参数确定新主题创建时创建多少个分区，主要是在启用自动主题创建时（这是默认设置）使用。此参数默认为一个分区。请记住，主题的分区数量只能增加，不能减少。这意味着如果主题需要比`num.partitions`更少的分区，就需要小心地手动创建主题（在[第12章](ch12.html#administering_kafka)中讨论）。'
- en: As described in [Chapter 1](ch01.html#meet_kafka), partitions are the way a
    topic is scaled within a Kafka cluster, which makes it important to use partition
    counts that will balance the message load across the entire cluster as brokers
    are added. Many users will have the partition count for a topic be equal to, or
    a multiple of, the number of brokers in the cluster. This allows the partitions
    to be evenly distributed to the brokers, which will evenly distribute the message
    load. For example, a topic with 10 partitions operating in a Kafka cluster with
    10 hosts with leadership balanced among all 10 hosts will have optimal throughput.
    This is not a requirement, however, as you can also balance message load in other
    ways, such as having multiple topics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#meet_kafka)中所述，分区是Kafka集群中扩展主题的方式，这使得使用能够平衡整个集群中的消息负载的分区计数变得重要，因为添加代理时会增加负载。许多用户将主题的分区计数设置为等于或是集群中代理数量的倍数。这样可以使分区均匀分布到代理中，从而均匀分布消息负载。例如，在Kafka集群中运行的具有10个分区的主题，如果有10个主机且领导权在所有10个主机之间平衡，将具有最佳吞吐量。然而，这并不是必须的，因为您也可以通过其他方式平衡消息负载，比如使用多个主题。
- en: With all this in mind, it’s clear that you want many partitions, but not too
    many. If you have some estimate regarding the target throughput of the topic and
    the expected throughput of the consumers, you can divide the target throughput
    by the expected consumer throughput and derive the number of partitions this way.
    So if we want to be able to write and read 1 GBps from a topic, and we know each
    consumer can only process 50 MBps, then we know we need at least 20 partitions.
    This way, we can have 20 consumers reading from the topic and achieve 1 GBps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一切，很明显您希望有很多分区，但不要太多。如果您对主题的目标吞吐量和消费者的预期吞吐量有一些估计，可以将目标吞吐量除以预期消费者吞吐量，以此确定分区数量。因此，如果我们希望能够从主题中写入和读取1GBps，并且我们知道每个消费者只能处理50MBps，那么我们知道至少需要20个分区。这样，我们可以有20个消费者从主题中读取，并实现1GBps的吞吐量。
- en: If you don’t have this detailed information, our experience suggests that limiting
    the size of the partition on the disk to less than 6 GB per day of retention often
    gives satisfactory results. Starting small and expanding as needed is easier than
    starting too large.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有这些详细信息，我们的经验表明，将磁盘上的分区大小限制在每天不到6GB的保留量通常会产生令人满意的结果。从小开始，根据需要扩展比从大开始更容易。
- en: default.replication.factor
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: default.replication.factor
- en: If auto-topic creation is enabled, this configuration sets what the replication
    factor should be for new topics. Replication strategy can vary depending on the
    desired durability or availability of a cluster and will be discussed more in
    later chapters. The following is a brief recommendation if you are running Kafka
    in a cluster that will prevent outages due to factors outside of Kafka’s internal
    capabilities, such as hardware failures.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了自动主题创建，此配置设置了新主题的复制因子应该是多少。复制策略可以根据集群的所需耐久性或可用性而变化，并将在后面的章节中进行更多讨论。如果您在集群中运行Kafka，可以防止由Kafka内部能力之外的因素（如硬件故障）导致的故障，以下是一个简要建议。
- en: It is highly recommended to set the replication factor to at least 1 above the
    `min.insync.replicas` setting. For more fault-resistant settings, if you have
    large enough clusters and enough hardware, setting your replication factor to
    2 above the `min.insync.replicas` (abbreviated as RF++) can be preferable. RF++
    will allow easier maintenance and prevent outages. The reasoning behind this recommendation
    is to allow for one planned outage within the replica set and one unplanned outage
    to occur simultaneously. For a typical cluster, this would mean you’d have a minimum
    of three replicas of every partition. An example of this is if there is a network
    switch outage, disk failure, or some other unplanned problem during a rolling
    deployment or upgrade of Kafka or the underlying OS, you can be assured there
    will still be an additional replica available. This will be discussed more in
    [Chapter 7](ch07.html#reliable_data_delivery).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议将复制因子设置为至少高于`min.insync.replicas`设置的1。对于更具容错性的设置，如果您有足够大的集群和足够的硬件，将复制因子设置为高于`min.insync.replicas`的2（简称为RF++）可能更可取。RF++将使维护更容易，并防止停机。这个建议的原因是允许在副本集中同时发生一个计划内的停机和一个非计划内的停机。对于典型的集群，这意味着每个分区至少有三个副本。例如，如果在Kafka或底层操作系统的滚动部署或升级期间发生网络交换机故障、磁盘故障或其他非计划问题，您可以确保仍然有额外的副本可用。这将在[第7章](ch07.html#reliable_data_delivery)中进一步讨论。
- en: log.retention.ms
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志保留时间（log.retention.ms）
- en: The most common configuration for how long Kafka will retain messages is by
    time. The default is specified in the configuration file using the `log.retention.hours`
    parameter, and it is set to 168 hours, or one week. However, there are two other
    parameters allowed, `log.retention.minutes` and `log.retention.ms`. All three
    of these control the same goal (the amount of time after which messages may be
    deleted), but the recommended parameter to use is `log.retention.ms`, as the smaller
    unit size will take precedence if more than one is specified. This will ensure
    that the value set for `log.retention.ms` is always the one used. If more than
    one is specified, the smaller unit size will take precedence.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka保留消息的最常见配置是按时间。默认值在配置文件中使用`log.retention.hours`参数指定，设置为168小时，即一周。然而，还允许使用另外两个参数，`log.retention.minutes`和`log.retention.ms`。所有这三个参数都控制相同的目标（消息可能被删除的时间），但建议使用的参数是`log.retention.ms`，因为如果指定了多个参数，较小的单位大小将优先。这将确保始终使用`log.retention.ms`设置的值。如果指定了多个参数，较小的单位大小将优先。
- en: Retention by Time and Last Modified Times
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按时间和最后修改时间保留
- en: Retention by time is performed by examining the last modified time (mtime) on
    each log segment file on disk. Under normal cluster operations, this is the time
    that the log segment was closed, and represents the timestamp of the last message
    in the file. However, when using administrative tools to move partitions between
    brokers, this time is not accurate and will result in excess retention for these
    partitions. For more information on this, see [Chapter 12](ch12.html#administering_kafka)
    discussing partition moves.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 按时间保留是通过检查磁盘上每个日志段文件的最后修改时间（mtime）来执行的。在正常的集群操作下，这是日志段关闭的时间，并代表文件中最后一条消息的时间戳。然而，当使用管理工具在代理之间移动分区时，这个时间是不准确的，会导致这些分区的过度保留。有关此信息，请参阅[第12章](ch12.html#administering_kafka)讨论分区移动。
- en: log.retention.bytes
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志保留字节数（log.retention.bytes）
- en: Another way to expire messages is based on the total number of bytes of messages
    retained. This value is set using the `log.retention.bytes` parameter, and it
    is applied per partition. This means that if you have a topic with 8 partitions,
    and `log.retention.bytes` is set to 1 GB, the amount of data retained for the
    topic will be 8 GB at most. Note that all retention is performed for individual
    partitions, not the topic. This means that should the number of partitions for
    a topic be expanded, the retention will also increase if `log.retention.bytes`
    is used. Setting the value to –1 will allow for infinite retention.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种过期消息的方法是基于保留的消息总字节数。这个值是使用`log.retention.bytes`参数设置的，它是针对每个分区应用的。这意味着如果您有一个包含8个分区的主题，并且`log.retention.bytes`设置为1GB，则主题保留的数据量最多为8GB。请注意，所有保留都是针对单个分区执行的，而不是主题。这意味着如果主题的分区数量扩大，使用`log.retention.bytes`时保留也会增加。将值设置为-1将允许无限保留。
- en: Configuring Retention by Size and Time
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按大小和时间配置保留
- en: If you have specified a value for both `log.retention.bytes` and `log.retention.ms`
    (or another parameter for retention by time), messages may be removed when either
    criteria is met. For example, if `log.retention.ms` is set to 86400000 (1 day)
    and `log.​reten⁠tion.bytes` is set to 1000000000 (1 GB), it is possible for messages
    that are less than 1 day old to get deleted if the total volume of messages over
    the course of the day is greater than 1 GB. Conversely, if the volume is less
    than 1 GB, messages can be deleted after 1 day even if the total size of the partition
    is less than 1 GB. It is recommended, for simplicity, to choose either size- or
    time-based retention—and not both—to prevent surprises and unwanted data loss,
    but both can be used for more advanced configurations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您同时为`log.retention.bytes`和`log.retention.ms`（或其他按时间保留的参数）指定了值，则在满足任一标准时可能会删除消息。例如，如果`log.retention.ms`设置为86400000（1天），而`log.retention.bytes`设置为1000000000（1GB），如果一天内的消息总量大于1GB，则可能会删除不到1天的消息。相反，如果总量小于1GB，则即使分区的总大小小于1GB，也可能在1天后删除消息。为了简单起见，建议选择基于大小或时间的保留方式，而不是两者兼用，以防止意外和不必要的数据丢失，但更高级的配置可以同时使用两者。
- en: log.segment.bytes
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志段字节数（log.segment.bytes）
- en: The log retention settings previously mentioned operate on log segments, not
    individual messages. As messages are produced to the Kafka broker, they are appended
    to the current log segment for the partition. Once the log segment has reached
    the size specified by the `log.segment.bytes` parameter, which defaults to 1 GB,
    the log segment is closed and a new one is opened. Once a log segment has been
    closed, it can be considered for expiration. A smaller log segment size means
    that files must be closed and allocated more often, which reduces the overall
    efficiency of disk writes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 先前提到的日志保留设置是针对日志段而不是单个消息的。当消息被生产到Kafka代理时，它们会附加到分区的当前日志段。一旦日志段达到由`log.segment.bytes`参数指定的大小（默认为1GB），日志段将关闭并打开一个新的日志段。一旦日志段关闭，就可以考虑将其过期。较小的日志段大小意味着文件必须更频繁地关闭和分配，这会降低磁盘写入的整体效率。
- en: Adjusting the size of the log segments can be important if topics have a low
    produce rate. For example, if a topic receives only 100 megabytes per day of messages,
    and `log.segment.bytes` is set to the default, it will take 10 days to fill one
    segment. As messages cannot be expired until the log segment is closed, if `log.retention.ms`
    is set to 604800000 (1 week), there will actually be up to 17 days of messages
    retained until the closed log segment expires. This is because once the log segment
    is closed with the current 10 days of messages, that log segment must be retained
    for 7 days before it expires based on the time policy (as the segment cannot be
    removed until the last message in the segment can be expired).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果主题的生产速率较低，则调整日志段的大小可能很重要。例如，如果一个主题每天只接收100兆字节的消息，并且`log.segment.bytes`设置为默认值，则需要10天才能填满一个段。由于消息直到日志段关闭后才能过期，如果`log.retention.ms`设置为604800000（1周），则实际上将保留多达17天的消息，直到关闭的日志段过期。这是因为一旦当前有10天的消息的日志段关闭，必须在根据时间策略过期之前保留该日志段7天（因为在最后一条消息过期之前无法删除该段）。
- en: Retrieving Offsets by Timestamp
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按时间戳检索偏移量
- en: The size of the log segment also affects the behavior of fetching offsets by
    timestamp. When requesting offsets for a partition at a specific timestamp, Kafka
    finds the log segment file that was being written at that time. It does this by
    using the creation and last modified time of the file, and looking for a file
    that was created before the timestamp specified and last modified after the timestamp.
    The offset at the beginning of that log segment (which is also the filename) is
    returned in the response.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 日志段的大小还会影响按时间戳获取偏移量的行为。当请求特定时间戳的分区偏移量时，Kafka会找到在该时间正在写入的日志段文件。它通过使用文件的创建时间和最后修改时间来执行此操作，并寻找在指定时间戳之前创建并在指定时间戳之后最后修改的文件。响应中返回该日志段开头的偏移量（也是文件名）。
- en: log.roll.ms
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: log.roll.ms
- en: Another way to control when log segments are closed is by using the `log.roll.ms`
    parameter, which specifies the amount of time after which a log segment should
    be closed. As with the `log.retention.bytes` and `log.retention.ms` parameters,
    `log.segment.bytes` and `log.roll.ms` are not mutually exclusive properties. Kafka
    will close a log segment either when the size limit is reached or when the time
    limit is reached, whichever comes first. By default, there is no setting for `log.roll.ms`,
    which results in only closing log segments by size.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种控制日志段何时关闭的方法是使用`log.roll.ms`参数，该参数指定多长时间后应关闭日志段。与`log.retention.bytes`和`log.retention.ms`参数一样，`log.segment.bytes`和`log.roll.ms`不是互斥的属性。Kafka将在达到大小限制或时间限制时关闭日志段，以先到者为准。默认情况下，没有`log.roll.ms`设置，这导致只按大小关闭日志段。
- en: Disk Performance When Using Time-Based Segments
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于时间的日志段时的磁盘性能
- en: When using a time-based log segment limit, it is important to consider the impact
    on disk performance when multiple log segments are closed simultaneously. This
    can happen when there are many partitions that never reach the size limit for
    log segments, as the clock for the time limit will start when the broker starts
    and will always execute at the same time for these low-volume partitions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用基于时间的日志段限制时，重要的是要考虑当多个日志段同时关闭时对磁盘性能的影响。当有许多分区从未达到日志段的大小限制时，会发生这种情况，因为时间限制的时钟将在代理启动时开始，并且对于这些低容量分区，它将始终在相同的时间执行。
- en: min.insync.replicas
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: min.insync.replicas
- en: When configuring your cluster for data durability, setting `min.insync.replicas`
    to 2 ensures that at least two replicas are caught up and “in sync” with the producer.
    This is used in tandem with setting the producer config to ack “all” requests.
    This will ensure that at least two replicas (leader and one other) acknowledge
    a write for it to be successful. This can prevent data loss in scenarios where
    the leader acks a write, then suffers a failure and leadership is transferred
    to a replica that does not have a successful write. Without these durable settings,
    the producer would think it successfully produced, and the message(s) would be
    dropped on the floor and lost. However, configuring for higher durability has
    the side effect of being less efficient due to the extra overhead involved, so
    clusters with high-throughput that can tolerate occasional message loss aren’t
    recommended to change this setting from the default of 1\. See [Chapter 7](ch07.html#reliable_data_delivery)
    for more information.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在为数据耐久性配置集群时，将`min.insync.replicas`设置为2可以确保至少有两个副本与生产者“同步”。这与将生产者配置设置为确认“所有”请求一起使用。这将确保至少有两个副本（领导者和另一个副本）确认写入才能成功。这可以防止数据丢失，例如领导者确认写入，然后发生故障并且领导权转移到没有成功写入的副本的情况。没有这些耐用的设置，生产者会认为它成功生产了，但消息会被丢弃和丢失。然而，配置更高的耐久性会导致效率降低，因为涉及额外的开销，因此不建议对可以容忍偶尔消息丢失的高吞吐量集群更改此设置。有关更多信息，请参见[第7章](ch07.html#reliable_data_delivery)。
- en: message.max.bytes
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: message.max.bytes
- en: The Kafka broker limits the maximum size of a message that can be produced,
    configured by the `message.max.bytes` parameter, which defaults to 1000000, or
    1 MB. A producer that tries to send a message larger than this will receive an
    error back from the broker, and the message will not be accepted. As with all
    byte sizes specified on the broker, this configuration deals with compressed message
    size, which means that producers can send messages that are much larger than this
    value uncompressed, provided they compress to under the configured `message.max.bytes`
    size.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka经纪限制了可以生成的消息的最大大小，由`message.max.bytes`参数配置，默认为1000000，即1MB。尝试发送大于此值的消息的生产者将从经纪那里收到错误，并且消息将不被接受。与经纪上指定的所有字节大小一样，此配置涉及压缩消息大小，这意味着生产者可以发送比此值大得多的未压缩消息，只要它们压缩到配置的`message.max.bytes`大小以下。
- en: There are noticeable performance impacts from increasing the allowable message
    size. Larger messages will mean that the broker threads that deal with processing
    network connections and requests will be working longer on each request. Larger
    messages also increase the size of disk writes, which will impact I/O throughput.
    Other storage solutions, such as blob stores and/or tiered storage, may be another
    method of addressing large disk write issues, but will not be covered in this
    chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 增加允许的消息大小会对性能产生明显影响。更大的消息意味着处理网络连接和请求的经纪线程将在每个请求上工作更长时间。更大的消息还会增加磁盘写入的大小，这将影响I/O吞吐量。其他存储解决方案，如blob存储和/或分层存储，可能是解决大容量磁盘写入问题的另一种方法，但本章不涉及这些内容。
- en: Coordinating Message Size Configurations
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协调消息大小配置
- en: The message size configured on the Kafka broker must be coordinated with the
    `fetch.message.max.bytes` configuration on consumer clients. If this value is
    smaller than `message.max.bytes`, then consumers that encounter larger messages
    will fail to fetch those messages, resulting in a situation where the consumer
    gets stuck and cannot proceed. The same rule applies to the `replica.fetch.max.bytes`
    configuration on the brokers when configured in a cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka经纪上配置的消息大小必须与消费者客户端上的`fetch.message.max.bytes`配置协调。如果这个值小于`message.max.bytes`，那么遇到更大消息的消费者将无法获取这些消息，导致消费者陷入僵局无法继续。当在集群中配置时，经纪上的`replica.fetch.max.bytes`配置也适用相同规则。
- en: Selecting Hardware
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择硬件
- en: 'Selecting an appropriate hardware configuration for a Kafka broker can be more
    art than science. Kafka itself has no strict requirement on a specific hardware
    configuration and will run without issue on most systems. Once performance becomes
    a concern, however, there are several factors that can contribute to the overall
    performance bottlenecks: disk throughput and capacity, memory, networking, and
    CPU. When scaling Kafka very large, there can also be constraints on the number
    of partitions that a single broker can handle due to the amount of metadata that
    needs to be updated. Once you have determined which performance types are the
    most critical for your environment, you can select an optimized hardware configuration
    appropriate for your budget.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为Kafka经纪选择适当的硬件配置可能更多地是一门艺术而不是科学。Kafka本身对特定硬件配置没有严格要求，并且在大多数系统上都可以正常运行。然而，一旦性能成为问题，有几个因素可能导致整体性能瓶颈：磁盘吞吐量和容量，内存，网络和CPU。当扩展Kafka到非常大规模时，由于需要更新的元数据量，单个经纪可以处理的分区数量也可能受到限制。一旦确定了哪些性能类型对您的环境最为关键，您可以选择一个适合预算的优化硬件配置。
- en: Disk Throughput
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘吞吐量
- en: The performance of producer clients will be most directly influenced by the
    throughput of the broker disk that is used for storing log segments. Kafka messages
    must be committed to local storage when they are produced, and most clients will
    wait until at least one broker has confirmed that messages have been committed
    before considering the send successful. This means that faster disk writes will
    equal lower produce latency.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者客户端的性能将受到用于存储日志段的经纪磁盘吞吐量的直接影响。Kafka消息在生成时必须提交到本地存储，大多数客户端将等待至少一个经纪确认消息已提交，然后才会考虑发送成功。这意味着更快的磁盘写入将等于更低的生成延迟。
- en: The obvious decision when it comes to disk throughput is whether to use traditional
    spinning hard disk drives (HDDs) or solid-state disks (SSDs). SSDs have drastically
    lower seek and access times and will provide the best performance. HDDs, on the
    other hand, are more economical and provide more capacity per unit. You can also
    improve the performance of HDDs by using more of them in a broker, whether by
    having multiple data directories or by setting up the drives in a redundant array
    of independent disks (RAID) configuration. Other factors, such as the specific
    drive technology (e.g., serial attached storage or serial ATA), as well as the
    quality of the drive controller, will affect throughput. Generally, observations
    show that HDD drives are typically more useful for clusters with very high storage
    needs but aren’t accessed as often, while SSDs are better options if there is
    a very large number of client connections.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在磁盘吞吐量方面的明显决定是选择传统的旋转硬盘驱动器（HDD）还是固态硬盘（SSD）。SSD具有极低的搜索和访问时间，并提供最佳性能。另一方面，HDD更经济，并且每单位提供更大的容量。您还可以通过在经纪中使用更多的HDD来提高性能，无论是通过拥有多个数据目录还是通过设置冗余独立磁盘阵列（RAID）配置来设置驱动器。其他因素，如特定的驱动器技术（例如串行附加存储或串行ATA），以及驱动器控制器的质量，都会影响吞吐量。一般来说，观察表明HDD驱动器通常对于存储需求非常高但访问频率不高的集群更有用，而如果有大量客户端连接，则SSD是更好的选择。
- en: Disk Capacity
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘容量
- en: Capacity is the other side of the storage discussion. The amount of disk capacity
    that is needed is determined by how many messages need to be retained at any time.
    If the broker is expected to receive 1 TB of traffic each day, with 7 days of
    retention, then the broker will need a minimum of 7 TB of usable storage for log
    segments. You should also factor in at least 10% overhead for other files, in
    addition to any buffer that you wish to maintain for fluctuations in traffic or
    growth over time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Storage capacity is one of the factors to consider when sizing a Kafka cluster
    and determining when to expand it. The total traffic for a cluster can be balanced
    across the cluster by having multiple partitions per topic, which will allow additional
    brokers to augment the available capacity if the density on a single broker will
    not suffice. The decision on how much disk capacity is needed will also be informed
    by the replication strategy chosen for the cluster (which is discussed in more
    detail in [Chapter 7](ch07.html#reliable_data_delivery)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The normal mode of operation for a Kafka consumer is reading from the end of
    the partitions, where the consumer is caught up and lagging behind the producers
    very little, if at all. In this situation, the messages the consumer is reading
    are optimally stored in the system’s page cache, resulting in faster reads than
    if the broker has to reread the messages from disk. Therefore, having more memory
    available to the system for page cache will improve the performance of consumer
    clients.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Kafka itself does not need much heap memory configured for the Java Virtual
    Machine (JVM). Even a broker that is handling 150,000 messages per second and
    a data rate of 200 megabits per second can run with a 5 GB heap. The rest of the
    system memory will be used by the page cache and will benefit Kafka by allowing
    the system to cache log segments in use. This is the main reason it is not recommended
    to have Kafka colocated on a system with any other significant application, as
    it will have to share the use of the page cache. This will decrease the consumer
    performance for Kafka.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The available network throughput will specify the maximum amount of traffic
    that Kafka can handle. This can be a governing factor, combined with disk storage,
    for cluster sizing. This is complicated by the inherent imbalance between inbound
    and outbound network usage that is created by Kafka’s support for multiple consumers.
    A producer may write 1 MB per second for a given topic, but there could be any
    number of consumers that create a multiplier on the outbound network usage. Other
    operations, such as cluster replication (covered in [Chapter 7](ch07.html#reliable_data_delivery))
    and mirroring (discussed in [Chapter 10](ch10.html#cross_cluster_mirroring)),
    will also increase requirements. Should the network interface become saturated,
    it is not uncommon for cluster replication to fall behind, which can leave the
    cluster in a vulnerable state. To prevent the network from being a major governing
    factor, it is recommended to run with at least 10 Gb NICs (Network Interface Cards).
    Older machines with 1 Gb NICs are easily saturated and aren’t recommended.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Processing power is not as important as disk and memory until you begin to scale
    Kafka very large, but it will affect overall performance of the broker to some
    extent. Ideally, clients should compress messages to optimize network and disk
    usage. The Kafka broker must decompress all message batches, however, in order
    to validate the `checksum` of the individual messages and assign offsets. It then
    needs to recompress the message batch in order to store it on disk. This is where
    most of Kafka’s requirement for processing power comes from. This should not be
    the primary factor in selecting hardware, however, unless clusters become very
    large with hundreds of nodes and millions of partitions in a single cluster. At
    that point, selecting more performant CPU can help reduce cluster sizes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Kafka in the Cloud
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, a more common installation for Kafka is within cloud computing
    environments, such as Microsoft Azure, Amazon’s AWS, or Google Cloud Platform.
    There are many options to have Kafka set up in the cloud and managed for you via
    vendors like Confluent or even through Azure’s own Kafka on HDInsight, but the
    following is some simple advice if you plan to manage your own Kafka clusters
    manually. In most cloud environments, you have a selection of many compute instances,
    each with a different combination of CPU, memory, IOPS, and disk. The various
    performance characteristics of Kafka must be prioritized in order to select the
    correct instance configuration to use.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Azure, you can manage the disks separately from the virtual machine (VM),
    so deciding your storage needs does not need to be related to the VM type selected.
    That being said, a good place to start on decisions is with the amount of data
    retention required, followed by the performance needed from the producers. If
    very low latency is necessary, I/O optimized instances utilizing premium SSD storage
    might be required. Otherwise, managed storage options (such as the Azure Managed
    Disks or the Azure Blob Storage) might be sufficient.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In real terms, experience in Azure shows that `Standard D16s v3` instance types
    are a good choice for smaller clusters and are performant enough for most use
    cases. To match high performant hardware and CPU needs, `D64s v4` instances have
    good performance that can scale for larger clusters. It is recommended to build
    out your cluster in an Azure availability set and balance partitions across Azure
    compute fault domains to ensure availability. Once you have a VM picked out, deciding
    on storage types can come next. It is highly recommended to use Azure Managed
    Disks rather than ephemeral disks. If a VM is moved, you run the risk of losing
    all the data on your Kafka broker. HDD Managed Disks are relatively inexpensive
    but do not have clearly defined SLAs from Microsoft on availability. Premium SSDs
    or Ultra SSD configurations are much more expensive but are much quicker and are
    well supported with 99.99% SLAs from Microsoft. Alternatively, using Microsoft
    Blob Storage is an option if you are not as latency sensitive.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In AWS, if very low latency is necessary, I/O optimized instances that have
    local SSD storage might be required. Otherwise, ephemeral storage (such as the
    Amazon Elastic Block Store) might be sufficient.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: A common choice in AWS is either the `m4` or `r3` instance types. The `m4` will
    allow for greater retention periods, but the throughput to the disk will be less
    because it is on elastic block storage. The `r3` instance will have much better
    throughput with local SSD drives, but those drives will limit the amount of data
    that can be retained. For the best of both worlds, it may be necessary to move
    up to either the `i2` or `d2` instance types, but they are significantly more
    expensive.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Kafka Clusters
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A single Kafka broker works well for local development work, or for a proof-of-concept
    system, but there are significant benefits to having multiple brokers configured
    as a cluster, as shown in [Figure 2-2](#fig-2-cluster). The biggest benefit is
    the ability to scale the load across multiple servers. A close second is using
    replication to guard against data loss due to single system failures. Replication
    will also allow for performing maintenance work on Kafka or the underlying systems
    while still maintaining availability for clients. This section focuses on the
    steps to configure a Kafka basic cluster. [Chapter 7](ch07.html#reliable_data_delivery)
    contains more information on replication of data and durability.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0202](assets/kdg2_0202.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. A simple Kafka cluster
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How Many Brokers?
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The appropriate size for a Kafka cluster is determined by several factors.
    Typically, the size of your cluster will be bound on the following key areas:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Disk capacity
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replica capacity per broker
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU capacity
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network capacity
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first factor to consider is how much disk capacity is required for retaining
    messages and how much storage is available on a single broker. If the cluster
    is required to retain 10 TB of data and a single broker can store 2 TB, then the
    minimum cluster size is 5 brokers. In addition, increasing the replication factor
    will increase the storage requirements by at least 100%, depending on the replication
    factor setting chosen (see [Chapter 7](ch07.html#reliable_data_delivery)). Replicas
    in this case refer to the number of different brokers a single partition is copied
    to. This means that this same cluster, configured with a replication of 2, now
    needs to contain at least 10 brokers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The other factor to consider is the capacity of the cluster to handle requests.
    This can exhibit through the other three bottlenecks mentioned earlier.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: If you have a 10-broker Kafka cluster but have over 1 million replicas (i.e.,
    500,000 partitions with a replication factor of 2) in your cluster, each broker
    is taking on approximately 100,000 replicas in an evenly balanced scenario. This
    can lead to bottlenecks in the produce, consume, and controller queues. In the
    past, official recommendations have been to have no more than 4,000 partition
    replicas per broker and no more than 200,000 partition replicas per cluster. However,
    advances in cluster efficiency have allowed Kafka to scale much larger. Currently,
    in a well-configured environment, it is recommended to not have more than 14,000
    partition replicas per broker and 1 million *replicas* per cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned in this chapter, CPU usually is not a major bottleneck
    for most use cases, but it can be if there is an excessive amount of client connections
    and requests on a broker. Keeping an eye on overall CPU usage based on how many
    unique clients and consumer groups there are, and expanding to meet those needs,
    can help to ensure better overall performance in large clusters. Speaking to network
    capacity, it is important to keep in mind the capacity of the network interfaces
    and whether they can handle the client traffic if there are multiple consumers
    of the data or if the traffic is not consistent over the retention period of the
    data (e.g., bursts of traffic during peak times). If the network interface on
    a single broker is used to 80% capacity at peak, and there are two consumers of
    that data, the consumers will not be able to keep up with peak traffic unless
    there are two brokers. If replication is being used in the cluster, this is an
    additional consumer of the data that must be taken into account. You may also
    want to scale out to more brokers in a cluster in order to handle performance
    concerns caused by lesser disk throughput or system memory available.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Broker Configuration
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are only two requirements in the broker configuration to allow multiple
    Kafka brokers to join a single cluster. The first is that all brokers must have
    the same configuration for the `zookeeper.connect` parameter. This specifies the
    ZooKeeper ensemble and path where the cluster stores metadata. The second requirement
    is that all brokers in the cluster must have a unique value for the `broker.id`
    parameter. If two brokers attempt to join the same cluster with the same `broker.id`,
    the second broker will log an error and fail to start. There are other configuration
    parameters used when running a cluster—specifically, parameters that control replication,
    which are covered in later chapters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: OS Tuning
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While most Linux distributions have an out-of-the-box configuration for the
    kernel-tuning parameters that will work fairly well for most applications, there
    are a few changes that can be made for a Kafka broker that will improve performance.
    These primarily revolve around the virtual memory and networking subsystems, as
    well as specific concerns for the disk mount point used for storing log segments.
    These parameters are typically configured in the */etc/sysctl.conf* file, but
    you should refer to your Linux distribution documentation for specific details
    regarding how to adjust the kernel configuration.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, the Linux virtual memory system will automatically adjust itself
    for the system workload. We can make some adjustments to how swap space is handled,
    as well as to dirty memory pages, to tune these for Kafka’s workload.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: As with most applications, specifically ones where throughput is a concern,
    it is best to avoid swapping at (almost) all costs. The cost incurred by having
    pages of memory swapped to disk will show up as a noticeable impact on all aspects
    of performance in Kafka. In addition, Kafka makes heavy use of the system page
    cache, and if the VM system is swapping to disk, there is not enough memory being
    allocated to page cache.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: One way to avoid swapping is simply not to configure any swap space at all.
    Having swap is not a requirement, but it does provide a safety net if something
    catastrophic happens on the system. Having swap can prevent the OS from abruptly
    killing a process due to an out-of-memory condition. For this reason, the recommendation
    is to set the `vm.swappiness` parameter to a very low value, such as 1\. The parameter
    is a percentage of how likely the VM subsystem is to use swap space rather than
    dropping pages from the page cache. It is preferable to reduce the amount of memory
    available for the page cache rather than utilize any amount of swap memory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Why Not Set Swappiness to Zero?
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, the recommendation for `vm.swappiness` was always to set it to 0\.
    This value used to mean “do not swap unless there is an out-of-memory condition.”
    However, the meaning of this value changed as of Linux kernel version 3.5-rc1,
    and that change was backported into many distributions, including Red Hat Enterprise
    Linux kernels as of version 2.6.32-303\. This changed the meaning of the value
    0 to “never swap under any circumstances.” This is why a value of 1 is now recommended.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: There is also a benefit to adjusting how the kernel handles dirty pages that
    must be flushed to disk. Kafka relies on disk I/O performance to provide good
    response times to producers. This is also the reason that the log segments are
    usually put on a fast disk, whether that is an individual disk with a fast response
    time (e.g., SSD) or a disk subsystem with significant NVRAM for caching (e.g.,
    RAID). The result is that the number of dirty pages that are allowed, before the
    flush background process starts writing them to disk, can be reduced. Do this
    by setting the `vm.dirty_background_ratio` value lower than the default of 10\.
    The value is a percentage of the total amount of system memory, and setting this
    value to 5 is appropriate in many situations. This setting should not be set to
    zero, however, as that would cause the kernel to continually flush pages, which
    would then eliminate the ability of the kernel to buffer disk writes against temporary
    spikes in the underlying device performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The total number of dirty pages allowed before the kernel forces synchronous
    operations to flush them to disk can also be increased by changing the value of
    `vm.dirty_ratio` to above the default of 20 (also a percentage of total system
    memory). There is a wide range of possible values for this setting, but between
    60 and 80 is a reasonable number. This setting does introduce a small amount of
    risk, both in regard to the amount of unflushed disk activity as well as the potential
    for long I/O pauses if synchronous flushes are forced. If a higher setting for
    `vm.dirty_ratio` is chosen, it is highly recommended that replication be used
    in the Kafka cluster to guard against system failures.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'When choosing values for these parameters, it is wise to review the number
    of dirty pages over time while the Kafka cluster is running under load, whether
    in production or simulated. The current number of dirty pages can be determined
    by checking the */proc/vmstat* file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Kafka uses file descriptors for log segments and open connections. If a broker
    has a lot of partitions, then that broker needs at least *(number_of_partitions)*
    × *(partition_size/segment_size)* to track all the log segments in addition to
    the number of connections the broker makes. As such, it is recommended to update
    the `vm.max_map_count` to a very large number based on the above calculation.
    Depending on the environment, changing this value to 400,000 or 600,000 has generally
    been successful. It is also recommended to set `vm.overcommit_memory` to 0\. Setting
    the default value of 0 indicates that the kernel determines the amount of free
    memory from an application. If the property is set to a value other than zero,
    it could lead the operating system to grab too much memory, depriving memory for
    Kafka to operate optimally. This is common for applications with high ingestion
    rates.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Disk
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Outside of selecting the disk device hardware, as well as the configuration
    of RAID if it is used, the choice of filesystem for this disk can have the next
    largest impact on performance. There are many different filesystems available,
    but the most common choices for local filesystems are either Ext4 (fourth extended
    filesystem) or Extents File System (XFS). XFS has become the default filesystem
    for many Linux distributions, and this is for good reason: it outperforms Ext4
    for most workloads with minimal tuning required. Ext4 can perform well but requires
    using tuning parameters that are considered less safe. This includes setting the
    commit interval to a longer time than the default of five to force less frequent
    flushes. Ext4 also introduced delayed allocation of blocks, which brings with
    it a greater chance of data loss and filesystem corruption in case of a system
    failure. The XFS filesystem also uses a delayed allocation algorithm, but it is
    generally safer than the one used by Ext4\. XFS also has better performance for
    Kafka’s workload without requiring tuning beyond the automatic tuning performed
    by the filesystem. It is also more efficient when batching disk writes, all of
    which combine to give better overall I/O throughput.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of which filesystem is chosen for the mount that holds the log segments,
    it is advisable to set the `noatime` mount option for the mount point. File metadata
    contains three timestamps: creation time (`ctime`), last modified time (`mtime`),
    and last access time (`atime`). By default, the `atime` is updated every time
    a file is read. This generates a large number of disk writes. The `atime` attribute
    is generally considered to be of little use, unless an application needs to know
    if a file has been accessed since it was last modified (in which case the `relatime`
    option can be used). The `atime` is not used by Kafka at all, so disabling it
    is safe. Setting `noatime` on the mount will prevent these timestamp updates from
    happening but will not affect the proper handling of the `ctime` and `mtime` attributes.
    Using the option `largeio` can also help improve efficiency for Kafka for when
    there are larger disk writes.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adjusting the default tuning of the Linux networking stack is common for any
    application that generates a high amount of network traffic, as the kernel is
    not tuned by default for large, high-speed data transfers. In fact, the recommended
    changes for Kafka are the same as those suggested for most web servers and other
    networking applications. The first adjustment is to change the default and maximum
    amount of memory allocated for the send and receive buffers for each socket. This
    will significantly increase performance for large transfers. The relevant parameters
    for the send and receive buffer default size per socket are `net.core.wmem_default`
    and `net.core.rmem_default`, and a reasonable setting for these parameters is
    131072, or 128 KiB. The parameters for the send and receive buffer maximum sizes
    are `net.core.wmem_max` and `net.core.rmem_max`, and a reasonable setting is 2097152,
    or 2 MiB. Keep in mind that the maximum size does not indicate that every socket
    will have this much buffer space allocated; it only allows up to that much if
    needed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the socket settings, the send and receive buffer sizes for TCP
    sockets must be set separately using the `net.ipv4.tcp_wmem` and `net.ipv4.tcp_rmem`
    parameters. These are set using three space-separated integers that specify the
    minimum, default, and maximum sizes, respectively. The maximum size cannot be
    larger than the values specified for all sockets using `net.core.wmem_max` and
    `net.core.rmem_max`. An example setting for each of these parameters is “4096
    65536 2048000,” which is a 4 KiB minimum, 64 KiB default, and 2 MiB maximum buffer.
    Based on the actual workload of your Kafka brokers, you may want to increase the
    maximum sizes to allow for greater buffering of the network connections.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: There are several other network tuning parameters that are useful to set. Enabling
    TCP window scaling by setting `net.ipv4.tcp_window_scaling` to 1 will allow clients
    to transfer data more efficiently, and allow that data to be buffered on the broker
    side. Increasing the value of `net.ipv4.tcp_max_syn_backlog` above the default
    of 1024 will allow a greater number of simultaneous connections to be accepted.
    Increasing the value of `net.core.netdev_max_backlog` to greater than the default
    of 1000 can assist with bursts of network traffic, specifically when using multigigabit
    network connection speeds, by allowing more packets to be queued for the kernel
    to process them.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Production Concerns
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you are ready to move your Kafka environment out of testing and into your
    production operations, there are a few more things to think about that will assist
    with setting up a reliable messaging service.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Garbage Collector Options
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tuning the Java garbage-collection options for an application has always been
    something of an art, requiring detailed information about how the application
    uses memory and a significant amount of observation and trial and error. Thankfully,
    this has changed with Java 7 and the introduction of the Garbage-First garbage
    collector (G1GC). While G1GC was considered unstable initially, it saw marked
    improvement in JDK8 and JDK11\. It is now recommended for Kafka to use G1GC as
    the default garbage collector. G1GC is designed to automatically adjust to different
    workloads and provide consistent pause times for garbage collection over the lifetime
    of the application. It also handles large heap sizes with ease by segmenting the
    heap into smaller zones and not collecting over the entire heap in each pause.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'G1GC does all of this with a minimal amount of configuration in normal operation.
    There are two configuration options for G1GC used to adjust its performance:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '`MaxGCPauseMillis`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: This option specifies the preferred pause time for each garbage-collection cycle.
    It is not a fixed maximum—G1GC can and will exceed this time if required. This
    value defaults to 200 milliseconds. This means that G1GC will attempt to schedule
    the frequency of garbage collector cycles, as well as the number of zones that
    are collected in each cycle, such that each cycle will take approximately 200
    ms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '`InitiatingHeapOccupancyPercent`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: This option specifies the percentage of the total heap that may be in use before
    G1GC will start a collection cycle. The default value is 45\. This means that
    G1GC will not start a collection cycle until after 45% of the heap is in use.
    This includes both the new (Eden) and old zone usage, in total.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka broker is fairly efficient with the way it utilizes heap memory and
    creates garbage objects, so it is possible to set these options lower. The garbage
    collector tuning options provided in this section have been found to be appropriate
    for a server with 64 GB of memory, running Kafka in a 5 GB heap. For `MaxGCPauseMillis`,
    this broker can be configured with a value of 20 ms. The value for `InitiatingHeap​Occu⁠pancyPercent`
    is set to 35, which causes garbage collection to run slightly earlier than with
    the default value.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka was originally released before the G1GC collector was available and considered
    stable. Therefore, Kafka defaults to using concurrent mark and sweep garbage collection
    to ensure compatibility with all JVMs. New best practice is to use G1GC for anything
    for Java 1.8 and later. The change is easy to make via environment variables.
    Using the `start` command from earlier in the chapter, modify it as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Datacenter Layout
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For testing and development environments, the physical location of the Kafka
    brokers within a datacenter is not as much of a concern, as there is not as severe
    an impact if the cluster is partially or completely unavailable for short periods
    of time. However, when serving production traffic, downtime usually means dollars
    lost, whether through loss of services to users or loss of telemetry on what the
    users are doing. This is when it becomes critical to configure replication within
    the Kafka cluster (see [Chapter 7](ch07.html#reliable_data_delivery)), which is
    also when it is important to consider the physical location of brokers in their
    racks in the datacenter. A datacenter environment that has a concept of fault
    zones is preferable. If not addressed prior to deploying Kafka, expensive maintenance
    to move servers around may be needed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Kafka can assign new partitions to brokers in a rack-aware manner, making sure
    that replicas for a single partition do not share a rack. To do this, the `broker.rack`
    configuration for each broker must be set properly. This config can be set to
    the fault domain in cloud environments as well for similar reasons. However, this
    only applies to partitions that are newly created. The Kafka cluster does not
    monitor for partitions that are no longer rack aware (for example, as a result
    of a partition reassignment), nor does it automatically correct this situation.
    It is recommend to use tools that keep your cluster balanced properly to maintain
    rack awareness, such as Cruise Control (see [Appendix B](app02.html#appendix_3rd_party_tools)).
    Configuring this properly will help to ensure continued rack awareness over time.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the best practice is to have each Kafka broker in a cluster installed
    in a different rack, or at the very least not share single points of failure for
    infrastructure services such as power and network. This typically means at least
    deploying the servers that will run brokers with dual power connections (to two
    different circuits) and dual network switches (with a bonded interface on the
    servers themselves to failover seamlessly). Even with dual connections, there
    is a benefit to having brokers in completely separate racks. From time to time,
    it may be necessary to perform physical maintenance on a rack or cabinet that
    requires it to be offline (such as moving servers around or rewiring power connections).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Colocating Applications on ZooKeeper
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka utilizes ZooKeeper for storing metadata information about the brokers,
    topics, and partitions. Writes to ZooKeeper are only performed on changes to the
    membership of consumer groups or on changes to the Kafka cluster itself. This
    amount of traffic is generally minimal, and it does not justify the use of a dedicated
    ZooKeeper ensemble for a single Kafka cluster. In fact, many deployments will
    use a single ZooKeeper ensemble for multiple Kafka clusters (using a chroot ZooKeeper
    path for each cluster, as described earlier in this chapter).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Consumers, Tooling, ZooKeeper, and You
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As time goes on, dependency on ZooKeeper is shrinking. In version 2.8.0, Kafka
    is introducing an early-access look at a completely ZooKeeper-less Kafka, but
    it is still not production ready. However, we can still see this reduced reliance
    on ZooKeeper in versions leading up to this. For example, in older versions of
    Kafka, consumers (in addition to the brokers) utilized ZooKeeper to directly store
    information about the composition of the consumer group and what topics it was
    consuming, and to periodically commit offsets for each partition being consumed
    (to enable failover between consumers in the group). With version 0.9.0.0, the
    consumer interface was changed, allowing this to be managed directly with the
    Kafka brokers. In each 2.x release of Kafka, we see additional steps to removing
    ZooKeeper from other required paths of Kafka. Administration tools now connect
    directly to the cluster and have deprecated the need to connect to ZooKeeper directly
    for operations such as topic creations, dynamic configuration changes, etc. As
    such, many of the command-line tools that previously used the `--zookeeper` flags
    have been updated to use the `--bootstrap-server` option. The `--zookeeper` options
    can still be used but have been deprecated and will be removed in the future when
    Kafka is no longer required to connect to ZooKeeper to create, manage, or consume
    from topics.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a concern with consumers and ZooKeeper under certain configurations.
    While the use of ZooKeeper for such purposes is deprecated, consumers have a configurable
    choice to use either ZooKeeper or Kafka for committing offsets, and they can also
    configure the interval between commits. If the consumer uses ZooKeeper for offsets,
    each consumer will perform a ZooKeeper write at every interval for every partition
    it consumes. A reasonable interval for offset commits is 1 minute, as this is
    the period of time over which a consumer group will read duplicate messages in
    the case of a consumer failure. These commits can be a significant amount of ZooKeeper
    traffic, especially in a cluster with many consumers, and will need to be taken
    into account. It may be necessary to use a longer commit interval if the ZooKeeper
    ensemble is not able to handle the traffic. However, it is recommended that consumers
    using the latest Kafka libraries use Kafka for committing offsets, removing the
    dependency on ZooKeeper.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Outside of using a single ensemble for multiple Kafka clusters, it is not recommended
    to share the ensemble with other applications, if it can be avoided. Kafka is
    sensitive to ZooKeeper latency and timeouts, and an interruption in communications
    with the ensemble will cause the brokers to behave unpredictably. This can easily
    cause multiple brokers to go offline at the same time should they lose ZooKeeper
    connections, which will result in offline partitions. It also puts stress on the
    cluster controller, which can show up as subtle errors long after the interruption
    has passed, such as when trying to perform a controlled shutdown of a broker.
    Other applications that can put stress on the ZooKeeper ensemble, either through
    heavy usage or improper operations, should be segregated to their own ensemble.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we learned how to get Apache Kafka up and running. We also covered
    picking the right hardware for your brokers, and specific concerns around getting
    set up in a production environment. Now that you have a Kafka cluster, we will
    walk through the basics of Kafka client applications. The next two chapters will
    cover how to create clients for both producing messages to Kafka ([Chapter 3](ch03.html#writing_messages_to_kafka))
    as well as consuming those messages out again ([Chapter 4](ch04.html#reading_data_from_kafka)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL

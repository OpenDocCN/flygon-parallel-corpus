- en: Chapter 2\. Installing Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。安装Kafka
- en: This chapter describes how to get started with the Apache Kafka broker, including
    how to set up Apache ZooKeeper, which is used by Kafka for storing metadata for
    the brokers. The chapter will also cover basic configuration options for Kafka
    deployments, as well as some suggestions for selecting the correct hardware to
    run the brokers on. Finally, we cover how to install multiple Kafka brokers as
    part of a single cluster and things you should know when using Kafka in a production
    environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了如何开始使用Apache Kafka代理，包括如何设置Apache ZooKeeper，Kafka用于存储代理的元数据。本章还将涵盖Kafka部署的基本配置选项，以及一些关于选择正确硬件来运行代理的建议。最后，我们将介绍如何在单个集群的一部分安装多个Kafka代理，以及在生产环境中使用Kafka时应该了解的一些事项。
- en: Environment Setup
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: Before using Apache Kafka, your environment needs to be set up with a few prerequisites
    to ensure it runs properly. The following sections will guide you through that
    process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Apache Kafka之前，您的环境需要设置一些先决条件，以确保其正常运行。以下部分将指导您完成这个过程。
- en: Choosing an Operating System
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择操作系统
- en: Apache Kafka is a Java application and can run on many operating systems. While
    Kafka is capable of being run on many OSs, including Windows, macOS, Linux, and
    others, Linux is the recommended OS for the general use case. The installation
    steps in this chapter will focus on setting up and using Kafka in a Linux environment.
    For information on installing Kafka on Windows and macOS, see [Appendix A](app01.html#appendix_installing_other_os).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个Java应用程序，可以在许多操作系统上运行。虽然Kafka可以在许多操作系统上运行，包括Windows、macOS、Linux和其他操作系统，但Linux是一般用例的推荐操作系统。本章中的安装步骤将重点介绍在Linux环境中设置和使用Kafka。有关在Windows和macOS上安装Kafka的信息，请参阅[附录A](app01.html#appendix_installing_other_os)。
- en: Installing Java
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Java
- en: Prior to installing either ZooKeeper or Kafka, you will need a Java environment
    set up and functioning. Kafka and ZooKeeper work well with all OpenJDK-based Java
    implementations, including Oracle JDK. The latest versions of Kafka support both
    Java 8 and Java 11\. The exact version installed can be the version provided by
    your OS or one directly downloaded from the web—for example, [the Oracle website
    for the Oracle version](https://www.oracle.com/java). Though ZooKeeper and Kafka
    will work with a runtime edition of Java, it is recommended when developing tools
    and applications to have the full Java Development Kit (JDK). It is recommended
    to install the latest released patch version of your Java environment, as older
    versions may have security vulnerabilities. The installation steps will assume
    you have installed JDK version 11 update 10 deployed at */usr/java/jdk-11.0.10*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装ZooKeeper或Kafka之前，您需要设置并运行Java环境。Kafka和ZooKeeper与所有基于OpenJDK的Java实现（包括Oracle
    JDK）兼容。最新版本的Kafka支持Java 8和Java 11。安装的确切版本可以是操作系统提供的版本，也可以是直接从网络下载的版本，例如[Oracle版本的Oracle网站](https://www.oracle.com/java)。虽然ZooKeeper和Kafka可以与运行时版本的Java一起工作，但在开发工具和应用程序时建议使用完整的Java开发工具包（JDK）。建议安装Java环境的最新发布的补丁版本，因为旧版本可能存在安全漏洞。安装步骤将假定您已经安装了部署在*/usr/java/jdk-11.0.10*的JDK版本11更新10。
- en: Installing ZooKeeper
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装ZooKeeper
- en: Apache Kafka uses Apache ZooKeeper to store metadata about the Kafka cluster,
    as well as consumer client details, as shown in [Figure 2-1](#fig-1-kafkazk).
    ZooKeeper is a centralized service for maintaining configuration information,
    naming, providing distributed synchronization, and providing group services. This
    book won’t go into extensive detail about ZooKeeper but will limit explanations
    to only what is needed to operate Kafka. While it is possible to run a ZooKeeper
    server using scripts contained in the Kafka distribution, it is trivial to install
    a full version of ZooKeeper from the distribution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka使用Apache ZooKeeper存储有关Kafka集群的元数据，以及消费者客户端详细信息，如[图2-1](#fig-1-kafkazk)所示。ZooKeeper是一个集中式服务，用于维护配置信息、命名、提供分布式同步和提供组服务。本书不会详细介绍ZooKeeper，而是将解释限制在操作Kafka所需的内容。虽然可以使用Kafka分发中包含的脚本运行ZooKeeper服务器，但从分发中安装完整版本的ZooKeeper是微不足道的。
- en: '![kdg2 0201](assets/kdg2_0201.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0201](assets/kdg2_0201.png)'
- en: Figure 2-1\. Kafka and ZooKeeper
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。Kafka和ZooKeeper
- en: Kafka has been tested extensively with the stable 3.5 release of ZooKeeper and
    is regularly updated to include the latest release. In this book, we will be using
    ZooKeeper 3.5.9, which can be downloaded from the [ZooKeeper website](https://oreil.ly/iMZjR).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka已经广泛测试了稳定的ZooKeeper 3.5版本，并定期更新以包括最新版本。在本书中，我们将使用ZooKeeper 3.5.9，可以从[ZooKeeper网站](https://oreil.ly/iMZjR)下载。
- en: Standalone server
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立服务器
- en: 'ZooKeeper comes with a base example config file that will work well for most
    use cases in */usr/local/zookeeper/config/zoo_sample.cfg*. However, we will manually
    create ours with some basic settings for demo purposes in this book. The following
    example installs ZooKeeper with a basic configuration in */usr/local/zookeeper*,
    storing its data in */var/lib/zookeeper*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper附带一个基本示例配置文件，对于大多数用例来说都可以正常工作，位于*/usr/local/zookeeper/config/zoo_sample.cfg*。但是，为了演示目的，我们将在本书中手动创建一些基本设置的配置文件。以下示例在*/usr/local/zookeeper*中使用基本配置安装ZooKeeper，并将其数据存储在*/var/lib/zookeeper*中：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can now validate that ZooKeeper is running correctly in standalone mode
    by connecting to the client port and sending the four-letter command `srvr`. This
    will return basic ZooKeeper information from the running server:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过连接到客户端端口并发送四字命令`srvr`来验证ZooKeeper是否在独立模式下正确运行。这将从运行的服务器返回基本的ZooKeeper信息：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ZooKeeper ensemble
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ZooKeeper集合
- en: ZooKeeper is designed to work as a cluster, called an *ensemble*, to ensure
    high availability. Due to the balancing algorithm used, it is recommended that
    ensembles contain an odd number of servers (e.g., 3, 5, and so on) as a majority
    of ensemble members (a *quorum*) must be working in order for ZooKeeper to respond
    to requests. This means that in a three-node ensemble, you can run with one node
    missing. With a five-node ensemble, you can run with two nodes missing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper被设计为作为一个名为*ensemble*的集群工作，以确保高可用性。由于使用的平衡算法，建议集群包含奇数个服务器（例如3、5等），因为大多数集群成员（*quorum*）必须正常工作才能响应ZooKeeper的请求。这意味着在一个三节点集群中，您可以运行一个节点缺失。在一个五节点集群中，您可以运行两个节点缺失。
- en: Sizing Your ZooKeeper Ensemble
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整ZooKeeper集群的大小
- en: Consider running ZooKeeper in a five-node ensemble. To make configuration changes
    to the ensemble, including swapping a node, you will need to reload nodes one
    at a time. If your ensemble cannot tolerate more than one node being down, doing
    maintenance work introduces additional risk. It is also not recommended to run
    more than seven nodes, as performance can start to degrade due to the nature of
    the consensus protocol.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在一个五节点集群中运行ZooKeeper。要对集群进行配置更改，包括交换节点，您需要逐个重新加载节点。如果您的集群不能容忍多于一个节点宕机，进行维护工作会增加额外的风险。此外，不建议运行超过七个节点，因为由于共识协议的性质，性能可能开始下降。
- en: Additionally, if you feel that five or seven nodes aren’t supporting the load
    due to too many client connections, consider adding additional observer nodes
    for help in balancing read-only traffic.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您觉得五个或七个节点无法支持由于太多客户端连接而产生的负载，请考虑添加额外的观察者节点来帮助平衡只读流量。
- en: 'To configure ZooKeeper servers in an ensemble, they must have a common configuration
    that lists all servers, and each server needs a *myid* file in the data directory
    that specifies the ID number of the server. If the hostnames of the servers in
    the ensemble are `zoo1.example.com`, `zoo2.example.com`, and `zoo3.example.com`,
    the configuration file might look like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要在集群中配置ZooKeeper服务器，它们必须有一个列出所有服务器的共同配置，每个服务器在数据目录中需要一个指定服务器ID号的*myid*文件。如果集群中的服务器的主机名是`zoo1.example.com`，`zoo2.example.com`和`zoo3.example.com`，配置文件可能如下所示：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this configuration, the `initLimit` is the amount of time to allow followers
    to connect with a leader. The `syncLimit` value limits how long out-of-sync followers
    can be with the leader. Both values are a number of `tickTime` units, which makes
    the `init​Li⁠mit` 20 × 2,000 ms, or 40 seconds. The configuration also lists each
    server in the ensemble. The servers are specified in the format `*server.X=hostname:peerPort:leaderPort*`,
    with the following parameters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，`initLimit`是允许跟随者与领导者连接的时间。`syncLimit`值限制了落后于领导者的跟随者可以有多久。这两个值都是`tickTime`单位的数字，这使得`init​Li⁠mit`为20×2,000毫秒，即40秒。配置还列出了集群中的每个服务器。服务器以`*server.X=hostname:peerPort:leaderPort*`的格式指定，具有以下参数：
- en: '`X`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`'
- en: The ID number of the server. This must be an integer, but it does not need to
    be zero-based or sequential.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的ID号。这必须是一个整数，但不需要基于零或连续。
- en: '`hostname`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostname`'
- en: The hostname or IP address of the server.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的主机名或IP地址。
- en: '`peerPort`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`peerPort`'
- en: The TCP port over which servers in the ensemble communicate with one another.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用于集群中的服务器相互通信的TCP端口。
- en: '`leaderPort`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`leaderPort`'
- en: The TCP port over which leader election is performed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行领导者选举的TCP端口。
- en: Clients only need to be able to connect to the ensemble over the `*clientPort*`,
    but the members of the ensemble must be able to communicate with one another over
    all three ports.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端只需要能够通过`*clientPort*`连接到集群，但集群的成员必须能够通过所有三个端口相互通信。
- en: In addition to the shared configuration file, each server must have a file in
    the *dataDir* directory with the name *myid*. This file must contain the ID number
    of the server, which must match the configuration file. Once these steps are complete,
    the servers will start up and communicate with one another in an ensemble.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了共享配置文件之外，每个服务器必须在*dataDir*目录中有一个名为*myid*的文件。该文件必须包含服务器的ID号，该号码必须与配置文件匹配。完成这些步骤后，服务器将启动并在集群中相互通信。
- en: Testing ZooKeeper Ensemble on a Single Machine
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单台机器上测试ZooKeeper集群
- en: It is possible to test and run a ZooKeeper ensemble on a single machine by specifying
    all hostnames in the config as `localhost` and have unique ports specified for
    `*peerPort*` and `*leaderPort*` for each instance. Additionally, a separate *zoo.cfg*
    would need to be created for each instance with a unique *dataDir* and `*clientPort*`
    defined for each instance. This can be useful for testing purposes only, but it
    is *not* recommended for production systems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在配置中指定所有主机名为`localhost`并为每个实例指定唯一的`*peerPort*`和`*leaderPort*`来在单台机器上测试和运行ZooKeeper集群。此外，每个实例都需要创建一个单独的*zoo.cfg*，其中为每个实例定义了唯一的*dataDir*和`*clientPort*`。这只对测试目的有用，但*不*建议用于生产系统。
- en: Installing a Kafka Broker
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kafka Broker
- en: Once Java and ZooKeeper are configured, you are ready to install Apache Kafka.
    The current release can be downloaded from the [Kafka website](https://oreil.ly/xLopS).
    At press time, that version is 2.8.0 running under Scala version 2.13.0\. The
    examples in this chapters are shown using version 2.7.0.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Java和ZooKeeper配置完成，您就可以安装Apache Kafka了。当前版本可以从[Kafka网站](https://oreil.ly/xLopS)下载。截至目前，该版本是在Scala版本2.13.0下运行的2.8.0版本。本章的示例是使用2.7.0版本显示的。
- en: 'The following example installs Kafka in */usr/local/kafka*, configured to use
    the ZooKeeper server started previously and to store the message log segments
    stored in */tmp/kafka-logs*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例在*/usr/local/kafka*中安装Kafka，配置为使用先前启动的ZooKeeper服务器，并将消息日志段存储在*/tmp/kafka-logs*中：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the Kafka broker is started, we can verify that it is working by performing
    some simple operations against the cluster: creating a test topic, producing some
    messages, and consuming the same messages.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Kafka经纪人启动，我们可以通过针对集群执行一些简单操作来验证它是否正常工作：创建一个测试主题，生成一些消息，并消费相同的消息。
- en: 'Create and verify a topic:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和验证主题：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Produce messages to a test topic (use Ctrl-C to stop the producer at any time):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 向测试主题生成消息（使用Ctrl-C随时停止生产者）：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Consume messages from a test topic:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从测试主题消费消息：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Deprecation of ZooKeeper Connections on Kafka CLI Utilities
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka CLI实用程序上的ZooKeeper连接的弃用
- en: If you are familiar with older versions of the Kafka utilities, you may be used
    to using a `--zookeeper` connection string. This has been deprecated in almost
    all cases. The current best practice is to use the newer `--bootstrap-server`
    option and connect directly to the Kafka broker. If you are running in a cluster,
    you can provide the host:port of any broker in the cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉Kafka实用程序的旧版本，您可能习惯于使用“--zookeeper”连接字符串。在几乎所有情况下，这已经被弃用。当前的最佳实践是使用更新的“--bootstrap-server”选项，并直接连接到Kafka经纪人。如果在集群中运行，可以提供集群中任何经纪人的主机:端口。
- en: Configuring the Broker
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置经纪人
- en: The example configuration provided with the Kafka distribution is sufficient
    to run a standalone server as a proof of concept, but most likely will not be
    sufficient for large installations. There are numerous configuration options for
    Kafka that control all aspects of setup and tuning. Most of the options can be
    left at the default settings, though, as they deal with tuning aspects of the
    Kafka broker that will not be applicable until you have a specific use case that
    requires adjusting these settings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka发行版提供的示例配置足以作为概念验证运行独立服务器，但很可能不足以满足大型安装的需求。Kafka有许多配置选项，可以控制设置和调整的各个方面。大多数选项可以保留默认设置，因为它们涉及Kafka经纪人的调整方面，直到您有特定的用例需要调整这些设置之前，这些设置都不适用。
- en: General Broker Parameters
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般经纪人参数
- en: There are several broker configuration parameters that should be reviewed when
    deploying Kafka for any environment other than a standalone broker on a single
    server. These parameters deal with the basic configuration of the broker, and
    most of them must be changed to run properly in a cluster with other brokers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署Kafka到除单个服务器上的独立经纪人之外的任何环境时，应该审查几个经纪人配置参数。这些参数涉及经纪人的基本配置，大多数必须更改才能在与其他经纪人的集群中正确运行。
- en: broker.id
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: broker.id
- en: Every Kafka broker must have an integer identifier, which is set using the `broker.id`
    configuration. By default, this integer is set to `0`, but it can be any value.
    It is essential that the integer must be unique for each broker within a single
    Kafka cluster. The selection of this number is technically arbitrary, and it can
    be moved between brokers if necessary for maintenance tasks. However, it is highly
    recommended to set this value to something intrinsic to the host so that when
    performing maintenance it is not onerous to map broker ID numbers to hosts. For
    example, if your hostnames contain a unique number (such as `host1.example.com`,
    `host2.example.com`, etc.), then `1` and `2` would be good choices for the `broker.id`
    values, respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Kafka经纪人必须有一个整数标识符，这是使用“broker.id”配置设置的。默认情况下，此整数设置为“0”，但可以是任何值。对于单个Kafka集群中的每个经纪人，这个整数必须是唯一的。选择这个数字在技术上是任意的，如果需要进行维护任务，它可以在经纪人之间移动。然而，强烈建议将此值设置为主机的某些固有值，以便在执行维护时，将经纪人ID号映射到主机上不是繁重的任务。例如，如果您的主机名包含唯一的数字（例如`host1.example.com`，`host2.example.com`等），那么分别为`broker.id`值选择`1`和`2`是不错的选择。
- en: listeners
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: listeners
- en: Older versions of Kafka used a simple `port` configuration. This can still be
    used as a backup for simple configurations but is a deprecated config. The example
    configuration file starts Kafka with a listener on TCP port 9092\. The new `listeners`
    config is a comma-separated list of URIs that we listen on with the listener names.
    If the listener name is not a common security protocol, then another config `listener.security.protocol.map`
    must also be configured. A listener is defined as `*<protocol>://<hostname>:<port>*`.
    An example of a legal `listener` config is `PLAINTEXT://localhost:9092,SSL://:9091`.
    Specifying the hostname as `0.0.0.0` will bind to all interfaces. Leaving the
    hostname empty will bind it to the default interface. Keep in mind that if a port
    lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root
    is not a recommended configuration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的旧版本使用简单的“端口”配置。这仍然可以作为简单配置的备份使用，但已经是一个不推荐的配置。示例配置文件在TCP端口9092上启动Kafka监听器。新的“listeners”配置是一个以逗号分隔的URI列表，我们使用监听器名称进行监听。如果监听器名称不是常见的安全协议，那么必须配置另一个配置“listener.security.protocol.map”。监听器被定义为“*<protocol>://<hostname>:<port>*”。合法的“listener”配置示例是“PLAINTEXT://localhost:9092,SSL://:9091”。将主机名指定为“0.0.0.0”将绑定到所有接口。将主机名留空将绑定到默认接口。请记住，如果选择低于1024的端口，Kafka必须以root用户身份启动。以root用户身份运行Kafka不是推荐的配置。
- en: zookeeper.connect
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: zookeeper.connect
- en: 'The location of the ZooKeeper used for storing the broker metadata is set using
    the `zookeeper.connect` configuration parameter. The example configuration uses
    a ZooKeeper running on port 2181 on the local host, which is specified as `localhost:2181`.
    The format for this parameter is a semicolon-separated list of `hostname:port/path`
    strings, which include:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 用于存储经纪人元数据的ZooKeeper的位置是使用“zookeeper.connect”配置参数设置的。示例配置使用在本地主机上端口2181上运行的ZooKeeper，指定为“localhost:2181”。此参数的格式是一个以分号分隔的“hostname:port/path”字符串列表，其中包括：
- en: '`hostname`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostname`'
- en: The hostname or IP address of the ZooKeeper server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper服务器的主机名或IP地址。
- en: '`port`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`port`'
- en: The client port number for the server.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的客户端端口号。
- en: '`/path`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`/path`'
- en: An optional ZooKeeper path to use as a chroot environment for the Kafka cluster.
    If it is omitted, the root path is used.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用作Kafka集群的chroot环境的可选ZooKeeper路径。如果省略，将使用根路径。
- en: If a chroot path (a path designated to act as the root directory for a given
    application) is specified and does not exist, it will be created by the broker
    when it starts up.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了chroot路径（指定为给定应用程序的根目录的路径）并且不存在，代理在启动时将创建它。
- en: Why Use a Chroot Path?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用Chroot路径？
- en: It is generally considered to be good practice to use a chroot path for the
    Kafka cluster. This allows the ZooKeeper ensemble to be shared with other applications,
    including other Kafka clusters, without a conflict. It is also best to specify
    multiple ZooKeeper servers (which are all part of the same ensemble) in this configuration.
    This allows the Kafka broker to connect to another member of the ZooKeeper ensemble
    in the event of server failure.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为使用Kafka集群的chroot路径是一个良好的做法。这允许ZooKeeper集合与其他应用程序共享，包括其他Kafka集群，而不会发生冲突。最好还要在此配置中指定多个ZooKeeper服务器（它们都是同一个集合的一部分）。这允许Kafka代理在服务器故障时连接到ZooKeeper集合的另一个成员。
- en: log.dirs
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: log.dirs
- en: Kafka persists all messages to disk, and these log segments are stored in the
    directory specified in the `log.dir` configuration. For multiple directories,
    the config `log.dirs` is preferable. If this value is not set, it will default
    back to `log.dir`. `log.dirs` is a comma-separated list of paths on the local
    system. If more than one path is specified, the broker will store partitions on
    them in a “least-used” fashion, with one partition’s log segments stored within
    the same path. Note that the broker will place a new partition in the path that
    has the least number of partitions currently stored in it, not the least amount
    of disk space used, so an even distribution of data across multiple directories
    is not guaranteed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka将所有消息持久化到磁盘，并将这些日志段存储在`log.dir`配置中指定的目录中。对于多个目录，首选使用`log.dirs`配置。如果未设置此值，它将默认回到`log.dir`。`log.dirs`是本地系统上路径的逗号分隔列表。如果指定了多个路径，代理将以“最少使用”的方式在其中存储分区，一个分区的日志段存储在同一路径中。请注意，代理将在当前存储的分区数量最少的路径中放置新的分区，而不是使用的磁盘空间最少，因此不能保证在多个目录中均匀分布数据。
- en: num.recovery.threads.per.data.dir
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: num.recovery.threads.per.data.dir
- en: 'Kafka uses a configurable pool of threads for handling log segments. Currently,
    this thread pool is used:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用可配置的线程池来处理日志段。目前，该线程池用于：
- en: When starting normally, to open each partition’s log segments
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常启动时，打开每个分区的日志段
- en: When starting after a failure, to check and truncate each partition’s log segments
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在故障后启动时，检查和截断每个分区的日志段
- en: When shutting down, to cleanly close log segments
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关闭时，清理关闭日志段
- en: By default, only one thread per log directory is used. As these threads are
    only used during startup and shutdown, it is reasonable to set a larger number
    of threads in order to parallelize operations. Specifically, when recovering from
    an unclean shutdown, this can mean the difference of several hours when restarting
    a broker with a large number of partitions! When setting this parameter, remember
    that the number configured is per log directory specified with `log.dirs`. This
    means that if `num.​recov⁠ery.threads.per.data.dir` is set to 8, and there are
    3 paths specified in `log.dirs`​, this is a total of 24 threads.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每个日志目录只使用一个线程。由于这些线程仅在启动和关闭期间使用，因此合理地设置更多的线程以并行化操作是合理的。特别是在从不干净的关闭中恢复时，这可能意味着在重新启动具有大量分区的代理时节省数小时的时间！设置此参数时，请记住配置的数量是指`log.dirs`指定的每个日志目录。这意味着如果`num.​recov⁠ery.threads.per.data.dir`设置为8，并且在`log.dirs`中指定了3个路径，则总共有24个线程。
- en: auto.create.topics.enable
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: auto.create.topics.enable
- en: 'The default Kafka configuration specifies that the broker should automatically
    create a topic under the following circumstances:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Kafka配置指定代理在以下情况下应自动创建主题：
- en: When a producer starts writing messages to the topic
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当生产者开始向主题写入消息时
- en: When a consumer starts reading messages from the topic
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当消费者开始从主题中读取消息时
- en: When any client requests metadata for the topic
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当任何客户端请求主题的元数据时
- en: In many situations, this can be undesirable behavior, especially as there is
    no way to validate the existence of a topic through the Kafka protocol without
    causing it to be created. If you are managing topic creation explicitly, whether
    manually or through a provisioning system, you can set the `auto.create.topics.enable`
    configuration to `false`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这可能是不希望的行为，特别是因为没有办法通过Kafka协议验证主题的存在而不导致其被创建。如果您正在显式管理主题创建，无论是手动还是通过配置系统，都可以将`auto.create.topics.enable`配置设置为`false`。
- en: auto.leader.rebalance.enable
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: auto.leader.rebalance.enable
- en: In order to ensure a Kafka cluster doesn’t become unbalanced by having all topic
    leadership on one broker, this config can be specified to ensure leadership is
    balanced as much as possible. It enables a background thread that checks the distribution
    of partitions at regular intervals (this interval is configurable via `leader.​imbal⁠ance.check.interval.seconds`).
    If leadership imbalance exceeds another config, `leader.imbalance.per.broker.percentage`,
    then a rebalance of preferred leaders for partitions is started.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保Kafka集群不会因为所有主题领导都在一个代理上而变得不平衡，可以指定此配置以尽可能平衡领导。它启用了一个后台线程，定期检查分区的分布（此间隔可通过`leader.​imbal⁠ance.check.interval.seconds`进行配置）。如果领导不平衡超过另一个配置`leader.imbalance.per.broker.percentage`，则会开始重新平衡分区的首选领导。
- en: delete.topic.enable
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: delete.topic.enable
- en: Depending on your environment and data retention guidelines, you may wish to
    lock down a cluster to prevent arbitrary deletions of topics. Disabling topic
    deletion can be set by setting this flag to `false`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的环境和数据保留指南，您可能希望锁定集群以防止任意删除主题。通过将此标志设置为`false`，可以禁用主题删除。
- en: Topic Defaults
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题默认值
- en: The Kafka server configuration specifies many default configurations for topics
    that are created. Several of these parameters, including partition counts and
    message retention, can be set per topic using the administrative tools (covered
    in [Chapter 12](ch12.html#administering_kafka)). The defaults in the server configuration
    should be set to baseline values that are appropriate for the majority of the
    topics in the cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka服务器配置指定了为创建的主题设置的许多默认配置。其中包括分区计数和消息保留等参数，可以使用管理工具（在[第12章](ch12.html#administering_kafka)中介绍）针对每个主题进行设置。服务器配置中的默认值应设置为适用于集群中大多数主题的基线值。
- en: Using Per-Topic Overrides
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用每个主题的覆盖
- en: In older versions of Kafka, it was possible to specify per-topic overrides for
    these configurations in the broker configuration using the parameters `log.retention.hours.per.topic`,
    `log.reten⁠tion.​bytes.per.topic`, and `log.segment.bytes.per.topic`. These parameters
    are no longer supported, and overrides must be specified using the administrative
    tools.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧版本的Kafka中，可以使用代理配置中的参数`log.retention.hours.per.topic`、`log.reten⁠tion.​bytes.per.topic`和`log.segment.bytes.per.topic`为这些配置指定每个主题的覆盖。这些参数不再受支持，必须使用管理工具指定覆盖。
- en: num.partitions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: num.partitions
- en: The `num.partitions` parameter determines how many partitions a new topic is
    created with, primarily when automatic topic creation is enabled (which is the
    default setting). This parameter defaults to one partition. Keep in mind that
    the number of partitions for a topic can only be increased, never decreased. This
    means that if a topic needs to have fewer partitions than `num.partitions`, care
    will need to be taken to manually create the topic (discussed in [Chapter 12](ch12.html#administering_kafka)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`num.partitions`参数确定新主题创建时创建多少个分区，主要是在启用自动主题创建时（这是默认设置）使用。此参数默认为一个分区。请记住，主题的分区数量只能增加，不能减少。这意味着如果主题需要比`num.partitions`更少的分区，就需要小心地手动创建主题（在[第12章](ch12.html#administering_kafka)中讨论）。'
- en: As described in [Chapter 1](ch01.html#meet_kafka), partitions are the way a
    topic is scaled within a Kafka cluster, which makes it important to use partition
    counts that will balance the message load across the entire cluster as brokers
    are added. Many users will have the partition count for a topic be equal to, or
    a multiple of, the number of brokers in the cluster. This allows the partitions
    to be evenly distributed to the brokers, which will evenly distribute the message
    load. For example, a topic with 10 partitions operating in a Kafka cluster with
    10 hosts with leadership balanced among all 10 hosts will have optimal throughput.
    This is not a requirement, however, as you can also balance message load in other
    ways, such as having multiple topics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#meet_kafka)中所述，分区是Kafka集群中扩展主题的方式，这使得使用能够平衡整个集群中的消息负载的分区计数变得重要，因为添加代理时会增加负载。许多用户将主题的分区计数设置为等于或是集群中代理数量的倍数。这样可以使分区均匀分布到代理中，从而均匀分布消息负载。例如，在Kafka集群中运行的具有10个分区的主题，如果有10个主机且领导权在所有10个主机之间平衡，将具有最佳吞吐量。然而，这并不是必须的，因为您也可以通过其他方式平衡消息负载，比如使用多个主题。
- en: With all this in mind, it’s clear that you want many partitions, but not too
    many. If you have some estimate regarding the target throughput of the topic and
    the expected throughput of the consumers, you can divide the target throughput
    by the expected consumer throughput and derive the number of partitions this way.
    So if we want to be able to write and read 1 GBps from a topic, and we know each
    consumer can only process 50 MBps, then we know we need at least 20 partitions.
    This way, we can have 20 consumers reading from the topic and achieve 1 GBps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一切，很明显您希望有很多分区，但不要太多。如果您对主题的目标吞吐量和消费者的预期吞吐量有一些估计，可以将目标吞吐量除以预期消费者吞吐量，以此确定分区数量。因此，如果我们希望能够从主题中写入和读取1GBps，并且我们知道每个消费者只能处理50MBps，那么我们知道至少需要20个分区。这样，我们可以有20个消费者从主题中读取，并实现1GBps的吞吐量。
- en: If you don’t have this detailed information, our experience suggests that limiting
    the size of the partition on the disk to less than 6 GB per day of retention often
    gives satisfactory results. Starting small and expanding as needed is easier than
    starting too large.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有这些详细信息，我们的经验表明，将磁盘上的分区大小限制在每天不到6GB的保留量通常会产生令人满意的结果。从小开始，根据需要扩展比从大开始更容易。
- en: default.replication.factor
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: default.replication.factor
- en: If auto-topic creation is enabled, this configuration sets what the replication
    factor should be for new topics. Replication strategy can vary depending on the
    desired durability or availability of a cluster and will be discussed more in
    later chapters. The following is a brief recommendation if you are running Kafka
    in a cluster that will prevent outages due to factors outside of Kafka’s internal
    capabilities, such as hardware failures.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了自动主题创建，此配置设置了新主题的复制因子应该是多少。复制策略可以根据集群的所需耐久性或可用性而变化，并将在后面的章节中进行更多讨论。如果您在集群中运行Kafka，可以防止由Kafka内部能力之外的因素（如硬件故障）导致的故障，以下是一个简要建议。
- en: It is highly recommended to set the replication factor to at least 1 above the
    `min.insync.replicas` setting. For more fault-resistant settings, if you have
    large enough clusters and enough hardware, setting your replication factor to
    2 above the `min.insync.replicas` (abbreviated as RF++) can be preferable. RF++
    will allow easier maintenance and prevent outages. The reasoning behind this recommendation
    is to allow for one planned outage within the replica set and one unplanned outage
    to occur simultaneously. For a typical cluster, this would mean you’d have a minimum
    of three replicas of every partition. An example of this is if there is a network
    switch outage, disk failure, or some other unplanned problem during a rolling
    deployment or upgrade of Kafka or the underlying OS, you can be assured there
    will still be an additional replica available. This will be discussed more in
    [Chapter 7](ch07.html#reliable_data_delivery).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议将复制因子设置为至少高于`min.insync.replicas`设置的1。对于更具容错性的设置，如果您有足够大的集群和足够的硬件，将复制因子设置为高于`min.insync.replicas`的2（简称为RF++）可能更可取。RF++将使维护更容易，并防止停机。这个建议的原因是允许在副本集中同时发生一个计划内的停机和一个非计划内的停机。对于典型的集群，这意味着每个分区至少有三个副本。例如，如果在Kafka或底层操作系统的滚动部署或升级期间发生网络交换机故障、磁盘故障或其他非计划问题，您可以确保仍然有额外的副本可用。这将在[第7章](ch07.html#reliable_data_delivery)中进一步讨论。
- en: log.retention.ms
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志保留时间（log.retention.ms）
- en: The most common configuration for how long Kafka will retain messages is by
    time. The default is specified in the configuration file using the `log.retention.hours`
    parameter, and it is set to 168 hours, or one week. However, there are two other
    parameters allowed, `log.retention.minutes` and `log.retention.ms`. All three
    of these control the same goal (the amount of time after which messages may be
    deleted), but the recommended parameter to use is `log.retention.ms`, as the smaller
    unit size will take precedence if more than one is specified. This will ensure
    that the value set for `log.retention.ms` is always the one used. If more than
    one is specified, the smaller unit size will take precedence.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka保留消息的最常见配置是按时间。默认值在配置文件中使用`log.retention.hours`参数指定，设置为168小时，即一周。然而，还允许使用另外两个参数，`log.retention.minutes`和`log.retention.ms`。所有这三个参数都控制相同的目标（消息可能被删除的时间），但建议使用的参数是`log.retention.ms`，因为如果指定了多个参数，较小的单位大小将优先。这将确保始终使用`log.retention.ms`设置的值。如果指定了多个参数，较小的单位大小将优先。
- en: Retention by Time and Last Modified Times
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按时间和最后修改时间保留
- en: Retention by time is performed by examining the last modified time (mtime) on
    each log segment file on disk. Under normal cluster operations, this is the time
    that the log segment was closed, and represents the timestamp of the last message
    in the file. However, when using administrative tools to move partitions between
    brokers, this time is not accurate and will result in excess retention for these
    partitions. For more information on this, see [Chapter 12](ch12.html#administering_kafka)
    discussing partition moves.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 按时间保留是通过检查磁盘上每个日志段文件的最后修改时间（mtime）来执行的。在正常的集群操作下，这是日志段关闭的时间，并代表文件中最后一条消息的时间戳。然而，当使用管理工具在代理之间移动分区时，这个时间是不准确的，会导致这些分区的过度保留。有关此信息，请参阅[第12章](ch12.html#administering_kafka)讨论分区移动。
- en: log.retention.bytes
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志保留字节数（log.retention.bytes）
- en: Another way to expire messages is based on the total number of bytes of messages
    retained. This value is set using the `log.retention.bytes` parameter, and it
    is applied per partition. This means that if you have a topic with 8 partitions,
    and `log.retention.bytes` is set to 1 GB, the amount of data retained for the
    topic will be 8 GB at most. Note that all retention is performed for individual
    partitions, not the topic. This means that should the number of partitions for
    a topic be expanded, the retention will also increase if `log.retention.bytes`
    is used. Setting the value to –1 will allow for infinite retention.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种过期消息的方法是基于保留的消息总字节数。这个值是使用`log.retention.bytes`参数设置的，它是针对每个分区应用的。这意味着如果您有一个包含8个分区的主题，并且`log.retention.bytes`设置为1GB，则主题保留的数据量最多为8GB。请注意，所有保留都是针对单个分区执行的，而不是主题。这意味着如果主题的分区数量扩大，使用`log.retention.bytes`时保留也会增加。将值设置为-1将允许无限保留。
- en: Configuring Retention by Size and Time
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按大小和时间配置保留
- en: If you have specified a value for both `log.retention.bytes` and `log.retention.ms`
    (or another parameter for retention by time), messages may be removed when either
    criteria is met. For example, if `log.retention.ms` is set to 86400000 (1 day)
    and `log.​reten⁠tion.bytes` is set to 1000000000 (1 GB), it is possible for messages
    that are less than 1 day old to get deleted if the total volume of messages over
    the course of the day is greater than 1 GB. Conversely, if the volume is less
    than 1 GB, messages can be deleted after 1 day even if the total size of the partition
    is less than 1 GB. It is recommended, for simplicity, to choose either size- or
    time-based retention—and not both—to prevent surprises and unwanted data loss,
    but both can be used for more advanced configurations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您同时为`log.retention.bytes`和`log.retention.ms`（或其他按时间保留的参数）指定了值，则在满足任一标准时可能会删除消息。例如，如果`log.retention.ms`设置为86400000（1天），而`log.retention.bytes`设置为1000000000（1GB），如果一天内的消息总量大于1GB，则可能会删除不到1天的消息。相反，如果总量小于1GB，则即使分区的总大小小于1GB，也可能在1天后删除消息。为了简单起见，建议选择基于大小或时间的保留方式，而不是两者兼用，以防止意外和不必要的数据丢失，但更高级的配置可以同时使用两者。
- en: log.segment.bytes
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志段字节数（log.segment.bytes）
- en: The log retention settings previously mentioned operate on log segments, not
    individual messages. As messages are produced to the Kafka broker, they are appended
    to the current log segment for the partition. Once the log segment has reached
    the size specified by the `log.segment.bytes` parameter, which defaults to 1 GB,
    the log segment is closed and a new one is opened. Once a log segment has been
    closed, it can be considered for expiration. A smaller log segment size means
    that files must be closed and allocated more often, which reduces the overall
    efficiency of disk writes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 先前提到的日志保留设置是针对日志段而不是单个消息的。当消息被生产到Kafka代理时，它们会附加到分区的当前日志段。一旦日志段达到由`log.segment.bytes`参数指定的大小（默认为1GB），日志段将关闭并打开一个新的日志段。一旦日志段关闭，就可以考虑将其过期。较小的日志段大小意味着文件必须更频繁地关闭和分配，这会降低磁盘写入的整体效率。
- en: Adjusting the size of the log segments can be important if topics have a low
    produce rate. For example, if a topic receives only 100 megabytes per day of messages,
    and `log.segment.bytes` is set to the default, it will take 10 days to fill one
    segment. As messages cannot be expired until the log segment is closed, if `log.retention.ms`
    is set to 604800000 (1 week), there will actually be up to 17 days of messages
    retained until the closed log segment expires. This is because once the log segment
    is closed with the current 10 days of messages, that log segment must be retained
    for 7 days before it expires based on the time policy (as the segment cannot be
    removed until the last message in the segment can be expired).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果主题的生产速率较低，则调整日志段的大小可能很重要。例如，如果一个主题每天只接收100兆字节的消息，并且`log.segment.bytes`设置为默认值，则需要10天才能填满一个段。由于消息直到日志段关闭后才能过期，如果`log.retention.ms`设置为604800000（1周），则实际上将保留多达17天的消息，直到关闭的日志段过期。这是因为一旦当前有10天的消息的日志段关闭，必须在根据时间策略过期之前保留该日志段7天（因为在最后一条消息过期之前无法删除该段）。
- en: Retrieving Offsets by Timestamp
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按时间戳检索偏移量
- en: The size of the log segment also affects the behavior of fetching offsets by
    timestamp. When requesting offsets for a partition at a specific timestamp, Kafka
    finds the log segment file that was being written at that time. It does this by
    using the creation and last modified time of the file, and looking for a file
    that was created before the timestamp specified and last modified after the timestamp.
    The offset at the beginning of that log segment (which is also the filename) is
    returned in the response.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 日志段的大小还会影响按时间戳获取偏移量的行为。当请求特定时间戳的分区偏移量时，Kafka会找到在该时间正在写入的日志段文件。它通过使用文件的创建时间和最后修改时间来执行此操作，并寻找在指定时间戳之前创建并在指定时间戳之后最后修改的文件。响应中返回该日志段开头的偏移量（也是文件名）。
- en: log.roll.ms
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: log.roll.ms
- en: Another way to control when log segments are closed is by using the `log.roll.ms`
    parameter, which specifies the amount of time after which a log segment should
    be closed. As with the `log.retention.bytes` and `log.retention.ms` parameters,
    `log.segment.bytes` and `log.roll.ms` are not mutually exclusive properties. Kafka
    will close a log segment either when the size limit is reached or when the time
    limit is reached, whichever comes first. By default, there is no setting for `log.roll.ms`,
    which results in only closing log segments by size.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种控制日志段何时关闭的方法是使用`log.roll.ms`参数，该参数指定多长时间后应关闭日志段。与`log.retention.bytes`和`log.retention.ms`参数一样，`log.segment.bytes`和`log.roll.ms`不是互斥的属性。Kafka将在达到大小限制或时间限制时关闭日志段，以先到者为准。默认情况下，没有`log.roll.ms`设置，这导致只按大小关闭日志段。
- en: Disk Performance When Using Time-Based Segments
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于时间的日志段时的磁盘性能
- en: When using a time-based log segment limit, it is important to consider the impact
    on disk performance when multiple log segments are closed simultaneously. This
    can happen when there are many partitions that never reach the size limit for
    log segments, as the clock for the time limit will start when the broker starts
    and will always execute at the same time for these low-volume partitions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用基于时间的日志段限制时，重要的是要考虑当多个日志段同时关闭时对磁盘性能的影响。当有许多分区从未达到日志段的大小限制时，会发生这种情况，因为时间限制的时钟将在代理启动时开始，并且对于这些低容量分区，它将始终在相同的时间执行。
- en: min.insync.replicas
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: min.insync.replicas
- en: When configuring your cluster for data durability, setting `min.insync.replicas`
    to 2 ensures that at least two replicas are caught up and “in sync” with the producer.
    This is used in tandem with setting the producer config to ack “all” requests.
    This will ensure that at least two replicas (leader and one other) acknowledge
    a write for it to be successful. This can prevent data loss in scenarios where
    the leader acks a write, then suffers a failure and leadership is transferred
    to a replica that does not have a successful write. Without these durable settings,
    the producer would think it successfully produced, and the message(s) would be
    dropped on the floor and lost. However, configuring for higher durability has
    the side effect of being less efficient due to the extra overhead involved, so
    clusters with high-throughput that can tolerate occasional message loss aren’t
    recommended to change this setting from the default of 1\. See [Chapter 7](ch07.html#reliable_data_delivery)
    for more information.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在为数据耐久性配置集群时，将`min.insync.replicas`设置为2可以确保至少有两个副本与生产者“同步”。这与将生产者配置设置为确认“所有”请求一起使用。这将确保至少有两个副本（领导者和另一个副本）确认写入才能成功。这可以防止数据丢失，例如领导者确认写入，然后发生故障并且领导权转移到没有成功写入的副本的情况。没有这些耐用的设置，生产者会认为它成功生产了，但消息会被丢弃和丢失。然而，配置更高的耐久性会导致效率降低，因为涉及额外的开销，因此不建议对可以容忍偶尔消息丢失的高吞吐量集群更改此设置。有关更多信息，请参见[第7章](ch07.html#reliable_data_delivery)。
- en: message.max.bytes
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: message.max.bytes
- en: The Kafka broker limits the maximum size of a message that can be produced,
    configured by the `message.max.bytes` parameter, which defaults to 1000000, or
    1 MB. A producer that tries to send a message larger than this will receive an
    error back from the broker, and the message will not be accepted. As with all
    byte sizes specified on the broker, this configuration deals with compressed message
    size, which means that producers can send messages that are much larger than this
    value uncompressed, provided they compress to under the configured `message.max.bytes`
    size.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka经纪限制了可以生成的消息的最大大小，由`message.max.bytes`参数配置，默认为1000000，即1MB。尝试发送大于此值的消息的生产者将从经纪那里收到错误，并且消息将不被接受。与经纪上指定的所有字节大小一样，此配置涉及压缩消息大小，这意味着生产者可以发送比此值大得多的未压缩消息，只要它们压缩到配置的`message.max.bytes`大小以下。
- en: There are noticeable performance impacts from increasing the allowable message
    size. Larger messages will mean that the broker threads that deal with processing
    network connections and requests will be working longer on each request. Larger
    messages also increase the size of disk writes, which will impact I/O throughput.
    Other storage solutions, such as blob stores and/or tiered storage, may be another
    method of addressing large disk write issues, but will not be covered in this
    chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 增加允许的消息大小会对性能产生明显影响。更大的消息意味着处理网络连接和请求的经纪线程将在每个请求上工作更长时间。更大的消息还会增加磁盘写入的大小，这将影响I/O吞吐量。其他存储解决方案，如blob存储和/或分层存储，可能是解决大容量磁盘写入问题的另一种方法，但本章不涉及这些内容。
- en: Coordinating Message Size Configurations
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协调消息大小配置
- en: The message size configured on the Kafka broker must be coordinated with the
    `fetch.message.max.bytes` configuration on consumer clients. If this value is
    smaller than `message.max.bytes`, then consumers that encounter larger messages
    will fail to fetch those messages, resulting in a situation where the consumer
    gets stuck and cannot proceed. The same rule applies to the `replica.fetch.max.bytes`
    configuration on the brokers when configured in a cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka经纪上配置的消息大小必须与消费者客户端上的`fetch.message.max.bytes`配置协调。如果这个值小于`message.max.bytes`，那么遇到更大消息的消费者将无法获取这些消息，导致消费者陷入僵局无法继续。当在集群中配置时，经纪上的`replica.fetch.max.bytes`配置也适用相同规则。
- en: Selecting Hardware
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择硬件
- en: 'Selecting an appropriate hardware configuration for a Kafka broker can be more
    art than science. Kafka itself has no strict requirement on a specific hardware
    configuration and will run without issue on most systems. Once performance becomes
    a concern, however, there are several factors that can contribute to the overall
    performance bottlenecks: disk throughput and capacity, memory, networking, and
    CPU. When scaling Kafka very large, there can also be constraints on the number
    of partitions that a single broker can handle due to the amount of metadata that
    needs to be updated. Once you have determined which performance types are the
    most critical for your environment, you can select an optimized hardware configuration
    appropriate for your budget.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为Kafka经纪选择适当的硬件配置可能更多地是一门艺术而不是科学。Kafka本身对特定硬件配置没有严格要求，并且在大多数系统上都可以正常运行。然而，一旦性能成为问题，有几个因素可能导致整体性能瓶颈：磁盘吞吐量和容量，内存，网络和CPU。当扩展Kafka到非常大规模时，由于需要更新的元数据量，单个经纪可以处理的分区数量也可能受到限制。一旦确定了哪些性能类型对您的环境最为关键，您可以选择一个适合预算的优化硬件配置。
- en: Disk Throughput
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘吞吐量
- en: The performance of producer clients will be most directly influenced by the
    throughput of the broker disk that is used for storing log segments. Kafka messages
    must be committed to local storage when they are produced, and most clients will
    wait until at least one broker has confirmed that messages have been committed
    before considering the send successful. This means that faster disk writes will
    equal lower produce latency.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者客户端的性能将受到用于存储日志段的经纪磁盘吞吐量的直接影响。Kafka消息在生成时必须提交到本地存储，大多数客户端将等待至少一个经纪确认消息已提交，然后才会考虑发送成功。这意味着更快的磁盘写入将等于更低的生成延迟。
- en: The obvious decision when it comes to disk throughput is whether to use traditional
    spinning hard disk drives (HDDs) or solid-state disks (SSDs). SSDs have drastically
    lower seek and access times and will provide the best performance. HDDs, on the
    other hand, are more economical and provide more capacity per unit. You can also
    improve the performance of HDDs by using more of them in a broker, whether by
    having multiple data directories or by setting up the drives in a redundant array
    of independent disks (RAID) configuration. Other factors, such as the specific
    drive technology (e.g., serial attached storage or serial ATA), as well as the
    quality of the drive controller, will affect throughput. Generally, observations
    show that HDD drives are typically more useful for clusters with very high storage
    needs but aren’t accessed as often, while SSDs are better options if there is
    a very large number of client connections.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在磁盘吞吐量方面的明显决定是选择传统的旋转硬盘驱动器（HDD）还是固态硬盘（SSD）。SSD具有极低的搜索和访问时间，并提供最佳性能。另一方面，HDD更经济，并且每单位提供更大的容量。您还可以通过在经纪中使用更多的HDD来提高性能，无论是通过拥有多个数据目录还是通过设置冗余独立磁盘阵列（RAID）配置来设置驱动器。其他因素，如特定的驱动器技术（例如串行附加存储或串行ATA），以及驱动器控制器的质量，都会影响吞吐量。一般来说，观察表明HDD驱动器通常对于存储需求非常高但访问频率不高的集群更有用，而如果有大量客户端连接，则SSD是更好的选择。
- en: Disk Capacity
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘容量
- en: Capacity is the other side of the storage discussion. The amount of disk capacity
    that is needed is determined by how many messages need to be retained at any time.
    If the broker is expected to receive 1 TB of traffic each day, with 7 days of
    retention, then the broker will need a minimum of 7 TB of usable storage for log
    segments. You should also factor in at least 10% overhead for other files, in
    addition to any buffer that you wish to maintain for fluctuations in traffic or
    growth over time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 容量是存储讨论的另一面。所需的磁盘容量取决于任何时候需要保留多少消息。如果预计经纪人每天将接收1TB的流量，并且保留7天，那么经纪人将需要至少7TB的可用存储来存储日志段。您还应该考虑至少10%的其他文件开销，以及您希望保持用于流量波动或随时间增长的任何缓冲区。
- en: Storage capacity is one of the factors to consider when sizing a Kafka cluster
    and determining when to expand it. The total traffic for a cluster can be balanced
    across the cluster by having multiple partitions per topic, which will allow additional
    brokers to augment the available capacity if the density on a single broker will
    not suffice. The decision on how much disk capacity is needed will also be informed
    by the replication strategy chosen for the cluster (which is discussed in more
    detail in [Chapter 7](ch07.html#reliable_data_delivery)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 存储容量是确定Kafka集群规模和确定何时扩展的因素之一。通过为每个主题设置多个分区，可以在集群中平衡总流量，这将允许额外的经纪人增加可用容量，如果单个经纪人上的密度不够。对所需磁盘容量的决定也将受到为集群选择的复制策略的影响（在[第7章](ch07.html#reliable_data_delivery)中有更详细的讨论）。
- en: Memory
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存
- en: The normal mode of operation for a Kafka consumer is reading from the end of
    the partitions, where the consumer is caught up and lagging behind the producers
    very little, if at all. In this situation, the messages the consumer is reading
    are optimally stored in the system’s page cache, resulting in faster reads than
    if the broker has to reread the messages from disk. Therefore, having more memory
    available to the system for page cache will improve the performance of consumer
    clients.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka消费者的正常操作模式是从分区的末尾读取，消费者赶上生产者并且滞后很少，如果有的话。在这种情况下，消费者正在读取的消息被最佳地存储在系统的页面缓存中，这会比经纪人不得不重新从磁盘读取消息时读取更快。因此，为系统提供更多的内存用于页面缓存将提高消费者客户端的性能。
- en: Kafka itself does not need much heap memory configured for the Java Virtual
    Machine (JVM). Even a broker that is handling 150,000 messages per second and
    a data rate of 200 megabits per second can run with a 5 GB heap. The rest of the
    system memory will be used by the page cache and will benefit Kafka by allowing
    the system to cache log segments in use. This is the main reason it is not recommended
    to have Kafka colocated on a system with any other significant application, as
    it will have to share the use of the page cache. This will decrease the consumer
    performance for Kafka.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka本身不需要为Java虚拟机（JVM）配置太多堆内存。即使处理每秒15万条消息和每秒200兆位的数据速率的经纪人也可以运行5GB堆。系统内存的其余部分将被页面缓存使用，并且通过允许系统缓存正在使用的日志段来使Kafka受益。这是不建议将Kafka与任何其他重要应用程序共存的主要原因，因为它将不得不共享页面缓存的使用。这将降低Kafka的消费者性能。
- en: Networking
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: The available network throughput will specify the maximum amount of traffic
    that Kafka can handle. This can be a governing factor, combined with disk storage,
    for cluster sizing. This is complicated by the inherent imbalance between inbound
    and outbound network usage that is created by Kafka’s support for multiple consumers.
    A producer may write 1 MB per second for a given topic, but there could be any
    number of consumers that create a multiplier on the outbound network usage. Other
    operations, such as cluster replication (covered in [Chapter 7](ch07.html#reliable_data_delivery))
    and mirroring (discussed in [Chapter 10](ch10.html#cross_cluster_mirroring)),
    will also increase requirements. Should the network interface become saturated,
    it is not uncommon for cluster replication to fall behind, which can leave the
    cluster in a vulnerable state. To prevent the network from being a major governing
    factor, it is recommended to run with at least 10 Gb NICs (Network Interface Cards).
    Older machines with 1 Gb NICs are easily saturated and aren’t recommended.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的网络吞吐量将指定Kafka可以处理的最大流量。这可以是集群规模的决定性因素，结合磁盘存储。这受Kafka支持多个消费者所造成的入站和出站网络使用之间固有不平衡的影响。生产者可能为给定主题每秒写入1MB，但可能有任意数量的消费者对出站网络使用量产生乘数效应。其他操作，如集群复制（在[第7章](ch07.html#reliable_data_delivery)中介绍）和镜像（在[第10章](ch10.html#cross_cluster_mirroring)中讨论），也会增加要求。如果网络接口变得饱和，集群复制落后是很常见的，这可能使集群处于脆弱状态。为防止网络成为主要决定因素，建议使用至少10Gb的网卡（网络接口卡）。老旧的配备1Gb网卡的机器很容易饱和，不建议使用。
- en: CPU
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU
- en: Processing power is not as important as disk and memory until you begin to scale
    Kafka very large, but it will affect overall performance of the broker to some
    extent. Ideally, clients should compress messages to optimize network and disk
    usage. The Kafka broker must decompress all message batches, however, in order
    to validate the `checksum` of the individual messages and assign offsets. It then
    needs to recompress the message batch in order to store it on disk. This is where
    most of Kafka’s requirement for processing power comes from. This should not be
    the primary factor in selecting hardware, however, unless clusters become very
    large with hundreds of nodes and millions of partitions in a single cluster. At
    that point, selecting more performant CPU can help reduce cluster sizes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展Kafka之前，处理能力并不像磁盘和内存那样重要，但它会在一定程度上影响经纪人的整体性能。理想情况下，客户端应该压缩消息以优化网络和磁盘使用。然而，Kafka经纪人必须解压所有消息批次，以验证各个消息的“校验和”并分配偏移量。然后，它需要重新压缩消息批次以将其存储在磁盘上。这就是Kafka对处理能力需求的大部分来源。然而，除非集群变得非常大，单个集群中有数百个节点和数百万个分区，否则这不应该是选择硬件的主要因素。在那时，选择性能更好的CPU可以帮助减少集群大小。
- en: Kafka in the Cloud
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云中的Kafka
- en: In recent years, a more common installation for Kafka is within cloud computing
    environments, such as Microsoft Azure, Amazon’s AWS, or Google Cloud Platform.
    There are many options to have Kafka set up in the cloud and managed for you via
    vendors like Confluent or even through Azure’s own Kafka on HDInsight, but the
    following is some simple advice if you plan to manage your own Kafka clusters
    manually. In most cloud environments, you have a selection of many compute instances,
    each with a different combination of CPU, memory, IOPS, and disk. The various
    performance characteristics of Kafka must be prioritized in order to select the
    correct instance configuration to use.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Kafka在云计算环境中的安装越来越普遍，例如微软Azure、亚马逊的AWS或谷歌云平台。有许多选项可以在云中设置并通过供应商（如Confluent）或甚至通过Azure自己的HDInsight上的Kafka进行管理，但以下是一些简单的建议，如果您计划手动管理自己的Kafka集群。在大多数云环境中，您可以选择许多计算实例，每个实例都具有不同的CPU、内存、IOPS和磁盘组合。必须优先考虑Kafka的各种性能特征，以选择正确的实例配置。
- en: Microsoft Azure
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软Azure
- en: In Azure, you can manage the disks separately from the virtual machine (VM),
    so deciding your storage needs does not need to be related to the VM type selected.
    That being said, a good place to start on decisions is with the amount of data
    retention required, followed by the performance needed from the producers. If
    very low latency is necessary, I/O optimized instances utilizing premium SSD storage
    might be required. Otherwise, managed storage options (such as the Azure Managed
    Disks or the Azure Blob Storage) might be sufficient.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure中，您可以单独管理磁盘和虚拟机（VM），因此决定您的存储需求不需要与所选的VM类型相关。也就是说，决策的一个好的起点是所需的数据保留量，然后是生产者所需的性能。如果需要非常低的延迟，可能需要使用优化了I/O的实例，利用高级SSD存储。否则，托管存储选项（如Azure托管磁盘或Azure
    Blob存储）可能就足够了。
- en: In real terms, experience in Azure shows that `Standard D16s v3` instance types
    are a good choice for smaller clusters and are performant enough for most use
    cases. To match high performant hardware and CPU needs, `D64s v4` instances have
    good performance that can scale for larger clusters. It is recommended to build
    out your cluster in an Azure availability set and balance partitions across Azure
    compute fault domains to ensure availability. Once you have a VM picked out, deciding
    on storage types can come next. It is highly recommended to use Azure Managed
    Disks rather than ephemeral disks. If a VM is moved, you run the risk of losing
    all the data on your Kafka broker. HDD Managed Disks are relatively inexpensive
    but do not have clearly defined SLAs from Microsoft on availability. Premium SSDs
    or Ultra SSD configurations are much more expensive but are much quicker and are
    well supported with 99.99% SLAs from Microsoft. Alternatively, using Microsoft
    Blob Storage is an option if you are not as latency sensitive.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况中，Azure的经验表明，“标准D16s v3”实例类型对于较小的集群是一个不错的选择，并且对于大多数用例来说性能足够好。为了匹配高性能硬件和CPU需求，“D64s
    v4”实例具有良好的性能，可以扩展到更大的集群。建议在Azure可用性集中构建您的集群，并在Azure计算故障域之间平衡分区，以确保可用性。一旦选择了VM，接下来可以决定存储类型。强烈建议使用Azure托管磁盘而不是临时磁盘。如果移动VM，您可能会面临丢失Kafka经纪人上所有数据的风险。HDD托管磁盘相对便宜，但微软对可用性没有明确定义的服务级别协议（SLA）。高级SSD或Ultra
    SSD配置要贵得多，但速度更快，并且得到了微软99.99%的SLA支持。或者，如果对延迟不那么敏感，可以使用Microsoft Blob存储。
- en: Amazon Web Services
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊网络服务
- en: In AWS, if very low latency is necessary, I/O optimized instances that have
    local SSD storage might be required. Otherwise, ephemeral storage (such as the
    Amazon Elastic Block Store) might be sufficient.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，如果需要非常低的延迟，可能需要使用具有本地SSD存储的I/O优化实例。否则，临时存储（如Amazon弹性块存储）可能就足够了。
- en: A common choice in AWS is either the `m4` or `r3` instance types. The `m4` will
    allow for greater retention periods, but the throughput to the disk will be less
    because it is on elastic block storage. The `r3` instance will have much better
    throughput with local SSD drives, but those drives will limit the amount of data
    that can be retained. For the best of both worlds, it may be necessary to move
    up to either the `i2` or `d2` instance types, but they are significantly more
    expensive.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，常见的选择是“m4”或“r3”实例类型。 “m4”将允许更长的保留期，但磁盘吞吐量会较低，因为它在弹性块存储上。 “r3”实例将具有更好的本地SSD驱动器吞吐量，但这些驱动器将限制可以保留的数据量。为了兼顾两者，可能需要升级到“i2”或“d2”实例类型，但它们的价格要高得多。
- en: Configuring Kafka Clusters
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Kafka集群
- en: A single Kafka broker works well for local development work, or for a proof-of-concept
    system, but there are significant benefits to having multiple brokers configured
    as a cluster, as shown in [Figure 2-2](#fig-2-cluster). The biggest benefit is
    the ability to scale the load across multiple servers. A close second is using
    replication to guard against data loss due to single system failures. Replication
    will also allow for performing maintenance work on Kafka or the underlying systems
    while still maintaining availability for clients. This section focuses on the
    steps to configure a Kafka basic cluster. [Chapter 7](ch07.html#reliable_data_delivery)
    contains more information on replication of data and durability.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 单个Kafka经纪人适用于本地开发工作，或用于概念验证系统，但将多个经纪人配置为集群有显著的好处，如[图2-2](#fig-2-cluster)所示。最大的好处是能够跨多台服务器分配负载。其次是使用复制来防范由于单个系统故障而导致的数据丢失。复制还将允许在维护Kafka或底层系统时仍保持对客户端的可用性。本节重点介绍了配置Kafka基本集群的步骤。[第7章](ch07.html#reliable_data_delivery)包含有关数据复制和持久性的更多信息。
- en: '![kdg2 0202](assets/kdg2_0202.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: ！[kdg2 0202]（assets/kdg2_0202.png）
- en: Figure 2-2\. A simple Kafka cluster
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。一个简单的Kafka集群
- en: How Many Brokers?
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多少个经纪人？
- en: 'The appropriate size for a Kafka cluster is determined by several factors.
    Typically, the size of your cluster will be bound on the following key areas:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群的适当大小由几个因素决定。通常，您的集群大小将受以下关键领域的限制：
- en: Disk capacity
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘容量
- en: Replica capacity per broker
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个经纪人的副本容量
- en: CPU capacity
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU容量
- en: Network capacity
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络容量
- en: The first factor to consider is how much disk capacity is required for retaining
    messages and how much storage is available on a single broker. If the cluster
    is required to retain 10 TB of data and a single broker can store 2 TB, then the
    minimum cluster size is 5 brokers. In addition, increasing the replication factor
    will increase the storage requirements by at least 100%, depending on the replication
    factor setting chosen (see [Chapter 7](ch07.html#reliable_data_delivery)). Replicas
    in this case refer to the number of different brokers a single partition is copied
    to. This means that this same cluster, configured with a replication of 2, now
    needs to contain at least 10 brokers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑的第一个因素是保留消息所需的磁盘容量以及单个经纪人上可用的存储空间。如果集群需要保留10 TB的数据，单个经纪人可以存储2 TB，那么最小的集群大小是5个经纪人。此外，增加复制因子将至少增加100%的存储需求，具体取决于所选择的复制因子设置（请参阅[第7章](ch07.html#reliable_data_delivery)）。在这种情况下，副本指的是单个分区复制到的不同经纪人的数量。这意味着相同的集群，配置为复制2，现在需要至少包含10个经纪人。
- en: The other factor to consider is the capacity of the cluster to handle requests.
    This can exhibit through the other three bottlenecks mentioned earlier.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑的另一个因素是集群处理请求的能力。这可以通过前面提到的其他三个瓶颈来展示。
- en: If you have a 10-broker Kafka cluster but have over 1 million replicas (i.e.,
    500,000 partitions with a replication factor of 2) in your cluster, each broker
    is taking on approximately 100,000 replicas in an evenly balanced scenario. This
    can lead to bottlenecks in the produce, consume, and controller queues. In the
    past, official recommendations have been to have no more than 4,000 partition
    replicas per broker and no more than 200,000 partition replicas per cluster. However,
    advances in cluster efficiency have allowed Kafka to scale much larger. Currently,
    in a well-configured environment, it is recommended to not have more than 14,000
    partition replicas per broker and 1 million *replicas* per cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个包含10个经纪人的Kafka集群，但在您的集群中有超过100万个副本（即，具有复制因子2的500,000个分区），在均衡的情况下，每个经纪人承担大约100,000个副本。这可能导致生产、消费和控制器队列中的瓶颈。过去，官方建议是每个经纪人不超过4,000个分区副本，每个集群不超过200,000个分区副本。然而，集群效率的提高使得Kafka能够扩展得更大。目前，在配置良好的环境中，建议每个经纪人不要超过14,000个分区副本，每个集群不要超过1百万个副本。
- en: As previously mentioned in this chapter, CPU usually is not a major bottleneck
    for most use cases, but it can be if there is an excessive amount of client connections
    and requests on a broker. Keeping an eye on overall CPU usage based on how many
    unique clients and consumer groups there are, and expanding to meet those needs,
    can help to ensure better overall performance in large clusters. Speaking to network
    capacity, it is important to keep in mind the capacity of the network interfaces
    and whether they can handle the client traffic if there are multiple consumers
    of the data or if the traffic is not consistent over the retention period of the
    data (e.g., bursts of traffic during peak times). If the network interface on
    a single broker is used to 80% capacity at peak, and there are two consumers of
    that data, the consumers will not be able to keep up with peak traffic unless
    there are two brokers. If replication is being used in the cluster, this is an
    additional consumer of the data that must be taken into account. You may also
    want to scale out to more brokers in a cluster in order to handle performance
    concerns caused by lesser disk throughput or system memory available.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面提到的，对于大多数用例来说，CPU通常不是主要瓶颈，但如果经纪人上有过多的客户端连接和请求，它可能会成为瓶颈。根据有多少个唯一的客户端和消费者组，以及扩展以满足这些需求，可以帮助确保大型集群的更好性能。谈到网络容量，重要的是要考虑网络接口的容量，以及它们是否能够处理客户端流量，如果有多个数据的消费者，或者数据在保留期内的流量不一致（例如，在高峰时段的流量突发）。如果单个经纪人的网络接口在高峰时期使用了80%的容量，并且有两个数据的消费者，那么除非有两个经纪人，否则消费者将无法跟上高峰期的流量。如果在集群中使用了复制，这是数据的另一个额外的消费者，必须考虑到。您可能还希望扩展到更多经纪人的集群，以处理由较低的磁盘吞吐量或系统可用内存引起的性能问题。
- en: Broker Configuration
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经纪人配置
- en: There are only two requirements in the broker configuration to allow multiple
    Kafka brokers to join a single cluster. The first is that all brokers must have
    the same configuration for the `zookeeper.connect` parameter. This specifies the
    ZooKeeper ensemble and path where the cluster stores metadata. The second requirement
    is that all brokers in the cluster must have a unique value for the `broker.id`
    parameter. If two brokers attempt to join the same cluster with the same `broker.id`,
    the second broker will log an error and fail to start. There are other configuration
    parameters used when running a cluster—specifically, parameters that control replication,
    which are covered in later chapters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在经纪人配置中，只有两个要求允许多个Kafka经纪人加入单个集群。第一个是所有经纪人必须对“zookeeper.connect”参数具有相同的配置。这指定了ZooKeeper集群和路径，集群在其中存储元数据。第二个要求是集群中的所有经纪人必须具有“broker.id”参数的唯一值。如果两个经纪人尝试使用相同的“broker.id”加入同一个集群，第二个经纪人将记录错误并无法启动。在运行集群时使用了其他配置参数，特别是控制复制的参数，这些将在后面的章节中介绍。
- en: OS Tuning
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作系统调优
- en: While most Linux distributions have an out-of-the-box configuration for the
    kernel-tuning parameters that will work fairly well for most applications, there
    are a few changes that can be made for a Kafka broker that will improve performance.
    These primarily revolve around the virtual memory and networking subsystems, as
    well as specific concerns for the disk mount point used for storing log segments.
    These parameters are typically configured in the */etc/sysctl.conf* file, but
    you should refer to your Linux distribution documentation for specific details
    regarding how to adjust the kernel configuration.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数Linux发行版都有适用于内核调优参数的开箱即用配置，对于Kafka经纪人，可以进行一些改变以提高性能。这些主要围绕虚拟内存和网络子系统以及用于存储日志段的磁盘挂载点的特定问题。这些参数通常在*/etc/sysctl.conf*文件中配置，但您应参考您的Linux发行版文档，了解如何调整内核配置的具体细节。
- en: Virtual memory
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟内存
- en: In general, the Linux virtual memory system will automatically adjust itself
    for the system workload. We can make some adjustments to how swap space is handled,
    as well as to dirty memory pages, to tune these for Kafka’s workload.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Linux虚拟内存系统会自动调整以适应系统工作负载。我们可以对交换空间的处理方式以及脏内存页面进行一些调整，以调整这些内容以适应Kafka的工作负载。
- en: As with most applications, specifically ones where throughput is a concern,
    it is best to avoid swapping at (almost) all costs. The cost incurred by having
    pages of memory swapped to disk will show up as a noticeable impact on all aspects
    of performance in Kafka. In addition, Kafka makes heavy use of the system page
    cache, and if the VM system is swapping to disk, there is not enough memory being
    allocated to page cache.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数应用程序一样，特别是对吞吐量有要求的应用程序，最好尽量避免交换。将内存页面交换到磁盘会导致Kafka在性能的各个方面都有明显的影响。此外，Kafka大量使用系统页缓存，如果VM系统交换到磁盘，那么分配给页缓存的内存不足。
- en: One way to avoid swapping is simply not to configure any swap space at all.
    Having swap is not a requirement, but it does provide a safety net if something
    catastrophic happens on the system. Having swap can prevent the OS from abruptly
    killing a process due to an out-of-memory condition. For this reason, the recommendation
    is to set the `vm.swappiness` parameter to a very low value, such as 1\. The parameter
    is a percentage of how likely the VM subsystem is to use swap space rather than
    dropping pages from the page cache. It is preferable to reduce the amount of memory
    available for the page cache rather than utilize any amount of swap memory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 避免交换的一种方法就是根本不配置任何交换空间。拥有交换空间并不是一个要求，但如果系统发生灾难性事件，交换空间可以提供一个安全网。拥有交换空间可以防止操作系统因内存不足而突然终止进程。因此，建议将“vm.swappiness”参数设置为一个非常低的值，比如1。该参数是VM子系统使用交换空间而不是从页缓存中丢弃页面的可能性的百分比。最好减少可用于页缓存的内存量，而不是利用任何交换内存。
- en: Why Not Set Swappiness to Zero?
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不将Swappiness设置为零？
- en: Previously, the recommendation for `vm.swappiness` was always to set it to 0\.
    This value used to mean “do not swap unless there is an out-of-memory condition.”
    However, the meaning of this value changed as of Linux kernel version 3.5-rc1,
    and that change was backported into many distributions, including Red Hat Enterprise
    Linux kernels as of version 2.6.32-303\. This changed the meaning of the value
    0 to “never swap under any circumstances.” This is why a value of 1 is now recommended.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，“vm.swappiness”的建议总是将其设置为0。这个值曾经意味着“除非出现内存不足的情况，否则不进行交换”。然而，随着Linux内核版本3.5-rc1的改变，这个值的含义发生了变化，并且这个改变被反向移植到了许多发行版中，包括Red
    Hat企业Linux内核版本2.6.32-303。这改变了值0的含义为“在任何情况下都不进行交换”。这就是为什么现在建议使用值1。
- en: There is also a benefit to adjusting how the kernel handles dirty pages that
    must be flushed to disk. Kafka relies on disk I/O performance to provide good
    response times to producers. This is also the reason that the log segments are
    usually put on a fast disk, whether that is an individual disk with a fast response
    time (e.g., SSD) or a disk subsystem with significant NVRAM for caching (e.g.,
    RAID). The result is that the number of dirty pages that are allowed, before the
    flush background process starts writing them to disk, can be reduced. Do this
    by setting the `vm.dirty_background_ratio` value lower than the default of 10\.
    The value is a percentage of the total amount of system memory, and setting this
    value to 5 is appropriate in many situations. This setting should not be set to
    zero, however, as that would cause the kernel to continually flush pages, which
    would then eliminate the ability of the kernel to buffer disk writes against temporary
    spikes in the underlying device performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 调整内核处理必须刷新到磁盘的脏页的方式也有好处。Kafka依赖于磁盘I/O性能来为生产者提供良好的响应时间。这也是日志段通常放在快速磁盘上的原因，无论是具有快速响应时间的单独磁盘（例如SSD）还是具有大量NVRAM用于缓存的磁盘子系统（例如RAID）。结果是，在后台刷新进程开始将脏页写入磁盘之前允许的脏页数量可以减少。通过将`vm.dirty_background_ratio`的值设置为低于默认值10来实现。该值是系统内存总量的百分比，将该值设置为5在许多情况下是合适的。但是，不应将此设置为零，因为这将导致内核不断刷新页面，从而消除内核对磁盘写入的缓冲，以应对底层设备性能的暂时性波动。
- en: The total number of dirty pages allowed before the kernel forces synchronous
    operations to flush them to disk can also be increased by changing the value of
    `vm.dirty_ratio` to above the default of 20 (also a percentage of total system
    memory). There is a wide range of possible values for this setting, but between
    60 and 80 is a reasonable number. This setting does introduce a small amount of
    risk, both in regard to the amount of unflushed disk activity as well as the potential
    for long I/O pauses if synchronous flushes are forced. If a higher setting for
    `vm.dirty_ratio` is chosen, it is highly recommended that replication be used
    in the Kafka cluster to guard against system failures.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核强制同步操作将脏页刷新到磁盘之前允许的脏页总数也可以通过将`vm.dirty_ratio`的值更改为默认值20以上（也是总系统内存的百分比）来增加。对于这个设置，有很多可能的值，但在60到80之间是一个合理的数字。这个设置确实会引入一定的风险，无论是未刷新的磁盘活动量还是强制同步刷新可能导致的长时间I/O暂停。如果选择更高的`vm.dirty_ratio`设置，强烈建议在Kafka集群中使用复制来防范系统故障。
- en: 'When choosing values for these parameters, it is wise to review the number
    of dirty pages over time while the Kafka cluster is running under load, whether
    in production or simulated. The current number of dirty pages can be determined
    by checking the */proc/vmstat* file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择这些参数的值时，明智的做法是在Kafka集群在负载下运行时（无论是在生产环境还是模拟环境下）随时间审查脏页的数量。当前的脏页数量可以通过检查*/proc/vmstat*文件来确定：
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Kafka uses file descriptors for log segments and open connections. If a broker
    has a lot of partitions, then that broker needs at least *(number_of_partitions)*
    × *(partition_size/segment_size)* to track all the log segments in addition to
    the number of connections the broker makes. As such, it is recommended to update
    the `vm.max_map_count` to a very large number based on the above calculation.
    Depending on the environment, changing this value to 400,000 or 600,000 has generally
    been successful. It is also recommended to set `vm.overcommit_memory` to 0\. Setting
    the default value of 0 indicates that the kernel determines the amount of free
    memory from an application. If the property is set to a value other than zero,
    it could lead the operating system to grab too much memory, depriving memory for
    Kafka to operate optimally. This is common for applications with high ingestion
    rates.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用文件描述符来跟踪日志段和打开的连接。如果一个代理有很多分区，那么该代理至少需要*(分区数)*×*(分区大小/段大小)*来跟踪所有日志段，另外还需要跟踪代理建立的连接数。因此，建议根据上述计算将`vm.max_map_count`更新为一个非常大的数字。根据环境的不同，将这个值更改为400,000或600,000通常是成功的。还建议将`vm.overcommit_memory`设置为0。将默认值设置为0表示内核从应用程序确定空闲内存的数量。如果将属性设置为非零值，可能会导致操作系统获取过多的内存，从而剥夺Kafka进行最佳操作所需的内存。这对于具有高摄入速率的应用程序是常见的。
- en: Disk
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 磁盘
- en: 'Outside of selecting the disk device hardware, as well as the configuration
    of RAID if it is used, the choice of filesystem for this disk can have the next
    largest impact on performance. There are many different filesystems available,
    but the most common choices for local filesystems are either Ext4 (fourth extended
    filesystem) or Extents File System (XFS). XFS has become the default filesystem
    for many Linux distributions, and this is for good reason: it outperforms Ext4
    for most workloads with minimal tuning required. Ext4 can perform well but requires
    using tuning parameters that are considered less safe. This includes setting the
    commit interval to a longer time than the default of five to force less frequent
    flushes. Ext4 also introduced delayed allocation of blocks, which brings with
    it a greater chance of data loss and filesystem corruption in case of a system
    failure. The XFS filesystem also uses a delayed allocation algorithm, but it is
    generally safer than the one used by Ext4\. XFS also has better performance for
    Kafka’s workload without requiring tuning beyond the automatic tuning performed
    by the filesystem. It is also more efficient when batching disk writes, all of
    which combine to give better overall I/O throughput.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择磁盘设备硬件以及如果使用RAID则配置RAID之外，为该磁盘选择文件系统可能对性能产生更大的影响。有许多不同的文件系统可用，但本地文件系统的最常见选择要么是Ext4（第四个扩展文件系统），要么是Extents文件系统（XFS）。
    XFS已成为许多Linux发行版的默认文件系统，这是有充分理由的：它在大多数工作负载下的性能优于Ext4，而且几乎不需要进行调整。 Ext4可以表现良好，但需要使用被认为不太安全的调整参数。这包括将提交间隔设置为比默认值五更长的时间，以强制较少的刷新。
    Ext4还引入了块的延迟分配，这增加了数据丢失和文件系统损坏的风险，以防系统故障。 XFS文件系统也使用延迟分配算法，但通常比Ext4使用的算法更安全。 XFS在Kafka的工作负载下也具有更好的性能，而无需进行文件系统执行的自动调整之外的调整。在批处理磁盘写入时，它也更有效，所有这些都结合在一起，提供更好的整体I/O吞吐量。
- en: 'Regardless of which filesystem is chosen for the mount that holds the log segments,
    it is advisable to set the `noatime` mount option for the mount point. File metadata
    contains three timestamps: creation time (`ctime`), last modified time (`mtime`),
    and last access time (`atime`). By default, the `atime` is updated every time
    a file is read. This generates a large number of disk writes. The `atime` attribute
    is generally considered to be of little use, unless an application needs to know
    if a file has been accessed since it was last modified (in which case the `relatime`
    option can be used). The `atime` is not used by Kafka at all, so disabling it
    is safe. Setting `noatime` on the mount will prevent these timestamp updates from
    happening but will not affect the proper handling of the `ctime` and `mtime` attributes.
    Using the option `largeio` can also help improve efficiency for Kafka for when
    there are larger disk writes.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种文件系统用于保存日志段的挂载点，建议为挂载点设置`noatime`挂载选项。文件元数据包含三个时间戳：创建时间（`ctime`），上次修改时间（`mtime`）和上次访问时间（`atime`）。默认情况下，每次读取文件时都会更新`atime`。这会产生大量的磁盘写入。`atime`属性通常被认为没有什么用，除非应用程序需要知道文件是否自上次修改以来已被访问（在这种情况下可以使用`relatime`选项）。
    Kafka根本不使用`atime`，因此禁用它是安全的。在挂载点上设置`noatime`将阻止这些时间戳的更新，但不会影响`ctime`和`mtime`属性的正确处理。使用`largeio`选项还可以帮助提高Kafka的效率，特别是在进行更大的磁盘写入时。
- en: Networking
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络
- en: Adjusting the default tuning of the Linux networking stack is common for any
    application that generates a high amount of network traffic, as the kernel is
    not tuned by default for large, high-speed data transfers. In fact, the recommended
    changes for Kafka are the same as those suggested for most web servers and other
    networking applications. The first adjustment is to change the default and maximum
    amount of memory allocated for the send and receive buffers for each socket. This
    will significantly increase performance for large transfers. The relevant parameters
    for the send and receive buffer default size per socket are `net.core.wmem_default`
    and `net.core.rmem_default`, and a reasonable setting for these parameters is
    131072, or 128 KiB. The parameters for the send and receive buffer maximum sizes
    are `net.core.wmem_max` and `net.core.rmem_max`, and a reasonable setting is 2097152,
    or 2 MiB. Keep in mind that the maximum size does not indicate that every socket
    will have this much buffer space allocated; it only allows up to that much if
    needed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 调整Linux网络堆栈的默认调整对于任何产生大量网络流量的应用程序都很常见，因为内核默认情况下未针对大型高速数据传输进行调整。实际上，Kafka的推荐更改与大多数Web服务器和其他网络应用程序建议的更改相同。第一个调整是更改为每个套接字分配的发送和接收缓冲区的默认和最大内存量。这将显着提高大型传输的性能。每个套接字的发送和接收缓冲区默认大小的相关参数是`net.core.wmem_default`和`net.core.rmem_default`，这些参数的合理设置是131072，或128
    KiB。发送和接收缓冲区的最大大小的参数是`net.core.wmem_max`和`net.core.rmem_max`，这些参数的合理设置是2097152，或2
    MiB。请记住，最大大小并不表示每个套接字都会分配这么多的缓冲区空间；它只允许在需要时分配多达这么多的空间。
- en: In addition to the socket settings, the send and receive buffer sizes for TCP
    sockets must be set separately using the `net.ipv4.tcp_wmem` and `net.ipv4.tcp_rmem`
    parameters. These are set using three space-separated integers that specify the
    minimum, default, and maximum sizes, respectively. The maximum size cannot be
    larger than the values specified for all sockets using `net.core.wmem_max` and
    `net.core.rmem_max`. An example setting for each of these parameters is “4096
    65536 2048000,” which is a 4 KiB minimum, 64 KiB default, and 2 MiB maximum buffer.
    Based on the actual workload of your Kafka brokers, you may want to increase the
    maximum sizes to allow for greater buffering of the network connections.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 除了套接字设置之外，TCP套接字的发送和接收缓冲区大小必须使用`net.ipv4.tcp_wmem`和`net.ipv4.tcp_rmem`参数分别设置。这些参数使用三个以空格分隔的整数来指定最小、默认和最大大小。最大大小不能大于使用`net.core.wmem_max`和`net.core.rmem_max`设置的所有套接字的值。每个参数的示例设置是“4096
    65536 2048000”，这是4 KiB最小，64 KiB默认和2 MiB最大缓冲区。根据Kafka经纪人的实际工作负载，您可能希望增加最大大小，以允许更大的网络连接缓冲。
- en: There are several other network tuning parameters that are useful to set. Enabling
    TCP window scaling by setting `net.ipv4.tcp_window_scaling` to 1 will allow clients
    to transfer data more efficiently, and allow that data to be buffered on the broker
    side. Increasing the value of `net.ipv4.tcp_max_syn_backlog` above the default
    of 1024 will allow a greater number of simultaneous connections to be accepted.
    Increasing the value of `net.core.netdev_max_backlog` to greater than the default
    of 1000 can assist with bursts of network traffic, specifically when using multigigabit
    network connection speeds, by allowing more packets to be queued for the kernel
    to process them.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个其他网络调优参数是有用的。通过将`net.ipv4.tcp_window_scaling`设置为1来启用TCP窗口缩放将允许客户端更有效地传输数据，并允许数据在代理端进行缓冲。将`net.ipv4.tcp_max_syn_backlog`的值增加到默认值1024以上将允许更多的同时连接被接受。将`net.core.netdev_max_backlog`的值增加到默认值1000以上可以在网络流量突发时提供帮助，特别是在使用多千兆网络连接速度时，通过允许更多的数据包排队等待内核处理它们。
- en: Production Concerns
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产方面的考虑
- en: Once you are ready to move your Kafka environment out of testing and into your
    production operations, there are a few more things to think about that will assist
    with setting up a reliable messaging service.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您准备将Kafka环境从测试中移出并投入到生产运营中，还有一些需要考虑的事项，这将有助于建立可靠的消息服务。
- en: Garbage Collector Options
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾收集器选项
- en: Tuning the Java garbage-collection options for an application has always been
    something of an art, requiring detailed information about how the application
    uses memory and a significant amount of observation and trial and error. Thankfully,
    this has changed with Java 7 and the introduction of the Garbage-First garbage
    collector (G1GC). While G1GC was considered unstable initially, it saw marked
    improvement in JDK8 and JDK11\. It is now recommended for Kafka to use G1GC as
    the default garbage collector. G1GC is designed to automatically adjust to different
    workloads and provide consistent pause times for garbage collection over the lifetime
    of the application. It also handles large heap sizes with ease by segmenting the
    heap into smaller zones and not collecting over the entire heap in each pause.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 调整应用程序的Java垃圾收集选项一直是一种艺术，需要详细了解应用程序如何使用内存以及大量的观察和试错。幸运的是，随着Java 7和Garbage-First垃圾收集器（G1GC）的引入，情况已经改变。虽然最初G1GC被认为不稳定，但在JDK8和JDK11中有了显著改进。现在建议Kafka使用G1GC作为默认的垃圾收集器。G1GC旨在自动调整不同的工作负载，并在应用程序的生命周期内提供一致的垃圾收集暂停时间。它还通过将堆分成较小的区域并不在每次暂停中收集整个堆来轻松处理大堆大小。
- en: 'G1GC does all of this with a minimal amount of configuration in normal operation.
    There are two configuration options for G1GC used to adjust its performance:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: G1GC在正常操作中只需进行最少量的配置。有两个用于调整其性能的G1GC配置选项：
- en: '`MaxGCPauseMillis`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxGCPauseMillis`'
- en: This option specifies the preferred pause time for each garbage-collection cycle.
    It is not a fixed maximum—G1GC can and will exceed this time if required. This
    value defaults to 200 milliseconds. This means that G1GC will attempt to schedule
    the frequency of garbage collector cycles, as well as the number of zones that
    are collected in each cycle, such that each cycle will take approximately 200
    ms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项指定每个垃圾回收周期的首选暂停时间。这不是一个固定的最大值——如果需要，G1GC可以超过这个时间。默认值为200毫秒。这意味着G1GC将尝试安排垃圾收集器周期的频率，以及在每个周期中收集的区域数量，以便每个周期大约需要200毫秒。
- en: '`InitiatingHeapOccupancyPercent`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`InitiatingHeapOccupancyPercent`'
- en: This option specifies the percentage of the total heap that may be in use before
    G1GC will start a collection cycle. The default value is 45\. This means that
    G1GC will not start a collection cycle until after 45% of the heap is in use.
    This includes both the new (Eden) and old zone usage, in total.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项指定在G1GC启动收集周期之前可以使用的堆总量的百分比。默认值为45。这意味着G1GC在堆使用45%之后才会启动收集周期。这包括新（Eden）和旧区域的使用总量。
- en: The Kafka broker is fairly efficient with the way it utilizes heap memory and
    creates garbage objects, so it is possible to set these options lower. The garbage
    collector tuning options provided in this section have been found to be appropriate
    for a server with 64 GB of memory, running Kafka in a 5 GB heap. For `MaxGCPauseMillis`,
    this broker can be configured with a value of 20 ms. The value for `InitiatingHeap​Occu⁠pancyPercent`
    is set to 35, which causes garbage collection to run slightly earlier than with
    the default value.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka代理在利用堆内存和创建垃圾对象的方式上相当高效，因此可以将这些选项设置得更低。本节提供的垃圾收集器调优选项已被证明适用于具有64GB内存的服务器，在5GB堆中运行Kafka。对于`MaxGCPauseMillis`，该代理可以配置为20毫秒的值。`InitiatingHeapOccupancyPercent`的值设置为35，这会导致垃圾收集比默认值稍早地运行。
- en: 'Kafka was originally released before the G1GC collector was available and considered
    stable. Therefore, Kafka defaults to using concurrent mark and sweep garbage collection
    to ensure compatibility with all JVMs. New best practice is to use G1GC for anything
    for Java 1.8 and later. The change is easy to make via environment variables.
    Using the `start` command from earlier in the chapter, modify it as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka最初发布时G1GC收集器尚不可用且不稳定。因此，Kafka默认使用并发标记和扫描垃圾回收以确保与所有JVM的兼容性。新的最佳实践是对于Java
    1.8及更高版本使用G1GC。通过环境变量很容易进行更改。使用本章前面的`start`命令，修改如下：
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Datacenter Layout
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中心布局
- en: For testing and development environments, the physical location of the Kafka
    brokers within a datacenter is not as much of a concern, as there is not as severe
    an impact if the cluster is partially or completely unavailable for short periods
    of time. However, when serving production traffic, downtime usually means dollars
    lost, whether through loss of services to users or loss of telemetry on what the
    users are doing. This is when it becomes critical to configure replication within
    the Kafka cluster (see [Chapter 7](ch07.html#reliable_data_delivery)), which is
    also when it is important to consider the physical location of brokers in their
    racks in the datacenter. A datacenter environment that has a concept of fault
    zones is preferable. If not addressed prior to deploying Kafka, expensive maintenance
    to move servers around may be needed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试和开发环境来说，Kafka经纪人在数据中心的物理位置并不是很重要，因为如果集群在短时间内部分或完全不可用，影响就不会那么严重。然而，在生产流量服务时，停机通常意味着损失，无论是因为用户服务的丢失还是因为对用户活动的遥测数据的丢失。这时就变得至关重要配置Kafka集群内的复制（参见[第7章](ch07.html#reliable_data_delivery)），同时也要考虑经纪人在数据中心机架中的物理位置。最好选择具有故障区域概念的数据中心环境。如果在部署Kafka之前没有解决这个问题，可能需要进行昂贵的维护来移动服务器。
- en: Kafka can assign new partitions to brokers in a rack-aware manner, making sure
    that replicas for a single partition do not share a rack. To do this, the `broker.rack`
    configuration for each broker must be set properly. This config can be set to
    the fault domain in cloud environments as well for similar reasons. However, this
    only applies to partitions that are newly created. The Kafka cluster does not
    monitor for partitions that are no longer rack aware (for example, as a result
    of a partition reassignment), nor does it automatically correct this situation.
    It is recommend to use tools that keep your cluster balanced properly to maintain
    rack awareness, such as Cruise Control (see [Appendix B](app02.html#appendix_3rd_party_tools)).
    Configuring this properly will help to ensure continued rack awareness over time.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka可以以机架感知的方式将新分区分配给经纪人，确保单个分区的副本不共享一个机架。为此，必须正确设置每个经纪人的`broker.rack`配置。出于类似的原因，在云环境中也可以将此配置设置为故障域。但是，这仅适用于新创建的分区。Kafka集群不会监视不再具有机架感知的分区（例如，由于分区重新分配而导致的情况），也不会自动纠正这种情况。建议使用工具来保持集群平衡，以保持机架感知，例如Cruise
    Control（参见[附录B](app02.html#appendix_3rd_party_tools)）。正确配置这一点将有助于确保随着时间的推移继续保持机架感知。
- en: Overall, the best practice is to have each Kafka broker in a cluster installed
    in a different rack, or at the very least not share single points of failure for
    infrastructure services such as power and network. This typically means at least
    deploying the servers that will run brokers with dual power connections (to two
    different circuits) and dual network switches (with a bonded interface on the
    servers themselves to failover seamlessly). Even with dual connections, there
    is a benefit to having brokers in completely separate racks. From time to time,
    it may be necessary to perform physical maintenance on a rack or cabinet that
    requires it to be offline (such as moving servers around or rewiring power connections).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，最佳实践是将集群中的每个Kafka经纪人安装在不同的机架上，或者至少不共享基础设施服务的单点故障，如电源和网络。这通常意味着至少部署将运行经纪人的服务器具有双电源连接（连接到两个不同的电路）和双网络交换机（服务器本身具有绑定接口以实现无缝故障转移）。即使有双重连接，将经纪人放在完全不同的机架中也是有益的。不时需要对机架或机柜进行物理维护，这可能需要将其脱机（例如移动服务器或重新布线电源连接）。
- en: Colocating Applications on ZooKeeper
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将应用程序放置在ZooKeeper上
- en: Kafka utilizes ZooKeeper for storing metadata information about the brokers,
    topics, and partitions. Writes to ZooKeeper are only performed on changes to the
    membership of consumer groups or on changes to the Kafka cluster itself. This
    amount of traffic is generally minimal, and it does not justify the use of a dedicated
    ZooKeeper ensemble for a single Kafka cluster. In fact, many deployments will
    use a single ZooKeeper ensemble for multiple Kafka clusters (using a chroot ZooKeeper
    path for each cluster, as described earlier in this chapter).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka利用ZooKeeper存储有关经纪人、主题和分区的元数据信息。对ZooKeeper的写入仅在消费者组的成员资格发生变化或Kafka集群本身发生变化时才执行。这种流量通常很小，不足以为单个Kafka集群使用专用的ZooKeeper集合。事实上，许多部署将为多个Kafka集群使用单个ZooKeeper集合（对于每个集群使用chroot
    ZooKeeper路径，如本章前面所述）。
- en: Kafka Consumers, Tooling, ZooKeeper, and You
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka消费者、工具、ZooKeeper和您
- en: As time goes on, dependency on ZooKeeper is shrinking. In version 2.8.0, Kafka
    is introducing an early-access look at a completely ZooKeeper-less Kafka, but
    it is still not production ready. However, we can still see this reduced reliance
    on ZooKeeper in versions leading up to this. For example, in older versions of
    Kafka, consumers (in addition to the brokers) utilized ZooKeeper to directly store
    information about the composition of the consumer group and what topics it was
    consuming, and to periodically commit offsets for each partition being consumed
    (to enable failover between consumers in the group). With version 0.9.0.0, the
    consumer interface was changed, allowing this to be managed directly with the
    Kafka brokers. In each 2.x release of Kafka, we see additional steps to removing
    ZooKeeper from other required paths of Kafka. Administration tools now connect
    directly to the cluster and have deprecated the need to connect to ZooKeeper directly
    for operations such as topic creations, dynamic configuration changes, etc. As
    such, many of the command-line tools that previously used the `--zookeeper` flags
    have been updated to use the `--bootstrap-server` option. The `--zookeeper` options
    can still be used but have been deprecated and will be removed in the future when
    Kafka is no longer required to connect to ZooKeeper to create, manage, or consume
    from topics.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，对ZooKeeper的依赖正在减少。在2.8.0版本中，Kafka引入了一个早期版本的完全无ZooKeeper的Kafka，但它仍未达到生产就绪状态。然而，在此之前的版本中，我们仍然可以看到对ZooKeeper依赖的减少。例如，在较早的Kafka版本中，消费者（除了经纪人）利用ZooKeeper直接存储有关消费者组成和正在消费的主题的信息，并定期提交每个正在消费的分区的偏移量（以实现组内消费者之间的故障转移）。从0.9.0.0版本开始，消费者接口发生了变化，允许直接由Kafka经纪人管理这些内容。在每个Kafka的2.x版本中，我们看到了进一步的步骤，以从Kafka的其他必需路径中删除ZooKeeper。管理工具现在直接连接到集群，并已经废弃了直接连接到ZooKeeper进行主题创建、动态配置更改等操作的需要。因此，许多先前使用`--zookeeper`标志的命令行工具已经更新为使用`--bootstrap-server`选项。`--zookeeper`选项仍然可以使用，但已被废弃，并将在未来删除，当Kafka不再需要连接到ZooKeeper来创建、管理或从主题中消费时。
- en: However, there is a concern with consumers and ZooKeeper under certain configurations.
    While the use of ZooKeeper for such purposes is deprecated, consumers have a configurable
    choice to use either ZooKeeper or Kafka for committing offsets, and they can also
    configure the interval between commits. If the consumer uses ZooKeeper for offsets,
    each consumer will perform a ZooKeeper write at every interval for every partition
    it consumes. A reasonable interval for offset commits is 1 minute, as this is
    the period of time over which a consumer group will read duplicate messages in
    the case of a consumer failure. These commits can be a significant amount of ZooKeeper
    traffic, especially in a cluster with many consumers, and will need to be taken
    into account. It may be necessary to use a longer commit interval if the ZooKeeper
    ensemble is not able to handle the traffic. However, it is recommended that consumers
    using the latest Kafka libraries use Kafka for committing offsets, removing the
    dependency on ZooKeeper.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些配置下，消费者和ZooKeeper存在一些问题。虽然使用ZooKeeper进行此类目的已经被废弃，但消费者可以配置选择使用ZooKeeper或Kafka来提交偏移量，并且还可以配置提交之间的间隔。如果消费者使用ZooKeeper来提交偏移量，每个消费者将在每个间隔内为其消费的每个分区执行ZooKeeper写入。偏移量提交的合理间隔是1分钟，因为这是消费者组在消费者故障的情况下读取重复消息的时间段。这些提交可能会产生大量的ZooKeeper流量，特别是在具有许多消费者的集群中，需要考虑到这一点。如果ZooKeeper集群无法处理流量，可能需要使用更长的提交间隔。然而，建议使用最新的Kafka库的消费者使用Kafka来提交偏移量，消除对ZooKeeper的依赖。
- en: Outside of using a single ensemble for multiple Kafka clusters, it is not recommended
    to share the ensemble with other applications, if it can be avoided. Kafka is
    sensitive to ZooKeeper latency and timeouts, and an interruption in communications
    with the ensemble will cause the brokers to behave unpredictably. This can easily
    cause multiple brokers to go offline at the same time should they lose ZooKeeper
    connections, which will result in offline partitions. It also puts stress on the
    cluster controller, which can show up as subtle errors long after the interruption
    has passed, such as when trying to perform a controlled shutdown of a broker.
    Other applications that can put stress on the ZooKeeper ensemble, either through
    heavy usage or improper operations, should be segregated to their own ensemble.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将单个集群用于多个Kafka集群之外，如果可以避免，不建议将集群与其他应用程序共享。Kafka对ZooKeeper的延迟和超时非常敏感，与集群的通信中断将导致经纪人的行为变得不可预测。这很容易导致多个经纪人同时下线，如果它们失去ZooKeeper连接，将导致离线分区。这也会给集群控制器带来压力，这可能会在中断过后的很长时间内显示出微妙的错误，例如在尝试对经纪人执行受控关闭时。其他应用程序可能会通过重度使用或不当操作对ZooKeeper集群施加压力，应将其隔离到自己的集群中。
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we learned how to get Apache Kafka up and running. We also covered
    picking the right hardware for your brokers, and specific concerns around getting
    set up in a production environment. Now that you have a Kafka cluster, we will
    walk through the basics of Kafka client applications. The next two chapters will
    cover how to create clients for both producing messages to Kafka ([Chapter 3](ch03.html#writing_messages_to_kafka))
    as well as consuming those messages out again ([Chapter 4](ch04.html#reading_data_from_kafka)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何启动和运行Apache Kafka。我们还涵盖了为经纪人选择合适的硬件以及在生产环境中设置的特定问题。现在您已经有了一个Kafka集群，我们将介绍Kafka客户端应用程序的基础知识。接下来的两章将介绍如何为生产消息到Kafka（[第3章](ch03.html#writing_messages_to_kafka)）以及再次消费这些消息（[第4章](ch04.html#reading_data_from_kafka)）创建客户端。

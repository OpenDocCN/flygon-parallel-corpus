["```java\nProperties kafkaProps = new Properties(); ![1](assets/1.png)\nkafkaProps.put(\"bootstrap.servers\", \"broker1:9092,broker2:9092\");\n\nkafkaProps.put(\"key.serializer\",\n    \"org.apache.kafka.common.serialization.StringSerializer\"); ![2](assets/2.png)\nkafkaProps.put(\"value.serializer\",\n    \"org.apache.kafka.common.serialization.StringSerializer\");\n\nproducer = new KafkaProducer<String, String>(kafkaProps); ![3](assets/3.png)\n```", "```java\nProducerRecord<String, String> record =\n    new ProducerRecord<>(\"CustomerCountry\", \"Precision Products\",\n        \"France\"); ![1](assets/1.png)\ntry {\n    producer.send(record); ![2](assets/2.png)\n} catch (Exception e) {\n    e.printStackTrace(); ![3](assets/3.png)\n}\n```", "```java\nProducerRecord<String, String> record =\n    new ProducerRecord<>(\"CustomerCountry\", \"Precision Products\", \"France\");\ntry {\n    producer.send(record).get(); ![1](assets/1.png)\n} catch (Exception e) {\n    e.printStackTrace(); ![2](assets/2.png)\n}\n```", "```java\nprivate class DemoProducerCallback implements Callback { ![1](assets/1.png)\n    @Override\n    public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n        if (e != null) {\n            e.printStackTrace(); ![2](assets/2.png)\n        }\n    }\n}\n\nProducerRecord<String, String> record =\n    new ProducerRecord<>(\"CustomerCountry\", \"Biomedical Materials\", \"USA\"); ![3](assets/3.png)\nproducer.send(record, new DemoProducerCallback()); ![4](assets/4.png)\n```", "```java\npublic class Customer {\n    private int customerID;\n    private String customerName;\n\n    public Customer(int ID, String name) {\n        this.customerID = ID;\n        this.customerName = name;\n    }\n\n    public int getID() {\n        return customerID;\n    }\n\n    public String getName() {\n        return customerName;\n    }\n}\n```", "```java\nimport org.apache.kafka.common.errors.SerializationException;\n\nimport java.nio.ByteBuffer;\nimport java.util.Map;\n\npublic class CustomerSerializer implements Serializer<Customer> {\n\n    @Override\n    public void configure(Map configs, boolean isKey) {\n        // nothing to configure\n    }\n\n    @Override\n    /**\n    We are serializing Customer as:\n    4 byte int representing customerId\n    4 byte int representing length of customerName in UTF-8 bytes (0 if\n        name is Null)\n    N bytes representing customerName in UTF-8\n    **/\n    public byte[] serialize(String topic, Customer data) {\n        try {\n            byte[] serializedName;\n            int stringSize;\n            if (data == null)\n                return null;\n            else {\n                if (data.getName() != null) {\n                    serializedName = data.getName().getBytes(\"UTF-8\");\n                    stringSize = serializedName.length;\n                } else {\n                    serializedName = new byte[0];\n                    stringSize = 0;\n                }\n            }\n\n            ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + stringSize);\n            buffer.putInt(data.getID());\n            buffer.putInt(stringSize);\n            buffer.put(serializedName);\n\n            return buffer.array();\n        } catch (Exception e) {\n            throw new SerializationException(\n                \"Error when serializing Customer to byte[] \" + e);\n        }\n    }\n\n    @Override\n    public void close() {\n        // nothing to close\n    }\n}\n```", "```java\n{\"namespace\": \"customerManagement.avro\",\n \"type\": \"record\",\n \"name\": \"Customer\",\n \"fields\": [\n     {\"name\": \"id\", \"type\": \"int\"},\n     {\"name\": \"name\",  \"type\": \"string\"},\n     {\"name\": \"faxNumber\", \"type\": [\"null\", \"string\"], \"default\": \"null\"} ![1](assets/1.png)\n ]\n}\n```", "```java\n{\"namespace\": \"customerManagement.avro\",\n \"type\": \"record\",\n \"name\": \"Customer\",\n \"fields\": [\n     {\"name\": \"id\", \"type\": \"int\"},\n     {\"name\": \"name\",  \"type\": \"string\"},\n     {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": \"null\"}\n ]\n}\n```", "```java\nProperties props = new Properties();\n\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"key.serializer\",\n   \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"value.serializer\",\n   \"io.confluent.kafka.serializers.KafkaAvroSerializer\"); ![1](assets/1.png)\nprops.put(\"schema.registry.url\", schemaUrl); ![2](assets/2.png)\n\nString topic = \"customerContacts\";\n\nProducer<String, Customer> producer = new KafkaProducer<>(props); ![3](assets/3.png)\n\n// We keep producing new events until someone ctrl-c\nwhile (true) {\n    Customer customer = CustomerGenerator.getNext(); ![4](assets/4.png)\n    System.out.println(\"Generated customer \" +\n        customer.toString());\n    ProducerRecord<String, Customer> record =\n        new ProducerRecord<>(topic, customer.getName(), customer); ![5](assets/5.png)\n    producer.send(record); ![6](assets/6.png)\n}\n```", "```java\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"key.serializer\",\n   \"io.confluent.kafka.serializers.KafkaAvroSerializer\"); ![1](assets/1.png)\nprops.put(\"value.serializer\",\n   \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"schema.registry.url\", url); ![2](assets/2.png)\n\nString schemaString =\n    \"{\\\"namespace\\\": \\\"customerManagement.avro\\\",\n     \"\\\"type\\\": \\\"record\\\", \" + ![3](assets/3.png)\n     \"\\\"name\\\": \\\"Customer\\\",\" +\n     \"\\\"fields\\\": [\" +\n      \"{\\\"name\\\": \\\"id\\\", \\\"type\\\": \\\"int\\\"},\" +\n      \"{\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"},\" +\n      \"{\\\"name\\\": \\\"email\\\", \\\"type\\\": \" + \"[\\\"null\\\",\\\"string\\\"], \" +\n       \"\\\"default\\\":\\\"null\\\" }\" +\n    \"]}\";\nProducer<String, GenericRecord> producer =\n   new KafkaProducer<String, GenericRecord>(props); ![4](assets/4.png)\n\nSchema.Parser parser = new Schema.Parser();\nSchema schema = parser.parse(schemaString);\n\nfor (int nCustomers = 0; nCustomers < customers; nCustomers++) {\n    String name = \"exampleCustomer\" + nCustomers;\n    String email = \"example \" + nCustomers + \"@example.com\";\n\n    GenericRecord customer = new GenericData.Record(schema); ![5](assets/5.png)\n    customer.put(\"id\", nCustomers);\n    customer.put(\"name\", name);\n    customer.put(\"email\", email);\n\n    ProducerRecord<String, GenericRecord> data =\n        new ProducerRecord<>(\"customerContacts\", name, customer);\n    producer.send(data);\n}\n```", "```java\nProducerRecord<String, String> record =\n    new ProducerRecord<>(\"CustomerCountry\", \"Laboratory Equipment\", \"USA\");\n```", "```java\nProducerRecord<String, String> record =\n    new ProducerRecord<>(\"CustomerCountry\", \"USA\"); ![1](assets/1.png)\n```", "```java\nimport org.apache.kafka.clients.producer.Partitioner;\nimport org.apache.kafka.common.Cluster;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.record.InvalidRecordException;\nimport org.apache.kafka.common.utils.Utils;\n\npublic class BananaPartitioner implements Partitioner {\n\n    public void configure(Map<String, ?> configs) {} ![1](assets/1.png)\n\n    public int partition(String topic, Object key, byte[] keyBytes,\n                         Object value, byte[] valueBytes,\n                         Cluster cluster) {\n        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n        int numPartitions = partitions.size();\n\n        if ((keyBytes == null) || (!(key instanceOf String))) ![2](assets/2.png)\n            throw new InvalidRecordException(\"We expect all messages \" +\n                \"to have customer name as key\");\n\n        if (((String) key).equals(\"Banana\"))\n            return numPartitions - 1; // Banana will always go to last partition\n\n        // Other records will get hashed to the rest of the partitions\n        return Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1);\n    }\n\n    public void close() {}\n}\n```", "```java\nProducerRecord<String, String> record =\n    new ProducerRecord<>(\"CustomerCountry\", \"Precision Products\", \"France\");\n\nrecord.headers().add(\"privacy-level\",\"YOLO\".getBytes(StandardCharsets.UTF_8));\n```", "```java\npublic class CountingProducerInterceptor implements ProducerInterceptor {\n\n  ScheduledExecutorService executorService =\n          Executors.newSingleThreadScheduledExecutor();\n  static AtomicLong numSent = new AtomicLong(0);\n  static AtomicLong numAcked = new AtomicLong(0);\n\n  public void configure(Map<String, ?> map) {\n      Long windowSize = Long.valueOf(\n              (String) map.get(\"counting.interceptor.window.size.ms\")); ![1](assets/1.png)\n      executorService.scheduleAtFixedRate(CountingProducerInterceptor::run,\n              windowSize, windowSize, TimeUnit.MILLISECONDS);\n  }\n\n  public ProducerRecord onSend(ProducerRecord producerRecord) {\n      numSent.incrementAndGet();\n      return producerRecord; ![2](assets/2.png)\n  }\n\n  public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {\n      numAcked.incrementAndGet(); ![3](assets/3.png)\n  }\n\n  public void close() {\n      executorService.shutdownNow(); ![4](assets/4.png)\n  }\n\n  public static void run() {\n      System.out.println(numSent.getAndSet(0));\n      System.out.println(numAcked.getAndSet(0));\n  }\n\n}\n```", "```java\nbin/kafka-configs  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024' --entity-name clientC --entity-type clients ![1](assets/1.png)\n\nbin/kafka-configs  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048' --entity-name user1 --entity-type users ![2](assets/2.png)\n\nbin/kafka-configs  --bootstrap-server localhost:9092 --alter --add-config 'consumer_byte_rate=2048' --entity-type users ![3](assets/3.png)\n```"]
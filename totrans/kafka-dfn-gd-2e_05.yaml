- en: 'Chapter 3\. Kafka Producers: Writing Messages to Kafka'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。Kafka生产者：向Kafka写入消息
- en: Whether you use Kafka as a queue, message bus, or data storage platform, you
    will always use Kafka by creating a producer that writes data to Kafka, a consumer
    that reads data from Kafka, or an application that serves both roles.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您将Kafka用作队列、消息总线还是数据存储平台，您始终会通过创建一个将数据写入Kafka的生产者、一个从Kafka读取数据的消费者或一个同时扮演这两个角色的应用程序来使用Kafka。
- en: For example, in a credit card transaction processing system, there will be a
    client application, perhaps an online store, responsible for sending each transaction
    to Kafka immediately when a payment is made. Another application is responsible
    for immediately checking this transaction against a rules engine and determining
    whether the transaction is approved or denied. The approve/deny response can then
    be written back to Kafka, and the response can propagate back to the online store
    where the transaction was initiated. A third application can read both transactions
    and the approval status from Kafka and store them in a database where analysts
    can later review the decisions and perhaps improve the rules engine.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在信用卡交易处理系统中，可能会有一个客户端应用程序，例如在线商店，负责在付款时立即将每笔交易发送到Kafka。另一个应用程序负责立即将此交易与规则引擎进行检查，并确定交易是否被批准或拒绝。批准/拒绝响应然后可以写回Kafka，并且响应可以传播回发起交易的在线商店。第三个应用程序可以从Kafka中读取交易和批准状态，并将它们存储在分析师稍后可以审查决策并可能改进规则引擎的数据库中。
- en: Apache Kafka ships with built-in client APIs that developers can use when developing
    applications that interact with Kafka.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka附带了内置的客户端API，开发人员在开发与Kafka交互的应用程序时可以使用这些API。
- en: In this chapter we will learn how to use the Kafka producer, starting with an
    overview of its design and components. We will show how to create `KafkaProducer`
    and `ProducerRecord` objects, how to send records to Kafka, and how to handle
    the errors that Kafka may return. We’ll then review the most important configuration
    options used to control the producer behavior. We’ll conclude with a deeper look
    at how to use different partitioning methods and serializers, and how to write
    your own serializers and partitioners.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用Kafka生产者，首先概述其设计和组件。我们将展示如何创建`KafkaProducer`和`ProducerRecord`对象，如何将记录发送到Kafka，以及如何处理Kafka可能返回的错误。然后，我们将回顾用于控制生产者行为的最重要的配置选项。最后，我们将深入了解如何使用不同的分区方法和序列化程序，以及如何编写自己的序列化程序和分区器。
- en: In [Chapter 4](ch04.html#reading_data_from_kafka), we will look at Kafka’s consumer
    client and reading data from Kafka.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#reading_data_from_kafka)中，我们将看一下Kafka的消费者客户端和从Kafka读取数据。
- en: Third-Party Clients
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三方客户端
- en: In addition to the built-in clients, Kafka has a binary wire protocol. This
    means that it is possible for applications to read messages from Kafka or write
    messages to Kafka simply by sending the correct byte sequences to Kafka’s network
    port. There are multiple clients that implement Kafka’s wire protocol in different
    programming languages, giving simple ways to use Kafka not just in Java applications
    but also in languages like C++, Python, Go, and many more. Those clients are not
    part of the Apache Kafka project, but a list of non-Java clients is maintained
    in the [project wiki](https://oreil.ly/9SbJr). The wire protocol and the external
    clients are outside the scope of the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内置的客户端，Kafka还具有二进制的传输协议。这意味着应用程序可以通过向Kafka的网络端口发送正确的字节序列来从Kafka读取消息或向Kafka写入消息。有多个客户端在不同的编程语言中实现了Kafka的传输协议，为使用Kafka提供了简单的方式，不仅可以在Java应用程序中使用Kafka，还可以在C++、Python、Go等语言中使用。这些客户端不是Apache
    Kafka项目的一部分，但非Java客户端的列表在[项目维基](https://oreil.ly/9SbJr)中进行了维护。传输协议和外部客户端不在本章的范围内。
- en: Producer Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产者概述
- en: 'There are many reasons an application might need to write messages to Kafka:
    recording user activities for auditing or analysis, recording metrics, storing
    log messages, recording information from smart appliances, communicating asynchronously
    with other applications, buffering information before writing to a database, and
    much more.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可能需要将消息写入Kafka的原因有很多：记录用户活动以进行审计或分析，记录指标，存储日志消息，记录智能设备的信息，与其他应用程序异步通信，在写入数据库之前缓冲信息等等。
- en: 'Those diverse use cases also imply diverse requirements: is every message critical,
    or can we tolerate loss of messages? Are we OK with accidentally duplicating messages?
    Are there any strict latency or throughput requirements we need to support?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的用例也意味着不同的要求：每条消息都很重要吗，还是我们可以容忍消息的丢失？我们可以意外复制消息吗？我们需要支持任何严格的延迟或吞吐量要求吗？
- en: In the credit card transaction processing example we introduced earlier, we
    can see that it is critical to never lose a single message or duplicate any messages.
    Latency should be low, but latencies up to 500 ms can be tolerated, and throughput
    should be very high—we expect to process up to a million messages a second.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前介绍的信用卡交易处理示例中，我们可以看到绝对不能丢失任何一条消息或重复任何消息是至关重要的。延迟应该很低，但可以容忍高达500毫秒的延迟，吞吐量应该非常高-我们预计每秒处理高达一百万条消息。
- en: A different use case might be to store click information from a website. In
    that case, some message loss or a few duplicates can be tolerated; latency can
    be high as long as there is no impact on the user experience. In other words,
    we don’t mind if it takes a few seconds for the message to arrive at Kafka, as
    long as the next page loads immediately after the user clicks on a link. Throughput
    will depend on the level of activity we anticipate on our website.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的用例可能是存储来自网站的点击信息。在这种情况下，可以容忍一些消息丢失或少量重复；延迟可以很高，只要不影响用户体验。换句话说，如果消息需要几秒钟才能到达Kafka，只要用户点击链接后下一页立即加载即可。吞吐量将取决于我们预期在网站上的活动水平。
- en: The different requirements will influence the way you use the producer API to
    write messages to Kafka and the configuration you use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的要求将影响您使用生产者API向Kafka写入消息的方式以及您使用的配置。
- en: While the producer API is very simple, there is a bit more that goes on under
    the hood of the producer when we send data. [Figure 3-1](#fig-1-overview) shows
    the main steps involved in sending data to Kafka.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然生产者API非常简单，但在发送数据时，在生产者的幕后会发生更多事情。[图3-1](#fig-1-overview)显示了发送数据到Kafka涉及的主要步骤。
- en: '![kdg2 0301](assets/kdg2_0301.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0301](assets/kdg2_0301.png)'
- en: Figure 3-1\. High-level overview of Kafka producer components
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。Kafka生产者组件的高级概述
- en: We start producing messages to Kafka by creating a `ProducerRecord`, which must
    include the topic we want to send the record to and a value. Optionally, we can
    also specify a key, a partition, a timestamp, and/or a collection of headers.
    Once we send the `ProducerRecord`, the first thing the producer will do is serialize
    the key and value objects to byte arrays so they can be sent over the network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建`ProducerRecord`开始向Kafka生产消息，其中必须包括我们要将记录发送到的主题和一个值。可选地，我们还可以指定一个键、一个分区、一个时间戳和/或一组标头。一旦我们发送`ProducerRecord`，生产者将首先将键和值对象序列化为字节数组，以便可以通过网络发送。
- en: Next, if we didn’t explicitly specify a partition, the data is sent to a partitioner.
    The partitioner will choose a partition for us, usually based on the `ProducerRecord`
    key. Once a partition is selected, the producer knows which topic and partition
    the record will go to. It then adds the record to a batch of records that will
    also be sent to the same topic and partition. A separate thread is responsible
    for sending those batches of records to the appropriate Kafka brokers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们没有明确指定分区，数据将被发送到分区器。分区器将为我们选择一个分区，通常基于`ProducerRecord`键。一旦选择了分区，生产者就知道记录将要发送到哪个主题和分区。然后，它将记录添加到一批记录中，这些记录也将发送到相同的主题和分区。一个单独的线程负责将这些记录批次发送到适当的Kafka代理。
- en: When the broker receives the messages, it sends back a response. If the messages
    were successfully written to Kafka, it will return a `RecordMetadata` object with
    the topic, partition, and the offset of the record within the partition. If the
    broker failed to write the messages, it will return an error. When the producer
    receives an error, it may retry sending the message a few more times before giving
    up and returning an error.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理接收到消息时，它会发送回一个响应。如果消息成功写入Kafka，它将返回一个带有记录所在主题、分区和偏移量的`RecordMetadata`对象。如果代理未能写入消息，它将返回一个错误。当生产者收到错误时，它可能会在放弃并返回错误之前尝试重新发送消息几次。
- en: Constructing a Kafka Producer
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Kafka生产者
- en: 'The first step in writing messages to Kafka is to create a producer object
    with the properties you want to pass to the producer. A Kafka producer has three
    mandatory properties:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向Kafka写入消息的第一步是创建一个具有要传递给生产者的属性的生产者对象。Kafka生产者有三个必填属性：
- en: '`bootstrap.servers`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap.servers`'
- en: List of `host:port` pairs of brokers that the producer will use to establish
    initial connection to the Kafka cluster. This list doesn’t need to include all
    brokers, since the producer will get more information after the initial connection.
    But it is recommended to include at least two, so in case one broker goes down,
    the producer will still be able to connect to the cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将用于建立与Kafka集群的初始连接的`host:port`对列表。此列表不需要包括所有代理，因为生产者在初始连接后会获取更多信息。但建议至少包括两个，这样如果一个代理宕机，生产者仍然能够连接到集群。
- en: '`key.serializer`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`key.serializer`'
- en: Name of a class that will be used to serialize the keys of the records we will
    produce to Kafka. Kafka brokers expect byte arrays as keys and values of messages.
    However, the producer interface allows, using parameterized types, any Java object
    to be sent as a key and value. This makes for very readable code, but it also
    means that the producer has to know how to convert these objects to byte arrays.
    `key.serializer` should be set to a name of a class that implements the `org.apache.kafka.common.serialization.Serializer`
    interface. The producer will use this class to serialize the key object to a byte
    array. The Kafka client package includes `ByteArraySerializer` (which doesn’t
    do much), `String​Serial⁠izer`, `IntegerSerializer`, and much more, so if you
    use common types, there is no need to implement your own serializers. Setting
    `key.serializer` is required even if you intend to send only values, but you can
    use the `Void` type for the key and the `VoidSerializer`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将用于将我们将要生产到Kafka的记录的键序列化的类的名称。Kafka代理期望消息的键和值为字节数组。但是，生产者接口允许使用参数化类型，将任何Java对象作为键和值发送。这使得代码非常易读，但也意味着生产者必须知道如何将这些对象转换为字节数组。`key.serializer`应设置为实现`org.apache.kafka.common.serialization.Serializer`接口的类的名称。生产者将使用此类将键对象序列化为字节数组。Kafka客户端包括`ByteArraySerializer`（几乎不做任何事情）、`String​Serial⁠izer`、`IntegerSerializer`等等，因此如果使用常见类型，则无需实现自己的序列化程序。即使您只打算发送值，也需要设置`key.serializer`，但是您可以使用`Void`类型作为键和`VoidSerializer`。
- en: '`value.serializer`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`value.serializer`'
- en: Name of a class that will be used to serialize the values of the records we
    will produce to Kafka. The same way you set `key.serializer` to a name of a class
    that will serialize the message key object to a byte array, you set `value.serializer`
    to a class that will serialize the message value object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将用于将我们将要生产到Kafka的记录的值序列化的类的名称。与设置`key.serializer`的方式相同，将`value.serializer`设置为将序列化消息值对象的类的名称。
- en: 'The following code snippet shows how to create a new producer by setting just
    the mandatory parameters and using defaults for everything else:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了如何通过仅设置必填参数并对其他所有内容使用默认值来创建新的生产者：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-1)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-1)'
- en: We start with a `Properties` object.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个`Properties`对象开始。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-2)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-2)'
- en: Since we plan on using strings for message key and value, we use the built-in
    `StringSerializer`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们打算使用字符串作为消息的键和值，我们使用内置的`StringSerializer`。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-3)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-3)'
- en: Here we create a new producer by setting the appropriate key and value types
    and passing the `Properties` object.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过设置适当的键和值类型并传递`Properties`对象来创建一个新的生产者。
- en: With such a simple interface, it is clear that most of the control over producer
    behavior is done by setting the correct configuration properties. Apache Kafka
    documentation covers all the [configuration options](http://bit.ly/2sMu1c8), and
    we will go over the important ones later in this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样一个简单的接口，很明显大部分对生产者行为的控制是通过设置正确的配置属性来完成的。Apache Kafka文档涵盖了所有的[配置选项](http://bit.ly/2sMu1c8)，我们稍后会在本章中讨论重要的选项。
- en: 'Once we instantiate a producer, it is time to start sending messages. There
    are three primary methods of sending messages:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们实例化了一个生产者，就是发送消息的时候了。有三种主要的发送消息的方法：
- en: Fire-and-forget
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 发送并忘
- en: We send a message to the server and don’t really care if it arrives successfully
    or not. Most of the time, it will arrive successfully, since Kafka is highly available
    and the producer will retry sending messages automatically. However, in case of
    nonretriable errors or timeout, messages will get lost and the application will
    not get any information or exceptions about this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向服务器发送一条消息，实际上并不在乎它是否成功到达。大多数情况下，它会成功到达，因为Kafka是高度可用的，生产者会自动重试发送消息。然而，在不可重试的错误或超时的情况下，消息将丢失，应用程序将不会得到任何关于此的信息或异常。
- en: Synchronous send
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同步发送
- en: Technically, Kafka producer is always asynchronous—we send a message and the
    `send()` method returns a `Future` object. However, we use `get()` to wait on
    the `Future` and see if the `send()` was successful or not before sending the
    next record.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，Kafka生产者总是异步的——我们发送一条消息，`send()`方法返回一个`Future`对象。然而，我们使用`get()`来等待`Future`，看看`send()`是否成功发送了下一条记录。
- en: Asynchronous send
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 异步发送
- en: We call the `send()` method with a callback function, which gets triggered when
    it receives a response from the Kafka broker.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`send()`方法并陦用一个回调函数，当它从Kafka代理接收到响应时会触发。
- en: In the examples that follow, we will see how to send messages using these methods
    and how to handle the different types of errors that might occur.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中，我们将看到如何使用这些方法发送消息以及如何处理可能发生的不同类型的错误。
- en: While all the examples in this chapter are single threaded, a producer object
    can be used by multiple threads to send messages.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章中的所有示例都是单线程的，但生产者对象可以被多个线程使用来发送消息。
- en: Sending a Message to Kafka
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发送消息到Kafka
- en: 'The simplest way to send a message is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 发送消息的最简单方法如下：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-1)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-1)'
- en: The producer accepts `ProducerRecord` objects, so we start by creating one.
    `ProducerRecord` has multiple constructors, which we will discuss later. Here
    we use one that requires the name of the topic we are sending data to, which is
    always a string, and the key and value we are sending to Kafka, which in this
    case are also strings. The types of the key and value must match our `key serializer`
    and `value serializer` objects.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者接受`ProducerRecord`对象，因此我们首先创建一个。`ProducerRecord`有多个构造函数，我们稍后会讨论。这里我们使用一个需要发送数据的主题名称（始终为字符串）以及我们要发送到Kafka的键和值的构造函数，这些键和值在这种情况下也是字符串。键和值的类型必须与我们的`key
    serializer`和`value serializer`对象匹配。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-2)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-2)'
- en: We use the producer object `send()` method to send the `ProducerRecord`. As
    we’ve seen in the producer architecture diagram in [Figure 3-1](#fig-1-overview),
    the message will be placed in a buffer and will be sent to the broker in a separate
    thread. The `send()` method returns a [Java `Future` object](http://bit.ly/2rG7Cg6)
    with `RecordMetadata`, but since we simply ignore the returned value, we have
    no way of knowing whether the message was sent successfully or not. This method
    of sending messages can be used when dropping a message silently is acceptable.
    This is not typically the case in production applications.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用生产者对象的`send()`方法来发送`ProducerRecord`。正如我们在[图3-1](#fig-1-overview)中看到的生产者架构图中一样，消息将被放入缓冲区，并将在单独的线程中发送到代理。`send()`方法返回一个带有`RecordMetadata`的[Java
    `Future`对象](http://bit.ly/2rG7Cg6)，但由于我们简单地忽略了返回值，我们无法知道消息是否成功发送。这种发送消息的方法可以在默默丢弃消息时使用。这在生产应用中通常不是这种情况。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-3)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-3)'
- en: While we ignore errors that may occur while sending messages to Kafka brokers
    or in the brokers themselves, we may still get an exception if the producer encountered
    errors before sending the message to Kafka. Those can be, for example, a `SerializationException`
    when it fails to serialize the message, a `Buffer​ExhaustedException` or `TimeoutException`
    if the buffer is full, or an `InterruptException` if the sending thread was interrupted.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们忽略了发送消息到Kafka代理或代理本身可能发生的错误，但如果生产者在发送消息到Kafka之前遇到错误，我们仍然可能会得到异常。例如，当无法序列化消息时会出现`SerializationException`，如果缓冲区已满会出现`Buffer​ExhaustedException`或`TimeoutException`，或者如果发送线程被中断会出现`InterruptException`。
- en: Sending a Message Synchronously
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步发送消息
- en: Sending a message synchronously is simple but still allows the producer to catch
    exceptions when Kafka responds to the produce request with an error, or when send
    retries were exhausted. The main trade-off involved is performance. Depending
    on how busy the Kafka cluster is, brokers can take anywhere from 2 ms to a few
    seconds to respond to produce requests. If you send messages synchronously, the
    sending thread will spend this time waiting and doing nothing else, not even sending
    additional messages. This leads to very poor performance, and as a result, synchronous
    sends are usually not used in production applications (but are very common in
    code examples).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同步发送消息很简单，但仍允许生产者在Kafka响应生产请求时出现错误或发送重试次数耗尽时捕获异常。涉及的主要权衡是性能。根据Kafka集群的繁忙程度，代理可能需要2毫秒到几秒钟的时间来响应生产请求。如果您同步发送消息，发送线程将花费这段时间等待，不做其他任何事情，甚至不发送其他消息。这会导致性能非常差，因此通常不会在生产应用程序中使用同步发送（但在代码示例中非常常见）。
- en: 'The simplest way to send a message synchronously is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同步发送消息的最简单方法如下：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO3-1)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO3-1)'
- en: Here, we are using `Future.get()` to wait for a reply from Kafka. This method
    will throw an exception if the record is not sent successfully to Kafka. If there
    were no errors, we will get a `RecordMetadata` object that we can use to retrieve
    the offset the message was written to and other metadata.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`Future.get()`来等待Kafka的回复。如果记录未成功发送到Kafka，此方法将抛出异常。如果没有错误，我们将获得一个`RecordMetadata`对象，可以用它来检索消息写入的偏移量和其他元数据。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO3-2)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO3-2)'
- en: If there were any errors before or while sending the record to Kafka, we will
    encounter an exception. In this case, we just print any exception we ran into.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在发送记录到Kafka之前或期间出现任何错误，我们将遇到异常。在这种情况下，我们只需打印我们遇到的任何异常。
- en: '`KafkaProducer` has two types of errors. *Retriable* errors are those that
    can be resolved by sending the message again. For example, a connection error
    can be resolved because the connection may get reestablished. A “not leader for
    partition” error can be resolved when a new leader is elected for the partition
    and the client metadata is refreshed. `KafkaProducer` can be configured to retry
    those errors automatically, so the application code will get retriable exceptions
    only when the number of retries was exhausted and the error was not resolved.
    Some errors will not be resolved by retrying—for example, “Message size too large.”
    In those cases, `KafkaProducer` will not attempt a retry and will return the exception
    immediately.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`KafkaProducer`有两种类型的错误。*可重试*错误是可以通过重新发送消息来解决的错误。例如，连接错误可以解决，因为连接可能会重新建立。当为分区选举新领导者并刷新客户端元数据时，“非分区领导者”错误可以解决。`KafkaProducer`可以配置为自动重试这些错误，因此应用程序代码只有在重试次数耗尽且错误未解决时才会收到可重试的异常。有些错误不会通过重试解决，例如“消息大小过大”。在这些情况下，`KafkaProducer`不会尝试重试，并将立即返回异常。'
- en: Sending a Message Asynchronously
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步发送消息
- en: Suppose the network round-trip time between our application and the Kafka cluster
    is 10 ms. If we wait for a reply after sending each message, sending 100 messages
    will take around 1 second. On the other hand, if we just send all our messages
    and not wait for any replies, then sending 100 messages will barely take any time
    at all. In most cases, we really don’t need a reply—Kafka sends back the topic,
    partition, and offset of the record after it was written, which is usually not
    required by the sending app. On the other hand, we do need to know when we failed
    to send a message completely so we can throw an exception, log an error, or perhaps
    write the message to an “errors” file for later analysis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的应用程序与Kafka集群之间的网络往返时间为10毫秒。如果我们在发送每条消息后等待回复，发送100条消息将花费大约1秒钟。另一方面，如果我们只发送所有消息而不等待任何回复，那么发送100条消息几乎不需要任何时间。在大多数情况下，我们确实不需要回复，Kafka在写入记录后会发送主题、分区和偏移量，通常发送应用程序不需要。另一方面，我们确实需要知道何时无法完全发送消息，以便我们可以抛出异常、记录错误，或者将消息写入“错误”文件以供以后分析。
- en: 'To send messages asynchronously and still handle error scenarios, the producer
    supports adding a callback when sending a record. Here is an example of how we
    use a callback:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要异步发送消息并仍然处理错误情况，生产者支持在发送记录时添加回调。以下是我们如何使用回调的示例：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-1)'
- en: To use callbacks, you need a class that implements the `org.apache.kafka.` `clients.producer.Callback`
    interface, which has a single function—`on​Com⁠ple⁠tion()`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用回调，您需要一个实现`org.apache.kafka.clients.producer.Callback`接口的类，该接口具有一个函数`on​Com⁠ple⁠tion()`。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-2)'
- en: If Kafka returned an error, `onCompletion()` will have a nonnull exception.
    Here we “handle” it by printing, but production code will probably have more robust
    error handling functions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Kafka返回错误，`onCompletion()`将有一个非空异常。在这里，我们通过打印来“处理”它，但生产代码可能会有更健壮的错误处理函数。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-3)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-3)'
- en: The records are the same as before.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 记录与以前相同。
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-4)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-4)'
- en: And we pass a `Callback` object along when sending the record.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 并在发送记录时传递一个`Callback`对象。
- en: Warning
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The callbacks execute in the producer’s main thread. This guarantees that when
    we send two messages to the same partition one after another, their callbacks
    will be executed in the same order that we sent them. But it also means that the
    callback should be reasonably fast to avoid delaying the producer and preventing
    other messages from being sent. It is not recommended to perform a blocking operation
    within the callback. Instead, you should use another thread to perform any blocking
    operation concurrently.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回调在生产者的主线程中执行。这保证了当我们连续向同一分区发送两条消息时，它们的回调将按照我们发送它们的顺序执行。但这也意味着回调应该相当快，以避免延迟生产者并阻止其他消息的发送。不建议在回调中执行阻塞操作。相反，您应该使用另一个线程并发执行任何阻塞操作。
- en: Configuring Producers
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置生产者
- en: So far we’ve seen very few configuration parameters for the producers—just the
    mandatory `bootstrap.servers` URI and serializers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对生产者的配置参数很少——只有强制的`bootstrap.servers` URI和序列化器。
- en: The producer has a large number of configuration parameters that are documented
    in [Apache Kafka documentation](https://oreil.ly/RkxSS), and many have reasonable
    defaults, so there is no reason to tinker with every single parameter. However,
    some of the parameters have a significant impact on memory use, performance, and
    reliability of the producers. We will review those here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者有大量的配置参数，这些参数在[Apache Kafka文档](https://oreil.ly/RkxSS)中有记录，许多参数都有合理的默认值，因此没有理由去调整每个参数。然而，一些参数对生产者的内存使用、性能和可靠性有重大影响。我们将在这里进行审查。
- en: client.id
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: client.id
- en: '`client.id` is a logical identifier for the client and the application it is
    used in. This can be any string and will be used by the brokers to identify messages
    sent from the client. It is used in logging and metrics and for quotas. Choosing
    a good client name will make troubleshooting much easier—it is the difference
    between “We are seeing a high rate of authentication failures from IP 104.27.155.134”
    and “Looks like the Order Validation service is failing to authenticate—can you
    ask Laura to take a look?”'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`client.id`是客户端和所使用的应用程序的逻辑标识符。这可以是任何字符串，并将被经纪人用于识别从客户端发送的消息。它用于日志记录和指标以及配额。选择一个好的客户端名称将使故障排除变得更容易——这是“我们看到IP
    104.27.155.134的身份验证失败率很高”和“看起来订单验证服务无法进行身份验证——你能让劳拉来看一下吗？”之间的区别。'
- en: acks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: acks
- en: 'The `acks` parameter controls how many partition replicas must receive the
    record before the producer can consider the write successful. By default, Kafka
    will respond that the record was written successfully after the leader received
    the record (release 3.0 of Apache Kafka is expected to change this default). This
    option has a significant impact on the durability of written messages, and depending
    on your use case, the default may not be the best choice. [Chapter 7](ch07.html#reliable_data_delivery)
    discusses Kafka’s reliability guarantees in depth, but for now let’s review the
    three allowed values for the `acks` parameter:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks`参数控制生产者在可以考虑写入成功之前必须接收记录的分区副本数量。默认情况下，Kafka将在领导者接收记录后回复记录已成功写入（预计Apache
    Kafka的3.0版本将更改此默认值）。此选项对写入消息的持久性有重大影响，根据您的用例，可能默认值不是最佳选择。[第7章](ch07.html#reliable_data_delivery)深入讨论了Kafka的可靠性保证，但现在让我们回顾一下`acks`参数的三个允许值：'
- en: '`acks=0`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=0`'
- en: The producer will not wait for a reply from the broker before assuming the message
    was sent successfully. This means that if something goes wrong and the broker
    does not receive the message, the producer will not know about it, and the message
    will be lost. However, because the producer is not waiting for any response from
    the server, it can send messages as fast as the network will support, so this
    setting can be used to achieve very high throughput.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者在假定消息成功发送之前不会等待经纪人的回复。这意味着如果出现问题，经纪人没有收到消息，生产者将不会知道，消息将丢失。然而，由于生产者不等待服务器的任何响应，它可以以网络支持的速度发送消息，因此可以使用此设置来实现非常高的吞吐量。
- en: '`acks=1`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=1`'
- en: The producer will receive a success response from the broker the moment the
    leader replica receives the message. If the message can’t be written to the leader
    (e.g., if the leader crashed and a new leader was not elected yet), the producer
    will receive an error response and can retry sending the message, avoiding potential
    loss of data. The message can still get lost if the leader crashes and the latest
    messages were not yet replicated to the new leader.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者将在领导者副本接收到消息时从经纪人那里收到成功响应。如果消息无法写入领导者（例如，如果领导者崩溃并且尚未选举出新的领导者），生产者将收到错误响应，并可以重试发送消息，避免数据的潜在丢失。如果领导者崩溃并且最新的消息尚未复制到新的领导者，消息仍可能丢失。
- en: '`acks=all`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=all`'
- en: The producer will receive a success response from the broker once all in sync
    replicas receive the message. This is the safest mode since you can make sure
    more than one broker has the message and that the message will survive even in
    case of a crash (more information on this in [Chapter 6](ch06.html#kafka_internals)).
    However, the latency we discussed in the `acks=1` case will be even higher, since
    we will be waiting for more than just one broker to receive the message.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有同步副本接收到消息，生产者将从经纪人那里收到成功响应。这是最安全的模式，因为您可以确保不止一个经纪人收到了消息，并且即使发生崩溃，消息也会存活下来（有关此信息的更多信息，请参见[第6章](ch06.html#kafka_internals)）。然而，我们在`acks=1`情况下讨论的延迟将更高，因为我们将等待不止一个经纪人接收消息。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You will see that with lower and less reliable `acks` configuration, the producer
    will be able to send records faster. This means that you trade off reliability
    for *producer latency*. However, *end-to-end latency* is measured from the time
    a record was produced until it is available for consumers to read and is identical
    for all three options. The reason is that, in order to maintain consistency, Kafka
    will not allow consumers to read records until they are written to all in sync
    replicas. Therefore, if you care about end-to-end latency, rather than just the
    producer latency, there is no trade-off to make: you will get the same end-to-end
    latency if you choose the most reliable option.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现，使用较低且不太可靠的`acks`配置，生产者将能够更快地发送记录。这意味着您在可靠性和*生产者延迟*之间进行权衡。但是，*端到端延迟*是从记录生成到可供消费者读取的时间，并且对于所有三个选项都是相同的。原因是，为了保持一致性，Kafka不会允许消费者读取记录，直到它们被写入所有同步副本。因此，如果您关心端到端延迟，而不仅仅是生产者延迟，那么就没有权衡可做：如果选择最可靠的选项，您将获得相同的端到端延迟。
- en: Message Delivery Time
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消息传递时间
- en: 'The producer has multiple configuration parameters that interact to control
    one of the behaviors that are of most interest to developers: how long will it
    take until a call to `send()` will succeed or fail. This is the time we are willing
    to spend until Kafka responds successfully, or until we are willing to give up
    and admit defeat.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者具有多个配置参数，这些参数相互作用以控制开发人员最感兴趣的行为之一：直到`send()`调用成功或失败需要多长时间。这是我们愿意花费的时间，直到Kafka成功响应，或者我们愿意放弃并承认失败。
- en: The configurations and their behaviors were modified several times over the
    years. We will describe here the latest implementation, introduced in Apache Kafka
    2.1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，配置及其行为已经多次修改。我们将在这里描述最新的实现，即Apache Kafka 2.1中引入的实现。
- en: 'Since Apache Kafka 2.1, we divide the time spent sending a `ProduceRecord`
    into two time intervals that are handled separately:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自Apache Kafka 2.1以来，我们将发送`ProduceRecord`的时间分为两个分别处理的时间间隔：
- en: Time until an async call to `send()` returns. During this interval, the thread
    that called `send()` will be blocked.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从调用`send()`的异步调用返回的时间。在此期间，调用`send()`的线程将被阻塞。
- en: From the time an async call to `send()` returned successfully until the callback
    is triggered (with success or failure). This is the same as from the point a `Produce​Re⁠cord`
    was placed in a batch for sending until Kafka responds with success, nonretriable
    failure, or we run out of time allocated for sending.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从异步调用`send()`成功返回直到触发回调（成功或失败）的时间。这与从将`Produce​Re⁠cord`放入批处理以进行发送直到Kafka以成功、不可重试的失败或我们用于发送的时间用完为止是相同的。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you use `send()` synchronously, the sending thread will block for both time
    intervals continuously, and you won’t be able to tell how much time was spent
    in each. We’ll discuss the common and recommended case, where `send()` is used
    asynchronously, with a callback.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您同步使用`send()`，发送线程将连续阻塞两个时间间隔，并且您将无法知道每个时间间隔花费了多少时间。我们将讨论常见和推荐的情况，即异步使用`send()`，并带有回调。
- en: The flow of data within the producer and how the different configuration parameters
    affect each other can be summarized in [Figure 3-2](#fig-2-delivery).^([1](ch03.html#idm45351110098016))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者内部数据流以及不同配置参数如何相互影响的流程可以在[图3-2](#fig-2-delivery)中总结。^([1](ch03.html#idm45351110098016))
- en: '![kdg2 0302](assets/kdg2_0302.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0302](assets/kdg2_0302.png)'
- en: Figure 3-2\. Sequence diagram of delivery time breakdown inside Kafka producer
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2。Kafka生产者内传递时间分解的序列图
- en: We’ll go through the different configuration parameters used to control the
    time spent waiting in these two intervals and how they interact.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍用于控制在这两个时间间隔中等待的时间以及它们如何相互作用的不同配置参数。
- en: max.block.ms
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: max.block.ms
- en: This parameter controls how long the producer may block when calling `send()`
    and when explicitly requesting metadata via `partitionsFor()`. Those methods may
    block when the producer’s send buffer is full or when metadata is not available.
    When `max.block.ms` is reached, a timeout exception is thrown.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数控制在调用`send()`时和通过`partitionsFor()`显式请求元数据时，生产者可能阻塞的时间。当生产者的发送缓冲区已满或元数据不可用时，这些方法可能会阻塞。当达到`max.block.ms`时，将抛出超时异常。
- en: delivery.timeout.ms
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: delivery.timeout.ms
- en: This configuration will limit the amount of time spent from the point a record
    is ready for sending (`send()` returned successfully and the record is placed
    in a batch) until either the broker responds or the client gives up, including
    time spent on retries. As you can see in [Figure 3-2](#fig-2-delivery), this time
    should be greater than `linger.ms` and `request.timeout.ms`. If you try to create
    a producer with an inconsistent timeout configuration, you will get an exception.
    Messages can be successfully sent much faster than `delivery.timeout.ms`, and
    typically will.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置将限制从记录准备发送（`send()`成功返回并将记录放入批处理）的时间，直到经纪人响应或客户端放弃，包括重试所花费的时间。如您在[图3-2](#fig-2-delivery)中所见，此时间应大于`linger.ms`和`request.timeout.ms`。如果尝试使用不一致的超时配置创建生产者，将会收到异常。消息可以成功发送的速度远快于`delivery.timeout.ms`，通常会更快。
- en: If the producer exceeds `delivery.timeout.ms` while retrying, the callback will
    be called with the exception that corresponds to the error that the broker returned
    before retrying. If `delivery.timeout.ms` is exceeded while the record batch was
    still waiting to be sent, the callback will be called with a timeout exception.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生产者在重试时超过`delivery.timeout.ms`，则回调将使用与重试前经纪人返回的错误对应的异常进行调用。如果在记录批次仍在等待发送时超过`delivery.timeout.ms`，则回调将使用超时异常进行调用。
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You can configure the delivery timeout to the maximum time you’ll want to wait
    for a message to be sent, typically a few minutes, and then leave the default
    number of retries (virtually infinite). With this configuration, the producer
    will keep retrying for as long as it has time to keep trying (or until it succeeds).
    This is a much more reasonable way to think about retries. Our normal process
    for tuning retries is: “In case of a broker crash, it typically takes leader election
    30 seconds to complete, so let’s keep retrying for 120 seconds just to be on the
    safe side.” Instead of converting this mental dialog to number of retries and
    time between retries, you just configure `deliver.timeout.ms` to 120.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将交付超时配置为您希望等待消息发送的最长时间，通常是几分钟，然后保持默认的重试次数（几乎是无限的）。使用这个配置，只要有时间继续尝试（或者直到成功），生产者将一直重试。这是一个更合理的重试方式。我们通常调整重试的过程是：“在发生代理崩溃的情况下，通常需要30秒才能完成领导者选举，所以让我们保持重试120秒，以防万一。”而不是将这种心理对话转化为重试次数和重试之间的时间，您只需将“deliver.timeout.ms”配置为120。
- en: request.timeout.ms
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: request.timeout.ms
- en: This parameter controls how long the producer will wait for a reply from the
    server when sending data. Note that this is the time spent waiting on each producer
    request before giving up; it does not include retries, time spent before sending,
    and so on. If the timeout is reached without reply, the producer will either retry
    sending or complete the callback with a `TimeoutException`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个参数控制生产者在发送数据时等待服务器回复的时间。请注意，这是在每个生产者请求等待回复的时间，而不包括重试、发送前的等待等。如果超时而没有回复，生产者将要么重试发送，要么用“TimeoutException”完成回调。
- en: retries and retry.backoff.ms
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重试和retry.backoff.ms
- en: When the producer receives an error message from the server, the error could
    be transient (e.g., a lack of leader for a partition). In this case, the value
    of the `retries` parameter will control how many times the producer will retry
    sending the message before giving up and notifying the client of an issue. By
    default, the producer will wait 100 ms between retries, but you can control this
    using the `retry.backoff.ms` parameter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当生产者从服务器收到错误消息时，错误可能是暂时的（例如，分区没有领导者）。在这种情况下，“重试”参数的值将控制生产者在放弃并通知客户端出现问题之前重试发送消息的次数。默认情况下，生产者在重试之间会等待100毫秒，但您可以使用“retry.backoff.ms”参数来控制这一点。
- en: We recommend against using these parameters in the current version of Kafka.
    Instead, test how long it takes to recover from a crashed broker (i.e., how long
    until all partitions get new leaders), and set `delivery.timeout.ms` such that
    the total amount of time spent retrying will be longer than the time it takes
    the Kafka cluster to recover from the crash—otherwise, the producer will give
    up too soon.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议不要在当前版本的Kafka中使用这些参数。相反，测试从崩溃的代理中恢复需要多长时间（即直到所有分区获得新领导者），并设置“delivery.timeout.ms”，使得重试的总时间长于Kafka集群从崩溃中恢复所需的时间——否则，生产者会放弃得太早。
- en: Not all errors will be retried by the producer. Some errors are not transient
    and will not cause retries (e.g., “message too large” error). In general, because
    the producer handles retries for you, there is no point in handling retries within
    your own application logic. You will want to focus your efforts on handling nonretriable
    errors or cases where retry attempts were exhausted.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的错误都会被生产者重试。一些错误不是暂时的，不会导致重试（例如，“消息过大”错误）。一般来说，因为生产者为您处理重试，所以在您自己的应用逻辑中处理重试是没有意义的。您将希望将精力集中在处理不可重试的错误或重试尝试耗尽的情况上。
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to completely disable retries, setting `retries=0` is the only way
    to do so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想完全禁用重试，将“retries=0”设置为唯一的方法。
- en: linger.ms
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: linger.ms
- en: '`linger.ms` controls the amount of time to wait for additional messages before
    sending the current batch. `KafkaProducer` sends a batch of messages either when
    the current batch is full or when the `linger.ms` limit is reached. By default,
    the producer will send messages as soon as there is a sender thread available
    to send them, even if there’s just one message in the batch. By setting `linger.ms`
    higher than 0, we instruct the producer to wait a few milliseconds to add additional
    messages to the batch before sending it to the brokers. This increases latency
    a little and significantly increases throughput—the overhead per message is much
    lower, and compression, if enabled, is much better.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: “linger.ms”控制在发送当前批次之前等待额外消息的时间。默认情况下，生产者会在当前批次已满或达到“linger.ms”限制时发送消息。默认情况下，只要有发送线程可用来发送消息，生产者就会立即发送消息，即使批次中只有一条消息。通过将“linger.ms”设置为大于0，我们指示生产者在将批次发送到代理之前等待几毫秒以添加额外的消息到批次中。这会稍微增加延迟，并显著增加吞吐量——每条消息的开销要低得多，如果启用了压缩，压缩效果会更好。
- en: buffer.memory
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: buffer.memory
- en: This config sets the amount of memory the producer will use to buffer messages
    waiting to be sent to brokers. If messages are sent by the application faster
    than they can be delivered to the server, the producer may run out of space, and
    additional `send()` calls will block for `max.block.ms` and wait for space to
    free up before throwing an exception. Note that unlike most producer exceptions,
    this timeout is thrown by `send()` and not by the resulting `Future`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置设置了生产者用来缓冲等待发送到代理的消息的内存量。如果应用程序发送消息的速度比它们被传递到服务器的速度快，生产者可能会用完空间，而额外的“send（）”调用将会阻塞“max.block.ms”并等待空间释放，然后才会抛出异常。请注意，与大多数生产者异常不同，这个超时是由“send（）”而不是由结果“Future”抛出的。
- en: compression.type
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: compression.type
- en: By default, messages are sent uncompressed. This parameter can be set to `snappy`,
    `gzip`, `lz4`, or `zstd`, in which case the corresponding compression algorithms
    will be used to compress the data before sending it to the brokers. Snappy compression
    was invented by Google to provide decent compression ratios with low CPU overhead
    and good performance, so it is recommended in cases where both performance and
    bandwidth are a concern. Gzip compression will typically use more CPU and time
    but results in better compression ratios, so it is recommended in cases where
    network bandwidth is more restricted. By enabling compression, you reduce network
    utilization and storage, which is often a bottleneck when sending messages to
    Kafka.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消息是未压缩的。此参数可以设置为`snappy`、`gzip`、`lz4`或`zstd`，在这种情况下，将使用相应的压缩算法对数据进行压缩，然后将其发送到经纪人。Snappy压缩是由Google发明的，以提供良好的压缩比和低CPU开销以及良好的性能，因此在性能和带宽都受到关注的情况下建议使用。Gzip压缩通常会使用更多的CPU和时间，但会产生更好的压缩比，因此在网络带宽更受限制的情况下建议使用。通过启用压缩，可以减少网络利用率和存储，这在向Kafka发送消息时通常是瓶颈。
- en: batch.size
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: batch.size
- en: When multiple records are sent to the same partition, the producer will batch
    them together. This parameter controls the amount of memory in bytes (not messages!)
    that will be used for each batch. When the batch is full, all the messages in
    the batch will be sent. However, this does not mean that the producer will wait
    for the batch to become full. The producer will send half-full batches and even
    batches with just a single message in them. Therefore, setting the batch size
    too large will not cause delays in sending messages; it will just use more memory
    for the batches. Setting the batch size too small will add some overhead because
    the producer will need to send messages more frequently.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当多条记录发送到同一分区时，生产者将它们批量处理在一起。此参数控制每个批次将用于的字节内存量（而不是消息！）。当批次满了，批次中的所有消息将被发送。但是，这并不意味着生产者会等待批次变满。生产者将发送半满的批次，甚至只有一条消息的批次。因此，将批次大小设置得太大不会导致发送消息的延迟；它只会使用更多的内存用于批次。将批次大小设置得太小会增加一些开销，因为生产者需要更频繁地发送消息。
- en: max.in.flight.requests.per.connection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.in.flight.requests.per.connection
- en: This controls how many message batches the producer will send to the server
    without receiving responses. Higher settings can increase memory usage while improving
    throughput. [Apache’s wiki experiments show](https://oreil.ly/NZmJ0) that in a
    single-DC environment, the throughput is maximized with only 2 in-flight requests;
    however, the default value is 5 and shows similar performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这控制生产者在未收到响应的情况下向服务器发送多少消息批次。较高的设置可以增加内存使用量，同时提高吞吐量。[Apache的维基实验显示](https://oreil.ly/NZmJ0)，在单个DC环境中，通过只有2个飞行请求可以实现最大吞吐量；然而，默认值为5并显示类似的性能。
- en: Ordering Guarantees
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序保证
- en: Apache Kafka preserves the order of messages within a partition. This means
    that if messages are sent from the producer in a specific order, the broker will
    write them to a partition in that order and all consumers will read them in that
    order. For some use cases, order is very important. There is a big difference
    between depositing $100 in an account and later withdrawing it, and the other
    way around! However, some use cases are less sensitive.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka保留分区内消息的顺序。这意味着如果消息按特定顺序从生产者发送，经纪人将按照该顺序将它们写入分区，并且所有消费者将按照该顺序读取它们。对于某些用例，顺序非常重要。在账户中存入100美元并稍后取款，与相反的顺序之间存在很大的区别！然而，某些用例则不太敏感。
- en: Setting the `retries` parameter to nonzero and the `max.in.​flight.requests.per.connection`
    to more than 1 means that it is possible that the broker will fail to write the
    first batch of messages, succeed in writing the second (which was already in-flight),
    and then retry the first batch and succeed, thereby reversing the order.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将“重试”参数设置为非零，并将“每个连接的最大飞行请求数”设置为大于1意味着可能会发生经纪人无法写入第一批消息，成功写入第二批（已经在飞行中），然后重试第一批并成功，从而颠倒顺序。
- en: Since we want at least two in-flight requests for performance reasons, and a
    high number of retries for reliability reasons, the best solution is to set `enable.idempotence=true`.
    This guarantees message ordering with up to five in-flight requests and also guarantees
    that retries will not introduce duplicates. [Chapter 8](ch08.html#exactly_once_semantics)
    discusses the idempotent producer in depth.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于出于性能原因，我们希望至少有两个飞行请求，并出于可靠性原因，希望有较高数量的重试，因此最佳解决方案是设置`enable.idempotence=true`。这可以保证最多有五个飞行请求的消息排序，并且保证重试不会引入重复。[第8章](ch08.html#exactly_once_semantics)深入讨论了幂等生产者。
- en: max.request.size
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.request.size
- en: This setting controls the size of a produce request sent by the producer. It
    caps both the size of the largest message that can be sent and the number of messages
    that the producer can send in one request. For example, with a default maximum
    request size of 1 MB, the largest message you can send is 1 MB, or the producer
    can batch 1,024 messages of size 1 KB each into one request. In addition, the
    broker has its own limit on the size of the largest message it will accept (`message.max.bytes`).
    It is usually a good idea to have these configurations match, so the producer
    will not attempt to send messages of a size that will be rejected by the broker.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置控制生产者发送的生产请求的大小。它限制了可以发送的最大消息的大小以及生产者可以在一个请求中发送的消息数量。例如，默认的最大请求大小为1 MB，您可以发送的最大消息为1
    MB，或者生产者可以将1,024条大小为1 KB的消息批量处理成一个请求。此外，经纪人对其将接受的最大消息大小也有限制（`message.max.bytes`）。通常最好将这些配置匹配起来，这样生产者就不会尝试发送经纪人拒绝的大小的消息。
- en: receive.buffer.bytes and send.buffer.bytes
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: receive.buffer.bytes和send.buffer.bytes
- en: These are the sizes of the TCP send and receive buffers used by the sockets
    when writing and reading data. If these are set to –1, the OS defaults will be
    used. It is a good idea to increase these when producers or consumers communicate
    with brokers in a different datacenter, because those network links typically
    have higher latency and lower bandwidth.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在写入和读取数据时套接字使用的TCP发送和接收缓冲区的大小。如果将它们设置为-1，将使用操作系统的默认值。当生产者或消费者与不同数据中心的代理进行通信时，建议增加这些值，因为这些网络链接通常具有更高的延迟和较低的带宽。
- en: enable.idempotence
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: enable.idempotence
- en: Starting in version 0.11, Kafka supports *exactly once* semantics. Exactly once
    is a fairly large topic, and we’ll dedicate an entire chapter to it, but idempotent
    producer is a simple and highly beneficial part of it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从0.11版本开始，Kafka支持*仅一次*语义。仅一次是一个相当大的主题，我们将专门为此撰写一整章，但幂等生产者是其中一个简单且非常有益的部分。
- en: 'Suppose you configure your producer to maximize reliability: `acks=all` and
    a decently large `delivery.timeout.ms` to allow sufficient retries. These make
    sure each message will be written to Kafka at least once. In some cases, this
    means that messages will be written to Kafka more than once. For example, imagine
    that a broker received a record from the producer, wrote it to local disk, and
    the record was successfully replicated to other brokers, but then the first broker
    crashed before sending a response to the producer. The producer will wait until
    it reaches `request.​time⁠out.ms` and then retry. The retry will go to the new
    leader that already has a copy of this record since the previous write was replicated
    successfully. You now have a duplicate record.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您配置生产者以最大化可靠性：`acks=all`和一个相当大的`delivery.timeout.ms`以允许足够的重试。这样可以确保每条消息至少会被写入Kafka一次。在某些情况下，这意味着消息将被写入Kafka多次。例如，假设代理从生产者接收到一条记录，将其写入本地磁盘，并成功地复制到其他代理，但然后第一个代理在发送响应给生产者之前崩溃了。生产者将等待直到达到`request.​time⁠out.ms`然后重试。重试将发送到已经成功复制了此记录的新领导者。现在您有了一个重复的记录。
- en: To avoid this, you can set `enable.idempotence=true`. When the idempotent producer
    is enabled, the producer will attach a sequence number to each record it sends.
    If the broker receives records with the same sequence number, it will reject the
    second copy and the producer will receive the harmless `DuplicateSequenceException`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，您可以设置`enable.idempotence=true`。启用幂等生产者后，生产者将为发送的每条记录附加一个序列号。如果代理接收到具有相同序列号的记录，它将拒绝第二份副本，生产者将收到无害的`DuplicateSequenceException`。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Enabling idempotence requires `max.in.flight.requests.per.​con⁠nection` to be
    less than or equal to 5, `retries` to be greater than 0, and `acks=all`. If incompatible
    values are set, a `ConfigException` will be thrown.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 启用幂等性要求`max.in.flight.requests.per.​con⁠nection`小于或等于5，`retries`大于0，`acks=all`。如果设置了不兼容的值，将抛出`ConfigException`。
- en: Serializers
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化程序
- en: As seen in previous examples, producer configuration includes mandatory serializers.
    We’ve seen how to use the default `String` serializer. Kafka also includes serializers
    for integers, `ByteArrays`, and many more, but this does not cover most use cases.
    Eventually, you will want to be able to serialize more generic records.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在之前的例子中所看到的，生产者配置包括强制性的序列化程序。我们已经看到了如何使用默认的`String`序列化程序。Kafka还包括整数、`ByteArrays`等许多序列化程序，但这并不能涵盖大多数用例。最终，您将希望能够序列化更通用的记录。
- en: We will start by showing how to write your own serializer and then introduce
    the Avro serializer as a recommended alternative.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示如何编写自己的序列化程序，然后介绍Avro序列化程序作为一个推荐的替代方案。
- en: Custom Serializers
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义序列化程序
- en: When the object you need to send to Kafka is not a simple string or integer,
    you have a choice of either using a generic serialization library like Avro, Thrift,
    or Protobuf to create records, or creating a custom serialization for objects
    you are already using. We highly recommend using a generic serialization library.
    In order to understand how the serializers work and why it is a good idea to use
    a serialization library, let’s see what it takes to write your own custom serializer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要发送到Kafka的对象不是简单的字符串或整数时，您可以选择使用通用序列化库（如Avro、Thrift或Protobuf）创建记录，或者为您已经使用的对象创建自定义序列化。我们强烈建议使用通用序列化库。为了理解序列化程序的工作原理以及为什么使用序列化库是一个好主意，让我们看看编写自己的自定义序列化程序需要做些什么。
- en: 'Suppose that instead of recording just the customer name, you create a simple
    class to represent customers:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不仅仅记录客户的姓名，而是创建一个简单的类来表示客户：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now suppose we want to create a custom serializer for this class. It will look
    something like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想为这个类创建一个自定义的序列化程序。它看起来会像这样：
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Configuring a producer with this `CustomerSerializer` will allow you to define
    `ProducerRecord<String, Customer>`, and send `Customer` data and pass `Customer`
    objects directly to the producer. This example is pretty simple, but you can see
    how fragile the code is. If we ever have too many customers, for example, and
    need to change `customerID` to `Long`, or if we ever decide to add a `startDate`
    field to `Customer`, we will have a serious issue in maintaining compatibility
    between old and new messages. Debugging compatibility issues between different
    versions of serializers and deserializers is fairly challenging: you need to compare
    arrays of raw bytes. To make matters even worse, if multiple teams in the same
    company end up writing `Customer` data to Kafka, they will all need to use the
    same serializers and modify the code at the exact same time.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`CustomerSerializer`配置生产者将允许您定义`ProducerRecord<String, Customer>`，并发送`Customer`数据并直接将`Customer`对象传递给生产者。这个例子很简单，但您可以看到代码是多么脆弱。例如，如果我们有太多的客户，并且需要将`customerID`更改为`Long`，或者如果我们决定向`Customer`添加一个`startDate`字段，那么在维护旧消息和新消息之间的兼容性方面将会出现严重问题。在不同版本的序列化程序和反序列化程序之间调试兼容性问题是相当具有挑战性的：您需要比较原始字节数组。更糟糕的是，如果同一家公司的多个团队最终都向Kafka写入`Customer`数据，他们都需要使用相同的序列化程序并同时修改代码。
- en: For these reasons, we recommend using existing serializers and deserializers
    such as JSON, Apache Avro, Thrift, or Protobuf. In the following section, we will
    describe Apache Avro and then show how to serialize Avro records and send them
    to Kafka.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议使用现有的序列化器和反序列化器，如JSON、Apache Avro、Thrift或Protobuf。在接下来的部分中，我们将描述Apache
    Avro，然后展示如何序列化Avro记录并将其发送到Kafka。
- en: Serializing Using Apache Avro
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Apache Avro进行序列化
- en: Apache Avro is a language-neutral data serialization format. The project was
    created by Doug Cutting to provide a way to share data files with a large audience.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Avro是一种语言中立的数据序列化格式。该项目由Doug Cutting创建，旨在为大众提供一种共享数据文件的方式。
- en: Avro data is described in a language-independent schema. The schema is usually
    described in JSON, and the serialization is usually to binary files, although
    serializing to JSON is also supported. Avro assumes that the schema is present
    when reading and writing files, usually by embedding the schema in the files themselves.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Avro数据是用语言无关的模式描述的。模式通常用JSON描述，序列化通常是到二进制文件，尽管也支持序列化到JSON。Avro假定在读取和写入文件时存在模式，通常是通过将模式嵌入文件本身来实现。
- en: One of the most interesting features of Avro, and what makes it a good fit for
    use in a messaging system like Kafka, is that when the application that is writing
    messages switches to a new but compatible schema, the applications reading the
    data can continue processing messages without requiring any change or update.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Avro最有趣的特性之一，也是使其适合在Kafka等消息系统中使用的原因之一，是当编写消息的应用程序切换到新的但兼容的模式时，读取数据的应用程序可以继续处理消息而无需任何更改或更新。
- en: 'Suppose the original schema was:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始模式是：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO5-1)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO5-1)'
- en: '`id` and `name` fields are mandatory, while `faxNumber` is optional and defaults
    to `null`.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`id`和`name`字段是必需的，而`faxNumber`是可选的，默认为`null`。'
- en: We used this schema for a few months and generated a few terabytes of data in
    this format. Now suppose we decide that in the new version, we will upgrade to
    the 21st century and will no longer include a fax number field and will instead
    use an email field.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个模式下使用了几个月，并以这种格式生成了几TB的数据。现在假设我们决定在新版本中，我们将升级到21世纪，不再包含传真号码字段，而是使用电子邮件字段。
- en: 'The new schema would be:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 新模式将是：
- en: '[PRE7]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, after upgrading to the new version, old records will contain `faxNumber`
    and new records will contain `email`. In many organizations, upgrades are done
    slowly and over many months. So we need to consider how pre-upgrade applications
    that still use the fax numbers and post-upgrade applications that use email will
    be able to handle all the events in Kafka.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在升级到新版本后，旧记录将包含`faxNumber`，新记录将包含`email`。在许多组织中，升级是缓慢进行的，需要花费很多个月的时间。因此，我们需要考虑如何处理仍然使用传真号码的升级前应用程序和使用电子邮件的升级后应用程序在Kafka中的所有事件。
- en: The reading application will contain calls to methods similar to `getName()`,
    `getId()`, and `getFaxNumber()`. If it encounters a message written with the new
    schema, `getName()` and `getId()` will continue working with no modification,
    but `getFax` `Number()` will return `null` because the message will not contain
    a fax number.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 读取应用程序将包含类似于`getName()`、`getId()`和`getFaxNumber()`的方法调用。如果遇到使用新模式编写的消息，`getName()`和`getId()`将继续工作而无需修改，但`getFaxNumber()`将返回`null`，因为消息不包含传真号码。
- en: Now suppose we upgrade our reading application and it no longer has the `getFax`
    `Number()` method but rather `getEmail()`. If it encounters a message written
    with the old schema, `getEmail()` will return `null` because the older messages
    do not contain an email address.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们升级了我们的读取应用程序，它不再具有`getFaxNumber()`方法，而是`getEmail()`。如果遇到使用旧模式编写的消息，`getEmail()`将返回`null`，因为旧消息不包含电子邮件地址。
- en: 'This example illustrates the benefit of using Avro: even though we changed
    the schema in the messages without changing all the applications reading the data,
    there will be no exceptions or breaking errors and no need for expensive updates
    of existing data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了使用Avro的好处：即使我们在消息中改变了模式，而不改变所有读取数据的应用程序，也不会出现异常或破坏错误，也不需要昂贵的现有数据更新。
- en: 'However, there are two caveats to this scenario:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种情况有两个注意事项：
- en: The schema used for writing the data and the schema expected by the reading
    application must be compatible. The Avro documentation includes [compatibility
    rules](http://bit.ly/2t9FmEb).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入数据使用的模式和读取应用程序期望的模式必须是兼容的。Avro文档包括[兼容性规则](http://bit.ly/2t9FmEb)。
- en: The deserializer will need access to the schema that was used when writing the
    data, even when it is different from the schema expected by the application that
    accesses the data. In Avro files, the writing schema is included in the file itself,
    but there is a better way to handle this for Kafka messages. We will look at that
    next.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反序列化器将需要访问写入数据时使用的模式，即使它与应用程序期望的模式不同。在Avro文件中，写入模式包含在文件本身中，但对于Kafka消息，有一种更好的处理方式。我们将在下面看到这一点。
- en: Using Avro Records with Kafka
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Avro记录与Kafka
- en: Unlike Avro files, where storing the entire schema in the data file is associated
    with a fairly reasonable overhead, storing the entire schema in each record will
    usually more than double the record size. However, Avro still requires the entire
    schema to be present when reading the record, so we need to locate the schema
    elsewhere. To achieve this, we follow a common architecture pattern and use a
    *Schema Registry*. The Schema Registry is not part of Apache Kafka, but there
    are several open source options to choose from. We’ll use the Confluent Schema
    Registry for this example. You can find the Schema Registry code on [GitHub](https://oreil.ly/htoZK),
    or you can install it as part of the [Confluent Platform](https://oreil.ly/n2V71).
    If you decide to use the Schema Registry, we recommend checking [the documentation
    on Confluent](https://oreil.ly/yFkTX).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Avro 文件不同，将整个模式存储在数据文件中与相当合理的开销相关联，将整个模式存储在每个记录中通常会使记录大小增加一倍以上。但是，Avro 仍然要求在读取记录时整个模式都存在，因此我们需要在其他地方定位模式。为了实现这一点，我们遵循一个常见的架构模式，并使用
    *模式注册表*。模式注册表不是 Apache Kafka 的一部分，但有几个开源选项可供选择。我们将在此示例中使用 Confluent Schema Registry。您可以在
    [GitHub](https://oreil.ly/htoZK) 上找到模式注册表代码，或者您可以将其作为 [Confluent Platform](https://oreil.ly/n2V71)
    的一部分安装。如果决定使用模式注册表，我们建议查看 [Confluent 上的文档](https://oreil.ly/yFkTX)。
- en: The idea is to store all the schemas used to write data to Kafka in the registry.
    Then we simply store the identifier for the schema in the record we produce to
    Kafka. The consumers can then use the identifier to pull the record out of the
    Schema Registry and deserialize the data. The key is that all this work—storing
    the schema in the registry and pulling it up when required—is done in the serializers
    and deserializers. The code that produces data to Kafka simply uses the Avro serializer
    just like it would any other serializer. [Figure 3-3](#fig-3-serializer) demonstrates
    this process.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有用于将数据写入 Kafka 的模式存储在注册表中。然后我们只需在我们产生到 Kafka 的记录中存储模式的标识符。消费者随后可以使用标识符从模式注册表中提取记录并反序列化数据。关键在于所有这些工作——将模式存储在注册表中并在需要时提取模式——都是在序列化器和反序列化器中完成的。将数据生成到
    Kafka 的代码就像使用任何其他序列化器一样使用 Avro 序列化器。[图 3-3](#fig-3-serializer) 展示了这个过程。
- en: '![kdg2 0303](assets/kdg2_0303.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0303](assets/kdg2_0303.png)'
- en: Figure 3-3\. Flow diagram of serialization and deserialization of Avro records
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. Avro 记录的序列化和反序列化流程
- en: 'Here is an example of how to produce generated Avro objects to Kafka (see the
    [Avro documentation](https://oreil.ly/klcjK) for how to generate objects from
    Avro schemas):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何将生成的 Avro 对象发送到 Kafka 的示例（请参阅 [Avro 文档](https://oreil.ly/klcjK) 了解如何从 Avro
    模式生成对象）：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-1)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: We use the `KafkaAvroSerializer` to serialize our objects with Avro. Note that
    the `KafkaAvroSerializer` can also handle primitives, which is why we can later
    use `String` as the record key and our `Customer` object as the value.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `KafkaAvroSerializer` 来使用 Avro 序列化我们的对象。请注意，`KafkaAvroSerializer` 也可以处理原始类型，这就是为什么我们后来可以使用
    `String` 作为记录键，而我们的 `Customer` 对象作为值。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-2)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)'
- en: '`schema.registry.url` is the configuration of the Avro serializer that will
    be passed to the serializer by the producer. It simply points to where we store
    the schemas.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`schema.registry.url` 是 Avro 序列化器的配置，将被生产者传递给序列化器。它简单地指向我们存储模式的位置。'
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-3)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)'
- en: '`Customer` is our generated object. We tell the producer that our records will
    contain `Customer` as the value.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`Customer` 是我们生成的对象。我们告诉生产者我们的记录将包含 `Customer` 作为值。'
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-4)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)'
- en: '`Customer` class is not a regular Java class (plain old Java object, or POJO)
    but rather a specialized Avro object, generated from a schema using Avro code
    generation. The Avro serializer can only serialize Avro objects, not POJO. Generating
    Avro classes can be done either using the *avro-tools.jar* or the Avro Maven plug-in,
    both part of Apache Avro. See the [Apache Avro Getting Started (Java) guide](https://oreil.ly/sHGEe)
    for details on how to generate Avro classes.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`Customer` 类不是常规的 Java 类（普通的旧的 Java 对象，或 POJO），而是一个专门的 Avro 对象，使用 Avro 代码生成从模式生成。Avro
    序列化器只能序列化 Avro 对象，而不是 POJO。生成 Avro 类可以使用 *avro-tools.jar* 或 Avro Maven 插件来完成，这两者都是
    Apache Avro 的一部分。有关如何生成 Avro 类的详细信息，请参阅 [Apache Avro 入门（Java）指南](https://oreil.ly/sHGEe)。'
- en: '[![5](assets/5.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-5)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)'
- en: We also instantiate `ProducerRecord` with `Customer` as the value type, and
    pass a `Customer` object when creating the new record.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 `Customer` 作为值类型来实例化 `ProducerRecord`，并在创建新记录时传递一个 `Customer` 对象。
- en: '[![6](assets/6.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-6)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)'
- en: That’s it. We send the record with our `Customer` object, and `KafkaAvro​Serial⁠izer`
    will handle the rest.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们发送包含我们的 `Customer` 对象的记录，`KafkaAvro​Serial⁠izer` 将处理其余部分。
- en: 'Avro also allows you to use generic Avro objects, that are used as key-value
    maps, rather than generated Avro objects with getters and setters that match the
    schema that was used to generate them. To use generic Avro objects, you just need
    to provide the schema:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 还允许您使用通用 Avro 对象，这些对象用作键值映射，而不是具有与用于生成它们的模式匹配的getter和setter的生成的 Avro 对象。要使用通用
    Avro 对象，您只需要提供模式：
- en: '[PRE9]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-1)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: We still use the same `KafkaAvroSerializer`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然使用相同的`KafkaAvroSerializer`。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-2)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-2)'
- en: And we provide the URI of the same Schema Registry.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供相同模式注册表的URI。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-3)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-3)'
- en: But now we also need to provide the Avro schema, since it is not provided by
    an Avro-generated object.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们还需要提供Avro模式，因为它不是由Avro生成的对象提供的。
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-4)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-4)'
- en: Our object type is an Avro `GenericRecord`, which we initialize with our schema
    and the data we want to write.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的对象类型是Avro `GenericRecord`，我们使用我们的模式和我们想要写入的数据初始化它。
- en: '[![5](assets/5.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-5)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-5)'
- en: Then the value of the `ProducerRecord` is simply a `GenericRecord` that contains
    our schema and data. The serializer will know how to get the schema from this
    record, store it in the Schema Registry, and serialize the object data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`ProducerRecord`的值只是包含我们的模式和数据的`GenericRecord`。序列化程序将知道如何从此记录中获取模式，将其存储在模式注册表中，并对对象数据进行序列化。
- en: Partitions
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区
- en: 'In previous examples, the `ProducerRecord` objects we created included a topic
    name, key, and value. Kafka messages are key-value pairs, and while it is possible
    to create a `ProducerRecord` with just a topic and a value, with the key set to
    `null` by default, most applications produce records with keys. Keys serve two
    goals: they are additional information that gets stored with the message, and
    they are typically also used to decide which one of the topic partitions the message
    will be written to (keys also play an important role in compacted topics—we’ll
    discuss those in [Chapter 6](ch06.html#kafka_internals)). All messages with the
    same key will go to the same partition. This means that if a process is reading
    only a subset of the partitions in a topic (more on that in [Chapter 4](ch04.html#reading_data_from_kafka)),
    all the records for a single key will be read by the same process. To create a
    key-value record, you simply create a `ProducerRecord` as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的示例中，我们创建的`ProducerRecord`对象包括主题名称、键和值。Kafka消息是键值对，虽然可以只使用主题和值创建`ProducerRecord`，并且默认情况下将键设置为`null`，但大多数应用程序都会生成带有键的记录。键有两个目标：它们是存储在消息中的附加信息，通常也用于决定消息将写入哪个主题分区（键在压缩主题中也起着重要作用，我们将在[第6章](ch06.html#kafka_internals)中讨论这些内容）。具有相同键的所有消息将进入同一分区。这意味着如果进程只读取主题中的一部分分区（有关详细信息，请参阅[第4章](ch04.html#reading_data_from_kafka)），则单个键的所有记录将由同一进程读取。要创建键值记录，只需创建`ProducerRecord`如下所示：
- en: '[PRE10]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When creating messages with a null key, you can simply leave the key out:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建具有空键的消息时，可以简单地将键省略：
- en: '[PRE11]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO8-1)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO8-1)'
- en: Here, the key will simply be set to `null`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，键将简单地设置为`null`。
- en: When the key is `null` and the default partitioner is used, the record will
    be sent to one of the available partitions of the topic at random. A round-robin
    algorithm will be used to balance the messages among the partitions. Starting
    in the Apache Kafka 2.4 producer, the round-robin algorithm used in the default
    partitioner when handling null keys is sticky. This means that it will fill a
    batch of messages sent to a single partition before switching to the next partition.
    This allows sending the same number of messages to Kafka in fewer requests, leading
    to lower latency and reduced CPU utilization on the broker.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当键为`null`且使用默认分区器时，记录将随机发送到主题的可用分区之一。循环算法将用于在分区之间平衡消息。从Apache Kafka 2.4生产者开始，默认分区器在处理空键时使用的循环算法是粘性的。这意味着它将在切换到下一个分区之前填充发送到单个分区的一批消息。这允许以更少的请求将相同数量的消息发送到Kafka，从而降低延迟并减少代理上的CPU利用率。
- en: If a key exists and the default partitioner is used, Kafka will hash the key
    (using its own hash algorithm, so hash values will not change when Java is upgraded)
    and use the result to map the message to a specific partition. Since it is important
    that a key is always mapped to the same partition, we use all the partitions in
    the topic to calculate the mapping—not just the available partitions. This means
    that if a specific partition is unavailable when you write data to it, you might
    get an error. This is fairly rare, as you will see in [Chapter 7](ch07.html#reliable_data_delivery)
    when we discuss Kafka’s replication and availability.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在键并且使用默认分区器，则Kafka将对键进行哈希处理（使用自己的哈希算法，因此当Java升级时哈希值不会更改），并使用结果将消息映射到特定分区。由于关键始终映射到同一分区很重要，因此我们使用主题中的所有分区来计算映射，而不仅仅是可用分区。这意味着如果在写入数据时特定分区不可用，则可能会出现错误。这是相当罕见的，正如您将在[第7章](ch07.html#reliable_data_delivery)中看到的，当我们讨论Kafka的复制和可用性时。
- en: In addition to the default partitioner, Apache Kafka clients also provide `RoundRobinPartitioner`
    and `UniformStickyPartitioner`. These provide random partition assignment and
    sticky random partition assignment even when messages have keys. These are useful
    when keys are important for the consuming application (for example, there are
    ETL applications that use the key from Kafka records as the primary key when loading
    data from Kafka to a relational database), but the workload may be skewed, so
    a single key may have a disproportionately large workload. Using the `UniformStickyPartitioner`
    will result in an even distribution of workload across all partitions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了默认的分区器，Apache Kafka客户端还提供了`RoundRobinPartitioner`和`UniformStickyPartitioner`。即使消息具有键，这些分配随机分区和粘性随机分区分配。当键对于消费应用程序很重要时（例如，有一些ETL应用程序使用Kafka记录的键作为从Kafka加载数据到关系数据库时的主键），但工作负载可能会倾斜，因此单个键可能具有不成比例的大工作负载。使用`UniformStickyPartitioner`将导致工作负载均匀分布在所有分区上。
- en: When the default partitioner is used, the mapping of keys to partitions is consistent
    only as long as the number of partitions in a topic does not change. So as long
    as the number of partitions is constant, you can be sure that, for example, records
    regarding user 045189 will always get written to partition 34\. This allows all
    kinds of optimization when reading data from partitions. However, the moment you
    add new partitions to the topic, this is no longer guaranteed—the old records
    will stay in partition 34 while new records may get written to a different partition.
    When partitioning keys is important, the easiest solution is to create topics
    with sufficient partitions (the Confluent blog contains suggestions on how to
    [choose the number of partitions](https://oreil.ly/ortRk)) and never add partitions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用默认分区器时，键到分区的映射仅在主题中的分区数量不变时才是一致的。因此，只要分区数量保持不变，您可以确保，例如，关于用户045189的记录将始终被写入分区34。这允许在从分区读取数据时进行各种优化。然而，一旦您向主题添加新的分区，这就不再保证——旧记录将保留在分区34，而新记录可能会被写入不同的分区。当分区键很重要时，最简单的解决方案是创建具有足够分区的主题（Confluent博客包含有关如何[选择分区数量](https://oreil.ly/ortRk)的建议），并且永远不要添加分区。
- en: Implementing a custom partitioning strategy
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施自定义分区策略
- en: So far, we have discussed the traits of the default partitioner, which is the
    one most commonly used. However, Kafka does not limit you to just hash partitions,
    and sometimes there are good reasons to partition data differently. For example,
    suppose that you are a B2B vendor and your biggest customer is a company that
    manufactures handheld devices called Bananas. Suppose that you do so much business
    with customer “Banana” that over 10% of your daily transactions are with this
    customer. If you use default hash partitioning, the Banana records will get allocated
    to the same partition as other accounts, resulting in one partition being much
    larger than the rest. This can cause servers to run out of space, processing to
    slow down, etc. What we really want is to give Banana its own partition and then
    use hash partitioning to map the rest of the accounts to all other partitions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了默认分区器的特性，这是最常用的分区器。然而，Kafka并不限制您只能使用哈希分区，有时分区数据的原因也是很好的。例如，假设您是一家B2B供应商，您最大的客户是一家制造名为Bananas的手持设备的公司。假设您与客户“Banana”的业务量如此之大，以至于您每天超过10%的交易都与该客户进行。如果您使用默认的哈希分区，Banana记录将被分配到与其他帐户相同的分区，导致一个分区比其他分区大得多。这可能导致服务器空间不足，处理速度变慢等问题。我们真正想要的是给Banana分配自己的分区，然后使用哈希分区将其余的帐户映射到所有其他分区。
- en: 'Here is an example of a custom partitioner:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个自定义分区器的示例：
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO9-1)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO9-1)'
- en: Partitioner interface includes `configure`, `partition`, and `close` methods.
    Here we only implement `partition`, although we really should have passed the
    special customer name through `configure` instead of hardcoding it in `partition`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 分区器接口包括`configure`、`partition`和`close`方法。在这里，我们只实现了`partition`，尽管我们真的应该通过`configure`传递特殊的客户名称，而不是在`partition`中硬编码它。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO9-2)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO9-2)'
- en: We only expect `String` keys, so we throw an exception if that is not the case.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只期望`String`键，因此如果不是这种情况，我们会抛出异常。
- en: Headers
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标题
- en: Records can, in addition to key and value, also include headers. Record headers
    give you the ability to add some metadata about the Kafka record, without adding
    any extra information to the key/value pair of the record itself. Headers are
    often used for lineage to indicate the source of the data in the record, and for
    routing or tracing messages based on header information without having to parse
    the message itself (perhaps the message is encrypted and the router doesn’t have
    permissions to access the data).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 记录除了键和值之外，还可以包括头。记录头使您能够向Kafka记录添加一些关于元数据的信息，而不向记录本身的键/值对添加任何额外信息。头通常用于表示记录中数据的来源的谱系，并根据头信息路由或跟踪消息，而无需解析消息本身（也许消息是加密的，路由器没有权限访问数据）。
- en: Headers are implemented as an ordered collection of key/value pairs. The keys
    are always a `String`, and the values can be any serialized object—just like the
    message value.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 头被实现为键/值对的有序集合。键始终是`String`，值可以是任何序列化对象，就像消息值一样。
- en: 'Here is a small example that shows how to add headers to a `ProduceRecord`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个小例子，展示了如何向`ProduceRecord`添加头：
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Interceptors
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拦截器
- en: There are times when you want to modify the behavior of your Kafka client application
    without modifying its code, perhaps because you want to add identical behavior
    to all applications in the organization. Or perhaps you don’t have access to the
    original code.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您希望修改Kafka客户端应用程序的行为，而无需修改其代码，也许是因为您希望向组织中的所有应用程序添加相同的行为。或者您可能无法访问原始代码。
- en: 'Kafka’s `ProducerInterceptor` interceptor includes two key methods:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的`ProducerInterceptor`拦截器包括两个关键方法：
- en: '`ProducerRecord<K, V> onSend(ProducerRecord<K, V> record)`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProducerRecord<K, V> onSend(ProducerRecord<K, V> record)`'
- en: This method will be called before the produced record is sent to Kafka, indeed
    before it is even serialized. When overriding this method, you can capture information
    about the sent record and even modify it. Just be sure to return a valid `ProducerRecord`
    from this method. The record that this method returns will be serialized and sent
    to Kafka.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在将生成的记录发送到Kafka之前，甚至在序列化之前，将调用此方法。重写此方法时，您可以捕获有关发送记录的信息，甚至修改它。只需确保从此方法返回有效的`ProducerRecord`。此方法返回的记录将被序列化并发送到Kafka。
- en: '`void onAcknowledgement(RecordMetadata metadata, Exception exception)`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`void onAcknowledgement(RecordMetadata metadata, Exception exception)`'
- en: This method will be called if and when Kafka responds with an acknowledgment
    for a send. The method does not allow modifying the response from Kafka, but you
    can capture information about the response.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Kafka响应发送的消息，则将调用此方法。该方法不允许修改来自Kafka的响应，但可以捕获有关响应的信息。
- en: Common use cases for producer interceptors include capturing monitoring and
    tracing information; enhancing the message with standard headers, especially for
    lineage tracking purposes; and redacting sensitive information.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者拦截器的常见用例包括捕获监视和跟踪信息；增强消息的标准头，特别是用于血统跟踪目的；以及删除敏感信息。
- en: 'Here is an example of a very simple producer interceptor. This one simply counts
    the messages sent and acks received within specific time windows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的生产者拦截器示例。这个示例只是在特定时间窗口内计算发送的消息和接收的确认：
- en: '[PRE14]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-1)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-1)'
- en: '`ProducerInterceptor` is a `Configurable` interface. You can override the `configure`
    method and setup before any other method is called. This method receives the entire
    producer configuration, and you can access any configuration parameter. In this
    case, we added a configuration of our own that we reference here.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProducerInterceptor`是一个`Configurable`接口。您可以重写`configure`方法并在调用任何其他方法之前进行设置。此方法接收整个生产者配置，您可以访问任何配置参数。在这种情况下，我们添加了自己的配置，并在此引用。'
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-2)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-2)'
- en: When a record is sent, we increment the record count and return the record without
    modifying it.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 发送记录时，我们增加记录计数并返回记录而不修改它。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-3)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-3)'
- en: When Kafka responds with an ack, we increment the acknowledgment count and don’t
    need to return anything.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kafka响应确认时，我们增加确认计数，不需要返回任何内容。
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-4)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-4)'
- en: This method is called when the producer closes, giving us a chance to clean
    up the interceptor state. In this case, we close the thread we created. If you
    opened file handles, connections to remote data stores, or similar, this is the
    place to close everything and avoid leaks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当生产者关闭时，将调用此方法，让我们有机会清理拦截器状态。在这种情况下，我们关闭了创建的线程。如果您打开了文件句柄、连接到远程数据存储或类似的内容，这是关闭所有内容并避免泄漏的地方。
- en: 'As we mentioned earlier, producer interceptors can be applied without any changes
    to the client code. To use the preceding interceptor with `kafka-console-producer`,
    an example application that ships with Apache Kafka, follow these three simple
    steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，生产者拦截器可以在不更改客户端代码的情况下应用。要在`kafka-console-producer`中使用前面的拦截器，这是一个随Apache
    Kafka一起提供的示例应用程序，请按照以下三个简单步骤操作：
- en: 'Add your jar to the classpath:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的jar添加到类路径：
- en: '`export CLASSPATH=$CLASSPATH:~./target/CountProducerInterceptor-1.0-SNAPSHOT.jar`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`export CLASSPATH=$CLASSPATH:~./target/CountProducerInterceptor-1.0-SNAPSHOT.jar`'
- en: 'Create a config file that includes:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建包含以下内容的配置文件：
- en: '`interceptor.classes=com.shapira.examples.interceptors.CountProducerInterceptor`
    `counting.interceptor.window.size.ms=10000`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`interceptor.classes=com.shapira.examples.interceptors.CountProducerInterceptor`
    `counting.interceptor.window.size.ms=10000`'
- en: 'Run the application as you normally would, but make sure to include the configuration
    that you created in the previous step:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像通常一样运行应用程序，但确保包含在上一步中创建的配置：
- en: '`bin/kafka-console-producer.sh --broker-list localhost:9092 --topic interceptor-test
    --producer.config producer.config`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`bin/kafka-console-producer.sh --broker-list localhost:9092 --topic interceptor-test
    --producer.config producer.config`'
- en: Quotas and Throttling
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配额和限流
- en: 'Kafka brokers have the ability to limit the rate at which messages are produced
    and consumed. This is done via the quota mechanism. Kafka has three quota types:
    produce, consume, and request. Produce and consume quotas limit the rate at which
    clients can send and receive data, measured in bytes per second. Request quotas
    limit the percentage of time the broker spends processing client requests.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka代理有能力限制消息的生产和消费速率。这是通过配额机制完成的。Kafka有三种配额类型：生产、消费和请求。生产和消费配额限制客户端发送和接收数据的速率，以每秒字节数为单位。请求配额限制代理处理客户端请求的时间百分比。
- en: Quotas can be applied to all clients by setting default quotas, specific client-ids,
    specific users, or both. User-specific quotas are only meaningful in clusters
    where security is configured and clients authenticate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过设置默认配额、特定客户端ID、特定用户或两者来应用配额到所有客户端。用户特定的配额只在配置了安全性并且客户端进行身份验证的集群中才有意义。
- en: 'The default produce and consume quotas that are applied to all clients are
    part of the Kafka broker configuration file. For example, to limit each producer
    to send no more than 2 MBps on average, add the following configuration to the
    broker configuration file: `quota.producer.default=2M`.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于所有客户端的默认生产和消费配额是Kafka代理配置文件的一部分。例如，要限制每个生产者平均发送不超过2 MBps，将以下配置添加到代理配置文件中：`quota.producer.default=2M`。
- en: 'While not recommended, you can also configure specific quotas for certain clients
    that override the default quotas in the broker configuration file. To allow clientA
    to produce 4 MBps and clientB 10 MBps, you can use the following: `quota.​pro⁠ducer.override="clientA:4M,clientB:10M"`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不建议，但您也可以为某些客户端配置特定的配额，这些配额会覆盖代理配置文件中的默认配额。要允许clientA产生4 MBps和clientB 10 MBps，您可以使用以下命令：`quota.producer.override="clientA:4M,clientB:10M"`
- en: Quotas that are specified in Kafka’s configuration file are static, and you
    can only modify them by changing the configuration and then restarting all the
    brokers. Since new clients can arrive at any time, this is very inconvenient.
    Therefore the usual method of applying quotas to specific clients is through dynamic
    configuration that can be set using `kafka-config.sh` or the AdminClient API.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka的配置文件中指定的配额是静态的，您只能通过更改配置然后重新启动所有代理来修改它们。由于新客户端可以随时到达，这非常不方便。因此，将配额应用于特定客户端的常规方法是通过可以使用`kafka-config.sh`或AdminClient
    API设置的动态配置。
- en: 'Let’s look at few examples:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个例子：
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-1)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-1)'
- en: Limiting clientC (identified by client-id) to produce only 1024 bytes per second
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将clientC（通过客户端ID标识）限制为每秒只能产生1024字节
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-2)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-2)'
- en: Limiting user1 (identified by authenticated principal) to produce only 1024
    bytes per second and consume only 2048 bytes per second.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 将user1（通过经过身份验证的主体标识）限制为每秒只能产生1024字节和每秒只能消耗2048字节。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-3)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-3)'
- en: Limiting all users to consume only 2048 bytes per second, except users with
    more specific override. This is the way to dynamically modify the default quota.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有用户限制为每秒只能消耗2048字节，除了具有更具体覆盖的用户。这是动态修改默认配额的方法。
- en: When a client reaches its quota, the broker will start throttling the client’s
    requests to prevent it from exceeding the quota. This means that the broker will
    delay responses to client requests; in most clients this will automatically reduce
    the request rate (since the number of in-flight requests is limited) and bring
    the client traffic down to a level allowed by the quota. To protect the broker
    from misbehaved clients sending additional requests while being throttled, the
    broker will also mute the communication channel with the client for the period
    of time needed to achieve compliance with the quota.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端达到配额时，代理将开始限制客户端的请求，以防止其超出配额。这意味着代理将延迟响应客户端的请求；在大多数客户端中，这将自动降低请求速率（因为在飞行请求的数量受限），并将客户端流量降至配额允许的水平。为了保护代理免受在被限制时发送额外请求的不良客户端，代理还将在所需时间内静音与客户端的通信通道，以达到符合配额的目的。
- en: The throttling behavior is exposed to clients via `produce-throttle-time-avg`,
    `produce-throttle-time-max`, `fetch-throttle-time-avg`, and `fetch-throttle-time-max`,
    the average and the maximum amount of time a produce request and fetch request
    was delayed due to throttling. Note that this time can represent throttling due
    to produce and consume throughput quotas, request time quotas, or both. Other
    types of client requests can only be throttled due to request time quotas, and
    those will also be exposed via similar metrics.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`produce-throttle-time-avg`、`produce-throttle-time-max`、`fetch-throttle-time-avg`和`fetch-throttle-time-max`向客户端公开了限流行为，这是由于限流而延迟生产请求和获取请求的平均和最大时间量。请注意，此时间可以代表由于生产和消费吞吐量配额、请求时间配额或两者而导致的限流。其他类型的客户端请求只能由于请求时间配额而被限流，这些请求也将通过类似的指标公开。
- en: Warning
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you use async `Producer.send()` and continue to send messages at a rate that
    is higher than the rate the broker can accept (whether due to quotas or just plain
    old capacity), the messages will first be queued in the client memory. If the
    rate of sending continues to be higher than the rate of accepting messages, the
    client will eventually run out of buffer space for storing the excess messages
    and will block the next `Producer.send()` call. If the timeout delay is insufficient
    to let the broker catch up to the producer and clear some space in the buffer,
    eventually `Producer.send()` will throw `TimeoutException`. Alternatively, some
    of the records that were already placed in batches will wait for longer than `delivery.timeout.ms`
    and expire, resulting in calling the `send()` callback with a `TimeoutException`.
    It is therefore important to plan and monitor to make sure that the broker capacity
    over time will match the rate at which producers are sending data.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用异步`Producer.send()`并继续以高于代理可以接受的速率发送消息（无论是因为配额还是容量不足），消息将首先排队在客户端内存中。如果发送速率继续高于接受消息的速率，客户端最终将耗尽用于存储多余消息的缓冲空间，并阻塞下一个`Producer.send()`调用。如果超时延迟不足以让代理赶上生产者并在缓冲区中清理一些空间，最终`Producer.send()`将抛出`TimeoutException`。或者，一些已经放入批处理中的记录将等待的时间超过`delivery.timeout.ms`并过期，导致使用`TimeoutException`调用`send()`回调。因此，重要的是要计划和监视，以确保代理的容量随时间匹配生产者发送数据的速率。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We began this chapter with a simple example of a producer—just 10 lines of code
    that send events to Kafka. We added to the simple example by adding error handling
    and experimenting with synchronous and asynchronous producing. We then explored
    the most important producer configuration parameters and saw how they modify the
    behavior of the producers. We discussed serializers, which let us control the
    format of the events we write to Kafka. We looked in-depth at Avro, one of many
    ways to serialize events but one that is very commonly used with Kafka. We concluded
    the chapter with a discussion of partitioning in Kafka and an example of an advanced
    custom partitioning technique.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的生产者示例开始，只有10行代码将事件发送到Kafka。我们通过添加错误处理和尝试同步和异步生产来扩展了简单示例。然后，我们探讨了最重要的生产者配置参数，并看到它们如何修改生产者的行为。我们讨论了序列化器，它让我们控制写入Kafka的事件的格式。我们深入研究了Avro，这是序列化事件的许多方式之一，但在Kafka中非常常用。我们在本章中讨论了Kafka中的分区以及高级自定义分区技术的示例。
- en: Now that we know how to write events to Kafka, in [Chapter 4](ch04.html#reading_data_from_kafka)
    we’ll learn all about consuming events from Kafka.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何将事件写入Kafka，在[第4章](ch04.html#reading_data_from_kafka)中，我们将学习有关从Kafka消费事件的所有内容。
- en: ^([1](ch03.html#idm45351110098016-marker)) Image contributed to the Apache Kafka
    project by Sumant Tambe under the ASLv2 license terms.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm45351110098016-marker)) 图像由Sumant Tambe根据ASLv2许可条款为Apache
    Kafka项目做出贡献。

- en: 'Chapter 3\. Kafka Producers: Writing Messages to Kafka'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。Kafka生产者：向Kafka写入消息
- en: Whether you use Kafka as a queue, message bus, or data storage platform, you
    will always use Kafka by creating a producer that writes data to Kafka, a consumer
    that reads data from Kafka, or an application that serves both roles.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您将Kafka用作队列、消息总线还是数据存储平台，您始终会通过创建一个将数据写入Kafka的生产者、一个从Kafka读取数据的消费者或一个同时扮演这两个角色的应用程序来使用Kafka。
- en: For example, in a credit card transaction processing system, there will be a
    client application, perhaps an online store, responsible for sending each transaction
    to Kafka immediately when a payment is made. Another application is responsible
    for immediately checking this transaction against a rules engine and determining
    whether the transaction is approved or denied. The approve/deny response can then
    be written back to Kafka, and the response can propagate back to the online store
    where the transaction was initiated. A third application can read both transactions
    and the approval status from Kafka and store them in a database where analysts
    can later review the decisions and perhaps improve the rules engine.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在信用卡交易处理系统中，可能会有一个客户端应用程序，例如在线商店，负责在付款时立即将每笔交易发送到Kafka。另一个应用程序负责立即将此交易与规则引擎进行检查，并确定交易是否被批准或拒绝。批准/拒绝响应然后可以写回Kafka，并且响应可以传播回发起交易的在线商店。第三个应用程序可以从Kafka中读取交易和批准状态，并将它们存储在分析师稍后可以审查决策并可能改进规则引擎的数据库中。
- en: Apache Kafka ships with built-in client APIs that developers can use when developing
    applications that interact with Kafka.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka附带了内置的客户端API，开发人员在开发与Kafka交互的应用程序时可以使用这些API。
- en: In this chapter we will learn how to use the Kafka producer, starting with an
    overview of its design and components. We will show how to create `KafkaProducer`
    and `ProducerRecord` objects, how to send records to Kafka, and how to handle
    the errors that Kafka may return. We’ll then review the most important configuration
    options used to control the producer behavior. We’ll conclude with a deeper look
    at how to use different partitioning methods and serializers, and how to write
    your own serializers and partitioners.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用Kafka生产者，首先概述其设计和组件。我们将展示如何创建`KafkaProducer`和`ProducerRecord`对象，如何将记录发送到Kafka，以及如何处理Kafka可能返回的错误。然后，我们将回顾用于控制生产者行为的最重要的配置选项。最后，我们将深入了解如何使用不同的分区方法和序列化程序，以及如何编写自己的序列化程序和分区器。
- en: In [Chapter 4](ch04.html#reading_data_from_kafka), we will look at Kafka’s consumer
    client and reading data from Kafka.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#reading_data_from_kafka)中，我们将看一下Kafka的消费者客户端和从Kafka读取数据。
- en: Third-Party Clients
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三方客户端
- en: In addition to the built-in clients, Kafka has a binary wire protocol. This
    means that it is possible for applications to read messages from Kafka or write
    messages to Kafka simply by sending the correct byte sequences to Kafka’s network
    port. There are multiple clients that implement Kafka’s wire protocol in different
    programming languages, giving simple ways to use Kafka not just in Java applications
    but also in languages like C++, Python, Go, and many more. Those clients are not
    part of the Apache Kafka project, but a list of non-Java clients is maintained
    in the [project wiki](https://oreil.ly/9SbJr). The wire protocol and the external
    clients are outside the scope of the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内置的客户端，Kafka还具有二进制的传输协议。这意味着应用程序可以通过向Kafka的网络端口发送正确的字节序列来从Kafka读取消息或向Kafka写入消息。有多个客户端在不同的编程语言中实现了Kafka的传输协议，为使用Kafka提供了简单的方式，不仅可以在Java应用程序中使用Kafka，还可以在C++、Python、Go等语言中使用。这些客户端不是Apache
    Kafka项目的一部分，但非Java客户端的列表在[项目维基](https://oreil.ly/9SbJr)中进行了维护。传输协议和外部客户端不在本章的范围内。
- en: Producer Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产者概述
- en: 'There are many reasons an application might need to write messages to Kafka:
    recording user activities for auditing or analysis, recording metrics, storing
    log messages, recording information from smart appliances, communicating asynchronously
    with other applications, buffering information before writing to a database, and
    much more.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可能需要将消息写入Kafka的原因有很多：记录用户活动以进行审计或分析，记录指标，存储日志消息，记录智能设备的信息，与其他应用程序异步通信，在写入数据库之前缓冲信息等等。
- en: 'Those diverse use cases also imply diverse requirements: is every message critical,
    or can we tolerate loss of messages? Are we OK with accidentally duplicating messages?
    Are there any strict latency or throughput requirements we need to support?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的用例也意味着不同的要求：每条消息都很重要吗，还是我们可以容忍消息的丢失？我们可以意外复制消息吗？我们需要支持任何严格的延迟或吞吐量要求吗？
- en: In the credit card transaction processing example we introduced earlier, we
    can see that it is critical to never lose a single message or duplicate any messages.
    Latency should be low, but latencies up to 500 ms can be tolerated, and throughput
    should be very high—we expect to process up to a million messages a second.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前介绍的信用卡交易处理示例中，我们可以看到绝对不能丢失任何一条消息或重复任何消息是至关重要的。延迟应该很低，但可以容忍高达500毫秒的延迟，吞吐量应该非常高-我们预计每秒处理高达一百万条消息。
- en: A different use case might be to store click information from a website. In
    that case, some message loss or a few duplicates can be tolerated; latency can
    be high as long as there is no impact on the user experience. In other words,
    we don’t mind if it takes a few seconds for the message to arrive at Kafka, as
    long as the next page loads immediately after the user clicks on a link. Throughput
    will depend on the level of activity we anticipate on our website.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的用例可能是存储来自网站的点击信息。在这种情况下，可以容忍一些消息丢失或少量重复；延迟可以很高，只要不影响用户体验。换句话说，如果消息需要几秒钟才能到达Kafka，只要用户点击链接后下一页立即加载即可。吞吐量将取决于我们预期在网站上的活动水平。
- en: The different requirements will influence the way you use the producer API to
    write messages to Kafka and the configuration you use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的要求将影响您使用生产者API向Kafka写入消息的方式以及您使用的配置。
- en: While the producer API is very simple, there is a bit more that goes on under
    the hood of the producer when we send data. [Figure 3-1](#fig-1-overview) shows
    the main steps involved in sending data to Kafka.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然生产者API非常简单，但在发送数据时，在生产者的幕后会发生更多事情。[图3-1](#fig-1-overview)显示了发送数据到Kafka涉及的主要步骤。
- en: '![kdg2 0301](assets/kdg2_0301.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0301](assets/kdg2_0301.png)'
- en: Figure 3-1\. High-level overview of Kafka producer components
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。Kafka生产者组件的高级概述
- en: We start producing messages to Kafka by creating a `ProducerRecord`, which must
    include the topic we want to send the record to and a value. Optionally, we can
    also specify a key, a partition, a timestamp, and/or a collection of headers.
    Once we send the `ProducerRecord`, the first thing the producer will do is serialize
    the key and value objects to byte arrays so they can be sent over the network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建`ProducerRecord`开始向Kafka生产消息，其中必须包括我们要将记录发送到的主题和一个值。可选地，我们还可以指定一个键、一个分区、一个时间戳和/或一组标头。一旦我们发送`ProducerRecord`，生产者将首先将键和值对象序列化为字节数组，以便可以通过网络发送。
- en: Next, if we didn’t explicitly specify a partition, the data is sent to a partitioner.
    The partitioner will choose a partition for us, usually based on the `ProducerRecord`
    key. Once a partition is selected, the producer knows which topic and partition
    the record will go to. It then adds the record to a batch of records that will
    also be sent to the same topic and partition. A separate thread is responsible
    for sending those batches of records to the appropriate Kafka brokers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们没有明确指定分区，数据将被发送到分区器。分区器将为我们选择一个分区，通常基于`ProducerRecord`键。一旦选择了分区，生产者就知道记录将要发送到哪个主题和分区。然后，它将记录添加到一批记录中，这些记录也将发送到相同的主题和分区。一个单独的线程负责将这些记录批次发送到适当的Kafka代理。
- en: When the broker receives the messages, it sends back a response. If the messages
    were successfully written to Kafka, it will return a `RecordMetadata` object with
    the topic, partition, and the offset of the record within the partition. If the
    broker failed to write the messages, it will return an error. When the producer
    receives an error, it may retry sending the message a few more times before giving
    up and returning an error.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理接收到消息时，它会发送回一个响应。如果消息成功写入Kafka，它将返回一个带有记录所在主题、分区和偏移量的`RecordMetadata`对象。如果代理未能写入消息，它将返回一个错误。当生产者收到错误时，它可能会在放弃并返回错误之前尝试重新发送消息几次。
- en: Constructing a Kafka Producer
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Kafka生产者
- en: 'The first step in writing messages to Kafka is to create a producer object
    with the properties you want to pass to the producer. A Kafka producer has three
    mandatory properties:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向Kafka写入消息的第一步是创建一个具有要传递给生产者的属性的生产者对象。Kafka生产者有三个必填属性：
- en: '`bootstrap.servers`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap.servers`'
- en: List of `host:port` pairs of brokers that the producer will use to establish
    initial connection to the Kafka cluster. This list doesn’t need to include all
    brokers, since the producer will get more information after the initial connection.
    But it is recommended to include at least two, so in case one broker goes down,
    the producer will still be able to connect to the cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将用于建立与Kafka集群的初始连接的`host:port`对列表。此列表不需要包括所有代理，因为生产者在初始连接后会获取更多信息。但建议至少包括两个，这样如果一个代理宕机，生产者仍然能够连接到集群。
- en: '`key.serializer`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`key.serializer`'
- en: Name of a class that will be used to serialize the keys of the records we will
    produce to Kafka. Kafka brokers expect byte arrays as keys and values of messages.
    However, the producer interface allows, using parameterized types, any Java object
    to be sent as a key and value. This makes for very readable code, but it also
    means that the producer has to know how to convert these objects to byte arrays.
    `key.serializer` should be set to a name of a class that implements the `org.apache.kafka.common.serialization.Serializer`
    interface. The producer will use this class to serialize the key object to a byte
    array. The Kafka client package includes `ByteArraySerializer` (which doesn’t
    do much), `String​Serial⁠izer`, `IntegerSerializer`, and much more, so if you
    use common types, there is no need to implement your own serializers. Setting
    `key.serializer` is required even if you intend to send only values, but you can
    use the `Void` type for the key and the `VoidSerializer`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将用于将我们将要生产到Kafka的记录的键序列化的类的名称。Kafka代理期望消息的键和值为字节数组。但是，生产者接口允许使用参数化类型，将任何Java对象作为键和值发送。这使得代码非常易读，但也意味着生产者必须知道如何将这些对象转换为字节数组。`key.serializer`应设置为实现`org.apache.kafka.common.serialization.Serializer`接口的类的名称。生产者将使用此类将键对象序列化为字节数组。Kafka客户端包括`ByteArraySerializer`（几乎不做任何事情）、`String​Serial⁠izer`、`IntegerSerializer`等等，因此如果使用常见类型，则无需实现自己的序列化程序。即使您只打算发送值，也需要设置`key.serializer`，但是您可以使用`Void`类型作为键和`VoidSerializer`。
- en: '`value.serializer`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`value.serializer`'
- en: Name of a class that will be used to serialize the values of the records we
    will produce to Kafka. The same way you set `key.serializer` to a name of a class
    that will serialize the message key object to a byte array, you set `value.serializer`
    to a class that will serialize the message value object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将用于将我们将要生产到Kafka的记录的值序列化的类的名称。与设置`key.serializer`的方式相同，将`value.serializer`设置为将序列化消息值对象的类的名称。
- en: 'The following code snippet shows how to create a new producer by setting just
    the mandatory parameters and using defaults for everything else:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了如何通过仅设置必填参数并对其他所有内容使用默认值来创建新的生产者：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-1)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-1)'
- en: We start with a `Properties` object.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个`Properties`对象开始。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-2)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Since we plan on using strings for message key and value, we use the built-in
    `StringSerializer`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO1-3)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Here we create a new producer by setting the appropriate key and value types
    and passing the `Properties` object.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: With such a simple interface, it is clear that most of the control over producer
    behavior is done by setting the correct configuration properties. Apache Kafka
    documentation covers all the [configuration options](http://bit.ly/2sMu1c8), and
    we will go over the important ones later in this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we instantiate a producer, it is time to start sending messages. There
    are three primary methods of sending messages:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Fire-and-forget
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: We send a message to the server and don’t really care if it arrives successfully
    or not. Most of the time, it will arrive successfully, since Kafka is highly available
    and the producer will retry sending messages automatically. However, in case of
    nonretriable errors or timeout, messages will get lost and the application will
    not get any information or exceptions about this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous send
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Technically, Kafka producer is always asynchronous—we send a message and the
    `send()` method returns a `Future` object. However, we use `get()` to wait on
    the `Future` and see if the `send()` was successful or not before sending the
    next record.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous send
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: We call the `send()` method with a callback function, which gets triggered when
    it receives a response from the Kafka broker.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In the examples that follow, we will see how to send messages using these methods
    and how to handle the different types of errors that might occur.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: While all the examples in this chapter are single threaded, a producer object
    can be used by multiple threads to send messages.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Sending a Message to Kafka
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way to send a message is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-1)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The producer accepts `ProducerRecord` objects, so we start by creating one.
    `ProducerRecord` has multiple constructors, which we will discuss later. Here
    we use one that requires the name of the topic we are sending data to, which is
    always a string, and the key and value we are sending to Kafka, which in this
    case are also strings. The types of the key and value must match our `key serializer`
    and `value serializer` objects.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-2)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: We use the producer object `send()` method to send the `ProducerRecord`. As
    we’ve seen in the producer architecture diagram in [Figure 3-1](#fig-1-overview),
    the message will be placed in a buffer and will be sent to the broker in a separate
    thread. The `send()` method returns a [Java `Future` object](http://bit.ly/2rG7Cg6)
    with `RecordMetadata`, but since we simply ignore the returned value, we have
    no way of knowing whether the message was sent successfully or not. This method
    of sending messages can be used when dropping a message silently is acceptable.
    This is not typically the case in production applications.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO2-3)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: While we ignore errors that may occur while sending messages to Kafka brokers
    or in the brokers themselves, we may still get an exception if the producer encountered
    errors before sending the message to Kafka. Those can be, for example, a `SerializationException`
    when it fails to serialize the message, a `Buffer​ExhaustedException` or `TimeoutException`
    if the buffer is full, or an `InterruptException` if the sending thread was interrupted.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Sending a Message Synchronously
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sending a message synchronously is simple but still allows the producer to catch
    exceptions when Kafka responds to the produce request with an error, or when send
    retries were exhausted. The main trade-off involved is performance. Depending
    on how busy the Kafka cluster is, brokers can take anywhere from 2 ms to a few
    seconds to respond to produce requests. If you send messages synchronously, the
    sending thread will spend this time waiting and doing nothing else, not even sending
    additional messages. This leads to very poor performance, and as a result, synchronous
    sends are usually not used in production applications (but are very common in
    code examples).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to send a message synchronously is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO3-1)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are using `Future.get()` to wait for a reply from Kafka. This method
    will throw an exception if the record is not sent successfully to Kafka. If there
    were no errors, we will get a `RecordMetadata` object that we can use to retrieve
    the offset the message was written to and other metadata.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO3-2)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: If there were any errors before or while sending the record to Kafka, we will
    encounter an exception. In this case, we just print any exception we ran into.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '`KafkaProducer` has two types of errors. *Retriable* errors are those that
    can be resolved by sending the message again. For example, a connection error
    can be resolved because the connection may get reestablished. A “not leader for
    partition” error can be resolved when a new leader is elected for the partition
    and the client metadata is refreshed. `KafkaProducer` can be configured to retry
    those errors automatically, so the application code will get retriable exceptions
    only when the number of retries was exhausted and the error was not resolved.
    Some errors will not be resolved by retrying—for example, “Message size too large.”
    In those cases, `KafkaProducer` will not attempt a retry and will return the exception
    immediately.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Sending a Message Asynchronously
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose the network round-trip time between our application and the Kafka cluster
    is 10 ms. If we wait for a reply after sending each message, sending 100 messages
    will take around 1 second. On the other hand, if we just send all our messages
    and not wait for any replies, then sending 100 messages will barely take any time
    at all. In most cases, we really don’t need a reply—Kafka sends back the topic,
    partition, and offset of the record after it was written, which is usually not
    required by the sending app. On the other hand, we do need to know when we failed
    to send a message completely so we can throw an exception, log an error, or perhaps
    write the message to an “errors” file for later analysis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'To send messages asynchronously and still handle error scenarios, the producer
    supports adding a callback when sending a record. Here is an example of how we
    use a callback:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: To use callbacks, you need a class that implements the `org.apache.kafka.` `clients.producer.Callback`
    interface, which has a single function—`on​Com⁠ple⁠tion()`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: If Kafka returned an error, `onCompletion()` will have a nonnull exception.
    Here we “handle” it by printing, but production code will probably have more robust
    error handling functions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-3)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The records are the same as before.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO4-4)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: And we pass a `Callback` object along when sending the record.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The callbacks execute in the producer’s main thread. This guarantees that when
    we send two messages to the same partition one after another, their callbacks
    will be executed in the same order that we sent them. But it also means that the
    callback should be reasonably fast to avoid delaying the producer and preventing
    other messages from being sent. It is not recommended to perform a blocking operation
    within the callback. Instead, you should use another thread to perform any blocking
    operation concurrently.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回调在生产者的主线程中执行。这保证了当我们连续向同一分区发送两条消息时，它们的回调将按照我们发送它们的顺序执行。但这也意味着回调应该相当快，以避免延迟生产者并阻止其他消息的发送。不建议在回调中执行阻塞操作。相反，您应该使用另一个线程并发执行任何阻塞操作。
- en: Configuring Producers
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置生产者
- en: So far we’ve seen very few configuration parameters for the producers—just the
    mandatory `bootstrap.servers` URI and serializers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对生产者的配置参数很少——只有强制的`bootstrap.servers` URI和序列化器。
- en: The producer has a large number of configuration parameters that are documented
    in [Apache Kafka documentation](https://oreil.ly/RkxSS), and many have reasonable
    defaults, so there is no reason to tinker with every single parameter. However,
    some of the parameters have a significant impact on memory use, performance, and
    reliability of the producers. We will review those here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者有大量的配置参数，这些参数在[Apache Kafka文档](https://oreil.ly/RkxSS)中有记录，许多参数都有合理的默认值，因此没有理由去调整每个参数。然而，一些参数对生产者的内存使用、性能和可靠性有重大影响。我们将在这里进行审查。
- en: client.id
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: client.id
- en: '`client.id` is a logical identifier for the client and the application it is
    used in. This can be any string and will be used by the brokers to identify messages
    sent from the client. It is used in logging and metrics and for quotas. Choosing
    a good client name will make troubleshooting much easier—it is the difference
    between “We are seeing a high rate of authentication failures from IP 104.27.155.134”
    and “Looks like the Order Validation service is failing to authenticate—can you
    ask Laura to take a look?”'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`client.id`是客户端和所使用的应用程序的逻辑标识符。这可以是任何字符串，并将被经纪人用于识别从客户端发送的消息。它用于日志记录和指标以及配额。选择一个好的客户端名称将使故障排除变得更容易——这是“我们看到IP
    104.27.155.134的身份验证失败率很高”和“看起来订单验证服务无法进行身份验证——你能让劳拉来看一下吗？”之间的区别。'
- en: acks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: acks
- en: 'The `acks` parameter controls how many partition replicas must receive the
    record before the producer can consider the write successful. By default, Kafka
    will respond that the record was written successfully after the leader received
    the record (release 3.0 of Apache Kafka is expected to change this default). This
    option has a significant impact on the durability of written messages, and depending
    on your use case, the default may not be the best choice. [Chapter 7](ch07.html#reliable_data_delivery)
    discusses Kafka’s reliability guarantees in depth, but for now let’s review the
    three allowed values for the `acks` parameter:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks`参数控制生产者在可以考虑写入成功之前必须接收记录的分区副本数量。默认情况下，Kafka将在领导者接收记录后回复记录已成功写入（预计Apache
    Kafka的3.0版本将更改此默认值）。此选项对写入消息的持久性有重大影响，根据您的用例，可能默认值不是最佳选择。[第7章](ch07.html#reliable_data_delivery)深入讨论了Kafka的可靠性保证，但现在让我们回顾一下`acks`参数的三个允许值：'
- en: '`acks=0`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=0`'
- en: The producer will not wait for a reply from the broker before assuming the message
    was sent successfully. This means that if something goes wrong and the broker
    does not receive the message, the producer will not know about it, and the message
    will be lost. However, because the producer is not waiting for any response from
    the server, it can send messages as fast as the network will support, so this
    setting can be used to achieve very high throughput.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者在假定消息成功发送之前不会等待经纪人的回复。这意味着如果出现问题，经纪人没有收到消息，生产者将不会知道，消息将丢失。然而，由于生产者不等待服务器的任何响应，它可以以网络支持的速度发送消息，因此可以使用此设置来实现非常高的吞吐量。
- en: '`acks=1`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=1`'
- en: The producer will receive a success response from the broker the moment the
    leader replica receives the message. If the message can’t be written to the leader
    (e.g., if the leader crashed and a new leader was not elected yet), the producer
    will receive an error response and can retry sending the message, avoiding potential
    loss of data. The message can still get lost if the leader crashes and the latest
    messages were not yet replicated to the new leader.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者将在领导者副本接收到消息时从经纪人那里收到成功响应。如果消息无法写入领导者（例如，如果领导者崩溃并且尚未选举出新的领导者），生产者将收到错误响应，并可以重试发送消息，避免数据的潜在丢失。如果领导者崩溃并且最新的消息尚未复制到新的领导者，消息仍可能丢失。
- en: '`acks=all`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=all`'
- en: The producer will receive a success response from the broker once all in sync
    replicas receive the message. This is the safest mode since you can make sure
    more than one broker has the message and that the message will survive even in
    case of a crash (more information on this in [Chapter 6](ch06.html#kafka_internals)).
    However, the latency we discussed in the `acks=1` case will be even higher, since
    we will be waiting for more than just one broker to receive the message.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有同步副本接收到消息，生产者将从经纪人那里收到成功响应。这是最安全的模式，因为您可以确保不止一个经纪人收到了消息，并且即使发生崩溃，消息也会存活下来（有关此信息的更多信息，请参见[第6章](ch06.html#kafka_internals)）。然而，我们在`acks=1`情况下讨论的延迟将更高，因为我们将等待不止一个经纪人接收消息。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You will see that with lower and less reliable `acks` configuration, the producer
    will be able to send records faster. This means that you trade off reliability
    for *producer latency*. However, *end-to-end latency* is measured from the time
    a record was produced until it is available for consumers to read and is identical
    for all three options. The reason is that, in order to maintain consistency, Kafka
    will not allow consumers to read records until they are written to all in sync
    replicas. Therefore, if you care about end-to-end latency, rather than just the
    producer latency, there is no trade-off to make: you will get the same end-to-end
    latency if you choose the most reliable option.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现，使用较低且不太可靠的`acks`配置，生产者将能够更快地发送记录。这意味着您在可靠性和*生产者延迟*之间进行权衡。但是，*端到端延迟*是从记录生成到可供消费者读取的时间，并且对于所有三个选项都是相同的。原因是，为了保持一致性，Kafka不会允许消费者读取记录，直到它们被写入所有同步副本。因此，如果您关心端到端延迟，而不仅仅是生产者延迟，那么就没有权衡可做：如果选择最可靠的选项，您将获得相同的端到端延迟。
- en: Message Delivery Time
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消息传递时间
- en: 'The producer has multiple configuration parameters that interact to control
    one of the behaviors that are of most interest to developers: how long will it
    take until a call to `send()` will succeed or fail. This is the time we are willing
    to spend until Kafka responds successfully, or until we are willing to give up
    and admit defeat.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者具有多个配置参数，这些参数相互作用以控制开发人员最感兴趣的行为之一：直到`send()`调用成功或失败需要多长时间。这是我们愿意花费的时间，直到Kafka成功响应，或者我们愿意放弃并承认失败。
- en: The configurations and their behaviors were modified several times over the
    years. We will describe here the latest implementation, introduced in Apache Kafka
    2.1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，配置及其行为已经多次修改。我们将在这里描述最新的实现，即Apache Kafka 2.1中引入的实现。
- en: 'Since Apache Kafka 2.1, we divide the time spent sending a `ProduceRecord`
    into two time intervals that are handled separately:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自Apache Kafka 2.1以来，我们将发送`ProduceRecord`的时间分为两个分别处理的时间间隔：
- en: Time until an async call to `send()` returns. During this interval, the thread
    that called `send()` will be blocked.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从调用`send()`的异步调用返回的时间。在此期间，调用`send()`的线程将被阻塞。
- en: From the time an async call to `send()` returned successfully until the callback
    is triggered (with success or failure). This is the same as from the point a `Produce​Re⁠cord`
    was placed in a batch for sending until Kafka responds with success, nonretriable
    failure, or we run out of time allocated for sending.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从异步调用`send()`成功返回直到触发回调（成功或失败）的时间。这与从将`Produce​Re⁠cord`放入批处理以进行发送直到Kafka以成功、不可重试的失败或我们用于发送的时间用完为止是相同的。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you use `send()` synchronously, the sending thread will block for both time
    intervals continuously, and you won’t be able to tell how much time was spent
    in each. We’ll discuss the common and recommended case, where `send()` is used
    asynchronously, with a callback.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您同步使用`send()`，发送线程将连续阻塞两个时间间隔，并且您将无法知道每个时间间隔花费了多少时间。我们将讨论常见和推荐的情况，即异步使用`send()`，并带有回调。
- en: The flow of data within the producer and how the different configuration parameters
    affect each other can be summarized in [Figure 3-2](#fig-2-delivery).^([1](ch03.html#idm45351110098016))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者内部数据流以及不同配置参数如何相互影响的流程可以在[图3-2](#fig-2-delivery)中总结。^([1](ch03.html#idm45351110098016))
- en: '![kdg2 0302](assets/kdg2_0302.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0302](assets/kdg2_0302.png)'
- en: Figure 3-2\. Sequence diagram of delivery time breakdown inside Kafka producer
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2。Kafka生产者内传递时间分解的序列图
- en: We’ll go through the different configuration parameters used to control the
    time spent waiting in these two intervals and how they interact.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍用于控制在这两个时间间隔中等待的时间以及它们如何相互作用的不同配置参数。
- en: max.block.ms
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: max.block.ms
- en: This parameter controls how long the producer may block when calling `send()`
    and when explicitly requesting metadata via `partitionsFor()`. Those methods may
    block when the producer’s send buffer is full or when metadata is not available.
    When `max.block.ms` is reached, a timeout exception is thrown.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数控制在调用`send()`时和通过`partitionsFor()`显式请求元数据时，生产者可能阻塞的时间。当生产者的发送缓冲区已满或元数据不可用时，这些方法可能会阻塞。当达到`max.block.ms`时，将抛出超时异常。
- en: delivery.timeout.ms
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: delivery.timeout.ms
- en: This configuration will limit the amount of time spent from the point a record
    is ready for sending (`send()` returned successfully and the record is placed
    in a batch) until either the broker responds or the client gives up, including
    time spent on retries. As you can see in [Figure 3-2](#fig-2-delivery), this time
    should be greater than `linger.ms` and `request.timeout.ms`. If you try to create
    a producer with an inconsistent timeout configuration, you will get an exception.
    Messages can be successfully sent much faster than `delivery.timeout.ms`, and
    typically will.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置将限制从记录准备发送（`send()`成功返回并将记录放入批处理）的时间，直到经纪人响应或客户端放弃，包括重试所花费的时间。如您在[图3-2](#fig-2-delivery)中所见，此时间应大于`linger.ms`和`request.timeout.ms`。如果尝试使用不一致的超时配置创建生产者，将会收到异常。消息可以成功发送的速度远快于`delivery.timeout.ms`，通常会更快。
- en: If the producer exceeds `delivery.timeout.ms` while retrying, the callback will
    be called with the exception that corresponds to the error that the broker returned
    before retrying. If `delivery.timeout.ms` is exceeded while the record batch was
    still waiting to be sent, the callback will be called with a timeout exception.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生产者在重试时超过`delivery.timeout.ms`，则回调将使用与重试前经纪人返回的错误对应的异常进行调用。如果在记录批次仍在等待发送时超过`delivery.timeout.ms`，则回调将使用超时异常进行调用。
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You can configure the delivery timeout to the maximum time you’ll want to wait
    for a message to be sent, typically a few minutes, and then leave the default
    number of retries (virtually infinite). With this configuration, the producer
    will keep retrying for as long as it has time to keep trying (or until it succeeds).
    This is a much more reasonable way to think about retries. Our normal process
    for tuning retries is: “In case of a broker crash, it typically takes leader election
    30 seconds to complete, so let’s keep retrying for 120 seconds just to be on the
    safe side.” Instead of converting this mental dialog to number of retries and
    time between retries, you just configure `deliver.timeout.ms` to 120.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将交付超时配置为您希望等待消息发送的最长时间，通常是几分钟，然后保持默认的重试次数（几乎是无限的）。使用这个配置，只要有时间继续尝试（或者直到成功），生产者将一直重试。这是一个更合理的重试方式。我们通常调整重试的过程是：“在发生代理崩溃的情况下，通常需要30秒才能完成领导者选举，所以让我们保持重试120秒，以防万一。”而不是将这种心理对话转化为重试次数和重试之间的时间，您只需将“deliver.timeout.ms”配置为120。
- en: request.timeout.ms
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: request.timeout.ms
- en: This parameter controls how long the producer will wait for a reply from the
    server when sending data. Note that this is the time spent waiting on each producer
    request before giving up; it does not include retries, time spent before sending,
    and so on. If the timeout is reached without reply, the producer will either retry
    sending or complete the callback with a `TimeoutException`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个参数控制生产者在发送数据时等待服务器回复的时间。请注意，这是在每个生产者请求等待回复的时间，而不包括重试、发送前的等待等。如果超时而没有回复，生产者将要么重试发送，要么用“TimeoutException”完成回调。
- en: retries and retry.backoff.ms
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重试和retry.backoff.ms
- en: When the producer receives an error message from the server, the error could
    be transient (e.g., a lack of leader for a partition). In this case, the value
    of the `retries` parameter will control how many times the producer will retry
    sending the message before giving up and notifying the client of an issue. By
    default, the producer will wait 100 ms between retries, but you can control this
    using the `retry.backoff.ms` parameter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当生产者从服务器收到错误消息时，错误可能是暂时的（例如，分区没有领导者）。在这种情况下，“重试”参数的值将控制生产者在放弃并通知客户端出现问题之前重试发送消息的次数。默认情况下，生产者在重试之间会等待100毫秒，但您可以使用“retry.backoff.ms”参数来控制这一点。
- en: We recommend against using these parameters in the current version of Kafka.
    Instead, test how long it takes to recover from a crashed broker (i.e., how long
    until all partitions get new leaders), and set `delivery.timeout.ms` such that
    the total amount of time spent retrying will be longer than the time it takes
    the Kafka cluster to recover from the crash—otherwise, the producer will give
    up too soon.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议不要在当前版本的Kafka中使用这些参数。相反，测试从崩溃的代理中恢复需要多长时间（即直到所有分区获得新领导者），并设置“delivery.timeout.ms”，使得重试的总时间长于Kafka集群从崩溃中恢复所需的时间——否则，生产者会放弃得太早。
- en: Not all errors will be retried by the producer. Some errors are not transient
    and will not cause retries (e.g., “message too large” error). In general, because
    the producer handles retries for you, there is no point in handling retries within
    your own application logic. You will want to focus your efforts on handling nonretriable
    errors or cases where retry attempts were exhausted.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的错误都会被生产者重试。一些错误不是暂时的，不会导致重试（例如，“消息过大”错误）。一般来说，因为生产者为您处理重试，所以在您自己的应用逻辑中处理重试是没有意义的。您将希望将精力集中在处理不可重试的错误或重试尝试耗尽的情况上。
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to completely disable retries, setting `retries=0` is the only way
    to do so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想完全禁用重试，将“retries=0”设置为唯一的方法。
- en: linger.ms
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: linger.ms
- en: '`linger.ms` controls the amount of time to wait for additional messages before
    sending the current batch. `KafkaProducer` sends a batch of messages either when
    the current batch is full or when the `linger.ms` limit is reached. By default,
    the producer will send messages as soon as there is a sender thread available
    to send them, even if there’s just one message in the batch. By setting `linger.ms`
    higher than 0, we instruct the producer to wait a few milliseconds to add additional
    messages to the batch before sending it to the brokers. This increases latency
    a little and significantly increases throughput—the overhead per message is much
    lower, and compression, if enabled, is much better.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: “linger.ms”控制在发送当前批次之前等待额外消息的时间。默认情况下，生产者会在当前批次已满或达到“linger.ms”限制时发送消息。默认情况下，只要有发送线程可用来发送消息，生产者就会立即发送消息，即使批次中只有一条消息。通过将“linger.ms”设置为大于0，我们指示生产者在将批次发送到代理之前等待几毫秒以添加额外的消息到批次中。这会稍微增加延迟，并显著增加吞吐量——每条消息的开销要低得多，如果启用了压缩，压缩效果会更好。
- en: buffer.memory
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: buffer.memory
- en: This config sets the amount of memory the producer will use to buffer messages
    waiting to be sent to brokers. If messages are sent by the application faster
    than they can be delivered to the server, the producer may run out of space, and
    additional `send()` calls will block for `max.block.ms` and wait for space to
    free up before throwing an exception. Note that unlike most producer exceptions,
    this timeout is thrown by `send()` and not by the resulting `Future`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置设置了生产者用来缓冲等待发送到代理的消息的内存量。如果应用程序发送消息的速度比它们被传递到服务器的速度快，生产者可能会用完空间，而额外的“send（）”调用将会阻塞“max.block.ms”并等待空间释放，然后才会抛出异常。请注意，与大多数生产者异常不同，这个超时是由“send（）”而不是由结果“Future”抛出的。
- en: compression.type
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: compression.type
- en: By default, messages are sent uncompressed. This parameter can be set to `snappy`,
    `gzip`, `lz4`, or `zstd`, in which case the corresponding compression algorithms
    will be used to compress the data before sending it to the brokers. Snappy compression
    was invented by Google to provide decent compression ratios with low CPU overhead
    and good performance, so it is recommended in cases where both performance and
    bandwidth are a concern. Gzip compression will typically use more CPU and time
    but results in better compression ratios, so it is recommended in cases where
    network bandwidth is more restricted. By enabling compression, you reduce network
    utilization and storage, which is often a bottleneck when sending messages to
    Kafka.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消息是未压缩的。此参数可以设置为`snappy`、`gzip`、`lz4`或`zstd`，在这种情况下，将使用相应的压缩算法对数据进行压缩，然后将其发送到经纪人。Snappy压缩是由Google发明的，以提供良好的压缩比和低CPU开销以及良好的性能，因此在性能和带宽都受到关注的情况下建议使用。Gzip压缩通常会使用更多的CPU和时间，但会产生更好的压缩比，因此在网络带宽更受限制的情况下建议使用。通过启用压缩，可以减少网络利用率和存储，这在向Kafka发送消息时通常是瓶颈。
- en: batch.size
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: batch.size
- en: When multiple records are sent to the same partition, the producer will batch
    them together. This parameter controls the amount of memory in bytes (not messages!)
    that will be used for each batch. When the batch is full, all the messages in
    the batch will be sent. However, this does not mean that the producer will wait
    for the batch to become full. The producer will send half-full batches and even
    batches with just a single message in them. Therefore, setting the batch size
    too large will not cause delays in sending messages; it will just use more memory
    for the batches. Setting the batch size too small will add some overhead because
    the producer will need to send messages more frequently.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当多条记录发送到同一分区时，生产者将它们批量处理在一起。此参数控制每个批次将用于的字节内存量（而不是消息！）。当批次满了，批次中的所有消息将被发送。但是，这并不意味着生产者会等待批次变满。生产者将发送半满的批次，甚至只有一条消息的批次。因此，将批次大小设置得太大不会导致发送消息的延迟；它只会使用更多的内存用于批次。将批次大小设置得太小会增加一些开销，因为生产者需要更频繁地发送消息。
- en: max.in.flight.requests.per.connection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.in.flight.requests.per.connection
- en: This controls how many message batches the producer will send to the server
    without receiving responses. Higher settings can increase memory usage while improving
    throughput. [Apache’s wiki experiments show](https://oreil.ly/NZmJ0) that in a
    single-DC environment, the throughput is maximized with only 2 in-flight requests;
    however, the default value is 5 and shows similar performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这控制生产者在未收到响应的情况下向服务器发送多少消息批次。较高的设置可以增加内存使用量，同时提高吞吐量。[Apache的维基实验显示](https://oreil.ly/NZmJ0)，在单个DC环境中，通过只有2个飞行请求可以实现最大吞吐量；然而，默认值为5并显示类似的性能。
- en: Ordering Guarantees
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序保证
- en: Apache Kafka preserves the order of messages within a partition. This means
    that if messages are sent from the producer in a specific order, the broker will
    write them to a partition in that order and all consumers will read them in that
    order. For some use cases, order is very important. There is a big difference
    between depositing $100 in an account and later withdrawing it, and the other
    way around! However, some use cases are less sensitive.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka保留分区内消息的顺序。这意味着如果消息按特定顺序从生产者发送，经纪人将按照该顺序将它们写入分区，并且所有消费者将按照该顺序读取它们。对于某些用例，顺序非常重要。在账户中存入100美元并稍后取款，与相反的顺序之间存在很大的区别！然而，某些用例则不太敏感。
- en: Setting the `retries` parameter to nonzero and the `max.in.​flight.requests.per.connection`
    to more than 1 means that it is possible that the broker will fail to write the
    first batch of messages, succeed in writing the second (which was already in-flight),
    and then retry the first batch and succeed, thereby reversing the order.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将“重试”参数设置为非零，并将“每个连接的最大飞行请求数”设置为大于1意味着可能会发生经纪人无法写入第一批消息，成功写入第二批（已经在飞行中），然后重试第一批并成功，从而颠倒顺序。
- en: Since we want at least two in-flight requests for performance reasons, and a
    high number of retries for reliability reasons, the best solution is to set `enable.idempotence=true`.
    This guarantees message ordering with up to five in-flight requests and also guarantees
    that retries will not introduce duplicates. [Chapter 8](ch08.html#exactly_once_semantics)
    discusses the idempotent producer in depth.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于出于性能原因，我们希望至少有两个飞行请求，并出于可靠性原因，希望有较高数量的重试，因此最佳解决方案是设置`enable.idempotence=true`。这可以保证最多有五个飞行请求的消息排序，并且保证重试不会引入重复。[第8章](ch08.html#exactly_once_semantics)深入讨论了幂等生产者。
- en: max.request.size
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.request.size
- en: This setting controls the size of a produce request sent by the producer. It
    caps both the size of the largest message that can be sent and the number of messages
    that the producer can send in one request. For example, with a default maximum
    request size of 1 MB, the largest message you can send is 1 MB, or the producer
    can batch 1,024 messages of size 1 KB each into one request. In addition, the
    broker has its own limit on the size of the largest message it will accept (`message.max.bytes`).
    It is usually a good idea to have these configurations match, so the producer
    will not attempt to send messages of a size that will be rejected by the broker.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置控制生产者发送的生产请求的大小。它限制了可以发送的最大消息的大小以及生产者可以在一个请求中发送的消息数量。例如，默认的最大请求大小为1 MB，您可以发送的最大消息为1
    MB，或者生产者可以将1,024条大小为1 KB的消息批量处理成一个请求。此外，经纪人对其将接受的最大消息大小也有限制（`message.max.bytes`）。通常最好将这些配置匹配起来，这样生产者就不会尝试发送经纪人拒绝的大小的消息。
- en: receive.buffer.bytes and send.buffer.bytes
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: receive.buffer.bytes和send.buffer.bytes
- en: These are the sizes of the TCP send and receive buffers used by the sockets
    when writing and reading data. If these are set to –1, the OS defaults will be
    used. It is a good idea to increase these when producers or consumers communicate
    with brokers in a different datacenter, because those network links typically
    have higher latency and lower bandwidth.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在写入和读取数据时套接字使用的TCP发送和接收缓冲区的大小。如果将它们设置为-1，将使用操作系统的默认值。当生产者或消费者与不同数据中心的代理进行通信时，建议增加这些值，因为这些网络链接通常具有更高的延迟和较低的带宽。
- en: enable.idempotence
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: enable.idempotence
- en: Starting in version 0.11, Kafka supports *exactly once* semantics. Exactly once
    is a fairly large topic, and we’ll dedicate an entire chapter to it, but idempotent
    producer is a simple and highly beneficial part of it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从0.11版本开始，Kafka支持*仅一次*语义。仅一次是一个相当大的主题，我们将专门为此撰写一整章，但幂等生产者是其中一个简单且非常有益的部分。
- en: 'Suppose you configure your producer to maximize reliability: `acks=all` and
    a decently large `delivery.timeout.ms` to allow sufficient retries. These make
    sure each message will be written to Kafka at least once. In some cases, this
    means that messages will be written to Kafka more than once. For example, imagine
    that a broker received a record from the producer, wrote it to local disk, and
    the record was successfully replicated to other brokers, but then the first broker
    crashed before sending a response to the producer. The producer will wait until
    it reaches `request.​time⁠out.ms` and then retry. The retry will go to the new
    leader that already has a copy of this record since the previous write was replicated
    successfully. You now have a duplicate record.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您配置生产者以最大化可靠性：`acks=all`和一个相当大的`delivery.timeout.ms`以允许足够的重试。这样可以确保每条消息至少会被写入Kafka一次。在某些情况下，这意味着消息将被写入Kafka多次。例如，假设代理从生产者接收到一条记录，将其写入本地磁盘，并成功地复制到其他代理，但然后第一个代理在发送响应给生产者之前崩溃了。生产者将等待直到达到`request.​time⁠out.ms`然后重试。重试将发送到已经成功复制了此记录的新领导者。现在您有了一个重复的记录。
- en: To avoid this, you can set `enable.idempotence=true`. When the idempotent producer
    is enabled, the producer will attach a sequence number to each record it sends.
    If the broker receives records with the same sequence number, it will reject the
    second copy and the producer will receive the harmless `DuplicateSequenceException`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，您可以设置`enable.idempotence=true`。启用幂等生产者后，生产者将为发送的每条记录附加一个序列号。如果代理接收到具有相同序列号的记录，它将拒绝第二份副本，生产者将收到无害的`DuplicateSequenceException`。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Enabling idempotence requires `max.in.flight.requests.per.​con⁠nection` to be
    less than or equal to 5, `retries` to be greater than 0, and `acks=all`. If incompatible
    values are set, a `ConfigException` will be thrown.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 启用幂等性要求`max.in.flight.requests.per.​con⁠nection`小于或等于5，`retries`大于0，`acks=all`。如果设置了不兼容的值，将抛出`ConfigException`。
- en: Serializers
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化程序
- en: As seen in previous examples, producer configuration includes mandatory serializers.
    We’ve seen how to use the default `String` serializer. Kafka also includes serializers
    for integers, `ByteArrays`, and many more, but this does not cover most use cases.
    Eventually, you will want to be able to serialize more generic records.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在之前的例子中所看到的，生产者配置包括强制性的序列化程序。我们已经看到了如何使用默认的`String`序列化程序。Kafka还包括整数、`ByteArrays`等许多序列化程序，但这并不能涵盖大多数用例。最终，您将希望能够序列化更通用的记录。
- en: We will start by showing how to write your own serializer and then introduce
    the Avro serializer as a recommended alternative.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示如何编写自己的序列化程序，然后介绍Avro序列化程序作为一个推荐的替代方案。
- en: Custom Serializers
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义序列化程序
- en: When the object you need to send to Kafka is not a simple string or integer,
    you have a choice of either using a generic serialization library like Avro, Thrift,
    or Protobuf to create records, or creating a custom serialization for objects
    you are already using. We highly recommend using a generic serialization library.
    In order to understand how the serializers work and why it is a good idea to use
    a serialization library, let’s see what it takes to write your own custom serializer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要发送到Kafka的对象不是简单的字符串或整数时，您可以选择使用通用序列化库（如Avro、Thrift或Protobuf）创建记录，或者为您已经使用的对象创建自定义序列化。我们强烈建议使用通用序列化库。为了理解序列化程序的工作原理以及为什么使用序列化库是一个好主意，让我们看看编写自己的自定义序列化程序需要做些什么。
- en: 'Suppose that instead of recording just the customer name, you create a simple
    class to represent customers:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不仅仅记录客户的姓名，而是创建一个简单的类来表示客户：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now suppose we want to create a custom serializer for this class. It will look
    something like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想为这个类创建一个自定义的序列化程序。它看起来会像这样：
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Configuring a producer with this `CustomerSerializer` will allow you to define
    `ProducerRecord<String, Customer>`, and send `Customer` data and pass `Customer`
    objects directly to the producer. This example is pretty simple, but you can see
    how fragile the code is. If we ever have too many customers, for example, and
    need to change `customerID` to `Long`, or if we ever decide to add a `startDate`
    field to `Customer`, we will have a serious issue in maintaining compatibility
    between old and new messages. Debugging compatibility issues between different
    versions of serializers and deserializers is fairly challenging: you need to compare
    arrays of raw bytes. To make matters even worse, if multiple teams in the same
    company end up writing `Customer` data to Kafka, they will all need to use the
    same serializers and modify the code at the exact same time.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`CustomerSerializer`配置生产者将允许您定义`ProducerRecord<String, Customer>`，并发送`Customer`数据并直接将`Customer`对象传递给生产者。这个例子很简单，但您可以看到代码是多么脆弱。例如，如果我们有太多的客户，并且需要将`customerID`更改为`Long`，或者如果我们决定向`Customer`添加一个`startDate`字段，那么在维护旧消息和新消息之间的兼容性方面将会出现严重问题。在不同版本的序列化程序和反序列化程序之间调试兼容性问题是相当具有挑战性的：您需要比较原始字节数组。更糟糕的是，如果同一家公司的多个团队最终都向Kafka写入`Customer`数据，他们都需要使用相同的序列化程序并同时修改代码。
- en: For these reasons, we recommend using existing serializers and deserializers
    such as JSON, Apache Avro, Thrift, or Protobuf. In the following section, we will
    describe Apache Avro and then show how to serialize Avro records and send them
    to Kafka.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议使用现有的序列化器和反序列化器，如JSON、Apache Avro、Thrift或Protobuf。在接下来的部分中，我们将描述Apache
    Avro，然后展示如何序列化Avro记录并将其发送到Kafka。
- en: Serializing Using Apache Avro
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Apache Avro进行序列化
- en: Apache Avro is a language-neutral data serialization format. The project was
    created by Doug Cutting to provide a way to share data files with a large audience.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Avro是一种语言中立的数据序列化格式。该项目由Doug Cutting创建，旨在为大众提供一种共享数据文件的方式。
- en: Avro data is described in a language-independent schema. The schema is usually
    described in JSON, and the serialization is usually to binary files, although
    serializing to JSON is also supported. Avro assumes that the schema is present
    when reading and writing files, usually by embedding the schema in the files themselves.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Avro数据是用语言无关的模式描述的。模式通常用JSON描述，序列化通常是到二进制文件，尽管也支持序列化到JSON。Avro假定在读取和写入文件时存在模式，通常是通过将模式嵌入文件本身来实现。
- en: One of the most interesting features of Avro, and what makes it a good fit for
    use in a messaging system like Kafka, is that when the application that is writing
    messages switches to a new but compatible schema, the applications reading the
    data can continue processing messages without requiring any change or update.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Avro最有趣的特性之一，也是使其适合在Kafka等消息系统中使用的原因之一，是当编写消息的应用程序切换到新的但兼容的模式时，读取数据的应用程序可以继续处理消息而无需任何更改或更新。
- en: 'Suppose the original schema was:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始模式是：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO5-1)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO5-1)'
- en: '`id` and `name` fields are mandatory, while `faxNumber` is optional and defaults
    to `null`.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`id`和`name`字段是必需的，而`faxNumber`是可选的，默认为`null`。'
- en: We used this schema for a few months and generated a few terabytes of data in
    this format. Now suppose we decide that in the new version, we will upgrade to
    the 21st century and will no longer include a fax number field and will instead
    use an email field.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个模式下使用了几个月，并以这种格式生成了几TB的数据。现在假设我们决定在新版本中，我们将升级到21世纪，不再包含传真号码字段，而是使用电子邮件字段。
- en: 'The new schema would be:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 新模式将是：
- en: '[PRE7]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, after upgrading to the new version, old records will contain `faxNumber`
    and new records will contain `email`. In many organizations, upgrades are done
    slowly and over many months. So we need to consider how pre-upgrade applications
    that still use the fax numbers and post-upgrade applications that use email will
    be able to handle all the events in Kafka.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在升级到新版本后，旧记录将包含`faxNumber`，新记录将包含`email`。在许多组织中，升级是缓慢进行的，需要花费很多个月的时间。因此，我们需要考虑如何处理仍然使用传真号码的升级前应用程序和使用电子邮件的升级后应用程序在Kafka中的所有事件。
- en: The reading application will contain calls to methods similar to `getName()`,
    `getId()`, and `getFaxNumber()`. If it encounters a message written with the new
    schema, `getName()` and `getId()` will continue working with no modification,
    but `getFax` `Number()` will return `null` because the message will not contain
    a fax number.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 读取应用程序将包含类似于`getName()`、`getId()`和`getFaxNumber()`的方法调用。如果遇到使用新模式编写的消息，`getName()`和`getId()`将继续工作而无需修改，但`getFaxNumber()`将返回`null`，因为消息不包含传真号码。
- en: Now suppose we upgrade our reading application and it no longer has the `getFax`
    `Number()` method but rather `getEmail()`. If it encounters a message written
    with the old schema, `getEmail()` will return `null` because the older messages
    do not contain an email address.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们升级了我们的读取应用程序，它不再具有`getFaxNumber()`方法，而是`getEmail()`。如果遇到使用旧模式编写的消息，`getEmail()`将返回`null`，因为旧消息不包含电子邮件地址。
- en: 'This example illustrates the benefit of using Avro: even though we changed
    the schema in the messages without changing all the applications reading the data,
    there will be no exceptions or breaking errors and no need for expensive updates
    of existing data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了使用Avro的好处：即使我们在消息中改变了模式，而不改变所有读取数据的应用程序，也不会出现异常或破坏错误，也不需要昂贵的现有数据更新。
- en: 'However, there are two caveats to this scenario:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种情况有两个注意事项：
- en: The schema used for writing the data and the schema expected by the reading
    application must be compatible. The Avro documentation includes [compatibility
    rules](http://bit.ly/2t9FmEb).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入数据使用的模式和读取应用程序期望的模式必须是兼容的。Avro文档包括[兼容性规则](http://bit.ly/2t9FmEb)。
- en: The deserializer will need access to the schema that was used when writing the
    data, even when it is different from the schema expected by the application that
    accesses the data. In Avro files, the writing schema is included in the file itself,
    but there is a better way to handle this for Kafka messages. We will look at that
    next.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反序列化器将需要访问写入数据时使用的模式，即使它与应用程序期望的模式不同。在Avro文件中，写入模式包含在文件本身中，但对于Kafka消息，有一种更好的处理方式。我们将在下面看到这一点。
- en: Using Avro Records with Kafka
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Avro记录与Kafka
- en: Unlike Avro files, where storing the entire schema in the data file is associated
    with a fairly reasonable overhead, storing the entire schema in each record will
    usually more than double the record size. However, Avro still requires the entire
    schema to be present when reading the record, so we need to locate the schema
    elsewhere. To achieve this, we follow a common architecture pattern and use a
    *Schema Registry*. The Schema Registry is not part of Apache Kafka, but there
    are several open source options to choose from. We’ll use the Confluent Schema
    Registry for this example. You can find the Schema Registry code on [GitHub](https://oreil.ly/htoZK),
    or you can install it as part of the [Confluent Platform](https://oreil.ly/n2V71).
    If you decide to use the Schema Registry, we recommend checking [the documentation
    on Confluent](https://oreil.ly/yFkTX).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Avro 文件不同，将整个模式存储在数据文件中与相当合理的开销相关联，将整个模式存储在每个记录中通常会使记录大小增加一倍以上。但是，Avro 仍然要求在读取记录时整个模式都存在，因此我们需要在其他地方定位模式。为了实现这一点，我们遵循一个常见的架构模式，并使用
    *模式注册表*。模式注册表不是 Apache Kafka 的一部分，但有几个开源选项可供选择。我们将在此示例中使用 Confluent Schema Registry。您可以在
    [GitHub](https://oreil.ly/htoZK) 上找到模式注册表代码，或者您可以将其作为 [Confluent Platform](https://oreil.ly/n2V71)
    的一部分安装。如果决定使用模式注册表，我们建议查看 [Confluent 上的文档](https://oreil.ly/yFkTX)。
- en: The idea is to store all the schemas used to write data to Kafka in the registry.
    Then we simply store the identifier for the schema in the record we produce to
    Kafka. The consumers can then use the identifier to pull the record out of the
    Schema Registry and deserialize the data. The key is that all this work—storing
    the schema in the registry and pulling it up when required—is done in the serializers
    and deserializers. The code that produces data to Kafka simply uses the Avro serializer
    just like it would any other serializer. [Figure 3-3](#fig-3-serializer) demonstrates
    this process.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有用于将数据写入 Kafka 的模式存储在注册表中。然后我们只需在我们产生到 Kafka 的记录中存储模式的标识符。消费者随后可以使用标识符从模式注册表中提取记录并反序列化数据。关键在于所有这些工作——将模式存储在注册表中并在需要时提取模式——都是在序列化器和反序列化器中完成的。将数据生成到
    Kafka 的代码就像使用任何其他序列化器一样使用 Avro 序列化器。[图 3-3](#fig-3-serializer) 展示了这个过程。
- en: '![kdg2 0303](assets/kdg2_0303.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0303](assets/kdg2_0303.png)'
- en: Figure 3-3\. Flow diagram of serialization and deserialization of Avro records
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. Avro 记录的序列化和反序列化流程
- en: 'Here is an example of how to produce generated Avro objects to Kafka (see the
    [Avro documentation](https://oreil.ly/klcjK) for how to generate objects from
    Avro schemas):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何将生成的 Avro 对象发送到 Kafka 的示例（请参阅 [Avro 文档](https://oreil.ly/klcjK) 了解如何从 Avro
    模式生成对象）：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-1)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: We use the `KafkaAvroSerializer` to serialize our objects with Avro. Note that
    the `KafkaAvroSerializer` can also handle primitives, which is why we can later
    use `String` as the record key and our `Customer` object as the value.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `KafkaAvroSerializer` 来使用 Avro 序列化我们的对象。请注意，`KafkaAvroSerializer` 也可以处理原始类型，这就是为什么我们后来可以使用
    `String` 作为记录键，而我们的 `Customer` 对象作为值。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-2)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)'
- en: '`schema.registry.url` is the configuration of the Avro serializer that will
    be passed to the serializer by the producer. It simply points to where we store
    the schemas.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`schema.registry.url` 是 Avro 序列化器的配置，将被生产者传递给序列化器。它简单地指向我们存储模式的位置。'
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-3)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)'
- en: '`Customer` is our generated object. We tell the producer that our records will
    contain `Customer` as the value.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`Customer` 是我们生成的对象。我们告诉生产者我们的记录将包含 `Customer` 作为值。'
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-4)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)'
- en: '`Customer` class is not a regular Java class (plain old Java object, or POJO)
    but rather a specialized Avro object, generated from a schema using Avro code
    generation. The Avro serializer can only serialize Avro objects, not POJO. Generating
    Avro classes can be done either using the *avro-tools.jar* or the Avro Maven plug-in,
    both part of Apache Avro. See the [Apache Avro Getting Started (Java) guide](https://oreil.ly/sHGEe)
    for details on how to generate Avro classes.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`Customer` 类不是常规的 Java 类（普通的旧的 Java 对象，或 POJO），而是一个专门的 Avro 对象，使用 Avro 代码生成从模式生成。Avro
    序列化器只能序列化 Avro 对象，而不是 POJO。生成 Avro 类可以使用 *avro-tools.jar* 或 Avro Maven 插件来完成，这两者都是
    Apache Avro 的一部分。有关如何生成 Avro 类的详细信息，请参阅 [Apache Avro 入门（Java）指南](https://oreil.ly/sHGEe)。'
- en: '[![5](assets/5.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-5)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)'
- en: We also instantiate `ProducerRecord` with `Customer` as the value type, and
    pass a `Customer` object when creating the new record.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 `Customer` 作为值类型来实例化 `ProducerRecord`，并在创建新记录时传递一个 `Customer` 对象。
- en: '[![6](assets/6.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO6-6)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)'
- en: That’s it. We send the record with our `Customer` object, and `KafkaAvro​Serial⁠izer`
    will handle the rest.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们发送包含我们的 `Customer` 对象的记录，`KafkaAvro​Serial⁠izer` 将处理其余部分。
- en: 'Avro also allows you to use generic Avro objects, that are used as key-value
    maps, rather than generated Avro objects with getters and setters that match the
    schema that was used to generate them. To use generic Avro objects, you just need
    to provide the schema:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 还允许您使用通用 Avro 对象，这些对象用作键值映射，而不是具有与用于生成它们的模式匹配的getter和setter的生成的 Avro 对象。要使用通用
    Avro 对象，您只需要提供模式：
- en: '[PRE9]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-1)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: We still use the same `KafkaAvroSerializer`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然使用相同的`KafkaAvroSerializer`。
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-2)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-2)'
- en: And we provide the URI of the same Schema Registry.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供相同模式注册表的URI。
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-3)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-3)'
- en: But now we also need to provide the Avro schema, since it is not provided by
    an Avro-generated object.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们还需要提供Avro模式，因为它不是由Avro生成的对象提供的。
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-4)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-4)'
- en: Our object type is an Avro `GenericRecord`, which we initialize with our schema
    and the data we want to write.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的对象类型是Avro `GenericRecord`，我们使用我们的模式和我们想要写入的数据初始化它。
- en: '[![5](assets/5.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-5)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO7-5)'
- en: Then the value of the `ProducerRecord` is simply a `GenericRecord` that contains
    our schema and data. The serializer will know how to get the schema from this
    record, store it in the Schema Registry, and serialize the object data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`ProducerRecord`的值只是包含我们的模式和数据的`GenericRecord`。序列化程序将知道如何从此记录中获取模式，将其存储在模式注册表中，并对对象数据进行序列化。
- en: Partitions
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区
- en: 'In previous examples, the `ProducerRecord` objects we created included a topic
    name, key, and value. Kafka messages are key-value pairs, and while it is possible
    to create a `ProducerRecord` with just a topic and a value, with the key set to
    `null` by default, most applications produce records with keys. Keys serve two
    goals: they are additional information that gets stored with the message, and
    they are typically also used to decide which one of the topic partitions the message
    will be written to (keys also play an important role in compacted topics—we’ll
    discuss those in [Chapter 6](ch06.html#kafka_internals)). All messages with the
    same key will go to the same partition. This means that if a process is reading
    only a subset of the partitions in a topic (more on that in [Chapter 4](ch04.html#reading_data_from_kafka)),
    all the records for a single key will be read by the same process. To create a
    key-value record, you simply create a `ProducerRecord` as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的示例中，我们创建的`ProducerRecord`对象包括主题名称、键和值。Kafka消息是键值对，虽然可以只使用主题和值创建`ProducerRecord`，并且默认情况下将键设置为`null`，但大多数应用程序都会生成带有键的记录。键有两个目标：它们是存储在消息中的附加信息，通常也用于决定消息将写入哪个主题分区（键在压缩主题中也起着重要作用，我们将在[第6章](ch06.html#kafka_internals)中讨论这些内容）。具有相同键的所有消息将进入同一分区。这意味着如果进程只读取主题中的一部分分区（有关详细信息，请参阅[第4章](ch04.html#reading_data_from_kafka)），则单个键的所有记录将由同一进程读取。要创建键值记录，只需创建`ProducerRecord`如下所示：
- en: '[PRE10]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When creating messages with a null key, you can simply leave the key out:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建具有空键的消息时，可以简单地将键省略：
- en: '[PRE11]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO8-1)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO8-1)'
- en: Here, the key will simply be set to `null`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，键将简单地设置为`null`。
- en: When the key is `null` and the default partitioner is used, the record will
    be sent to one of the available partitions of the topic at random. A round-robin
    algorithm will be used to balance the messages among the partitions. Starting
    in the Apache Kafka 2.4 producer, the round-robin algorithm used in the default
    partitioner when handling null keys is sticky. This means that it will fill a
    batch of messages sent to a single partition before switching to the next partition.
    This allows sending the same number of messages to Kafka in fewer requests, leading
    to lower latency and reduced CPU utilization on the broker.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当键为`null`且使用默认分区器时，记录将随机发送到主题的可用分区之一。循环算法将用于在分区之间平衡消息。从Apache Kafka 2.4生产者开始，默认分区器在处理空键时使用的循环算法是粘性的。这意味着它将在切换到下一个分区之前填充发送到单个分区的一批消息。这允许以更少的请求将相同数量的消息发送到Kafka，从而降低延迟并减少代理上的CPU利用率。
- en: If a key exists and the default partitioner is used, Kafka will hash the key
    (using its own hash algorithm, so hash values will not change when Java is upgraded)
    and use the result to map the message to a specific partition. Since it is important
    that a key is always mapped to the same partition, we use all the partitions in
    the topic to calculate the mapping—not just the available partitions. This means
    that if a specific partition is unavailable when you write data to it, you might
    get an error. This is fairly rare, as you will see in [Chapter 7](ch07.html#reliable_data_delivery)
    when we discuss Kafka’s replication and availability.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在键并且使用默认分区器，则Kafka将对键进行哈希处理（使用自己的哈希算法，因此当Java升级时哈希值不会更改），并使用结果将消息映射到特定分区。由于关键始终映射到同一分区很重要，因此我们使用主题中的所有分区来计算映射，而不仅仅是可用分区。这意味着如果在写入数据时特定分区不可用，则可能会出现错误。这是相当罕见的，正如您将在[第7章](ch07.html#reliable_data_delivery)中看到的，当我们讨论Kafka的复制和可用性时。
- en: In addition to the default partitioner, Apache Kafka clients also provide `RoundRobinPartitioner`
    and `UniformStickyPartitioner`. These provide random partition assignment and
    sticky random partition assignment even when messages have keys. These are useful
    when keys are important for the consuming application (for example, there are
    ETL applications that use the key from Kafka records as the primary key when loading
    data from Kafka to a relational database), but the workload may be skewed, so
    a single key may have a disproportionately large workload. Using the `UniformStickyPartitioner`
    will result in an even distribution of workload across all partitions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: When the default partitioner is used, the mapping of keys to partitions is consistent
    only as long as the number of partitions in a topic does not change. So as long
    as the number of partitions is constant, you can be sure that, for example, records
    regarding user 045189 will always get written to partition 34\. This allows all
    kinds of optimization when reading data from partitions. However, the moment you
    add new partitions to the topic, this is no longer guaranteed—the old records
    will stay in partition 34 while new records may get written to a different partition.
    When partitioning keys is important, the easiest solution is to create topics
    with sufficient partitions (the Confluent blog contains suggestions on how to
    [choose the number of partitions](https://oreil.ly/ortRk)) and never add partitions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a custom partitioning strategy
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have discussed the traits of the default partitioner, which is the
    one most commonly used. However, Kafka does not limit you to just hash partitions,
    and sometimes there are good reasons to partition data differently. For example,
    suppose that you are a B2B vendor and your biggest customer is a company that
    manufactures handheld devices called Bananas. Suppose that you do so much business
    with customer “Banana” that over 10% of your daily transactions are with this
    customer. If you use default hash partitioning, the Banana records will get allocated
    to the same partition as other accounts, resulting in one partition being much
    larger than the rest. This can cause servers to run out of space, processing to
    slow down, etc. What we really want is to give Banana its own partition and then
    use hash partitioning to map the rest of the accounts to all other partitions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a custom partitioner:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO9-1)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Partitioner interface includes `configure`, `partition`, and `close` methods.
    Here we only implement `partition`, although we really should have passed the
    special customer name through `configure` instead of hardcoding it in `partition`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO9-2)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: We only expect `String` keys, so we throw an exception if that is not the case.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Headers
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Records can, in addition to key and value, also include headers. Record headers
    give you the ability to add some metadata about the Kafka record, without adding
    any extra information to the key/value pair of the record itself. Headers are
    often used for lineage to indicate the source of the data in the record, and for
    routing or tracing messages based on header information without having to parse
    the message itself (perhaps the message is encrypted and the router doesn’t have
    permissions to access the data).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Headers are implemented as an ordered collection of key/value pairs. The keys
    are always a `String`, and the values can be any serialized object—just like the
    message value.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a small example that shows how to add headers to a `ProduceRecord`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Interceptors
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when you want to modify the behavior of your Kafka client application
    without modifying its code, perhaps because you want to add identical behavior
    to all applications in the organization. Or perhaps you don’t have access to the
    original code.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka’s `ProducerInterceptor` interceptor includes two key methods:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '`ProducerRecord<K, V> onSend(ProducerRecord<K, V> record)`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: This method will be called before the produced record is sent to Kafka, indeed
    before it is even serialized. When overriding this method, you can capture information
    about the sent record and even modify it. Just be sure to return a valid `ProducerRecord`
    from this method. The record that this method returns will be serialized and sent
    to Kafka.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '`void onAcknowledgement(RecordMetadata metadata, Exception exception)`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: This method will be called if and when Kafka responds with an acknowledgment
    for a send. The method does not allow modifying the response from Kafka, but you
    can capture information about the response.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Common use cases for producer interceptors include capturing monitoring and
    tracing information; enhancing the message with standard headers, especially for
    lineage tracking purposes; and redacting sensitive information.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a very simple producer interceptor. This one simply counts
    the messages sent and acks received within specific time windows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-1)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '`ProducerInterceptor` is a `Configurable` interface. You can override the `configure`
    method and setup before any other method is called. This method receives the entire
    producer configuration, and you can access any configuration parameter. In this
    case, we added a configuration of our own that we reference here.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-2)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: When a record is sent, we increment the record count and return the record without
    modifying it.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-3)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: When Kafka responds with an ack, we increment the acknowledgment count and don’t
    need to return anything.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO10-4)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: This method is called when the producer closes, giving us a chance to clean
    up the interceptor state. In this case, we close the thread we created. If you
    opened file handles, connections to remote data stores, or similar, this is the
    place to close everything and avoid leaks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, producer interceptors can be applied without any changes
    to the client code. To use the preceding interceptor with `kafka-console-producer`,
    an example application that ships with Apache Kafka, follow these three simple
    steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Add your jar to the classpath:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`export CLASSPATH=$CLASSPATH:~./target/CountProducerInterceptor-1.0-SNAPSHOT.jar`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a config file that includes:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`interceptor.classes=com.shapira.examples.interceptors.CountProducerInterceptor`
    `counting.interceptor.window.size.ms=10000`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the application as you normally would, but make sure to include the configuration
    that you created in the previous step:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`bin/kafka-console-producer.sh --broker-list localhost:9092 --topic interceptor-test
    --producer.config producer.config`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Quotas and Throttling
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka brokers have the ability to limit the rate at which messages are produced
    and consumed. This is done via the quota mechanism. Kafka has three quota types:
    produce, consume, and request. Produce and consume quotas limit the rate at which
    clients can send and receive data, measured in bytes per second. Request quotas
    limit the percentage of time the broker spends processing client requests.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Quotas can be applied to all clients by setting default quotas, specific client-ids,
    specific users, or both. User-specific quotas are only meaningful in clusters
    where security is configured and clients authenticate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'The default produce and consume quotas that are applied to all clients are
    part of the Kafka broker configuration file. For example, to limit each producer
    to send no more than 2 MBps on average, add the following configuration to the
    broker configuration file: `quota.producer.default=2M`.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'While not recommended, you can also configure specific quotas for certain clients
    that override the default quotas in the broker configuration file. To allow clientA
    to produce 4 MBps and clientB 10 MBps, you can use the following: `quota.​pro⁠ducer.override="clientA:4M,clientB:10M"`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Quotas that are specified in Kafka’s configuration file are static, and you
    can only modify them by changing the configuration and then restarting all the
    brokers. Since new clients can arrive at any time, this is very inconvenient.
    Therefore the usual method of applying quotas to specific clients is through dynamic
    configuration that can be set using `kafka-config.sh` or the AdminClient API.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at few examples:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-1)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Limiting clientC (identified by client-id) to produce only 1024 bytes per second
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-2)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Limiting user1 (identified by authenticated principal) to produce only 1024
    bytes per second and consume only 2048 bytes per second.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_producers__writing__span_class__keep_together__messages_to_kafka__span__CO11-3)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Limiting all users to consume only 2048 bytes per second, except users with
    more specific override. This is the way to dynamically modify the default quota.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: When a client reaches its quota, the broker will start throttling the client’s
    requests to prevent it from exceeding the quota. This means that the broker will
    delay responses to client requests; in most clients this will automatically reduce
    the request rate (since the number of in-flight requests is limited) and bring
    the client traffic down to a level allowed by the quota. To protect the broker
    from misbehaved clients sending additional requests while being throttled, the
    broker will also mute the communication channel with the client for the period
    of time needed to achieve compliance with the quota.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The throttling behavior is exposed to clients via `produce-throttle-time-avg`,
    `produce-throttle-time-max`, `fetch-throttle-time-avg`, and `fetch-throttle-time-max`,
    the average and the maximum amount of time a produce request and fetch request
    was delayed due to throttling. Note that this time can represent throttling due
    to produce and consume throughput quotas, request time quotas, or both. Other
    types of client requests can only be throttled due to request time quotas, and
    those will also be exposed via similar metrics.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you use async `Producer.send()` and continue to send messages at a rate that
    is higher than the rate the broker can accept (whether due to quotas or just plain
    old capacity), the messages will first be queued in the client memory. If the
    rate of sending continues to be higher than the rate of accepting messages, the
    client will eventually run out of buffer space for storing the excess messages
    and will block the next `Producer.send()` call. If the timeout delay is insufficient
    to let the broker catch up to the producer and clear some space in the buffer,
    eventually `Producer.send()` will throw `TimeoutException`. Alternatively, some
    of the records that were already placed in batches will wait for longer than `delivery.timeout.ms`
    and expire, resulting in calling the `send()` callback with a `TimeoutException`.
    It is therefore important to plan and monitor to make sure that the broker capacity
    over time will match the rate at which producers are sending data.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用异步`Producer.send()`并继续以高于代理可以接受的速率发送消息（无论是因为配额还是容量不足），消息将首先排队在客户端内存中。如果发送速率继续高于接受消息的速率，客户端最终将耗尽用于存储多余消息的缓冲空间，并阻塞下一个`Producer.send()`调用。如果超时延迟不足以让代理赶上生产者并在缓冲区中清理一些空间，最终`Producer.send()`将抛出`TimeoutException`。或者，一些已经放入批处理中的记录将等待的时间超过`delivery.timeout.ms`并过期，导致使用`TimeoutException`调用`send()`回调。因此，重要的是要计划和监视，以确保代理的容量随时间匹配生产者发送数据的速率。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We began this chapter with a simple example of a producer—just 10 lines of code
    that send events to Kafka. We added to the simple example by adding error handling
    and experimenting with synchronous and asynchronous producing. We then explored
    the most important producer configuration parameters and saw how they modify the
    behavior of the producers. We discussed serializers, which let us control the
    format of the events we write to Kafka. We looked in-depth at Avro, one of many
    ways to serialize events but one that is very commonly used with Kafka. We concluded
    the chapter with a discussion of partitioning in Kafka and an example of an advanced
    custom partitioning technique.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的生产者示例开始，只有10行代码将事件发送到Kafka。我们通过添加错误处理和尝试同步和异步生产来扩展了简单示例。然后，我们探讨了最重要的生产者配置参数，并看到它们如何修改生产者的行为。我们讨论了序列化器，它让我们控制写入Kafka的事件的格式。我们深入研究了Avro，这是序列化事件的许多方式之一，但在Kafka中非常常用。我们在本章中讨论了Kafka中的分区以及高级自定义分区技术的示例。
- en: Now that we know how to write events to Kafka, in [Chapter 4](ch04.html#reading_data_from_kafka)
    we’ll learn all about consuming events from Kafka.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何将事件写入Kafka，在[第4章](ch04.html#reading_data_from_kafka)中，我们将学习有关从Kafka消费事件的所有内容。
- en: ^([1](ch03.html#idm45351110098016-marker)) Image contributed to the Apache Kafka
    project by Sumant Tambe under the ASLv2 license terms.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm45351110098016-marker)) 图像由Sumant Tambe根据ASLv2许可条款为Apache
    Kafka项目做出贡献。

- en: 'Chapter 4\. Kafka Consumers: Reading Data from Kafka'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications that need to read data from Kafka use a `KafkaConsumer` to subscribe
    to Kafka topics and receive messages from these topics. Reading data from Kafka
    is a bit different than reading data from other messaging systems, and there are
    a few unique concepts and ideas involved. It can be difficult to understand how
    to use the Consumer API without understanding these concepts first. We’ll start
    by explaining some of the important concepts, and then we’ll go through some examples
    that show the different ways Consumer APIs can be used to implement applications
    with varying requirements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Consumer Concepts
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how to read data from Kafka, you first need to understand its
    consumers and consumer groups. The following sections cover those concepts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Consumers and Consumer Groups
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you have an application that needs to read messages from a Kafka topic,
    run some validations against them, and write the results to another data store.
    In this case, your application will create a consumer object, subscribe to the
    appropriate topic, and start receiving messages, validating them, and writing
    the results. This may work well for a while, but what if the rate at which producers
    write messages to the topic exceeds the rate at which your application can validate
    them? If you are limited to a single consumer reading and processing the data,
    your application may fall further and further behind, unable to keep up with the
    rate of incoming messages. Obviously there is a need to scale consumption from
    topics. Just like multiple producers can write to the same topic, we need to allow
    multiple consumers to read from the same topic, splitting the data among them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Kafka consumers are typically part of a *consumer group*. When multiple consumers
    are subscribed to a topic and belong to the same consumer group, each consumer
    in the group will receive messages from a different subset of the partitions in
    the topic.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take topic T1 with four partitions. Now suppose we created a new consumer,
    C1, which is the only consumer in group G1, and use it to subscribe to topic T1\.
    Consumer C1 will get all messages from all four T1 partitions. See [Figure 4-1](#T1_four_partitions).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0401](assets/kdg2_0401.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. One consumer group with four partitions
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we add another consumer, C2, to group G1, each consumer will only get messages
    from two partitions. Perhaps messages from partition 0 and 2 go to C1, and messages
    from partitions 1 and 3 go to consumer C2\. See [Figure 4-2](#T1_two_groups).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0402](assets/kdg2_0402.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Four partitions split to two consumers in a group
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If G1 has four consumers, then each will read messages from a single partition.
    See [Figure 4-3](#T1_four_partition_group).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0403](assets/kdg2_0403.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Four consumers in a group with one partition each
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we add more consumers to a single group with a single topic than we have
    partitions, some of the consumers will be idle and get no messages at all. See
    [Figure 4-4](#T1_overflow_nomessage).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0404](assets/kdg2_0404.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. More consumers in a group than partitions means idle consumers
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The main way we scale data consumption from a Kafka topic is by adding more
    consumers to a consumer group. It is common for Kafka consumers to do high-latency
    operations such as write to a database or a time-consuming computation on the
    data. In these cases, a single consumer can’t possibly keep up with the rate data
    flows into a topic, and adding more consumers that share the load by having each
    consumer own just a subset of the partitions and messages is our main method of
    scaling. This is a good reason to create topics with a large number of partitions—it
    allows adding more consumers when the load increases. Keep in mind that there
    is no point in adding more consumers than you have partitions in a topic—some
    of the consumers will just be idle. [Chapter 2](ch02.html#installing_kafka) includes
    some suggestions on how to choose the number of partitions in a topic.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In addition to adding consumers in order to scale a single application, it is
    very common to have multiple applications that need to read data from the same
    topic. In fact, one of the main design goals in Kafka was to make the data produced
    to Kafka topics available for many use cases throughout the organization. In those
    cases, we want each application to get all of the messages, rather than just a
    subset. To make sure an application gets all the messages in a topic, ensure the
    application has its own consumer group. Unlike many traditional messaging systems,
    Kafka scales to a large number of consumers and consumer groups without reducing
    performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, if we add a new consumer group (G2) with a single consumer,
    this consumer will get all the messages in topic T1 independent of what G1 is
    doing. G2 can have more than a single consumer, in which case they will each get
    a subset of partitions, just like we showed for G1, but G2 as a whole will still
    get all the messages regardless of other consumer groups. See [Figure 4-5](#receive_all_messages).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0405](assets/kdg2_0405.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Adding a new consumer group, both groups receive all messages
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To summarize, you create a new consumer group for each application that needs
    all the messages from one or more topics. You add consumers to an existing consumer
    group to scale the reading and processing of messages from the topics, so each
    additional consumer in a group will only get a subset of the messages.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Consumer Groups and Partition Rebalance
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the previous section, consumers in a consumer group share ownership
    of the partitions in the topics they subscribe to. When we add a new consumer
    to the group, it starts consuming messages from partitions previously consumed
    by another consumer. The same thing happens when a consumer shuts down or crashes;
    it leaves the group, and the partitions it used to consume will be consumed by
    one of the remaining consumers. Reassignment of partitions to consumers also happens
    when the topics the consumer group is consuming are modified (e.g., if an administrator
    adds new partitions).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Moving partition ownership from one consumer to another is called a *rebalance*.
    Rebalances are important because they provide the consumer group with high availability
    and scalability (allowing us to easily and safely add and remove consumers), but
    in the normal course of events they can be fairly undesirable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of rebalances, depending on the partition assignment strategy
    that the consumer group uses:^([1](ch04.html#idm45351109673920))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Eager rebalances
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'During an eager rebalance, all consumers stop consuming, give up their ownership
    of all partitions, rejoin the consumer group, and get a brand-new partition assignment.
    This is essentially a short window of unavailability of the entire consumer group.
    The length of the window depends on the size of the consumer group as well as
    on several configuration parameters. [Figure 4-6](#fig-6-eager-rebalance) shows
    how eager rebalances have two distinct phases: first, all consumers give up their
    partition assigning, and second, after they all complete this and rejoin the group,
    they get new partition assignments and can resume consuming.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在急切重新平衡期间，所有消费者停止消费，放弃它们对所有分区的所有权，重新加入消费者组，并获得全新的分区分配。这实质上是整个消费者组的短暂不可用窗口。这个窗口的长度取决于消费者组的大小以及几个配置参数。[图4-6](#fig-6-eager-rebalance)显示了急切重新平衡有两个明显的阶段：首先，所有消费者放弃它们的分区分配，然后，在它们都完成这一步并重新加入组后，它们获得新的分区分配并可以继续消费。
- en: '![kdg2 0406](assets/kdg2_0406.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0406](assets/kdg2_0406.png)'
- en: Figure 4-6\. Eager rebalance revokes all partitions, pauses consumption, and
    reassigns them
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 急切重新平衡会撤销所有分区，暂停消费，并重新分配它们
- en: Cooperative rebalances
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 合作重新平衡
- en: Cooperative rebalances (also called *incremental rebalances*) typically involve
    reassigning only a small subset of the partitions from one consumer to another,
    and allowing consumers to continue processing records from all the partitions
    that are not reassigned. This is achieved by rebalancing in two or more phases.
    Initially, the consumer group leader informs all the consumers that they will
    lose ownership of a subset of their partitions, then the consumers stop consuming
    from these partitions and give up their ownership in them. In the second phase,
    the consumer group leader assigns these now orphaned partitions to their new owners.
    This incremental approach may take a few iterations until a stable partition assignment
    is achieved, but it avoids the complete “stop the world” unavailability that occurs
    with the eager approach. This is especially important in large consumer groups
    where rebalances can take a significant amount of time. [Figure 4-7](#fig-7-cooperative-rebalance)
    shows how cooperative rebalances are incremental and that only a subset of the
    consumers and partitions are involved.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 合作重新平衡（也称为*增量重新平衡*）通常涉及将一小部分分区从一个消费者重新分配给另一个消费者，并允许消费者继续处理未被重新分配的所有分区的记录。这是通过分两个或更多阶段进行重新平衡来实现的。最初，消费者组领导者通知所有消费者它们将失去对一部分分区的所有权，然后消费者停止从这些分区消费并放弃它们的所有权。在第二阶段，消费者组领导者将这些现在被遗弃的分区分配给它们的新所有者。这种增量方法可能需要几次迭代，直到实现稳定的分区分配，但它避免了急切方法中发生的完全“停止世界”不可用性。这在大型消费者组中尤为重要，因为重新平衡可能需要大量时间。[图4-7](#fig-7-cooperative-rebalance)显示了合作重新平衡是增量的，只涉及部分消费者和分区。
- en: '![kdg2 0407](assets/kdg2_0407.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0407](assets/kdg2_0407.png)'
- en: Figure 4-7\. Cooperative rebalance only pauses consumption for the subset of
    partitions that will be reassigned
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. 合作重新平衡只暂停将被重新分配的分区的消费
- en: Consumers maintain membership in a consumer group and ownership of the partitions
    assigned to them by sending *heartbeats* to a Kafka broker designated as the *group
    coordinator* (this broker can be different for different consumer groups). The
    heartbeats are sent by a background thread of the consumer, and as long as the
    consumer is sending heartbeats at regular intervals, it is assumed to be alive.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者通过向被指定为*组协调者*的Kafka代理发送*心跳*来维护消费者组的成员资格和分配给它们的分区的所有权（对于不同的消费者组，这个代理可能是不同的）。消费者通过后台线程发送心跳，只要消费者以固定的间隔发送心跳，就假定它是活动的。
- en: If the consumer stops sending heartbeats for long enough, its session will timeout
    and the group coordinator will consider it dead and trigger a rebalance. If a
    consumer crashed and stopped processing messages, it will take the group coordinator
    a few seconds without heartbeats to decide it is dead and trigger the rebalance.
    During those seconds, no messages will be processed from the partitions owned
    by the dead consumer. When closing a consumer cleanly, the consumer will notify
    the group coordinator that it is leaving, and the group coordinator will trigger
    a rebalance immediately, reducing the gap in processing. Later in this chapter,
    we will discuss configuration options that control heartbeat frequency, session
    timeouts, and other configuration parameters that can be used to fine-tune the
    consumer behavior.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者停止发送心跳足够长的时间，它的会话将超时，组协调者将认为它已经死亡并触发重新平衡。如果消费者崩溃并停止处理消息，组协调者将需要几秒钟没有心跳来判断它已经死亡并触发重新平衡。在这几秒钟内，来自已死亡消费者所拥有的分区的消息将不会被处理。当消费者正常关闭时，消费者将通知组协调者它正在离开，组协调者将立即触发重新平衡，减少处理的间隙。在本章的后面，我们将讨论控制心跳频率、会话超时和其他配置参数的配置选项，这些配置选项可以用来微调消费者的行为。
- en: How Does the Process of Assigning Partitions to Consumers Work?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将分区分配给消费者的过程是如何工作的？
- en: When a consumer wants to join a group, it sends a `JoinGroup` request to the
    group coordinator. The first consumer to join the group becomes the group *leader*.
    The leader receives a list of all consumers in the group from the group coordinator
    (this will include all consumers that sent a heartbeat recently and that are therefore
    considered alive) and is responsible for assigning a subset of partitions to each
    consumer. It uses an implementation of `PartitionAssignor` to decide which partitions
    should be handled by which consumer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当消费者想要加入一个组时，它会向组协调者发送一个“JoinGroup”请求。第一个加入组的消费者成为组的*领导者*。领导者从组协调者那里接收到组中所有消费者的列表（这将包括最近发送心跳并因此被认为是活动的所有消费者），并负责将一部分分区分配给每个消费者。它使用`PartitionAssignor`的实现来决定哪些分区应该由哪些消费者处理。
- en: Kafka has few built-in partition assignment policies, which we will discuss
    in more depth in the configuration section. After deciding on the partition assignment,
    the consumer group leader sends the list of assignments to the `GroupCoordinator`,
    which sends this information to all the consumers. Each consumer only sees its
    own assignment—the leader is the only client process that has the full list of
    consumers in the group and their assignments. This process repeats every time
    a rebalance happens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka有一些内置的分区分配策略，我们将在配置部分更深入地讨论。在决定分区分配之后，消费者组领导者将分配的列表发送给`GroupCoordinator`，后者将此信息发送给所有消费者。每个消费者只能看到自己的分配
    - 领导者是唯一拥有完整消费者列表及其分配的客户端进程。每次发生重新平衡时，这个过程都会重复。
- en: Static Group Membership
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态组成员资格
- en: By default, the identity of a consumer as a member of its consumer group is
    transient. When consumers leave a consumer group, the partitions that were assigned
    to the consumer are revoked, and when it rejoins, it is assigned a new member
    ID and a new set of partitions through the rebalance protocol.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消费者作为其消费者组的成员的身份是瞬时的。当消费者离开消费者组时，分配给消费者的分区将被撤销，当它重新加入时，通过重新平衡协议，它将被分配一个新的成员ID和一组新的分区。
- en: All this is true unless you configure a consumer with a unique `group.instance.id`,
    which makes the consumer a *static* member of the group. When a consumer first
    joins a consumer group as a static member of the group, it is assigned a set of
    partitions according to the partition assignment strategy the group is using,
    as normal. However, when this consumer shuts down, it does not automatically leave
    the group—it remains a member of the group until its session times out. When the
    consumer rejoins the group, it is recognized with its static identity and is reassigned
    the same partitions it previously held without triggering a rebalance. The group
    coordinator that caches the assignment for each member of the group does not need
    to trigger a rebalance but can just send the cache assignment to the rejoining
    static member.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您配置具有唯一`group.instance.id`的消费者，否则所有这些都是真实的，这使得消费者成为组的*静态*成员。当消费者首次以组的静态成员身份加入消费者组时，它将根据组使用的分区分配策略被分配一组分区，就像正常情况下一样。然而，当此消费者关闭时，它不会自动离开组
    - 直到其会话超时之前，它仍然是组的成员。当消费者重新加入组时，它将以其静态身份被识别，并且重新分配之前持有的相同分区，而不会触发重新平衡。缓存组的协调者不需要触发重新平衡，而只需将缓存分配发送给重新加入的静态成员。
- en: If two consumers join the same group with the same `group.instance.id`, the
    second consumer will get an error saying that a consumer with this ID already
    exists.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个消费者以相同的`group.instance.id`加入同一组，第二个消费者将收到一个错误，指出已经存在具有此ID的消费者。
- en: Static group membership is useful when your application maintains local state
    or cache that is populated by the partitions that are assigned to each consumer.
    When re-creating this cache is time-consuming, you don’t want this process to
    happen every time a consumer restarts. On the flip side, it is important to remember
    that the partitions owned by each consumer will not get reassigned when a consumer
    is restarted. For a certain duration, no consumer will consume messages from these
    partitions, and when the consumer finally starts back up, it will lag behind the
    latest messages in these partitions. You should be confident that the consumer
    that owns these partitions will be able to catch up with the lag after the restart.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 静态组成员资格在您的应用程序维护由分配给每个消费者的分区填充的本地状态或缓存时非常有用。当重新创建此缓存需要耗费时间时，您不希望每次消费者重新启动时都发生这个过程。另一方面，重要的是要记住，当消费者重新启动时，每个消费者拥有的分区将不会重新分配。在一定的时间内，没有消费者会从这些分区中消费消息，当消费者最终重新启动时，它将落后于这些分区中的最新消息。您应该确信，拥有这些分区的消费者将能够在重新启动后赶上滞后。
- en: It is important to note that static members of consumer groups do not leave
    the group proactively when they shut down, and detecting when they are “really
    gone” depends on the `session.timeout.ms` configuration. You’ll want to set it
    high enough to avoid triggering rebalances on a simple application restart but
    low enough to allow automatic reassignment of their partitions when there is more
    significant downtime, to avoid large gaps in processing these partitions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，消费者组的静态成员在关闭时不会主动离开组，而是依赖于`session.timeout.ms`配置来检测它们何时“真正离开”。您需要将其设置得足够高，以避免在简单应用程序重新启动时触发重新平衡，但又要足够低，以允许在有更长时间停机时自动重新分配它们的分区，以避免处理这些分区时出现较大的间隙。
- en: Creating a Kafka Consumer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Kafka消费者
- en: 'The first step to start consuming records is to create a `KafkaConsumer` instance.
    Creating a `KafkaConsumer` is very similar to creating a `KafkaProducer`—you create
    a Java `Properties` instance with the properties you want to pass to the consumer.
    We will discuss all the properties in depth later in the chapter. To start, we
    just need to use the three mandatory properties: `bootstrap.servers`, `key.deserializer`,
    and `value.deserializer`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 开始消费记录的第一步是创建一个`KafkaConsumer`实例。创建`KafkaConsumer`与创建`KafkaProducer`非常相似 - 您需要创建一个带有要传递给消费者的属性的Java
    `Properties`实例。我们将在本章后面详细讨论所有属性。首先，我们只需要使用三个必需的属性：`bootstrap.servers`，`key.deserializer`和`value.deserializer`。
- en: The first property, `bootstrap.servers`, is the connection string to a Kafka
    cluster. It is used the exact same way as in `KafkaProducer` (refer to [Chapter 3](ch03.html#writing_messages_to_kafka)
    for details on how this is defined). The other two properties, `key.deserializer`
    and `value.​dese⁠rial⁠izer`, are similar to the `serializers` defined for the
    producer, but rather than specifying classes that turn Java objects to byte arrays,
    you need to specify classes that can take a byte array and turn it into a Java
    object.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: There is a fourth property, which is not strictly mandatory but very commonly
    used. The property is `group.id`, and it specifies the consumer group the `Kafka``Consumer`
    instance belongs to. While it is possible to create consumers that do not belong
    to any consumer group, this is uncommon, so for most of the chapter we will assume
    the consumer is part of a group.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to create a `KafkaConsumer`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Most of what you see here should be familiar if you’ve read [Chapter 3](ch03.html#writing_messages_to_kafka)
    on creating producers. We assume that the records we consume will have `String`
    objects as both the key and the value of the record. The only new property here
    is `group.id`, which is the name of the consumer group this consumer belongs to.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Subscribing to Topics
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we create a consumer, the next step is to subscribe to one or more topics.
    The `subscribe()` method takes a list of topics as a parameter, so it’s pretty
    simple to use:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO1-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we simply create a list with a single element: the topic name `customerCountries`.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to call `subscribe` with a regular expression. The expression
    can match multiple topic names, and if someone creates a new topic with a name
    that matches, a rebalance will happen almost immediately and the consumers will
    start consuming from the new topic. This is useful for applications that need
    to consume from multiple topics and can handle the different types of data the
    topics will contain. Subscribing to multiple topics using a regular expression
    is most commonly used in applications that replicate data between Kafka and another
    system or streams processing applications.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to subscribe to all test topics, we can call:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Warning
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your Kafka cluster has large number of partitions, perhaps 30,000 or more,
    you should be aware that the filtering of topics for the subscription is done
    on the client side. This means that when you subscribe to a subset of topics via
    a regular expression rather than via an explicit list, the consumer will request
    the list of all topics and their partitions from the broker in regular intervals.
    The client will then use this list to detect new topics that it should include
    in its subscription and subscribe to them. When the topic list is large and there
    are many consumers, the size of the list of topics and partitions is significant,
    and the regular expression subscription has significant overhead on the broker,
    client, and network. There are cases where the bandwidth used by the topic metadata
    is larger than the bandwidth used to send data. This also means that in order
    to subscribe with a regular expression, the client needs permissions to describe
    all topics in the cluster—that is, a full `describe` grant on the entire cluster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The Poll Loop
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the heart of the Consumer API is a simple loop for polling the server for
    more data. The main body of a consumer will look as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: This is indeed an infinite loop. Consumers are usually long-running applications
    that continuously poll Kafka for more data. We will show later in the chapter
    how to cleanly exit the loop and close the consumer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: This is the most important line in the chapter. The same way that sharks must
    keep moving or they die, consumers must keep polling Kafka or they will be considered
    dead and the partitions they are consuming will be handed to another consumer
    in the group to continue consuming. The parameter we pass to `poll()` is a timeout
    interval and controls how long `poll()` will block if data is not available in
    the consumer buffer. If this is set to 0 or if there are records available already,
    `poll()` will return immediately; otherwise, it will wait for the specified number
    of milliseconds.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章中最重要的一行。就像鲨鱼必须不断移动，否则它们就会死一样，消费者必须不断轮询Kafka，否则它们将被视为死亡，并且它们正在消费的分区将被交给组中的另一个消费者继续消费。我们传递给`poll()`的参数是超时间隔，控制如果消费者缓冲区中没有数据，`poll()`将阻塞多长时间。如果设置为0或者已经有记录可用，`poll()`将立即返回；否则，它将等待指定的毫秒数。
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-3)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-3)'
- en: '`poll()` returns a list of records. Each record contains the topic and partition
    the record came from, the offset of the record within the partition, and, of course,
    the key and the value of the record. Typically, we want to iterate over the list
    and process the records individually.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`poll()`返回一个记录列表。每个记录包含记录来自的主题和分区，记录在分区内的偏移量，当然还有记录的键和值。通常，我们希望遍历列表并逐个处理记录。'
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-4)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-4)'
- en: Processing usually ends in writing a result in a data store or updating a stored
    record. Here, the goal is to keep a running count of customers from each country,
    so we update a hash table and print the result as JSON. A more realistic example
    would store the updates result in a data store.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 处理通常以在数据存储中写入结果或更新存储的记录结束。在这里，目标是保持每个国家客户的运行计数，因此我们更新哈希表并将结果打印为JSON。一个更现实的例子会将更新的结果存储在数据存储中。
- en: The `poll` loop does a lot more than just get data. The first time you call
    `poll()` with a new consumer, it is responsible for finding the `GroupCoordinator`,
    joining the consumer group, and receiving a partition assignment. If a rebalance
    is triggered, it will be handled inside the poll loop as well, including related
    callbacks. This means that almost everything that can go wrong with a consumer
    or in the callbacks used in its listeners is likely to show up as an exception
    thrown by `poll()`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`poll`循环做的远不止获取数据。第一次使用新的消费者调用`poll()`时，它负责找到`GroupCoordinator`，加入消费者组，并接收分区分配。如果触发了重新平衡，它也将在`poll`循环中处理，包括相关的回调。这意味着几乎所有可能出错的消费者或其监听器中使用的回调都可能显示为`poll()`抛出的异常。'
- en: Keep in mind that if `poll()` is not invoked for longer than `max.poll.interval.ms`,
    the consumer will be considered dead and evicted from the consumer group, so avoid
    doing anything that can block for unpredictable intervals inside the poll loop.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果`poll()`的调用时间超过`max.poll.interval.ms`，则消费者将被视为死亡并从消费者组中驱逐，因此避免在`poll`循环内部阻塞不可预测的时间。
- en: Thread Safety
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程安全
- en: You can’t have multiple consumers that belong to the same group in one thread,
    and you can’t have multiple threads safely use the same consumer. One consumer
    per thread is the rule. To run multiple consumers in the same group in one application,
    you will need to run each in its own thread. It is useful to wrap the consumer
    logic in its own object and then use Java’s `ExecutorService` to start multiple
    threads, each with its own consumer. The Confluent blog has a [tutorial](https://oreil.ly/8YOVe)
    that shows how to do just that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能在一个线程中拥有属于同一组的多个消费者，也不能安全地让多个线程使用同一个消费者。每个线程一个消费者是规则。要在同一应用程序中运行同一组中的多个消费者，您需要在每个消费者中运行各自的线程。将消费者逻辑封装在自己的对象中，然后使用Java的`ExecutorService`启动多个线程，每个线程都有自己的消费者是很有用的。Confluent博客有一个[教程](https://oreil.ly/8YOVe)展示了如何做到这一点。
- en: Warning
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In older versions of Kafka, the full method signature was `poll(long)`; this
    signature is now deprecated and the new API is `poll(Duration)`. In addition to
    the change of argument type, the semantics of how the method blocks subtly changed.
    The original method, `poll(long)`, will block as long as it takes to get the needed
    metadata from Kafka, even if this is longer than the timeout duration. The new
    method, `poll(Duration)`, will adhere to the timeout restrictions and not wait
    for metadata. If you have existing consumer code that uses `poll(0)` as a method
    to force Kafka to get the metadata without consuming any records (a rather common
    hack), you can’t just change it to `poll(Duration.ofMillis(0))` and expect the
    same behavior. You’ll need to figure out a new way to achieve your goals. Often
    the solution is placing the logic in the `rebalanceListener.onPartitionAssignment()`
    method, which is guaranteed to get called after you have metadata for the assigned
    partitions but before records start arriving. Another solution was documented
    by Jesse Anderson in his blog post [“Kafka’s Got a Brand-New Poll”](https://oreil.ly/zN6ek).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧版本的Kafka中，完整的方法签名是`poll(long)`；这个签名现在已经被弃用，新的API是`poll(Duration)`。除了参数类型的更改，方法阻塞的语义也略有变化。原始方法`poll(long)`将阻塞，直到从Kafka获取所需的元数据，即使这比超时持续时间更长。新方法`poll(Duration)`将遵守超时限制，不会等待元数据。如果您有现有的消费者代码，使用`poll(0)`作为一种强制Kafka获取元数据而不消耗任何记录的方法（这是一种相当常见的黑客行为），您不能只是将其更改为`poll(Duration.ofMillis(0))`并期望相同的行为。您需要找出一种新的方法来实现您的目标。通常的解决方案是将逻辑放在`rebalanceListener.onPartitionAssignment()`方法中，在分配分区的元数据后但记录开始到达之前，这个方法保证会被调用。Jesse
    Anderson在他的博客文章[“Kafka’s Got a Brand-New Poll”](https://oreil.ly/zN6ek)中也记录了另一种解决方案。
- en: Another approach can be to have one consumer populate a queue of events and
    have multiple worker threads perform work from this queue. You can see an example
    of this pattern in a [blog post](https://oreil.ly/uMzj1) from Igor Buzatović.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是让一个消费者填充一个事件队列，并让多个工作线程从这个队列中执行工作。您可以在[Igor Buzatović的博客文章](https://oreil.ly/uMzj1)中看到这种模式的示例。
- en: Configuring Consumers
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置消费者
- en: So far we have focused on learning the Consumer API, but we’ve only looked at
    a few of the configuration properties—just the mandatory `bootstrap.servers`,
    `group.id`, `key.deserializer`, and `value.deserializer`. All of the consumer
    configuration is documented in the [Apache Kafka documentation](https://oreil.ly/Y00Gl).
    Most of the parameters have reasonable defaults and do not require modification,
    but some have implications on the performance and availability of the consumers.
    Let’s take a look at some of the more important properties.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经专注于学习消费者 API，但我们只看了一些配置属性——只是强制性的`bootstrap.servers`、`group.id`、`key.deserializer`和`value.deserializer`。所有的消费者配置都在[Apache
    Kafka文档](https://oreil.ly/Y00Gl)中有记录。大多数参数都有合理的默认值，不需要修改，但有些对消费者的性能和可用性有影响。让我们来看看一些更重要的属性。
- en: fetch.min.bytes
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fetch.min.bytes
- en: This property allows a consumer to specify the minimum amount of data that it
    wants to receive from the broker when fetching records, by default one byte. If
    a broker receives a request for records from a consumer but the new records amount
    to fewer bytes than `fetch.min.bytes`, the broker will wait until more messages
    are available before sending the records back to the consumer. This reduces the
    load on both the consumer and the broker, as they have to handle fewer back-and-forth
    messages in cases where the topics don’t have much new activity (or for lower-activity
    hours of the day). You will want to set this parameter higher than the default
    if the consumer is using too much CPU when there isn’t much data available, or
    reduce load on the brokers when you have a large number of consumers—although
    keep in mind that increasing this value can increase latency for low-throughput
    cases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性允许消费者指定从代理获取记录时要接收的最小数据量，默认为一个字节。如果代理收到来自消费者的记录请求，但新的记录量少于`fetch.min.bytes`，代理将等待更多消息可用后再将记录发送回消费者。这减少了在主题没有太多新活动（或者在一天中活动较少的时间）的情况下，消费者和代理处理来回消息的负载。如果消费者在没有太多数据可用时使用了太多
    CPU，或者在有大量消费者时减少了对代理的负载，您将希望将此参数设置得比默认值更高——尽管请记住，增加此值可能会增加低吞吐量情况下的延迟。
- en: fetch.max.wait.ms
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fetch.max.wait.ms
- en: By setting `fetch.min.bytes`, you tell Kafka to wait until it has enough data
    to send before responding to the consumer. `fetch.max.wait.ms` lets you control
    how long to wait. By default, Kafka will wait up to 500 ms. This results in up
    to 500 ms of extra latency in case there is not enough data flowing to the Kafka
    topic to satisfy the minimum amount of data to return. If you want to limit the
    potential latency (usually due to SLAs controlling the maximum latency of the
    application), you can set `fetch.max.wait.ms` to a lower value. If you set `fetch.max.wait.ms`
    to 100 ms and `fetch.min.bytes` to 1 MB, Kafka will receive a fetch request from
    the consumer and will respond with data either when it has 1 MB of data to return
    or after 100 ms, whichever happens first.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`fetch.min.bytes`，您告诉 Kafka 在响应消费者之前等待足够的数据。`fetch.max.wait.ms`允许您控制等待的时间。默认情况下，Kafka
    将等待最多500毫秒。如果 Kafka 主题中没有足够的数据流动以满足返回的最小数据量，这将导致最多500毫秒的额外延迟。如果要限制潜在的延迟（通常是由 SLA
    控制应用程序的最大延迟引起的），可以将`fetch.max.wait.ms`设置为较低的值。如果将`fetch.max.wait.ms`设置为100毫秒，并将`fetch.min.bytes`设置为1
    MB，Kafka 将从消费者接收获取请求，并在有1 MB数据要返回或者在100毫秒后，以先到者为准，响应数据。
- en: fetch.max.bytes
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fetch.max.bytes
- en: This property lets you specify the maximum bytes that Kafka will return whenever
    the consumer polls a broker (50 MB by default). It is used to limit the size of
    memory that the consumer will use to store data that was returned from the server,
    irrespective of how many partitions or messages were returned. Note that records
    are sent to the client in batches, and if the first record-batch that the broker
    has to send exceeds this size, the batch will be sent and the limit will be ignored.
    This guarantees that the consumer can continue making progress. It’s worth noting
    that there is a matching broker configuration that allows the Kafka administrator
    to limit the maximum fetch size as well. The broker configuration can be useful
    because requests for large amounts of data can result in large reads from disk
    and long sends over the network, which can cause contention and increase load
    on the broker.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性允许您指定 Kafka 在消费者轮询代理时将返回的最大字节数（默认为50 MB）。它用于限制消费者用于存储从服务器返回的数据的内存大小，而不管返回了多少个分区或消息。请注意，记录是以批量形式发送到客户端的，如果代理必须发送的第一个记录批次超过了这个大小，批次将被发送并且限制将被忽略。这保证了消费者可以继续取得进展。值得注意的是，还有一个匹配的代理配置，允许
    Kafka 管理员限制最大获取大小。代理配置可能很有用，因为对大量数据的请求可能导致从磁盘读取大量数据，并且在网络上传输时间较长，这可能会导致争用并增加代理的负载。
- en: max.poll.records
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.poll.records
- en: This property controls the maximum number of records that a single call to `poll()`
    will return. Use this to control the amount of data (but not the size of data)
    your application will need to process in one iteration of the poll loop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性控制了单次`poll()`调用将返回的最大记录数。使用它来控制应用程序在一次轮询循环中需要处理的数据量（但不是数据大小）。
- en: max.partition.fetch.bytes
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.partition.fetch.bytes
- en: This property controls the maximum number of bytes the server will return per
    partition (1 MB by default). When `KafkaConsumer.poll()` returns `ConsumerRecords`,
    the record object will use at most `max.partition.fetch.bytes` per partition assigned
    to the consumer. Note that controlling memory usage using this configuration can
    be quite complex, as you have no control over how many partitions will be included
    in the broker response. Therefore, we highly recommend using `fetch.max.bytes`
    instead, unless you have special reasons to try and process similar amounts of
    data from each partition.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性控制服务器每个分区返回的最大字节数（默认为 1 MB）。当 `KafkaConsumer.poll()` 返回 `ConsumerRecords`
    时，记录对象将使用分配给消费者的每个分区的最多 `max.partition.fetch.bytes`。请注意，使用这个配置来控制内存使用可能非常复杂，因为您无法控制经纪人响应中将包括多少分区。因此，我们强烈建议使用
    `fetch.max.bytes`，除非您有特殊原因要尝试从每个分区处理类似数量的数据。
- en: session.timeout.ms and heartbeat.interval.ms
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 会话超时时间和心跳间隔时间
- en: The amount of time a consumer can be out of contact with the brokers while still
    considered alive defaults to 10 seconds. If more than `session.timeout.ms` passes
    without the consumer sending a heartbeat to the group coordinator, it is considered
    dead and the group coordinator will trigger a rebalance of the consumer group
    to allocate partitions from the dead consumer to the other consumers in the group.
    This property is closely related to `heartbeat.interval.ms`, which controls how
    frequently the Kafka consumer will send a heartbeat to the group coordinator,
    whereas `ses⁠sion.timeout.ms` controls how long a consumer can go without sending
    a heartbeat. Therefore, those two properties are typically modified together—`heartbeat.​interval.ms`
    must be lower than `session.timeout.ms` and is usually set to one-third of the
    timeout value. So if `session.timeout.ms` is 3 seconds, `heartbeat.​inter⁠val.ms`
    should be 1 second. Setting `session.timeout.ms` lower than the default will allow
    consumer groups to detect and recover from failure sooner but may also cause unwanted
    rebalances. Setting `session.timeout.ms` higher will reduce the chance of accidental
    rebalance but also means it will take longer to detect a real failure.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者在与经纪人失去联系时被认为仍然存活的时间默认为 10 秒。如果消费者在没有向组协调器发送心跳的情况下经过了超过 `session.timeout.ms`
    的时间，那么它被认为已经死亡，组协调器将触发消费者组的重新平衡，将死亡消费者的分区分配给组中的其他消费者。这个属性与 `heartbeat.interval.ms`
    密切相关，它控制 Kafka 消费者向组协调器发送心跳的频率，而 `session.timeout.ms` 控制消费者可以多久不发送心跳。因此，这两个属性通常一起修改——`heartbeat.​interval.ms`
    必须低于 `session.timeout.ms`，通常设置为超时值的三分之一。因此，如果 `session.timeout.ms` 是 3 秒，`heartbeat.​inter⁠val.ms`
    应该是 1 秒。将 `session.timeout.ms` 设置得比默认值低将允许消费者组更快地检测和从故障中恢复，但也可能导致不必要的重新平衡。将 `session.timeout.ms`
    设置得更高将减少意外重新平衡的机会，但也意味着检测真正故障的时间会更长。
- en: max.poll.interval.ms
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大轮询间隔时间
- en: This property lets you set the length of time during which the consumer can
    go without polling before it is considered dead. As mentioned earlier, heartbeats
    and session timeouts are the main mechanism by which Kafka detects dead consumers
    and takes their partitions away. However, we also mentioned that heartbeats are
    sent by a background thread. There is a possibility that the main thread consuming
    from Kafka is deadlocked, but the background thread is still sending heartbeats.
    This means that records from partitions owned by this consumer are not being processed.
    The easiest way to know whether the consumer is still processing records is to
    check whether it is asking for more records. However, the intervals between requests
    for more records are difficult to predict and depend on the amount of available
    data, the type of processing done by the consumer, and sometimes on the latency
    of additional services. In applications that need to do time-consuming processing
    on each record that is returned, `max.poll.records` is used to limit the amount
    of data returned and therefore limit the duration before the application is available
    to `poll()` again. Even with `max.poll.records` defined, the interval between
    calls to `poll()` is difficult to predict, and `max.poll.interval.ms` is used
    as a fail-safe or backstop. It has to be an interval large enough that it will
    very rarely be reached by a healthy consumer but low enough to avoid significant
    impact from a hanging consumer. The default value is 5 minutes. When the timeout
    is hit, the background thread will send a “leave group” request to let the broker
    know that the consumer is dead and the group must rebalance, and then stop sending
    heartbeats.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性允许您设置消费者在轮询之前可以多久不进行轮询被认为已经死亡的时间。正如前面提到的，心跳和会话超时是 Kafka 检测死亡消费者并夺走它们分区的主要机制。然而，我们也提到了心跳是由后台线程发送的。主线程从
    Kafka 消费可能被死锁，但后台线程仍在发送心跳。这意味着由该消费者拥有的分区的记录没有被处理。了解消费者是否仍在处理记录的最简单方法是检查它是否正在请求更多记录。然而，请求更多记录之间的间隔很难预测，取决于可用数据的数量，消费者所做的处理类型，有时还取决于其他服务的延迟。在需要对返回的每条记录进行耗时处理的应用程序中，`max.poll.records`
    用于限制返回的数据量，从而限制应用程序在再次可用于 `poll()` 之前的持续时间。即使定义了 `max.poll.records`，`poll()` 调用之间的间隔也很难预测，`max.poll.interval.ms`
    用作故障安全或后备。它必须是足够大的间隔，以至于健康的消费者很少会达到，但又足够低，以避免悬挂消费者的重大影响。默认值为 5 分钟。当超时时，后台线程将发送“离开组”请求，让经纪人知道消费者已经死亡，组必须重新平衡，然后停止发送心跳。
- en: default.api.timeout.ms
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 默认 API 超时时间
- en: This is the timeout that will apply to (almost) all API calls made by the consumer
    when you don’t specify an explicit timeout while calling the API. The default
    is 1 minute, and since it is higher than the request timeout default, it will
    include a retry when needed. The notable exception to APIs that use this default
    is the `poll()` method that always requires an explicit timeout.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: request.timeout.ms
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the maximum amount of time the consumer will wait for a response from
    the broker. If the broker does not respond within this time, the client will assume
    the broker will not respond at all, close the connection, and attempt to reconnect.
    This configuration defaults to 30 seconds, and it is recommended not to lower
    it. It is important to leave the broker with enough time to process the request
    before giving up—there is little to gain by resending requests to an already overloaded
    broker, and the act of disconnecting and reconnecting adds even more overhead.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: auto.offset.reset
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This property controls the behavior of the consumer when it starts reading a
    partition for which it doesn’t have a committed offset, or if the committed offset
    it has is invalid (usually because the consumer was down for so long that the
    record with that offset was already aged out of the broker). The default is “latest,”
    which means that lacking a valid offset, the consumer will start reading from
    the newest records (records that were written after the consumer started running).
    The alternative is “earliest,” which means that lacking a valid offset, the consumer
    will read all the data in the partition, starting from the very beginning. Setting
    `auto.offset.reset` to `none` will cause an exception to be thrown when attempting
    to consume from an invalid offset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: enable.auto.commit
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This parameter controls whether the consumer will commit offsets automatically,
    and defaults to `true`. Set it to `false` if you prefer to control when offsets
    are committed, which is necessary to minimize duplicates and avoid missing data.
    If you set `enable.auto.commit` to `true`, then you might also want to control
    how frequently offsets will be committed using `auto.commit.interval.ms`. We’ll
    discuss the different options for committing offsets in more depth later in this
    chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: partition.assignment.strategy
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We learned that partitions are assigned to consumers in a consumer group. A
    `PartitionAssignor` is a class that, given consumers and topics they subscribed
    to, decides which partitions will be assigned to which consumer. By default, Kafka
    has the following assignment strategies:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Range
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Assigns to each consumer a consecutive subset of partitions from each topic
    it subscribes to. So if consumers C1 and C2 are subscribed to two topics, T1 and
    T2, and each of the topics has three partitions, then C1 will be assigned partitions
    0 and 1 from topics T1 and T2, while C2 will be assigned partition 2 from those
    topics. Because each topic has an uneven number of partitions and the assignment
    is done for each topic independently, the first consumer ends up with more partitions
    than the second. This happens whenever Range assignment is used and the number
    of consumers does not divide the number of partitions in each topic neatly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: RoundRobin
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Takes all the partitions from all subscribed topics and assigns them to consumers
    sequentially, one by one. If C1 and C2 described previously used RoundRobin assignment,
    C1 would have partitions 0 and 2 from topic T1, and partition 1 from topic T2\.
    C2 would have partition 1 from topic T1, and partitions 0 and 2 from topic T2\.
    In general, if all consumers are subscribed to the same topics (a very common
    scenario), RoundRobin assignment will end up with all consumers having the same
    number of partitions (or at most one partition difference).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Sticky
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'The Sticky Assignor has two goals: the first is to have an assignment that
    is as balanced as possible, and the second is that in case of a rebalance, it
    will leave as many assignments as possible in place, minimizing the overhead associated
    with moving partition assignments from one consumer to another. In the common
    case where all consumers are subscribed to the same topic, the initial assignment
    from the Sticky Assignor will be as balanced as that of the RoundRobin Assignor.
    Subsequent assignments will be just as balanced but will reduce the number of
    partition movements. In cases where consumers in the same group subscribe to different
    topics, the assignment achieved by Sticky Assignor is more balanced than that
    of the RoundRobin Assignor.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative Sticky
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: This assignment strategy is identical to that of the Sticky Assignor but supports
    cooperative rebalances in which consumers can continue consuming from the partitions
    that are not reassigned. See [“Consumer Groups and Partition Rebalance”](#partition-rebalances)
    to read more about cooperative rebalancing, and note that if you are upgrading
    from a version older than 2.3, you’ll need to follow a specific upgrade path in
    order to enable the cooperative sticky assignment strategy, so pay extra attention
    to the [upgrade guide](https://oreil.ly/klMI6).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The `partition.assignment.strategy` allows you to choose a partition assignment
    strategy. The default is `org.apache.kafka.clients.consumer.RangeAssignor`, which
    implements the Range strategy described earlier. You can replace it with `org.apache.kafka.clients.consumer.RoundRobinAssignor`,
    `org.apache.kafka.​clients.consumer.StickyAssignor`, or `org.apache.kafka.clients.consumer.​CooperativeStickyAssignor`.
    A more advanced option is to implement your own assignment strategy, in which
    case `partition.assignment.strategy` should point to the name of your class.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: client.id
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This can be any string, and will be used by the brokers to identify requests
    sent from the client, such as fetch requests. It is used in logging and metrics,
    and for quotas.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: client.rack
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, consumers will fetch messages from the leader replica of each partition.
    However, when the cluster spans multiple datacenters or multiple cloud availability
    zones, there are advantages both in performance and in cost to fetching messages
    from a replica that is located in the same zone as the consumer. To enable fetching
    from the closest replica, you need to set the `client.rack` configuration and
    identify the zone in which the client is located. Then you can configure the brokers
    to replace the default `replica.selector.class` with `org.apache.kafka.common.replica.RackAwareReplicaSelector`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: You can also implement your own `replica.selector.class` with custom logic for
    choosing the best replica to consume from, based on client metadata and partition
    metadata.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: group.instance.id
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This can be any unique string and is used to provide a consumer with [static
    group membership](#static-group-membership).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: receive.buffer.bytes and send.buffer.bytes
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are the sizes of the TCP send and receive buffers used by the sockets
    when writing and reading data. If these are set to –1, the OS defaults will be
    used. It can be a good idea to increase these when producers or consumers communicate
    with brokers in a different datacenter, because those network links typically
    have higher latency and lower bandwidth.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: offsets.retention.minutes
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a broker configuration, but it is important to be aware of it due to
    its impact on consumer behavior. As long as a consumer group has active members
    (i.e., members that are actively maintaining membership in the group by sending
    heartbeats), the last offset committed by the group for each partition will be
    retained by Kafka, so it can be retrieved in case of reassignment or restart.
    However, once a group becomes empty, Kafka will only retain its committed offsets
    to the duration set by this configuration—7 days by default. Once the offsets
    are deleted, if the group becomes active again it will behave like a brand-new
    consumer group with no memory of anything it consumed in the past. Note that this
    behavior changed a few times, so if you use versions older than 2.1.0, check the
    documentation for your version for the expected behavior.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Commits and Offsets
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we call `poll()`, it returns records written to Kafka that consumers
    in our group have not read yet. This means that we have a way of tracking which
    records were read by a consumer of the group. As discussed before, one of Kafka’s
    unique characteristics is that it does not track acknowledgments from consumers
    the way many JMS queues do. Instead, it allows consumers to use Kafka to track
    their position (offset) in each partition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We call the action of updating the current position in the partition an `offset`
    `commit`. Unlike traditional message queues, Kafka does not commit records individually.
    Instead, consumers commit the last message they’ve successfully processed from
    a partition and implicitly assume that every message before the last was also
    successfully processed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: How does a consumer commit an offset? It sends a message to Kafka, which updates
    a special ``__consumer_offsets`` topic with the committed offset for each partition.
    As long as all your consumers are up, running, and churning away, this will have
    no impact. However, if a consumer crashes or a new consumer joins the consumer
    group, this will *trigger a rebalance*. After a rebalance, each consumer may be
    assigned a new set of partitions than the one it processed before. In order to
    know where to pick up the work, the consumer will read the latest committed offset
    of each partition and continue from there.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: If the committed offset is smaller than the offset of the last message the client
    processed, the messages between the last processed offset and the committed offset
    will be processed twice. See [Figure 4-8](#reprocessed_offsets).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0408](assets/kdg2_0408.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Reprocessed messages
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the committed offset is larger than the offset of the last message the client
    actually processed, all messages between the last processed offset and the committed
    offset will be missed by the consumer group. See [Figure 4-9](#missed_messages_offsets).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0409](assets/kdg2_0409.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Missed messages between offsets
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, managing offsets has a big impact on the client application. The `KafkaConsumer`
    API provides multiple ways of committing offsets.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Which Offset Is Committed?
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When committing offsets either automatically or without specifying the intended
    offsets, the default behavior is to commit the offset after the last offset that
    was returned by `poll()`. This is important to keep in mind when attempting to
    manually commit specific offsets or seek to commit specific offsets. However,
    it is also tedious to repeatedly read “Commit the offset that is one larger than
    the last offset the client received from `poll()`,” and 99% of the time it does
    not matter. So, we are going to write “Commit the last offset” when we refer to
    the default behavior, and if you need to manually manipulate offsets, please keep
    this note in mind.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Commit
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to commit offsets is to allow the consumer to do it for you.
    If you configure `enable.auto.commit=true`, then every five seconds the consumer
    will commit the latest offset that your client received from `poll()`. The five-second
    interval is the default and is controlled by setting `auto.commit.interval.ms`.
    Just like everything else in the consumer, the automatic commits are driven by
    the poll loop. Whenever you poll, the consumer checks if it is time to commit,
    and if it is, it will commit the offsets it returned in the last poll.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Before using this convenient option, however, it is important to understand
    the consequences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Consider that, by default, automatic commits occur every five seconds. Suppose
    that we are three seconds after the most recent commit our consumer crashed. After
    the rebalancing, the surviving consumers will start consuming the partitions that
    were previously owned by the crashed broker. But they will start from the last
    offset committed. In this case, the offset is three seconds old, so all the events
    that arrived in those three seconds will be processed twice. It is possible to
    configure the commit interval to commit more frequently and reduce the window
    in which records will be duplicated, but it is impossible to completely eliminate
    them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: With autocommit enabled, when it is time to commit offsets, the next poll will
    commit the last offset returned by the previous poll. It doesn’t know which events
    were actually processed, so it is critical to always process all the events returned
    by `poll()` before calling `poll()` again. (Just like `poll()`, `close()` also
    commits offsets automatically.) This is usually not an issue, but pay attention
    when you handle exceptions or exit the poll loop prematurely.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Automatic commits are convenient, but they don’t give developers enough control
    to avoid duplicate messages.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Commit Current Offset
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most developers exercise more control over the time at which offsets are committed—both
    to eliminate the possibility of missing messages and to reduce the number of messages
    duplicated during rebalancing. The Consumer API has the option of committing the
    current offset at a point that makes sense to the application developer rather
    than based on a timer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: By setting `enable.auto.commit=false`, offsets will only be committed when the
    application explicitly chooses to do so. The simplest and most reliable of the
    commit APIs is `commitSync()`. This API will commit the latest offset returned
    by `poll()` and return once the offset is committed, throwing an exception if
    the commit fails for some reason.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that `commitSync()` will commit the latest offset
    returned by `poll()`, so if you call `commitSync()` before you are done processing
    all the records in the collection, you risk missing the messages that were committed
    but not processed, in case the application crashes. If the application crashes
    while it is still processing records in the collection, all the messages from
    the beginning of the most recent batch until the time of the rebalance will be
    processed twice—this may or may not be preferable to missing messages.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we would use `commitSync` to commit offsets after we finished processing
    the latest batch of messages:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO3-1)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that by printing the contents of a record, we are done processing
    it. Your application will likely do a lot more with the records—modify them, enrich
    them, aggregate them, display them on a dashboard, or notify users of important
    events. You should determine when you are “done” with a record according to your
    use case.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO3-2)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Once we are done “processing” all the records in the current batch, we call
    `commitSync` to commit the last offset in the batch, before polling for additional
    messages.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO3-3)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`commitSync` retries committing as long as there is no error that can’t be
    recovered. If this happens, there is not much we can do except log an error.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous Commit
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One drawback of manual commit is that the application is blocked until the broker
    responds to the commit request. This will limit the throughput of the application.
    Throughput can be improved by committing less frequently, but then we are increasing
    the number of potential duplicates that a rebalance may create.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option is the asynchronous commit API. Instead of waiting for the broker
    to respond to a commit, we just send the request and continue on:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO4-1)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Commit the last offset and carry on.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The drawback is that while `commitSync()` will retry the commit until it either
    succeeds or encounters a nonretriable failure, `commitAsync()` will not retry.
    The reason it does not retry is that by the time `commitAsync()` receives a response
    from the server, there may have been a later commit that was already successful.
    Imagine that we sent a request to commit offset 2000\. There is a temporary communication
    problem, so the broker never gets the request and therefore never responds. Meanwhile,
    we processed another batch and successfully committed offset 3000\. If `commit​Async()`
    now retries the previously failed commit, it might succeed in committing offset
    2000 *after* offset 3000 was already processed and committed. In the case of a
    rebalance, this will cause more duplicates.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'We mention this complication and the importance of correct order of commits
    because `commitAsync()` also gives you an option to pass in a callback that will
    be triggered when the broker responds. It is common to use the callback to log
    commit errors or to count them in a metric, but if you want to use the callback
    for retries, you need to be aware of the problem with commit order:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO5-1)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: We send the commit and carry on, but if the commit fails, the failure and the
    offsets will be logged.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Retrying Async Commits
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple pattern to get the commit order right for asynchronous retries is to
    use a monotonically increasing sequence number. Increase the sequence number every
    time you commit, and add the sequence number at the time of the commit to the
    `commitAsync` callback. When you’re getting ready to send a retry, check if the
    commit sequence number the callback got is equal to the instance variable; if
    it is, there was no newer commit and it is safe to retry. If the instance sequence
    number is higher, don’t retry because a newer commit was already sent.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Combining Synchronous and Asynchronous Commits
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, occasional failures to commit without retrying are not a huge problem
    because if the problem is temporary, the following commit will be successful.
    But if we know that this is the last commit before we close the consumer, or before
    a rebalance, we want to make extra sure that the commit succeeds.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a common pattern is to combine `commitAsync()` with `commitSync()`
    just before shutdown. Here is how it works (we will discuss how to commit just
    before rebalance when we get to the section about rebalance listeners):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO6-1)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: While everything is fine, we use `commitAsync`. It is faster, and if one commit
    fails, the next commit will serve as a retry.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO6-2)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: But if we are closing, there is no “next commit.” We call `commitSync()`, because
    it will retry until it succeeds or suffers unrecoverable failure.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Committing a Specified Offset
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Committing the latest offset only allows you to commit as often as you finish
    processing batches. But what if you want to commit more frequently than that?
    What if `poll()` returns a huge batch and you want to commit offsets in the middle
    of the batch to avoid having to process all those rows again if a rebalance occurs?
    You can’t just call `commitSync()` or `commitAsync()`—this will commit the last
    offset returned, which you didn’t get to process yet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the Consumer API allows you to call `commitSync()` and `commitAsync()`
    and pass a map of partitions and offsets that you wish to commit. If you are in
    the middle of processing a batch of records, and the last message you got from
    partition 3 in topic “customers” has offset 5000, you can call `commitSync()`
    to commit offset 5001 for partition 3 in topic “customers.” Since your consumer
    may be consuming more than a single partition, you will need to track offsets
    on all of them, which adds complexity to your code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what a commit of specific offsets looks like:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-1)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: This is the map we will use to manually track offsets.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-2)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Remember, `println` is a stand-in for whatever processing you do for the records
    you consume.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-3)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: After reading each record, we update the offsets map with the offset of the
    next message we expect to process. The committed offset should always be the offset
    of the next message that your application will read. This is where we’ll start
    reading next time we start.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-4)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Here, we decide to commit current offsets every 1,000 records. In your application,
    you can commit based on time or perhaps content of the records.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-5)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: I chose to call `commitAsync()` (without a callback, therefore the second parameter
    is `null`), but `commitSync()` is also completely valid here. Of course, when
    committing specific offsets you still need to perform all the error handling we’ve
    seen in previous sections.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Rebalance Listeners
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the previous section about committing offsets, a consumer
    will want to do some cleanup work before exiting and also before partition rebalancing.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: If you know your consumer is about to lose ownership of a partition, you will
    want to commit offsets of the last event you’ve processed. Perhaps you also need
    to close file handles, database connections, and such.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'The Consumer API allows you to run your own code when partitions are added
    or removed from the consumer. You do this by passing a `ConsumerRebalanceListener`
    when calling the `subscribe()` method we discussed previously. `ConsumerRebalanceListener`
    has three methods you can implement:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '`public void onPartitionsAssigned(Collection<TopicPartition> partitions)`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Called after partitions have been reassigned to the consumer but before the
    consumer starts consuming messages. This is where you prepare or load any state
    that you want to use with the partition, seek to the correct offsets if needed,
    or similar. Any preparation done here should be guaranteed to return within `max.poll.timeout.ms`
    so the consumer can successfully join the group.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '`public void onPartitionsRevoked(Collection<TopicPartition> partitions)`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Called when the consumer has to give up partitions that it previously owned—either
    as a result of a rebalance or when the consumer is being closed. In the common
    case, when an eager rebalancing algorithm is used, this method is invoked before
    the rebalancing starts and after the consumer stopped consuming messages. If a
    cooperative rebalancing algorithm is used, this method is invoked at the end of
    the rebalance, with just the subset of partitions that the consumer has to give
    up. This is where you want to commit offsets, so whoever gets this partition next
    will know where to start.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '`public void onPartitionsLost(Collection<TopicPartition> partitions)`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Only called when a cooperative rebalancing algorithm is used, and only in exceptional
    cases where the partitions were assigned to other consumers without first being
    revoked by the rebalance algorithm (in normal cases, `onPartitions​Revoked()`
    will be called). This is where you clean up any state or resources that are used
    with these partitions. Note that this has to be done carefully—the new owner of
    the partitions may have already saved its own state, and you’ll need to avoid
    conflicts. Note that if you don’t implement this method, `onPartitions​Revoked()`
    will be called instead.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you use a cooperative rebalancing algorithm, note that:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '`onPartitionsAssigned()` will be invoked on every rebalance, as a way of notifying
    the consumer that a rebalance happened. However, if there are no new partitions
    assigned to the consumer, it will be called with an empty collection.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onPartitionsRevoked()` will be invoked in normal rebalancing conditions, but
    only if the consumer gave up the ownership of partitions. It will not be called
    with an empty collection.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onPartitionsLost()` will be invoked in exceptional rebalancing conditions,
    and the partitions in the collection will already have new owners by the time
    the method is invoked.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you implemented all three methods, you are guaranteed that during a normal
    rebalance, `onPartitionsAssigned()` will be called by the new owner of the partitions
    that are reassigned only after the previous owner completed `onPartitionsRevoked()`
    and gave up its ownership.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will show how to use `onPartitionsRevoked()` to commit offsets
    before losing ownership of a partition:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-1)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: We start by implementing a `ConsumerRebalanceListener`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-2)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: In this example we don’t need to do anything when we get a new partition; we’ll
    just start consuming messages.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-3)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: However, when we are about to lose a partition due to rebalancing, we need to
    commit offsets. We are committing offsets for all partitions, not just the partitions
    we are about to lose—because the offsets are for events that were already processed,
    there is no harm in that. And we are using `commitSync()` to make sure the offsets
    are committed before the rebalance proceeds.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-4)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important part: pass the `ConsumerRebalanceListener` to the `subscribe()`
    method so it will get invoked by the consumer.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Consuming Records with Specific Offsets
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we’ve seen how to use `poll()` to start consuming messages from the last
    committed offset in each partition and to proceed in processing all messages in
    sequence. However, sometimes you want to start reading at a different offset.
    Kafka offers a variety of methods that cause the next `poll()` to start consuming
    in a different offset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to start reading all messages from the beginning of the partition,
    or you want to skip all the way to the end of the partition and start consuming
    only new messages, there are APIs specifically for that: `seekToBeginning(Collection<TopicPartition>
    tp)` and `seekToEnd(Collection<TopicPartition> tp)`.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka API also lets you seek a specific offset. This ability can be used
    in a variety of ways; for example, a time-sensitive application could skip ahead
    a few records when falling behind, or a consumer that writes data to a file could
    be reset back to a specific point in time in order to recover data if the file
    was lost.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick example of how to set the current offset on all partitions to
    records that were produced at a specific point in time:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-1)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: We create a map from all the partitions assigned to this consumer (via `consumer.assignment()`)
    to the timestamp we wanted to revert the consumers to.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-2)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Then we get the offsets that were current at these timestamps. This method sends
    a request to the broker where a timestamp index is used to return the relevant
    offsets.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-3)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we reset the offset on each partition to the offset that was returned
    in the previous step.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: But How Do We Exit?
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, when we discussed the poll loop, we told you not to
    worry about the fact that the consumer polls in an infinite loop, and that we
    would discuss how to exit the loop cleanly. So, let’s discuss how to exit cleanly.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: When you decide to shut down the consumer, and you want to exit immediately
    even though the consumer may be waiting on a long `poll()`, you will need another
    thread to call `consumer.wakeup()`. If you are running the consumer loop in the
    main thread, this can be done from `ShutdownHook`. Note that `consumer.wakeup()`
    is the only consumer method that is safe to call from a different thread. Calling
    `wakeup` will cause `poll()` to exit with `WakeupException`, or if `consumer.wakeup()`
    was called while the thread was not waiting on poll, the exception will be thrown
    on the next iteration when `poll()` is called. The `WakeupException` doesn’t need
    to be handled, but before exiting the thread, you must call `consumer.close()`.
    Closing the consumer will commit offsets if needed and will send the group coordinator
    a message that the consumer is leaving the group. The consumer coordinator will
    trigger rebalancing immediately, and you won’t need to wait for the session to
    timeout before partitions from the consumer you are closing will be assigned to
    another consumer in the group.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the exit code will look like if the consumer is running in the
    main application thread. This example is a bit truncated, but you can view the
    full example [on GitHub](http://bit.ly/2u47e9A):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '`ShutdownHook` runs in a separate thread, so the only safe action you can take
    is to call `wakeup` to break out of the `poll` loop.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-2)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: A particularly long poll timeout. If the poll loop is short enough and you don’t
    mind waiting a bit before exiting, you don’t need to call `wakeup`—just checking
    an atomic boolean in each iteration would be enough. Long poll timeouts are useful
    when consuming low-throughput topics; this way, the client uses less CPU for constantly
    looping while the broker has no new data to return.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-3)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Another thread calling `wakeup` will cause poll to throw a `WakeupException`.
    You’ll want to catch the exception to make sure your application doesn’t exit
    unexpectedly, but there is no need to do anything with it.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-4)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Before exiting the consumer, make sure you close it cleanly.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Deserializers
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, Kafka producers require *serializers*
    to convert objects into byte arrays that are then sent to Kafka. Similarly, Kafka
    consumers require *deserializers* to convert byte arrays received from Kafka into
    Java objects. In previous examples, we just assumed that both the key and the
    value of each message are strings, and we used the default `StringDeserializer`
    in the consumer configuration.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#writing_messages_to_kafka) about the Kafka producer,
    we saw how to serialize custom types and how to use Avro and `AvroSerializers`
    to generate Avro objects from schema definitions and then serialize them when
    producing messages to Kafka. We will now look at how to create custom deserializers
    for your own objects and how to use Avro and its deserializers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: It should be obvious that the serializer used to produce events to Kafka must
    match the deserializer that will be used when consuming events. Serializing with
    `IntSerializer` and then deserializing with `StringDeserializer` will not end
    well. This means that, as a developer, you need to keep track of which serializers
    were used to write into each topic and make sure each topic only contains data
    that the deserializers you use can interpret. This is one of the benefits of using
    Avro and the Schema Registry for serializing and deserializing—the `AvroSerializer`
    can make sure that all the data written to a specific topic is compatible with
    the schema of the topic, which means it can be deserialized with the matching
    deserializer and schema. Any errors in compatibility—on the producer or the consumer
    side—will be caught easily with an appropriate error message, which means you
    will not need to try to debug byte arrays for serialization errors.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We will start by quickly showing how to write a custom deserializer, even though
    this is the less common method, and then we will move on to an example of how
    to use Avro to deserialize message keys and values.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Custom Deserializers
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take the same custom object we serialized in [Chapter 3](ch03.html#writing_messages_to_kafka)
    and write a deserializer for it:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The custom deserializer will look as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO11-1)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The consumer also needs the implementation of the `Customer` class, and both
    the class and the serializer need to match on the producing and consuming applications.
    In a large organization with many consumers and producers sharing access to the
    data, this can become challenging.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO11-2)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We are just reversing the logic of the serializer here—we get the customer ID
    and name out of the byte array and use them to construct the object we need.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'The consumer code that uses this deserializer will look similar to this example:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Again, it is important to note that implementing a custom serializer and deserializer
    is not recommended. It tightly couples producers and consumers and is fragile
    and error prone. A better solution would be to use a standard message format,
    such as JSON, Thrift, Protobuf, or Avro. We’ll now see how to use Avro deserializers
    with the Kafka consumer. For background on Apache Avro, its schemas, and schema-compatibility
    capabilities, refer back to [Chapter 3](ch03.html#writing_messages_to_kafka).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Using Avro Deserialization with Kafka Consumer
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume we are using the implementation of the `Customer` class in Avro
    that was shown in [Chapter 3](ch03.html#writing_messages_to_kafka). In order to
    consume those objects from Kafka, you want to implement a consuming application
    similar to this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-1)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: We use `KafkaAvroDeserializer` to deserialize the Avro messages.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-2)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '`schema.registry.url` is a new parameter. This simply points to where we store
    the schemas. This way, the consumer can use the schema that was registered by
    the producer to deserialize the message.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-3)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: We specify the generated class, `Customer`, as the type for the record value.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-4)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '`record.value()` is a `Customer` instance, and we can use it accordingly.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Standalone Consumer: Why and How to Use a Consumer Without a Group'
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed consumer groups, which are where partitions are assigned
    automatically to consumers and are rebalanced automatically when consumers are
    added or removed from the group. Typically, this behavior is just what you want,
    but in some cases you want something much simpler. Sometimes you know you have
    a single consumer that always needs to read data from all the partitions in a
    topic, or from a specific partition in a topic. In this case, there is no reason
    for groups or rebalances—just assign the consumer-specific topic and/or partitions,
    consume messages, and commit offsets on occasion (although you still need to configure
    `group.id` to commit offsets, without calling *subscribe* the consumer won’t join
    any group).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: When you know exactly which partitions the consumer should read, you don’t *subscribe*
    to a topic—instead, you *assign* yourself a few partitions. A consumer can either
    subscribe to topics (and be part of a consumer group) or assign itself partitions,
    but not both at the same time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how a consumer can assign itself all partitions of a
    specific topic and consume from them:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO13-1)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: We start by asking the cluster for the partitions available in the topic. If
    you only plan on consuming a specific partition, you can skip this part.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO13-2)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Once we know which partitions we want, we call `assign()` with the list.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Other than the lack of rebalances and the need to manually find the partitions,
    everything else is business as usual. Keep in mind that if someone adds new partitions
    to the topic, the consumer will not be notified. You will need to handle this
    by checking `consumer.partitionsFor()` periodically or simply by bouncing the
    application whenever partitions are added.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with an in-depth explanation of Kafka’s consumer groups
    and the way they allow multiple consumers to share the work of reading events
    from topics. We followed the theoretical discussion with a practical example of
    a consumer subscribing to a topic and continuously reading events. We then looked
    into the most important consumer configuration parameters and how they affect
    consumer behavior. We dedicated a large part of the chapter to discussing offsets
    and how consumers keep track of them. Understanding how consumers commit offsets
    is critical when writing reliable consumers, so we took time to explain the different
    ways this can be done. We then discussed additional parts of the Consumer APIs,
    handling rebalances, and closing the consumer.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: We concluded by discussing the deserializers used by consumers to turn bytes
    stored in Kafka into Java objects that the applications can process. We discussed
    Avro deserializers in some detail, even though they are just one type of deserializer
    you can use, because these are most commonly used with Kafka.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.html#idm45351109673920-marker)) Diagrams by Sophie Blee-Goldman,
    from her May 2020 blog post, [“From Eager to Smarter in Apache Kafka Consumer
    Rebalances”](https://oreil.ly/fZzac).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL

- en: 'Chapter 4\. Kafka Consumers: Reading Data from Kafka'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications that need to read data from Kafka use a `KafkaConsumer` to subscribe
    to Kafka topics and receive messages from these topics. Reading data from Kafka
    is a bit different than reading data from other messaging systems, and there are
    a few unique concepts and ideas involved. It can be difficult to understand how
    to use the Consumer API without understanding these concepts first. We’ll start
    by explaining some of the important concepts, and then we’ll go through some examples
    that show the different ways Consumer APIs can be used to implement applications
    with varying requirements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Consumer Concepts
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how to read data from Kafka, you first need to understand its
    consumers and consumer groups. The following sections cover those concepts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Consumers and Consumer Groups
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you have an application that needs to read messages from a Kafka topic,
    run some validations against them, and write the results to another data store.
    In this case, your application will create a consumer object, subscribe to the
    appropriate topic, and start receiving messages, validating them, and writing
    the results. This may work well for a while, but what if the rate at which producers
    write messages to the topic exceeds the rate at which your application can validate
    them? If you are limited to a single consumer reading and processing the data,
    your application may fall further and further behind, unable to keep up with the
    rate of incoming messages. Obviously there is a need to scale consumption from
    topics. Just like multiple producers can write to the same topic, we need to allow
    multiple consumers to read from the same topic, splitting the data among them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Kafka consumers are typically part of a *consumer group*. When multiple consumers
    are subscribed to a topic and belong to the same consumer group, each consumer
    in the group will receive messages from a different subset of the partitions in
    the topic.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take topic T1 with four partitions. Now suppose we created a new consumer,
    C1, which is the only consumer in group G1, and use it to subscribe to topic T1\.
    Consumer C1 will get all messages from all four T1 partitions. See [Figure 4-1](#T1_four_partitions).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0401](assets/kdg2_0401.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. One consumer group with four partitions
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we add another consumer, C2, to group G1, each consumer will only get messages
    from two partitions. Perhaps messages from partition 0 and 2 go to C1, and messages
    from partitions 1 and 3 go to consumer C2\. See [Figure 4-2](#T1_two_groups).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0402](assets/kdg2_0402.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Four partitions split to two consumers in a group
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If G1 has four consumers, then each will read messages from a single partition.
    See [Figure 4-3](#T1_four_partition_group).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0403](assets/kdg2_0403.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Four consumers in a group with one partition each
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we add more consumers to a single group with a single topic than we have
    partitions, some of the consumers will be idle and get no messages at all. See
    [Figure 4-4](#T1_overflow_nomessage).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0404](assets/kdg2_0404.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. More consumers in a group than partitions means idle consumers
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The main way we scale data consumption from a Kafka topic is by adding more
    consumers to a consumer group. It is common for Kafka consumers to do high-latency
    operations such as write to a database or a time-consuming computation on the
    data. In these cases, a single consumer can’t possibly keep up with the rate data
    flows into a topic, and adding more consumers that share the load by having each
    consumer own just a subset of the partitions and messages is our main method of
    scaling. This is a good reason to create topics with a large number of partitions—it
    allows adding more consumers when the load increases. Keep in mind that there
    is no point in adding more consumers than you have partitions in a topic—some
    of the consumers will just be idle. [Chapter 2](ch02.html#installing_kafka) includes
    some suggestions on how to choose the number of partitions in a topic.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩展Kafka主题的数据消费的主要方法是向消费者组添加更多的消费者。Kafka消费者通常执行高延迟操作，例如向数据库写入或对数据进行耗时计算。在这些情况下，单个消费者不可能跟上数据流入主题的速度，通过添加更多的消费者来共享负载，使每个消费者仅拥有分区和消息的子集是我们扩展的主要方法。这是创建具有大量分区的主题的一个很好的理由——它允许在负载增加时添加更多的消费者。请记住，在主题中添加更多的消费者是没有意义的——一些消费者将处于空闲状态。[第2章](ch02.html#installing_kafka)包括一些建议，关于如何选择主题中的分区数量。
- en: In addition to adding consumers in order to scale a single application, it is
    very common to have multiple applications that need to read data from the same
    topic. In fact, one of the main design goals in Kafka was to make the data produced
    to Kafka topics available for many use cases throughout the organization. In those
    cases, we want each application to get all of the messages, rather than just a
    subset. To make sure an application gets all the messages in a topic, ensure the
    application has its own consumer group. Unlike many traditional messaging systems,
    Kafka scales to a large number of consumers and consumer groups without reducing
    performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了添加消费者以扩展单个应用程序之外，非常常见的是有多个应用程序需要从同一个主题中读取数据。事实上，Kafka的主要设计目标之一是使生产到Kafka主题的数据对组织中的许多用例都可用。在这些情况下，我们希望每个应用程序获取主题中的所有消息，而不仅仅是一个子集。为了确保应用程序获取主题中的所有消息，请确保应用程序有自己的消费者组。与许多传统的消息系统不同，Kafka可以扩展到大量的消费者和消费者组，而不会降低性能。
- en: In the previous example, if we add a new consumer group (G2) with a single consumer,
    this consumer will get all the messages in topic T1 independent of what G1 is
    doing. G2 can have more than a single consumer, in which case they will each get
    a subset of partitions, just like we showed for G1, but G2 as a whole will still
    get all the messages regardless of other consumer groups. See [Figure 4-5](#receive_all_messages).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，如果我们添加一个新的消费者组（G2）并有一个单独的消费者，这个消费者将独立于G1的操作获取主题T1中的所有消息。G2可以有多个消费者，这样它们将分别获取分区的子集，就像我们为G1所示的那样，但是G2作为一个整体仍将获取所有消息，而不受其他消费者组的影响。参见[图4-5](#receive_all_messages)。
- en: '![kdg2 0405](assets/kdg2_0405.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0405](assets/kdg2_0405.png)'
- en: Figure 4-5\. Adding a new consumer group, both groups receive all messages
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。添加一个新的消费者组，两个组都接收所有消息
- en: To summarize, you create a new consumer group for each application that needs
    all the messages from one or more topics. You add consumers to an existing consumer
    group to scale the reading and processing of messages from the topics, so each
    additional consumer in a group will only get a subset of the messages.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，为每个需要从一个或多个主题中获取所有消息的应用程序创建一个新的消费者组。向现有的消费者组添加消费者以扩展对主题中消息的读取和处理，因此组中的每个额外的消费者将只获取消息的子集。
- en: Consumer Groups and Partition Rebalance
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消费者组和分区重新平衡
- en: As we saw in the previous section, consumers in a consumer group share ownership
    of the partitions in the topics they subscribe to. When we add a new consumer
    to the group, it starts consuming messages from partitions previously consumed
    by another consumer. The same thing happens when a consumer shuts down or crashes;
    it leaves the group, and the partitions it used to consume will be consumed by
    one of the remaining consumers. Reassignment of partitions to consumers also happens
    when the topics the consumer group is consuming are modified (e.g., if an administrator
    adds new partitions).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中看到的，消费者组中的消费者共享他们订阅的主题中分区的所有权。当我们向组中添加新的消费者时，它开始消费先前由另一个消费者消费的分区的消息。当消费者关闭或崩溃时也会发生同样的情况；它离开了组，它以前消费的分区将被剩下的消费者之一消费。当消费者组正在消费的主题被修改时（例如，如果管理员添加了新的分区），分区也会重新分配给消费者。
- en: Moving partition ownership from one consumer to another is called a *rebalance*.
    Rebalances are important because they provide the consumer group with high availability
    and scalability (allowing us to easily and safely add and remove consumers), but
    in the normal course of events they can be fairly undesirable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将分区所有权从一个消费者转移到另一个消费者称为*重新平衡*。重新平衡很重要，因为它为消费者组提供了高可用性和可伸缩性（允许我们轻松安全地添加和删除消费者），但在正常情况下，它们可能是相当不受欢迎的。
- en: There are two types of rebalances, depending on the partition assignment strategy
    that the consumer group uses:^([1](ch04.html#idm45351109673920))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种重新平衡的类型，取决于消费者组使用的分区分配策略：^([1](ch04.html#idm45351109673920))
- en: Eager rebalances
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 急切的重新平衡
- en: 'During an eager rebalance, all consumers stop consuming, give up their ownership
    of all partitions, rejoin the consumer group, and get a brand-new partition assignment.
    This is essentially a short window of unavailability of the entire consumer group.
    The length of the window depends on the size of the consumer group as well as
    on several configuration parameters. [Figure 4-6](#fig-6-eager-rebalance) shows
    how eager rebalances have two distinct phases: first, all consumers give up their
    partition assigning, and second, after they all complete this and rejoin the group,
    they get new partition assignments and can resume consuming.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在急切重新平衡期间，所有消费者停止消费，放弃它们对所有分区的所有权，重新加入消费者组，并获得全新的分区分配。这实质上是整个消费者组的短暂不可用窗口。这个窗口的长度取决于消费者组的大小以及几个配置参数。[图4-6](#fig-6-eager-rebalance)显示了急切重新平衡有两个明显的阶段：首先，所有消费者放弃它们的分区分配，然后，在它们都完成这一步并重新加入组后，它们获得新的分区分配并可以继续消费。
- en: '![kdg2 0406](assets/kdg2_0406.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0406](assets/kdg2_0406.png)'
- en: Figure 4-6\. Eager rebalance revokes all partitions, pauses consumption, and
    reassigns them
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 急切重新平衡会撤销所有分区，暂停消费，并重新分配它们
- en: Cooperative rebalances
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 合作重新平衡
- en: Cooperative rebalances (also called *incremental rebalances*) typically involve
    reassigning only a small subset of the partitions from one consumer to another,
    and allowing consumers to continue processing records from all the partitions
    that are not reassigned. This is achieved by rebalancing in two or more phases.
    Initially, the consumer group leader informs all the consumers that they will
    lose ownership of a subset of their partitions, then the consumers stop consuming
    from these partitions and give up their ownership in them. In the second phase,
    the consumer group leader assigns these now orphaned partitions to their new owners.
    This incremental approach may take a few iterations until a stable partition assignment
    is achieved, but it avoids the complete “stop the world” unavailability that occurs
    with the eager approach. This is especially important in large consumer groups
    where rebalances can take a significant amount of time. [Figure 4-7](#fig-7-cooperative-rebalance)
    shows how cooperative rebalances are incremental and that only a subset of the
    consumers and partitions are involved.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 合作重新平衡（也称为*增量重新平衡*）通常涉及将一小部分分区从一个消费者重新分配给另一个消费者，并允许消费者继续处理未被重新分配的所有分区的记录。这是通过分两个或更多阶段进行重新平衡来实现的。最初，消费者组领导者通知所有消费者它们将失去对一部分分区的所有权，然后消费者停止从这些分区消费并放弃它们的所有权。在第二阶段，消费者组领导者将这些现在被遗弃的分区分配给它们的新所有者。这种增量方法可能需要几次迭代，直到实现稳定的分区分配，但它避免了急切方法中发生的完全“停止世界”不可用性。这在大型消费者组中尤为重要，因为重新平衡可能需要大量时间。[图4-7](#fig-7-cooperative-rebalance)显示了合作重新平衡是增量的，只涉及部分消费者和分区。
- en: '![kdg2 0407](assets/kdg2_0407.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0407](assets/kdg2_0407.png)'
- en: Figure 4-7\. Cooperative rebalance only pauses consumption for the subset of
    partitions that will be reassigned
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. 合作重新平衡只暂停将被重新分配的分区的消费
- en: Consumers maintain membership in a consumer group and ownership of the partitions
    assigned to them by sending *heartbeats* to a Kafka broker designated as the *group
    coordinator* (this broker can be different for different consumer groups). The
    heartbeats are sent by a background thread of the consumer, and as long as the
    consumer is sending heartbeats at regular intervals, it is assumed to be alive.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者通过向被指定为*组协调者*的Kafka代理发送*心跳*来维护消费者组的成员资格和分配给它们的分区的所有权（对于不同的消费者组，这个代理可能是不同的）。消费者通过后台线程发送心跳，只要消费者以固定的间隔发送心跳，就假定它是活动的。
- en: If the consumer stops sending heartbeats for long enough, its session will timeout
    and the group coordinator will consider it dead and trigger a rebalance. If a
    consumer crashed and stopped processing messages, it will take the group coordinator
    a few seconds without heartbeats to decide it is dead and trigger the rebalance.
    During those seconds, no messages will be processed from the partitions owned
    by the dead consumer. When closing a consumer cleanly, the consumer will notify
    the group coordinator that it is leaving, and the group coordinator will trigger
    a rebalance immediately, reducing the gap in processing. Later in this chapter,
    we will discuss configuration options that control heartbeat frequency, session
    timeouts, and other configuration parameters that can be used to fine-tune the
    consumer behavior.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者停止发送心跳足够长的时间，它的会话将超时，组协调者将认为它已经死亡并触发重新平衡。如果消费者崩溃并停止处理消息，组协调者将需要几秒钟没有心跳来判断它已经死亡并触发重新平衡。在这几秒钟内，来自已死亡消费者所拥有的分区的消息将不会被处理。当消费者正常关闭时，消费者将通知组协调者它正在离开，组协调者将立即触发重新平衡，减少处理的间隙。在本章的后面，我们将讨论控制心跳频率、会话超时和其他配置参数的配置选项，这些配置选项可以用来微调消费者的行为。
- en: How Does the Process of Assigning Partitions to Consumers Work?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将分区分配给消费者的过程是如何工作的？
- en: When a consumer wants to join a group, it sends a `JoinGroup` request to the
    group coordinator. The first consumer to join the group becomes the group *leader*.
    The leader receives a list of all consumers in the group from the group coordinator
    (this will include all consumers that sent a heartbeat recently and that are therefore
    considered alive) and is responsible for assigning a subset of partitions to each
    consumer. It uses an implementation of `PartitionAssignor` to decide which partitions
    should be handled by which consumer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当消费者想要加入一个组时，它会向组协调者发送一个“JoinGroup”请求。第一个加入组的消费者成为组的*领导者*。领导者从组协调者那里接收到组中所有消费者的列表（这将包括最近发送心跳并因此被认为是活动的所有消费者），并负责将一部分分区分配给每个消费者。它使用`PartitionAssignor`的实现来决定哪些分区应该由哪些消费者处理。
- en: Kafka has few built-in partition assignment policies, which we will discuss
    in more depth in the configuration section. After deciding on the partition assignment,
    the consumer group leader sends the list of assignments to the `GroupCoordinator`,
    which sends this information to all the consumers. Each consumer only sees its
    own assignment—the leader is the only client process that has the full list of
    consumers in the group and their assignments. This process repeats every time
    a rebalance happens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka有一些内置的分区分配策略，我们将在配置部分更深入地讨论。在决定分区分配之后，消费者组领导者将分配的列表发送给`GroupCoordinator`，后者将此信息发送给所有消费者。每个消费者只能看到自己的分配
    - 领导者是唯一拥有完整消费者列表及其分配的客户端进程。每次发生重新平衡时，这个过程都会重复。
- en: Static Group Membership
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态组成员资格
- en: By default, the identity of a consumer as a member of its consumer group is
    transient. When consumers leave a consumer group, the partitions that were assigned
    to the consumer are revoked, and when it rejoins, it is assigned a new member
    ID and a new set of partitions through the rebalance protocol.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消费者作为其消费者组的成员的身份是瞬时的。当消费者离开消费者组时，分配给消费者的分区将被撤销，当它重新加入时，通过重新平衡协议，它将被分配一个新的成员ID和一组新的分区。
- en: All this is true unless you configure a consumer with a unique `group.instance.id`,
    which makes the consumer a *static* member of the group. When a consumer first
    joins a consumer group as a static member of the group, it is assigned a set of
    partitions according to the partition assignment strategy the group is using,
    as normal. However, when this consumer shuts down, it does not automatically leave
    the group—it remains a member of the group until its session times out. When the
    consumer rejoins the group, it is recognized with its static identity and is reassigned
    the same partitions it previously held without triggering a rebalance. The group
    coordinator that caches the assignment for each member of the group does not need
    to trigger a rebalance but can just send the cache assignment to the rejoining
    static member.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您配置具有唯一`group.instance.id`的消费者，否则所有这些都是真实的，这使得消费者成为组的*静态*成员。当消费者首次以组的静态成员身份加入消费者组时，它将根据组使用的分区分配策略被分配一组分区，就像正常情况下一样。然而，当此消费者关闭时，它不会自动离开组
    - 直到其会话超时之前，它仍然是组的成员。当消费者重新加入组时，它将以其静态身份被识别，并且重新分配之前持有的相同分区，而不会触发重新平衡。缓存组的协调者不需要触发重新平衡，而只需将缓存分配发送给重新加入的静态成员。
- en: If two consumers join the same group with the same `group.instance.id`, the
    second consumer will get an error saying that a consumer with this ID already
    exists.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个消费者以相同的`group.instance.id`加入同一组，第二个消费者将收到一个错误，指出已经存在具有此ID的消费者。
- en: Static group membership is useful when your application maintains local state
    or cache that is populated by the partitions that are assigned to each consumer.
    When re-creating this cache is time-consuming, you don’t want this process to
    happen every time a consumer restarts. On the flip side, it is important to remember
    that the partitions owned by each consumer will not get reassigned when a consumer
    is restarted. For a certain duration, no consumer will consume messages from these
    partitions, and when the consumer finally starts back up, it will lag behind the
    latest messages in these partitions. You should be confident that the consumer
    that owns these partitions will be able to catch up with the lag after the restart.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 静态组成员资格在您的应用程序维护由分配给每个消费者的分区填充的本地状态或缓存时非常有用。当重新创建此缓存需要耗费时间时，您不希望每次消费者重新启动时都发生这个过程。另一方面，重要的是要记住，当消费者重新启动时，每个消费者拥有的分区将不会重新分配。在一定的时间内，没有消费者会从这些分区中消费消息，当消费者最终重新启动时，它将落后于这些分区中的最新消息。您应该确信，拥有这些分区的消费者将能够在重新启动后赶上滞后。
- en: It is important to note that static members of consumer groups do not leave
    the group proactively when they shut down, and detecting when they are “really
    gone” depends on the `session.timeout.ms` configuration. You’ll want to set it
    high enough to avoid triggering rebalances on a simple application restart but
    low enough to allow automatic reassignment of their partitions when there is more
    significant downtime, to avoid large gaps in processing these partitions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，消费者组的静态成员在关闭时不会主动离开组，而是依赖于`session.timeout.ms`配置来检测它们何时“真正离开”。您需要将其设置得足够高，以避免在简单应用程序重新启动时触发重新平衡，但又要足够低，以允许在有更长时间停机时自动重新分配它们的分区，以避免处理这些分区时出现较大的间隙。
- en: Creating a Kafka Consumer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Kafka消费者
- en: 'The first step to start consuming records is to create a `KafkaConsumer` instance.
    Creating a `KafkaConsumer` is very similar to creating a `KafkaProducer`—you create
    a Java `Properties` instance with the properties you want to pass to the consumer.
    We will discuss all the properties in depth later in the chapter. To start, we
    just need to use the three mandatory properties: `bootstrap.servers`, `key.deserializer`,
    and `value.deserializer`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 开始消费记录的第一步是创建一个`KafkaConsumer`实例。创建`KafkaConsumer`与创建`KafkaProducer`非常相似 - 您需要创建一个带有要传递给消费者的属性的Java
    `Properties`实例。我们将在本章后面详细讨论所有属性。首先，我们只需要使用三个必需的属性：`bootstrap.servers`，`key.deserializer`和`value.deserializer`。
- en: The first property, `bootstrap.servers`, is the connection string to a Kafka
    cluster. It is used the exact same way as in `KafkaProducer` (refer to [Chapter 3](ch03.html#writing_messages_to_kafka)
    for details on how this is defined). The other two properties, `key.deserializer`
    and `value.​dese⁠rial⁠izer`, are similar to the `serializers` defined for the
    producer, but rather than specifying classes that turn Java objects to byte arrays,
    you need to specify classes that can take a byte array and turn it into a Java
    object.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个属性`bootstrap.servers`是连接到Kafka集群的连接字符串。它的使用方式与`KafkaProducer`中的完全相同（有关其定义的详细信息，请参阅[第3章](ch03.html#writing_messages_to_kafka)）。另外两个属性`key.deserializer`和`value.deserializer`与生产者定义的`serializers`类似，但不是指定将Java对象转换为字节数组的类，而是需要指定能够将字节数组转换为Java对象的类。
- en: There is a fourth property, which is not strictly mandatory but very commonly
    used. The property is `group.id`, and it specifies the consumer group the `Kafka``Consumer`
    instance belongs to. While it is possible to create consumers that do not belong
    to any consumer group, this is uncommon, so for most of the chapter we will assume
    the consumer is part of a group.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 还有第四个属性，虽然不是严格必需的，但非常常用。该属性是`group.id`，它指定`Kafka``Consumer`实例所属的消费者组。虽然可以创建不属于任何消费者组的消费者，但这是不常见的，因此在本章的大部分内容中，我们将假设消费者是消费者组的一部分。
- en: 'The following code snippet shows how to create a `KafkaConsumer`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了如何创建`KafkaConsumer`：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Most of what you see here should be familiar if you’ve read [Chapter 3](ch03.html#writing_messages_to_kafka)
    on creating producers. We assume that the records we consume will have `String`
    objects as both the key and the value of the record. The only new property here
    is `group.id`, which is the name of the consumer group this consumer belongs to.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您阅读过[第3章](ch03.html#writing_messages_to_kafka)中关于创建生产者的内容，那么您在这里看到的大部分内容应该是熟悉的。我们假设我们消费的记录将作为记录的键和值都是`String`对象。这里唯一的新属性是`group.id`，它是这个消费者所属的消费者组的名称。
- en: Subscribing to Topics
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 订阅主题
- en: 'Once we create a consumer, the next step is to subscribe to one or more topics.
    The `subscribe()` method takes a list of topics as a parameter, so it’s pretty
    simple to use:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 创建消费者后，下一步是订阅一个或多个主题。`subscribe()`方法接受一个主题列表作为参数，所以使用起来非常简单：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO1-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO1-1)'
- en: 'Here we simply create a list with a single element: the topic name `customerCountries`.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是创建了一个包含单个元素的列表：主题名称`customerCountries`。
- en: It is also possible to call `subscribe` with a regular expression. The expression
    can match multiple topic names, and if someone creates a new topic with a name
    that matches, a rebalance will happen almost immediately and the consumers will
    start consuming from the new topic. This is useful for applications that need
    to consume from multiple topics and can handle the different types of data the
    topics will contain. Subscribing to multiple topics using a regular expression
    is most commonly used in applications that replicate data between Kafka and another
    system or streams processing applications.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用正则表达式调用`subscribe`。表达式可以匹配多个主题名称，如果有人创建了一个与名称匹配的新主题，重新平衡将几乎立即发生，消费者将开始从新主题中消费。这对需要从多个主题中消费并且可以处理主题将包含的不同类型数据的应用程序非常有用。使用正则表达式订阅多个主题最常用于在Kafka和另一个系统之间复制数据或流处理应用程序的应用程序。
- en: 'For example, to subscribe to all test topics, we can call:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要订阅所有测试主题，我们可以调用：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Warning
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If your Kafka cluster has large number of partitions, perhaps 30,000 or more,
    you should be aware that the filtering of topics for the subscription is done
    on the client side. This means that when you subscribe to a subset of topics via
    a regular expression rather than via an explicit list, the consumer will request
    the list of all topics and their partitions from the broker in regular intervals.
    The client will then use this list to detect new topics that it should include
    in its subscription and subscribe to them. When the topic list is large and there
    are many consumers, the size of the list of topics and partitions is significant,
    and the regular expression subscription has significant overhead on the broker,
    client, and network. There are cases where the bandwidth used by the topic metadata
    is larger than the bandwidth used to send data. This also means that in order
    to subscribe with a regular expression, the client needs permissions to describe
    all topics in the cluster—that is, a full `describe` grant on the entire cluster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的Kafka集群有大量分区，可能是30,000个或更多，您应该知道订阅主题的过滤是在客户端上完成的。这意味着当您通过正则表达式订阅主题的子集而不是通过显式列表时，消费者将定期请求来自代理的所有主题及其分区的列表。然后客户端将使用此列表来检测应该包括在其订阅中的新主题并订阅它们。当主题列表很大且有许多消费者时，主题和分区列表的大小是显著的，并且正则表达式订阅对代理、客户端和网络有显著的开销。有些情况下，主题元数据使用的带宽大于发送数据使用的带宽。这也意味着为了使用正则表达式订阅，客户端需要有权限描述集群中的所有主题——也就是说，对整个集群的完整`describe`授权。
- en: The Poll Loop
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮询循环
- en: 'At the heart of the Consumer API is a simple loop for polling the server for
    more data. The main body of a consumer will look as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者API的核心是一个简单的循环，用于从服务器轮询更多数据。消费者的主体将如下所示：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-1)'
- en: This is indeed an infinite loop. Consumers are usually long-running applications
    that continuously poll Kafka for more data. We will show later in the chapter
    how to cleanly exit the loop and close the consumer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个无限循环。消费者通常是长时间运行的应用程序，不断地轮询Kafka以获取更多数据。我们将在本章后面展示如何清洁地退出循环并关闭消费者。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-2)'
- en: This is the most important line in the chapter. The same way that sharks must
    keep moving or they die, consumers must keep polling Kafka or they will be considered
    dead and the partitions they are consuming will be handed to another consumer
    in the group to continue consuming. The parameter we pass to `poll()` is a timeout
    interval and controls how long `poll()` will block if data is not available in
    the consumer buffer. If this is set to 0 or if there are records available already,
    `poll()` will return immediately; otherwise, it will wait for the specified number
    of milliseconds.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章中最重要的一行。就像鲨鱼必须不断移动，否则它们就会死一样，消费者必须不断轮询Kafka，否则它们将被视为死亡，并且它们正在消费的分区将被交给组中的另一个消费者继续消费。我们传递给`poll()`的参数是超时间隔，控制如果消费者缓冲区中没有数据，`poll()`将阻塞多长时间。如果设置为0或者已经有记录可用，`poll()`将立即返回；否则，它将等待指定的毫秒数。
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-3)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-3)'
- en: '`poll()` returns a list of records. Each record contains the topic and partition
    the record came from, the offset of the record within the partition, and, of course,
    the key and the value of the record. Typically, we want to iterate over the list
    and process the records individually.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`poll()`返回一个记录列表。每个记录包含记录来自的主题和分区，记录在分区内的偏移量，当然还有记录的键和值。通常，我们希望遍历列表并逐个处理记录。'
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-4)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO2-4)'
- en: Processing usually ends in writing a result in a data store or updating a stored
    record. Here, the goal is to keep a running count of customers from each country,
    so we update a hash table and print the result as JSON. A more realistic example
    would store the updates result in a data store.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 处理通常以在数据存储中写入结果或更新存储的记录结束。在这里，目标是保持每个国家客户的运行计数，因此我们更新哈希表并将结果打印为JSON。一个更现实的例子会将更新的结果存储在数据存储中。
- en: The `poll` loop does a lot more than just get data. The first time you call
    `poll()` with a new consumer, it is responsible for finding the `GroupCoordinator`,
    joining the consumer group, and receiving a partition assignment. If a rebalance
    is triggered, it will be handled inside the poll loop as well, including related
    callbacks. This means that almost everything that can go wrong with a consumer
    or in the callbacks used in its listeners is likely to show up as an exception
    thrown by `poll()`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`poll`循环做的远不止获取数据。第一次使用新的消费者调用`poll()`时，它负责找到`GroupCoordinator`，加入消费者组，并接收分区分配。如果触发了重新平衡，它也将在`poll`循环中处理，包括相关的回调。这意味着几乎所有可能出错的消费者或其监听器中使用的回调都可能显示为`poll()`抛出的异常。'
- en: Keep in mind that if `poll()` is not invoked for longer than `max.poll.interval.ms`,
    the consumer will be considered dead and evicted from the consumer group, so avoid
    doing anything that can block for unpredictable intervals inside the poll loop.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果`poll()`的调用时间超过`max.poll.interval.ms`，则消费者将被视为死亡并从消费者组中驱逐，因此避免在`poll`循环内部阻塞不可预测的时间。
- en: Thread Safety
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程安全
- en: You can’t have multiple consumers that belong to the same group in one thread,
    and you can’t have multiple threads safely use the same consumer. One consumer
    per thread is the rule. To run multiple consumers in the same group in one application,
    you will need to run each in its own thread. It is useful to wrap the consumer
    logic in its own object and then use Java’s `ExecutorService` to start multiple
    threads, each with its own consumer. The Confluent blog has a [tutorial](https://oreil.ly/8YOVe)
    that shows how to do just that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能在一个线程中拥有属于同一组的多个消费者，也不能安全地让多个线程使用同一个消费者。每个线程一个消费者是规则。要在同一应用程序中运行同一组中的多个消费者，您需要在每个消费者中运行各自的线程。将消费者逻辑封装在自己的对象中，然后使用Java的`ExecutorService`启动多个线程，每个线程都有自己的消费者是很有用的。Confluent博客有一个[教程](https://oreil.ly/8YOVe)展示了如何做到这一点。
- en: Warning
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In older versions of Kafka, the full method signature was `poll(long)`; this
    signature is now deprecated and the new API is `poll(Duration)`. In addition to
    the change of argument type, the semantics of how the method blocks subtly changed.
    The original method, `poll(long)`, will block as long as it takes to get the needed
    metadata from Kafka, even if this is longer than the timeout duration. The new
    method, `poll(Duration)`, will adhere to the timeout restrictions and not wait
    for metadata. If you have existing consumer code that uses `poll(0)` as a method
    to force Kafka to get the metadata without consuming any records (a rather common
    hack), you can’t just change it to `poll(Duration.ofMillis(0))` and expect the
    same behavior. You’ll need to figure out a new way to achieve your goals. Often
    the solution is placing the logic in the `rebalanceListener.onPartitionAssignment()`
    method, which is guaranteed to get called after you have metadata for the assigned
    partitions but before records start arriving. Another solution was documented
    by Jesse Anderson in his blog post [“Kafka’s Got a Brand-New Poll”](https://oreil.ly/zN6ek).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧版本的Kafka中，完整的方法签名是`poll(long)`；这个签名现在已经被弃用，新的API是`poll(Duration)`。除了参数类型的更改，方法阻塞的语义也略有变化。原始方法`poll(long)`将阻塞，直到从Kafka获取所需的元数据，即使这比超时持续时间更长。新方法`poll(Duration)`将遵守超时限制，不会等待元数据。如果您有现有的消费者代码，使用`poll(0)`作为一种强制Kafka获取元数据而不消耗任何记录的方法（这是一种相当常见的黑客行为），您不能只是将其更改为`poll(Duration.ofMillis(0))`并期望相同的行为。您需要找出一种新的方法来实现您的目标。通常的解决方案是将逻辑放在`rebalanceListener.onPartitionAssignment()`方法中，在分配分区的元数据后但记录开始到达之前，这个方法保证会被调用。Jesse
    Anderson在他的博客文章[“Kafka’s Got a Brand-New Poll”](https://oreil.ly/zN6ek)中也记录了另一种解决方案。
- en: Another approach can be to have one consumer populate a queue of events and
    have multiple worker threads perform work from this queue. You can see an example
    of this pattern in a [blog post](https://oreil.ly/uMzj1) from Igor Buzatović.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是让一个消费者填充一个事件队列，并让多个工作线程从这个队列中执行工作。您可以在[Igor Buzatović的博客文章](https://oreil.ly/uMzj1)中看到这种模式的示例。
- en: Configuring Consumers
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置消费者
- en: So far we have focused on learning the Consumer API, but we’ve only looked at
    a few of the configuration properties—just the mandatory `bootstrap.servers`,
    `group.id`, `key.deserializer`, and `value.deserializer`. All of the consumer
    configuration is documented in the [Apache Kafka documentation](https://oreil.ly/Y00Gl).
    Most of the parameters have reasonable defaults and do not require modification,
    but some have implications on the performance and availability of the consumers.
    Let’s take a look at some of the more important properties.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经专注于学习消费者 API，但我们只看了一些配置属性——只是强制性的`bootstrap.servers`、`group.id`、`key.deserializer`和`value.deserializer`。所有的消费者配置都在[Apache
    Kafka文档](https://oreil.ly/Y00Gl)中有记录。大多数参数都有合理的默认值，不需要修改，但有些对消费者的性能和可用性有影响。让我们来看看一些更重要的属性。
- en: fetch.min.bytes
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fetch.min.bytes
- en: This property allows a consumer to specify the minimum amount of data that it
    wants to receive from the broker when fetching records, by default one byte. If
    a broker receives a request for records from a consumer but the new records amount
    to fewer bytes than `fetch.min.bytes`, the broker will wait until more messages
    are available before sending the records back to the consumer. This reduces the
    load on both the consumer and the broker, as they have to handle fewer back-and-forth
    messages in cases where the topics don’t have much new activity (or for lower-activity
    hours of the day). You will want to set this parameter higher than the default
    if the consumer is using too much CPU when there isn’t much data available, or
    reduce load on the brokers when you have a large number of consumers—although
    keep in mind that increasing this value can increase latency for low-throughput
    cases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性允许消费者指定从代理获取记录时要接收的最小数据量，默认为一个字节。如果代理收到来自消费者的记录请求，但新的记录量少于`fetch.min.bytes`，代理将等待更多消息可用后再将记录发送回消费者。这减少了在主题没有太多新活动（或者在一天中活动较少的时间）的情况下，消费者和代理处理来回消息的负载。如果消费者在没有太多数据可用时使用了太多
    CPU，或者在有大量消费者时减少了对代理的负载，您将希望将此参数设置得比默认值更高——尽管请记住，增加此值可能会增加低吞吐量情况下的延迟。
- en: fetch.max.wait.ms
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fetch.max.wait.ms
- en: By setting `fetch.min.bytes`, you tell Kafka to wait until it has enough data
    to send before responding to the consumer. `fetch.max.wait.ms` lets you control
    how long to wait. By default, Kafka will wait up to 500 ms. This results in up
    to 500 ms of extra latency in case there is not enough data flowing to the Kafka
    topic to satisfy the minimum amount of data to return. If you want to limit the
    potential latency (usually due to SLAs controlling the maximum latency of the
    application), you can set `fetch.max.wait.ms` to a lower value. If you set `fetch.max.wait.ms`
    to 100 ms and `fetch.min.bytes` to 1 MB, Kafka will receive a fetch request from
    the consumer and will respond with data either when it has 1 MB of data to return
    or after 100 ms, whichever happens first.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`fetch.min.bytes`，您告诉 Kafka 在响应消费者之前等待足够的数据。`fetch.max.wait.ms`允许您控制等待的时间。默认情况下，Kafka
    将等待最多500毫秒。如果 Kafka 主题中没有足够的数据流动以满足返回的最小数据量，这将导致最多500毫秒的额外延迟。如果要限制潜在的延迟（通常是由 SLA
    控制应用程序的最大延迟引起的），可以将`fetch.max.wait.ms`设置为较低的值。如果将`fetch.max.wait.ms`设置为100毫秒，并将`fetch.min.bytes`设置为1
    MB，Kafka 将从消费者接收获取请求，并在有1 MB数据要返回或者在100毫秒后，以先到者为准，响应数据。
- en: fetch.max.bytes
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fetch.max.bytes
- en: This property lets you specify the maximum bytes that Kafka will return whenever
    the consumer polls a broker (50 MB by default). It is used to limit the size of
    memory that the consumer will use to store data that was returned from the server,
    irrespective of how many partitions or messages were returned. Note that records
    are sent to the client in batches, and if the first record-batch that the broker
    has to send exceeds this size, the batch will be sent and the limit will be ignored.
    This guarantees that the consumer can continue making progress. It’s worth noting
    that there is a matching broker configuration that allows the Kafka administrator
    to limit the maximum fetch size as well. The broker configuration can be useful
    because requests for large amounts of data can result in large reads from disk
    and long sends over the network, which can cause contention and increase load
    on the broker.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性允许您指定 Kafka 在消费者轮询代理时将返回的最大字节数（默认为50 MB）。它用于限制消费者用于存储从服务器返回的数据的内存大小，而不管返回了多少个分区或消息。请注意，记录是以批量形式发送到客户端的，如果代理必须发送的第一个记录批次超过了这个大小，批次将被发送并且限制将被忽略。这保证了消费者可以继续取得进展。值得注意的是，还有一个匹配的代理配置，允许
    Kafka 管理员限制最大获取大小。代理配置可能很有用，因为对大量数据的请求可能导致从磁盘读取大量数据，并且在网络上传输时间较长，这可能会导致争用并增加代理的负载。
- en: max.poll.records
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.poll.records
- en: This property controls the maximum number of records that a single call to `poll()`
    will return. Use this to control the amount of data (but not the size of data)
    your application will need to process in one iteration of the poll loop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性控制了单次`poll()`调用将返回的最大记录数。使用它来控制应用程序在一次轮询循环中需要处理的数据量（但不是数据大小）。
- en: max.partition.fetch.bytes
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max.partition.fetch.bytes
- en: This property controls the maximum number of bytes the server will return per
    partition (1 MB by default). When `KafkaConsumer.poll()` returns `ConsumerRecords`,
    the record object will use at most `max.partition.fetch.bytes` per partition assigned
    to the consumer. Note that controlling memory usage using this configuration can
    be quite complex, as you have no control over how many partitions will be included
    in the broker response. Therefore, we highly recommend using `fetch.max.bytes`
    instead, unless you have special reasons to try and process similar amounts of
    data from each partition.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性控制服务器每个分区返回的最大字节数（默认为 1 MB）。当 `KafkaConsumer.poll()` 返回 `ConsumerRecords`
    时，记录对象将使用分配给消费者的每个分区的最多 `max.partition.fetch.bytes`。请注意，使用这个配置来控制内存使用可能非常复杂，因为您无法控制经纪人响应中将包括多少分区。因此，我们强烈建议使用
    `fetch.max.bytes`，除非您有特殊原因要尝试从每个分区处理类似数量的数据。
- en: session.timeout.ms and heartbeat.interval.ms
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 会话超时时间和心跳间隔时间
- en: The amount of time a consumer can be out of contact with the brokers while still
    considered alive defaults to 10 seconds. If more than `session.timeout.ms` passes
    without the consumer sending a heartbeat to the group coordinator, it is considered
    dead and the group coordinator will trigger a rebalance of the consumer group
    to allocate partitions from the dead consumer to the other consumers in the group.
    This property is closely related to `heartbeat.interval.ms`, which controls how
    frequently the Kafka consumer will send a heartbeat to the group coordinator,
    whereas `ses⁠sion.timeout.ms` controls how long a consumer can go without sending
    a heartbeat. Therefore, those two properties are typically modified together—`heartbeat.​interval.ms`
    must be lower than `session.timeout.ms` and is usually set to one-third of the
    timeout value. So if `session.timeout.ms` is 3 seconds, `heartbeat.​inter⁠val.ms`
    should be 1 second. Setting `session.timeout.ms` lower than the default will allow
    consumer groups to detect and recover from failure sooner but may also cause unwanted
    rebalances. Setting `session.timeout.ms` higher will reduce the chance of accidental
    rebalance but also means it will take longer to detect a real failure.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者在与经纪人失去联系时被认为仍然存活的时间默认为 10 秒。如果消费者在没有向组协调器发送心跳的情况下经过了超过 `session.timeout.ms`
    的时间，那么它被认为已经死亡，组协调器将触发消费者组的重新平衡，将死亡消费者的分区分配给组中的其他消费者。这个属性与 `heartbeat.interval.ms`
    密切相关，它控制 Kafka 消费者向组协调器发送心跳的频率，而 `session.timeout.ms` 控制消费者可以多久不发送心跳。因此，这两个属性通常一起修改——`heartbeat.​interval.ms`
    必须低于 `session.timeout.ms`，通常设置为超时值的三分之一。因此，如果 `session.timeout.ms` 是 3 秒，`heartbeat.​inter⁠val.ms`
    应该是 1 秒。将 `session.timeout.ms` 设置得比默认值低将允许消费者组更快地检测和从故障中恢复，但也可能导致不必要的重新平衡。将 `session.timeout.ms`
    设置得更高将减少意外重新平衡的机会，但也意味着检测真正故障的时间会更长。
- en: max.poll.interval.ms
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大轮询间隔时间
- en: This property lets you set the length of time during which the consumer can
    go without polling before it is considered dead. As mentioned earlier, heartbeats
    and session timeouts are the main mechanism by which Kafka detects dead consumers
    and takes their partitions away. However, we also mentioned that heartbeats are
    sent by a background thread. There is a possibility that the main thread consuming
    from Kafka is deadlocked, but the background thread is still sending heartbeats.
    This means that records from partitions owned by this consumer are not being processed.
    The easiest way to know whether the consumer is still processing records is to
    check whether it is asking for more records. However, the intervals between requests
    for more records are difficult to predict and depend on the amount of available
    data, the type of processing done by the consumer, and sometimes on the latency
    of additional services. In applications that need to do time-consuming processing
    on each record that is returned, `max.poll.records` is used to limit the amount
    of data returned and therefore limit the duration before the application is available
    to `poll()` again. Even with `max.poll.records` defined, the interval between
    calls to `poll()` is difficult to predict, and `max.poll.interval.ms` is used
    as a fail-safe or backstop. It has to be an interval large enough that it will
    very rarely be reached by a healthy consumer but low enough to avoid significant
    impact from a hanging consumer. The default value is 5 minutes. When the timeout
    is hit, the background thread will send a “leave group” request to let the broker
    know that the consumer is dead and the group must rebalance, and then stop sending
    heartbeats.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性允许您设置消费者在轮询之前可以多久不进行轮询被认为已经死亡的时间。正如前面提到的，心跳和会话超时是 Kafka 检测死亡消费者并夺走它们分区的主要机制。然而，我们也提到了心跳是由后台线程发送的。主线程从
    Kafka 消费可能被死锁，但后台线程仍在发送心跳。这意味着由该消费者拥有的分区的记录没有被处理。了解消费者是否仍在处理记录的最简单方法是检查它是否正在请求更多记录。然而，请求更多记录之间的间隔很难预测，取决于可用数据的数量，消费者所做的处理类型，有时还取决于其他服务的延迟。在需要对返回的每条记录进行耗时处理的应用程序中，`max.poll.records`
    用于限制返回的数据量，从而限制应用程序在再次可用于 `poll()` 之前的持续时间。即使定义了 `max.poll.records`，`poll()` 调用之间的间隔也很难预测，`max.poll.interval.ms`
    用作故障安全或后备。它必须是足够大的间隔，以至于健康的消费者很少会达到，但又足够低，以避免悬挂消费者的重大影响。默认值为 5 分钟。当超时时，后台线程将发送“离开组”请求，让经纪人知道消费者已经死亡，组必须重新平衡，然后停止发送心跳。
- en: default.api.timeout.ms
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 默认 API 超时时间
- en: This is the timeout that will apply to (almost) all API calls made by the consumer
    when you don’t specify an explicit timeout while calling the API. The default
    is 1 minute, and since it is higher than the request timeout default, it will
    include a retry when needed. The notable exception to APIs that use this default
    is the `poll()` method that always requires an explicit timeout.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个超时时间，适用于消费者在调用API时没有指定显式超时的情况下（几乎）所有API调用。默认值为1分钟，由于它高于请求超时的默认值，因此在需要时会包括重试。使用此默认值的API的一个显着例外是始终需要显式超时的`poll()`方法。
- en: request.timeout.ms
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: request.timeout.ms
- en: This is the maximum amount of time the consumer will wait for a response from
    the broker. If the broker does not respond within this time, the client will assume
    the broker will not respond at all, close the connection, and attempt to reconnect.
    This configuration defaults to 30 seconds, and it is recommended not to lower
    it. It is important to leave the broker with enough time to process the request
    before giving up—there is little to gain by resending requests to an already overloaded
    broker, and the act of disconnecting and reconnecting adds even more overhead.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是消费者等待来自代理的响应的最长时间。如果代理在此时间内没有响应，客户端将假定代理根本不会响应，关闭连接并尝试重新连接。此配置默认为30秒，建议不要降低它。在放弃之前，留给代理足够的时间来处理请求非常重要——重新发送请求到已经过载的代理并没有太多好处，而断开连接和重新连接会增加更多的开销。
- en: auto.offset.reset
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: auto.offset.reset
- en: This property controls the behavior of the consumer when it starts reading a
    partition for which it doesn’t have a committed offset, or if the committed offset
    it has is invalid (usually because the consumer was down for so long that the
    record with that offset was already aged out of the broker). The default is “latest,”
    which means that lacking a valid offset, the consumer will start reading from
    the newest records (records that were written after the consumer started running).
    The alternative is “earliest,” which means that lacking a valid offset, the consumer
    will read all the data in the partition, starting from the very beginning. Setting
    `auto.offset.reset` to `none` will cause an exception to be thrown when attempting
    to consume from an invalid offset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此属性控制消费者在开始读取没有提交偏移量的分区时的行为，或者如果其已提交的偏移量无效（通常是因为消费者停机时间太长，以至于该偏移量的记录已经从代理中删除）。默认值为“latest”，这意味着在缺少有效偏移量时，消费者将从最新的记录开始读取（在消费者开始运行后编写的记录）。另一种选择是“earliest”，这意味着在缺少有效偏移量时，消费者将从分区中读取所有数据，从最开始开始。将`auto.offset.reset`设置为`none`将导致在尝试从无效偏移量处消费时抛出异常。
- en: enable.auto.commit
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: enable.auto.commit
- en: This parameter controls whether the consumer will commit offsets automatically,
    and defaults to `true`. Set it to `false` if you prefer to control when offsets
    are committed, which is necessary to minimize duplicates and avoid missing data.
    If you set `enable.auto.commit` to `true`, then you might also want to control
    how frequently offsets will be committed using `auto.commit.interval.ms`. We’ll
    discuss the different options for committing offsets in more depth later in this
    chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数控制消费者是否会自动提交偏移量，默认为`true`。如果您希望控制何时提交偏移量，以最小化重复数据和避免丢失数据，则将其设置为`false`是必要的。如果将`enable.auto.commit`设置为`true`，则可能还希望使用`auto.commit.interval.ms`控制偏移量的提交频率。我们将在本章后面更深入地讨论提交偏移量的不同选项。
- en: partition.assignment.strategy
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: partition.assignment.strategy
- en: 'We learned that partitions are assigned to consumers in a consumer group. A
    `PartitionAssignor` is a class that, given consumers and topics they subscribed
    to, decides which partitions will be assigned to which consumer. By default, Kafka
    has the following assignment strategies:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到分区是分配给消费者组中的消费者的。`PartitionAssignor`是一个类，它根据消费者和它们订阅的主题决定将哪些分区分配给哪些消费者。默认情况下，Kafka具有以下分配策略：
- en: Range
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Range
- en: Assigns to each consumer a consecutive subset of partitions from each topic
    it subscribes to. So if consumers C1 and C2 are subscribed to two topics, T1 and
    T2, and each of the topics has three partitions, then C1 will be assigned partitions
    0 and 1 from topics T1 and T2, while C2 will be assigned partition 2 from those
    topics. Because each topic has an uneven number of partitions and the assignment
    is done for each topic independently, the first consumer ends up with more partitions
    than the second. This happens whenever Range assignment is used and the number
    of consumers does not divide the number of partitions in each topic neatly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个消费者分配其订阅主题的连续子集的分区。因此，如果消费者C1和C2订阅了两个主题T1和T2，并且每个主题都有三个分区，那么C1将从主题T1和T2分配分区0和1，而C2将从这些主题分配分区2。由于每个主题的分区数量不均匀，并且分配是针对每个主题独立进行的，第一个消费者最终会比第二个消费者拥有更多的分区。当使用Range分配并且消费者的数量不能完全整除每个主题的分区数量时，就会发生这种情况。
- en: RoundRobin
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: RoundRobin
- en: Takes all the partitions from all subscribed topics and assigns them to consumers
    sequentially, one by one. If C1 and C2 described previously used RoundRobin assignment,
    C1 would have partitions 0 and 2 from topic T1, and partition 1 from topic T2\.
    C2 would have partition 1 from topic T1, and partitions 0 and 2 from topic T2\.
    In general, if all consumers are subscribed to the same topics (a very common
    scenario), RoundRobin assignment will end up with all consumers having the same
    number of partitions (or at most one partition difference).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从所有订阅的主题中获取所有分区，并将它们依次分配给消费者。如果先前描述的C1和C2使用了RoundRobin分配，C1将从主题T1获取分区0和2，从主题T2获取分区1。C2将从主题T1获取分区1，从主题T2获取分区0和2。通常情况下，如果所有消费者都订阅了相同的主题（这是一个非常常见的场景），RoundRobin分配将导致所有消费者拥有相同数量的分区（或者最多有一个分区的差异）。
- en: Sticky
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Sticky
- en: 'The Sticky Assignor has two goals: the first is to have an assignment that
    is as balanced as possible, and the second is that in case of a rebalance, it
    will leave as many assignments as possible in place, minimizing the overhead associated
    with moving partition assignments from one consumer to another. In the common
    case where all consumers are subscribed to the same topic, the initial assignment
    from the Sticky Assignor will be as balanced as that of the RoundRobin Assignor.
    Subsequent assignments will be just as balanced but will reduce the number of
    partition movements. In cases where consumers in the same group subscribe to different
    topics, the assignment achieved by Sticky Assignor is more balanced than that
    of the RoundRobin Assignor.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 粘性分配器有两个目标：第一个是尽可能平衡的分配，第二个是在重新平衡的情况下，尽可能保留尽可能多的分配，最小化将分区分配从一个消费者移动到另一个消费者所带来的开销。在所有消费者都订阅相同主题的常见情况下，粘性分配器的初始分配将与RoundRobin分配器一样平衡。后续的分配将同样平衡，但会减少分区移动的数量。在同一组中的消费者订阅不同主题的情况下，粘性分配器实现的分配比RoundRobin分配器更加平衡。
- en: Cooperative Sticky
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 合作粘性
- en: This assignment strategy is identical to that of the Sticky Assignor but supports
    cooperative rebalances in which consumers can continue consuming from the partitions
    that are not reassigned. See [“Consumer Groups and Partition Rebalance”](#partition-rebalances)
    to read more about cooperative rebalancing, and note that if you are upgrading
    from a version older than 2.3, you’ll need to follow a specific upgrade path in
    order to enable the cooperative sticky assignment strategy, so pay extra attention
    to the [upgrade guide](https://oreil.ly/klMI6).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分配策略与粘性分配器相同，但支持合作重新平衡，消费者可以继续从未重新分配的分区中消费。请参阅“消费者组和分区重新平衡”以了解更多关于合作重新平衡的信息，并注意，如果您正在从早于2.3版本升级，您需要按照特定的升级路径来启用合作粘性分配策略，因此请特别注意升级指南。
- en: The `partition.assignment.strategy` allows you to choose a partition assignment
    strategy. The default is `org.apache.kafka.clients.consumer.RangeAssignor`, which
    implements the Range strategy described earlier. You can replace it with `org.apache.kafka.clients.consumer.RoundRobinAssignor`,
    `org.apache.kafka.​clients.consumer.StickyAssignor`, or `org.apache.kafka.clients.consumer.​CooperativeStickyAssignor`.
    A more advanced option is to implement your own assignment strategy, in which
    case `partition.assignment.strategy` should point to the name of your class.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`partition.assignment.strategy`允许您选择分区分配策略。默认值为`org.apache.kafka.clients.consumer.RangeAssignor`，它实现了前面描述的Range策略。您可以将其替换为`org.apache.kafka.clients.consumer.RoundRobinAssignor`、`org.apache.kafka.​clients.consumer.StickyAssignor`或`org.apache.kafka.clients.consumer.​CooperativeStickyAssignor`。更高级的选项是实现自己的分配策略，在这种情况下，`partition.assignment.strategy`应指向您的类的名称。'
- en: client.id
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: client.id
- en: This can be any string, and will be used by the brokers to identify requests
    sent from the client, such as fetch requests. It is used in logging and metrics,
    and for quotas.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是任何字符串，并将被代理用于识别从客户端发送的请求，例如获取请求。它用于记录和度量，以及配额。
- en: client.rack
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: client.rack
- en: By default, consumers will fetch messages from the leader replica of each partition.
    However, when the cluster spans multiple datacenters or multiple cloud availability
    zones, there are advantages both in performance and in cost to fetching messages
    from a replica that is located in the same zone as the consumer. To enable fetching
    from the closest replica, you need to set the `client.rack` configuration and
    identify the zone in which the client is located. Then you can configure the brokers
    to replace the default `replica.selector.class` with `org.apache.kafka.common.replica.RackAwareReplicaSelector`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，消费者将从每个分区的领导副本获取消息。但是，当集群跨越多个数据中心或多个云可用区时，从与消费者位于同一区域的副本获取消息在性能和成本上都有优势。要启用从最近的副本获取消息，您需要设置`client.rack`配置，并标识客户端所在的区域。然后，您可以配置代理将默认的`replica.selector.class`替换为`org.apache.kafka.common.replica.RackAwareReplicaSelector`。
- en: You can also implement your own `replica.selector.class` with custom logic for
    choosing the best replica to consume from, based on client metadata and partition
    metadata.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用自己的`replica.selector.class`实现自定义逻辑，根据客户端元数据和分区元数据选择最佳副本进行消费。
- en: group.instance.id
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: group.instance.id
- en: This can be any unique string and is used to provide a consumer with [static
    group membership](#static-group-membership).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是任何唯一的字符串，用于提供消费者静态组成员身份。
- en: receive.buffer.bytes and send.buffer.bytes
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: receive.buffer.bytes和send.buffer.bytes
- en: These are the sizes of the TCP send and receive buffers used by the sockets
    when writing and reading data. If these are set to –1, the OS defaults will be
    used. It can be a good idea to increase these when producers or consumers communicate
    with brokers in a different datacenter, because those network links typically
    have higher latency and lower bandwidth.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在写入和读取数据时套接字使用的TCP发送和接收缓冲区的大小。如果将它们设置为-1，则将使用操作系统默认值。当生产者或消费者与不同数据中心的代理进行通信时，增加这些值可能是个好主意，因为这些网络链接通常具有更高的延迟和较低的带宽。
- en: offsets.retention.minutes
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: offsets.retention.minutes
- en: This is a broker configuration, but it is important to be aware of it due to
    its impact on consumer behavior. As long as a consumer group has active members
    (i.e., members that are actively maintaining membership in the group by sending
    heartbeats), the last offset committed by the group for each partition will be
    retained by Kafka, so it can be retrieved in case of reassignment or restart.
    However, once a group becomes empty, Kafka will only retain its committed offsets
    to the duration set by this configuration—7 days by default. Once the offsets
    are deleted, if the group becomes active again it will behave like a brand-new
    consumer group with no memory of anything it consumed in the past. Note that this
    behavior changed a few times, so if you use versions older than 2.1.0, check the
    documentation for your version for the expected behavior.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个代理配置，但由于对消费者行为的影响，了解这一点很重要。只要消费者组有活跃成员（即通过发送心跳来积极维护组成员资格的成员），组对每个分区提交的最后偏移量将被Kafka保留，以便在重新分配或重新启动时检索。然而，一旦组变为空，Kafka只会保留其提交的偏移量到此配置设置的持续时间——默认为7天。一旦偏移量被删除，如果组再次变为活跃，它将表现得像一个全新的消费者组，对其过去消费的任何内容一无所知。请注意，这种行为已经多次更改，因此如果您使用的是早于2.1.0版本的版本，请查看您版本的文档以了解预期的行为。
- en: Commits and Offsets
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交和偏移量
- en: Whenever we call `poll()`, it returns records written to Kafka that consumers
    in our group have not read yet. This means that we have a way of tracking which
    records were read by a consumer of the group. As discussed before, one of Kafka’s
    unique characteristics is that it does not track acknowledgments from consumers
    the way many JMS queues do. Instead, it allows consumers to use Kafka to track
    their position (offset) in each partition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们调用`poll()`时，它会返回写入Kafka的记录，这些记录消费者组中的消费者尚未读取。这意味着我们有一种跟踪消费者组中哪些记录被消费者读取的方法。正如前面讨论的，Kafka的一个独特特性是它不像许多JMS队列那样跟踪消费者的确认。相反，它允许消费者使用Kafka来跟踪它们在每个分区中的位置（偏移量）。
- en: We call the action of updating the current position in the partition an `offset`
    `commit`. Unlike traditional message queues, Kafka does not commit records individually.
    Instead, consumers commit the last message they’ve successfully processed from
    a partition and implicitly assume that every message before the last was also
    successfully processed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称更新分区中的当前位置为`偏移量提交`。与传统的消息队列不同，Kafka不会逐个提交记录。相反，消费者提交他们从分区中成功处理的最后一条消息，并隐含地假设在最后一条消息之前的每条消息也都被成功处理。
- en: How does a consumer commit an offset? It sends a message to Kafka, which updates
    a special ``__consumer_offsets`` topic with the committed offset for each partition.
    As long as all your consumers are up, running, and churning away, this will have
    no impact. However, if a consumer crashes or a new consumer joins the consumer
    group, this will *trigger a rebalance*. After a rebalance, each consumer may be
    assigned a new set of partitions than the one it processed before. In order to
    know where to pick up the work, the consumer will read the latest committed offset
    of each partition and continue from there.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者如何提交偏移量？它向Kafka发送一条消息，更新一个特殊的`__consumer_offsets`主题，其中包含每个分区的提交偏移量。只要您的所有消费者都在运行并不断工作，这不会产生影响。但是，如果一个消费者崩溃或新的消费者加入消费者组，这将*触发重新平衡*。重新平衡后，每个消费者可能被分配一个新的分区集，而不是之前处理的分区集。为了知道从哪里开始工作，消费者将读取每个分区的最新提交偏移量，并从那里继续。
- en: If the committed offset is smaller than the offset of the last message the client
    processed, the messages between the last processed offset and the committed offset
    will be processed twice. See [Figure 4-8](#reprocessed_offsets).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提交的偏移量小于客户端处理的最后一条消息的偏移量，那么在最后处理的偏移量和提交的偏移量之间的所有消息将被处理两次。参见[图4-8](#reprocessed_offsets)。
- en: '![kdg2 0408](assets/kdg2_0408.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0408](assets/kdg2_0408.png)'
- en: Figure 4-8\. Reprocessed messages
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8。重新处理的消息
- en: If the committed offset is larger than the offset of the last message the client
    actually processed, all messages between the last processed offset and the committed
    offset will be missed by the consumer group. See [Figure 4-9](#missed_messages_offsets).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提交的偏移量大于客户端实际处理的最后一条消息的偏移量，那么在最后处理的偏移量和提交的偏移量之间的所有消息都将被消费者组错过。参见[图4-9](#missed_messages_offsets)。
- en: '![kdg2 0409](assets/kdg2_0409.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0409](assets/kdg2_0409.png)'
- en: Figure 4-9\. Missed messages between offsets
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9。偏移量之间的遗漏消息
- en: Clearly, managing offsets has a big impact on the client application. The `KafkaConsumer`
    API provides multiple ways of committing offsets.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，管理偏移量对客户端应用程序有很大影响。`KafkaConsumer` API提供了多种提交偏移量的方式。
- en: Which Offset Is Committed?
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交的是哪个偏移量？
- en: When committing offsets either automatically or without specifying the intended
    offsets, the default behavior is to commit the offset after the last offset that
    was returned by `poll()`. This is important to keep in mind when attempting to
    manually commit specific offsets or seek to commit specific offsets. However,
    it is also tedious to repeatedly read “Commit the offset that is one larger than
    the last offset the client received from `poll()`,” and 99% of the time it does
    not matter. So, we are going to write “Commit the last offset” when we refer to
    the default behavior, and if you need to manually manipulate offsets, please keep
    this note in mind.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动提交偏移量或者没有指定预期偏移量时，默认行为是在`poll()`返回的最后一个偏移量之后提交偏移量。在尝试手动提交特定偏移量或寻求提交特定偏移量时，这一点很重要。然而，反复阅读“提交比客户端从`poll()`接收到的最后一个偏移量大一”的说明也很繁琐，而且99%的情况下并不重要。因此，当我们提到默认行为时，我们将写“提交最后一个偏移量”，如果您需要手动操作偏移量，请记住这一点。
- en: Automatic Commit
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动提交
- en: The easiest way to commit offsets is to allow the consumer to do it for you.
    If you configure `enable.auto.commit=true`, then every five seconds the consumer
    will commit the latest offset that your client received from `poll()`. The five-second
    interval is the default and is controlled by setting `auto.commit.interval.ms`.
    Just like everything else in the consumer, the automatic commits are driven by
    the poll loop. Whenever you poll, the consumer checks if it is time to commit,
    and if it is, it will commit the offsets it returned in the last poll.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Before using this convenient option, however, it is important to understand
    the consequences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Consider that, by default, automatic commits occur every five seconds. Suppose
    that we are three seconds after the most recent commit our consumer crashed. After
    the rebalancing, the surviving consumers will start consuming the partitions that
    were previously owned by the crashed broker. But they will start from the last
    offset committed. In this case, the offset is three seconds old, so all the events
    that arrived in those three seconds will be processed twice. It is possible to
    configure the commit interval to commit more frequently and reduce the window
    in which records will be duplicated, but it is impossible to completely eliminate
    them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: With autocommit enabled, when it is time to commit offsets, the next poll will
    commit the last offset returned by the previous poll. It doesn’t know which events
    were actually processed, so it is critical to always process all the events returned
    by `poll()` before calling `poll()` again. (Just like `poll()`, `close()` also
    commits offsets automatically.) This is usually not an issue, but pay attention
    when you handle exceptions or exit the poll loop prematurely.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Automatic commits are convenient, but they don’t give developers enough control
    to avoid duplicate messages.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Commit Current Offset
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most developers exercise more control over the time at which offsets are committed—both
    to eliminate the possibility of missing messages and to reduce the number of messages
    duplicated during rebalancing. The Consumer API has the option of committing the
    current offset at a point that makes sense to the application developer rather
    than based on a timer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: By setting `enable.auto.commit=false`, offsets will only be committed when the
    application explicitly chooses to do so. The simplest and most reliable of the
    commit APIs is `commitSync()`. This API will commit the latest offset returned
    by `poll()` and return once the offset is committed, throwing an exception if
    the commit fails for some reason.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that `commitSync()` will commit the latest offset
    returned by `poll()`, so if you call `commitSync()` before you are done processing
    all the records in the collection, you risk missing the messages that were committed
    but not processed, in case the application crashes. If the application crashes
    while it is still processing records in the collection, all the messages from
    the beginning of the most recent batch until the time of the rebalance will be
    processed twice—this may or may not be preferable to missing messages.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we would use `commitSync` to commit offsets after we finished processing
    the latest batch of messages:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO3-1)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that by printing the contents of a record, we are done processing
    it. Your application will likely do a lot more with the records—modify them, enrich
    them, aggregate them, display them on a dashboard, or notify users of important
    events. You should determine when you are “done” with a record according to your
    use case.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO3-2)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Once we are done “processing” all the records in the current batch, we call
    `commitSync` to commit the last offset in the batch, before polling for additional
    messages.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO3-3)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`commitSync` retries committing as long as there is no error that can’t be
    recovered. If this happens, there is not much we can do except log an error.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`commitSync`在没有无法恢复的错误时重试提交。如果发生这种情况，除了记录错误外，我们无能为力。'
- en: Asynchronous Commit
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步提交
- en: One drawback of manual commit is that the application is blocked until the broker
    responds to the commit request. This will limit the throughput of the application.
    Throughput can be improved by committing less frequently, but then we are increasing
    the number of potential duplicates that a rebalance may create.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 手动提交的一个缺点是应用程序会被阻塞，直到代理响应提交请求。这将限制应用程序的吞吐量。通过减少提交的频率可以提高吞吐量，但这样会增加重新平衡可能创建的潜在重复数量。
- en: 'Another option is the asynchronous commit API. Instead of waiting for the broker
    to respond to a commit, we just send the request and continue on:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是异步提交API。我们不等待代理响应提交，只是发送请求并继续进行：
- en: '[PRE5]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO4-1)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO4-1)'
- en: Commit the last offset and carry on.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 提交最后的偏移量并继续进行。
- en: The drawback is that while `commitSync()` will retry the commit until it either
    succeeds or encounters a nonretriable failure, `commitAsync()` will not retry.
    The reason it does not retry is that by the time `commitAsync()` receives a response
    from the server, there may have been a later commit that was already successful.
    Imagine that we sent a request to commit offset 2000\. There is a temporary communication
    problem, so the broker never gets the request and therefore never responds. Meanwhile,
    we processed another batch and successfully committed offset 3000\. If `commit​Async()`
    now retries the previously failed commit, it might succeed in committing offset
    2000 *after* offset 3000 was already processed and committed. In the case of a
    rebalance, this will cause more duplicates.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是，虽然`commitSync()`会重试提交，直到成功或遇到不可重试的失败，但`commitAsync()`不会重试。它不重试的原因是，当`commitAsync()`从服务器接收到响应时，可能已经有一个后续的提交成功了。假设我们发送了一个提交偏移量2000的请求。出现了临时通信问题，因此代理从未收到请求，也从未响应。与此同时，我们处理了另一个批次，并成功提交了偏移量3000。如果`commitAsync()`现在重试之前失败的提交，它可能会在处理和提交偏移量3000之后成功提交偏移量2000。在重新平衡的情况下，这将导致更多的重复。
- en: 'We mention this complication and the importance of correct order of commits
    because `commitAsync()` also gives you an option to pass in a callback that will
    be triggered when the broker responds. It is common to use the callback to log
    commit errors or to count them in a metric, but if you want to use the callback
    for retries, you need to be aware of the problem with commit order:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到这个复杂性和正确的提交顺序的重要性，因为`commitAsync()`还提供了一个选项，可以传递一个回调，当代理响应时将触发该回调。通常使用回调来记录提交错误或在指标中计数，但如果要使用回调进行重试，就需要注意提交顺序的问题。
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO5-1)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO5-1)'
- en: We send the commit and carry on, but if the commit fails, the failure and the
    offsets will be logged.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发送提交并继续进行，但如果提交失败，将记录失败和偏移量。
- en: Retrying Async Commits
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重试异步提交
- en: A simple pattern to get the commit order right for asynchronous retries is to
    use a monotonically increasing sequence number. Increase the sequence number every
    time you commit, and add the sequence number at the time of the commit to the
    `commitAsync` callback. When you’re getting ready to send a retry, check if the
    commit sequence number the callback got is equal to the instance variable; if
    it is, there was no newer commit and it is safe to retry. If the instance sequence
    number is higher, don’t retry because a newer commit was already sent.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确处理异步重试的提交顺序，一个简单的模式是使用单调递增的序列号。每次提交时增加序列号，并在提交时将序列号添加到`commitAsync`回调中。当准备发送重试时，检查回调得到的提交序列号是否等于实例变量；如果是，表示没有更新的提交，可以安全地重试。如果实例序列号更高，则不要重试，因为已经发送了更新的提交。
- en: Combining Synchronous and Asynchronous Commits
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合同步和异步提交
- en: Normally, occasional failures to commit without retrying are not a huge problem
    because if the problem is temporary, the following commit will be successful.
    But if we know that this is the last commit before we close the consumer, or before
    a rebalance, we want to make extra sure that the commit succeeds.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，偶尔的提交失败而不重试并不是一个很大的问题，因为如果问题是暂时的，后续的提交将成功。但是，如果我们知道这是在关闭消费者之前的最后一次提交，或者在重新平衡之前，我们希望确保提交成功。
- en: 'Therefore, a common pattern is to combine `commitAsync()` with `commitSync()`
    just before shutdown. Here is how it works (we will discuss how to commit just
    before rebalance when we get to the section about rebalance listeners):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个常见的模式是在关闭之前将`commitAsync()`与`commitSync()`结合在一起。下面是它的工作原理（当我们讨论重新平衡监听器时，我们将讨论如何在重新平衡之前提交）：
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO6-1)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO6-1)'
- en: While everything is fine, we use `commitAsync`. It is faster, and if one commit
    fails, the next commit will serve as a retry.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一切正常时，我们使用`commitAsync`。它更快，如果一个提交失败，下一个提交将作为重试。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO6-2)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO6-2)'
- en: But if we are closing, there is no “next commit.” We call `commitSync()`, because
    it will retry until it succeeds or suffers unrecoverable failure.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们正在关闭，就没有“下一个提交”。我们调用`commitSync()`，因为它会重试直到成功或遇到无法恢复的失败。
- en: Committing a Specified Offset
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定偏移量
- en: Committing the latest offset only allows you to commit as often as you finish
    processing batches. But what if you want to commit more frequently than that?
    What if `poll()` returns a huge batch and you want to commit offsets in the middle
    of the batch to avoid having to process all those rows again if a rebalance occurs?
    You can’t just call `commitSync()` or `commitAsync()`—this will commit the last
    offset returned, which you didn’t get to process yet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 仅提交最新的偏移量只允许您在完成处理批次时提交。但是，如果您想更频繁地提交呢？如果`poll()`返回一个巨大的批次，并且您希望在批次中间提交偏移量，以避免在重新平衡发生时再次处理所有这些行怎么办？您不能只调用`commitSync()`或`commitAsync()`——这将提交您尚未处理的最后一个偏移量。
- en: Fortunately, the Consumer API allows you to call `commitSync()` and `commitAsync()`
    and pass a map of partitions and offsets that you wish to commit. If you are in
    the middle of processing a batch of records, and the last message you got from
    partition 3 in topic “customers” has offset 5000, you can call `commitSync()`
    to commit offset 5001 for partition 3 in topic “customers.” Since your consumer
    may be consuming more than a single partition, you will need to track offsets
    on all of them, which adds complexity to your code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，消费者API允许您调用`commitSync()`和`commitAsync()`并传递您希望提交的分区和偏移量的映射。如果您正在处理一批记录，并且您从主题“customers”的分区3中得到的最后一条消息的偏移量为5000，您可以调用`commitSync()`来提交主题“customers”中分区3的偏移量5001。由于您的消费者可能会消费多个分区，因此您需要跟踪所有分区的偏移量，这会增加代码的复杂性。
- en: 'Here is what a commit of specific offsets looks like:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提交特定偏移量的样子：
- en: '[PRE8]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-1)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-1)'
- en: This is the map we will use to manually track offsets.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将用于手动跟踪偏移量的映射。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-2)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-2)'
- en: Remember, `println` is a stand-in for whatever processing you do for the records
    you consume.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`println`是您为消耗的记录执行的任何处理的替代品。
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-3)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-3)'
- en: After reading each record, we update the offsets map with the offset of the
    next message we expect to process. The committed offset should always be the offset
    of the next message that your application will read. This is where we’ll start
    reading next time we start.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取每条记录后，我们使用下一条消息的偏移量更新偏移量映射。提交的偏移量应始终是您的应用程序将读取的下一条消息的偏移量。这是我们下次开始阅读的地方。
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-4)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-4)'
- en: Here, we decide to commit current offsets every 1,000 records. In your application,
    you can commit based on time or perhaps content of the records.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们决定每1,000条记录提交当前偏移量。在您的应用程序中，您可以根据时间或记录的内容进行提交。
- en: '[![5](assets/5.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-5)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_kafka_consumers__reading_data_from_kafka_CO7-5)'
- en: I chose to call `commitAsync()` (without a callback, therefore the second parameter
    is `null`), but `commitSync()` is also completely valid here. Of course, when
    committing specific offsets you still need to perform all the error handling we’ve
    seen in previous sections.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择调用`commitAsync()`（没有回调，因此第二个参数是`null`），但在这里也完全有效的是`commitSync()`。当然，当提交特定的偏移量时，您仍然需要执行我们在前几节中看到的所有错误处理。
- en: Rebalance Listeners
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新平衡监听器
- en: As we mentioned in the previous section about committing offsets, a consumer
    will want to do some cleanup work before exiting and also before partition rebalancing.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节关于提交偏移量中提到的，消费者在退出之前和分区重新平衡之前都希望进行一些清理工作。
- en: If you know your consumer is about to lose ownership of a partition, you will
    want to commit offsets of the last event you’ve processed. Perhaps you also need
    to close file handles, database connections, and such.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您知道您的消费者即将失去对分区的所有权，您将希望提交您已处理的最后一个事件的偏移量。也许您还需要关闭文件句柄、数据库连接等。
- en: 'The Consumer API allows you to run your own code when partitions are added
    or removed from the consumer. You do this by passing a `ConsumerRebalanceListener`
    when calling the `subscribe()` method we discussed previously. `ConsumerRebalanceListener`
    has three methods you can implement:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者API允许您在从消费者中添加或删除分区时运行自己的代码。您可以通过在调用我们之前讨论的`subscribe()`方法时传递`ConsumerRebalanceListener`来实现这一点。`ConsumerRebalanceListener`有三种方法可以实现：
- en: '`public void onPartitionsAssigned(Collection<TopicPartition> partitions)`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`public void onPartitionsAssigned(Collection<TopicPartition> partitions)`'
- en: Called after partitions have been reassigned to the consumer but before the
    consumer starts consuming messages. This is where you prepare or load any state
    that you want to use with the partition, seek to the correct offsets if needed,
    or similar. Any preparation done here should be guaranteed to return within `max.poll.timeout.ms`
    so the consumer can successfully join the group.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在将分区重新分配给消费者但在消费者开始消费消息之前调用。这是您准备或加载要与分区一起使用的任何状态，如果需要，寻找正确的偏移量或类似操作的地方。在这里做的任何准备工作都应该保证在`max.poll.timeout.ms`内返回，以便消费者可以成功加入组。
- en: '`public void onPartitionsRevoked(Collection<TopicPartition> partitions)`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`public void onPartitionsRevoked(Collection<TopicPartition> partitions)`'
- en: Called when the consumer has to give up partitions that it previously owned—either
    as a result of a rebalance or when the consumer is being closed. In the common
    case, when an eager rebalancing algorithm is used, this method is invoked before
    the rebalancing starts and after the consumer stopped consuming messages. If a
    cooperative rebalancing algorithm is used, this method is invoked at the end of
    the rebalance, with just the subset of partitions that the consumer has to give
    up. This is where you want to commit offsets, so whoever gets this partition next
    will know where to start.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当消费者必须放弃之前拥有的分区时调用——无论是作为重新平衡的结果还是消费者被关闭的结果。在通常情况下，当使用急切的重新平衡算法时，这个方法在重新平衡开始之前和消费者停止消费消息之后被调用。如果使用合作式重新平衡算法，这个方法在重新平衡结束时被调用，只有消费者必须放弃的分区的子集。这是你想要提交偏移量的地方，所以下一个得到这个分区的人将知道从哪里开始。
- en: '`public void onPartitionsLost(Collection<TopicPartition> partitions)`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`public void onPartitionsLost(Collection<TopicPartition> partitions)`'
- en: Only called when a cooperative rebalancing algorithm is used, and only in exceptional
    cases where the partitions were assigned to other consumers without first being
    revoked by the rebalance algorithm (in normal cases, `onPartitions​Revoked()`
    will be called). This is where you clean up any state or resources that are used
    with these partitions. Note that this has to be done carefully—the new owner of
    the partitions may have already saved its own state, and you’ll need to avoid
    conflicts. Note that if you don’t implement this method, `onPartitions​Revoked()`
    will be called instead.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在使用合作式重新平衡算法，并且在分区被重新分配给其他消费者之前没有被重新平衡算法撤销的异常情况下才会被调用（在正常情况下，将会调用`onPartitions​Revoked()`）。这是你清理任何与这些分区使用的状态或资源的地方。请注意，这必须小心进行——分区的新所有者可能已经保存了自己的状态，你需要避免冲突。请注意，如果你不实现这个方法，将会调用`onPartitions​Revoked()`。
- en: Tip
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you use a cooperative rebalancing algorithm, note that:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用合作式重新平衡算法，请注意：
- en: '`onPartitionsAssigned()` will be invoked on every rebalance, as a way of notifying
    the consumer that a rebalance happened. However, if there are no new partitions
    assigned to the consumer, it will be called with an empty collection.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onPartitionsAssigned()`将在每次重新平衡时被调用，作为通知消费者重新平衡发生的方式。然而，如果没有新的分区分配给消费者，它将被调用并传入一个空集合。'
- en: '`onPartitionsRevoked()` will be invoked in normal rebalancing conditions, but
    only if the consumer gave up the ownership of partitions. It will not be called
    with an empty collection.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onPartitionsRevoked()`将在正常的重新平衡条件下被调用，但只有在消费者放弃分区所有权时才会被调用。它不会被传入一个空集合。'
- en: '`onPartitionsLost()` will be invoked in exceptional rebalancing conditions,
    and the partitions in the collection will already have new owners by the time
    the method is invoked.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onPartitionsLost()`将在异常的重新平衡条件下被调用，而在方法被调用时，集合中的分区已经有了新的所有者。'
- en: If you implemented all three methods, you are guaranteed that during a normal
    rebalance, `onPartitionsAssigned()` will be called by the new owner of the partitions
    that are reassigned only after the previous owner completed `onPartitionsRevoked()`
    and gave up its ownership.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你实现了所有三种方法，你可以确保在正常的重新平衡过程中，`onPartitionsAssigned()`将被重新分配的分区的新所有者调用，只有在之前的所有者完成了`onPartitionsRevoked()`并放弃了它的所有权之后才会被调用。
- en: 'This example will show how to use `onPartitionsRevoked()` to commit offsets
    before losing ownership of a partition:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将展示如何使用`onPartitionsRevoked()`在失去分区所有权之前提交偏移量：
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-1)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-1)'
- en: We start by implementing a `ConsumerRebalanceListener`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实现一个`ConsumerRebalanceListener`。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-2)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-2)'
- en: In this example we don’t need to do anything when we get a new partition; we’ll
    just start consuming messages.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，当我们获得一个新的分区时，我们不需要做任何事情；我们将直接开始消费消息。
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-3)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-3)'
- en: However, when we are about to lose a partition due to rebalancing, we need to
    commit offsets. We are committing offsets for all partitions, not just the partitions
    we are about to lose—because the offsets are for events that were already processed,
    there is no harm in that. And we are using `commitSync()` to make sure the offsets
    are committed before the rebalance proceeds.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们即将因重新平衡而失去一个分区时，我们需要提交偏移量。我们为所有分区提交偏移量，而不仅仅是我们即将失去的分区——因为这些偏移量是已经处理过的事件，所以没有害处。我们使用`commitSync()`来确保在重新平衡进行之前提交偏移量。
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-4)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO8-4)'
- en: 'The most important part: pass the `ConsumerRebalanceListener` to the `subscribe()`
    method so it will get invoked by the consumer.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的部分：将`ConsumerRebalanceListener`传递给`subscribe()`方法，这样它将被消费者调用。
- en: Consuming Records with Specific Offsets
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特定偏移量消费记录
- en: So far we’ve seen how to use `poll()` to start consuming messages from the last
    committed offset in each partition and to proceed in processing all messages in
    sequence. However, sometimes you want to start reading at a different offset.
    Kafka offers a variety of methods that cause the next `poll()` to start consuming
    in a different offset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何使用`poll()`从每个分区的最后提交的偏移量开始消费消息，并按顺序处理所有消息。然而，有时你想从不同的偏移量开始读取。Kafka提供了各种方法，可以使下一个`poll()`从不同的偏移量开始消费。
- en: 'If you want to start reading all messages from the beginning of the partition,
    or you want to skip all the way to the end of the partition and start consuming
    only new messages, there are APIs specifically for that: `seekToBeginning(Collection<TopicPartition>
    tp)` and `seekToEnd(Collection<TopicPartition> tp)`.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从分区的开头开始读取所有消息，或者你想跳过到分区的末尾并只开始消费新消息，有专门的API可以实现：`seekToBeginning(Collection<TopicPartition>
    tp)`和`seekToEnd(Collection<TopicPartition> tp)`。
- en: The Kafka API also lets you seek a specific offset. This ability can be used
    in a variety of ways; for example, a time-sensitive application could skip ahead
    a few records when falling behind, or a consumer that writes data to a file could
    be reset back to a specific point in time in order to recover data if the file
    was lost.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka API还允许您寻找特定的偏移量。这种能力可以以各种方式使用；例如，一个时间敏感的应用程序在落后时可以跳过几条记录，或者将数据写入文件的消费者可以在特定时间点重置以恢复数据，如果文件丢失。
- en: 'Here’s a quick example of how to set the current offset on all partitions to
    records that were produced at a specific point in time:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何将所有分区的当前偏移量设置为特定时间点产生的记录的快速示例：
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-1)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-1)'
- en: We create a map from all the partitions assigned to this consumer (via `consumer.assignment()`)
    to the timestamp we wanted to revert the consumers to.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从分配给该消费者的所有分区（通过`consumer.assignment()`）创建一个映射，以便将消费者恢复到我们想要的时间戳。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-2)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-2)'
- en: Then we get the offsets that were current at these timestamps. This method sends
    a request to the broker where a timestamp index is used to return the relevant
    offsets.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到了这些时间戳的当前偏移量。这种方法会向经纪人发送一个请求，其中时间戳索引用于返回相关的偏移量。
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-3)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO9-3)'
- en: Finally, we reset the offset on each partition to the offset that was returned
    in the previous step.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将每个分区的偏移量重置为上一步返回的偏移量。
- en: But How Do We Exit?
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但是我们如何退出？
- en: Earlier in this chapter, when we discussed the poll loop, we told you not to
    worry about the fact that the consumer polls in an infinite loop, and that we
    would discuss how to exit the loop cleanly. So, let’s discuss how to exit cleanly.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，当我们讨论轮询循环时，我们告诉您不要担心消费者在无限循环中轮询的事实，并且我们将讨论如何干净地退出循环。因此，让我们讨论如何干净地退出。
- en: When you decide to shut down the consumer, and you want to exit immediately
    even though the consumer may be waiting on a long `poll()`, you will need another
    thread to call `consumer.wakeup()`. If you are running the consumer loop in the
    main thread, this can be done from `ShutdownHook`. Note that `consumer.wakeup()`
    is the only consumer method that is safe to call from a different thread. Calling
    `wakeup` will cause `poll()` to exit with `WakeupException`, or if `consumer.wakeup()`
    was called while the thread was not waiting on poll, the exception will be thrown
    on the next iteration when `poll()` is called. The `WakeupException` doesn’t need
    to be handled, but before exiting the thread, you must call `consumer.close()`.
    Closing the consumer will commit offsets if needed and will send the group coordinator
    a message that the consumer is leaving the group. The consumer coordinator will
    trigger rebalancing immediately, and you won’t need to wait for the session to
    timeout before partitions from the consumer you are closing will be assigned to
    another consumer in the group.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当您决定关闭消费者，并且希望立即退出，即使消费者可能正在等待长时间的`poll()`，您需要另一个线程调用`consumer.wakeup()`。如果您在主线程中运行消费者循环，可以从`ShutdownHook`中完成。请注意，`consumer.wakeup()`是唯一可以从不同线程调用的消费者方法。调用`wakeup`将导致`poll()`以`WakeupException`退出，或者如果在线程不在等待`poll`时调用了`consumer.wakeup()`，则在下一次迭代调用`poll()`时将抛出异常。不需要处理`WakeupException`，但在退出线程之前，您必须调用`consumer.close()`。关闭消费者将提交偏移量（如果需要），并向组协调器发送一条消息，说明消费者正在离开该组。消费者协调器将立即触发重新平衡，您无需等待会话超时，以便将要关闭的消费者的分区分配给组中的另一个消费者。
- en: 'Here is what the exit code will look like if the consumer is running in the
    main application thread. This example is a bit truncated, but you can view the
    full example [on GitHub](http://bit.ly/2u47e9A):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者在主应用程序线程中运行，退出代码将如下所示。这个例子有点截断，但你可以在[GitHub](http://bit.ly/2u47e9A)上查看完整的例子：
- en: '[PRE11]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-1)'
- en: '`ShutdownHook` runs in a separate thread, so the only safe action you can take
    is to call `wakeup` to break out of the `poll` loop.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`ShutdownHook`在单独的线程中运行，因此您可以采取的唯一安全操作是调用`wakeup`以退出`poll`循环。'
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-2)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-2)'
- en: A particularly long poll timeout. If the poll loop is short enough and you don’t
    mind waiting a bit before exiting, you don’t need to call `wakeup`—just checking
    an atomic boolean in each iteration would be enough. Long poll timeouts are useful
    when consuming low-throughput topics; this way, the client uses less CPU for constantly
    looping while the broker has no new data to return.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 特别长的轮询超时。如果轮询循环足够短，而且您不介意在退出之前等待一会儿，您不需要调用`wakeup`——只需在每次迭代中检查原子布尔值就足够了。长轮询超时在消费低吞吐量主题时非常有用；这样，客户端在经纪人没有新数据返回时使用更少的CPU不断循环。
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-3)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-3)'
- en: Another thread calling `wakeup` will cause poll to throw a `WakeupException`.
    You’ll want to catch the exception to make sure your application doesn’t exit
    unexpectedly, but there is no need to do anything with it.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个调用`wakeup`的线程将导致`poll`抛出`WakeupException`。您需要捕获异常以确保应用程序不会意外退出，但不需要对其进行任何处理。
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-4)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO10-4)'
- en: Before exiting the consumer, make sure you close it cleanly.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在退出消费者之前，请确保干净地关闭它。
- en: Deserializers
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反序列化器
- en: As discussed in the previous chapter, Kafka producers require *serializers*
    to convert objects into byte arrays that are then sent to Kafka. Similarly, Kafka
    consumers require *deserializers* to convert byte arrays received from Kafka into
    Java objects. In previous examples, we just assumed that both the key and the
    value of each message are strings, and we used the default `StringDeserializer`
    in the consumer configuration.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所讨论的，Kafka生产者需要*序列化器*将对象转换为字节数组，然后将其发送到Kafka。同样，Kafka消费者需要*反序列化器*将从Kafka接收的字节数组转换为Java对象。在先前的示例中，我们只假设每条消息的键和值都是字符串，并且在消费者配置中使用了默认的`StringDeserializer`。
- en: In [Chapter 3](ch03.html#writing_messages_to_kafka) about the Kafka producer,
    we saw how to serialize custom types and how to use Avro and `AvroSerializers`
    to generate Avro objects from schema definitions and then serialize them when
    producing messages to Kafka. We will now look at how to create custom deserializers
    for your own objects and how to use Avro and its deserializers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#writing_messages_to_kafka)中关于Kafka生产者，我们看到了如何序列化自定义类型以及如何使用Avro和`AvroSerializers`从模式定义生成Avro对象，然后在生产消息到Kafka时对它们进行序列化。我们现在将看看如何为您自己的对象创建自定义反序列化器以及如何使用Avro及其反序列化器。
- en: It should be obvious that the serializer used to produce events to Kafka must
    match the deserializer that will be used when consuming events. Serializing with
    `IntSerializer` and then deserializing with `StringDeserializer` will not end
    well. This means that, as a developer, you need to keep track of which serializers
    were used to write into each topic and make sure each topic only contains data
    that the deserializers you use can interpret. This is one of the benefits of using
    Avro and the Schema Registry for serializing and deserializing—the `AvroSerializer`
    can make sure that all the data written to a specific topic is compatible with
    the schema of the topic, which means it can be deserialized with the matching
    deserializer and schema. Any errors in compatibility—on the producer or the consumer
    side—will be caught easily with an appropriate error message, which means you
    will not need to try to debug byte arrays for serialization errors.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，用`IntSerializer`进行序列化然后用`StringDeserializer`进行反序列化将不会有好结果。这意味着作为开发人员，您需要跟踪用于写入每个主题的序列化器，并确保每个主题只包含您使用的反序列化器可以解释的数据。这是使用Avro和模式注册表进行序列化和反序列化的好处之一——`AvroSerializer`可以确保写入特定主题的所有数据与主题的模式兼容，这意味着它可以与匹配的反序列化器和模式进行反序列化。生产者或消费者端的任何兼容性错误都将很容易地通过适当的错误消息捕获，这意味着您不需要尝试调试字节数组以解决序列化错误。
- en: We will start by quickly showing how to write a custom deserializer, even though
    this is the less common method, and then we will move on to an example of how
    to use Avro to deserialize message keys and values.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先快速展示如何编写自定义反序列化器，尽管这是较少见的方法，然后我们将继续介绍如何使用Avro来反序列化消息键和值的示例。
- en: Custom Deserializers
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义反序列化器
- en: 'Let’s take the same custom object we serialized in [Chapter 3](ch03.html#writing_messages_to_kafka)
    and write a deserializer for it:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用在[第3章](ch03.html#writing_messages_to_kafka)中序列化的相同自定义对象，并为其编写一个反序列化器：
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The custom deserializer will look as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义反序列化器将如下所示：
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO11-1)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO11-1)'
- en: The consumer also needs the implementation of the `Customer` class, and both
    the class and the serializer need to match on the producing and consuming applications.
    In a large organization with many consumers and producers sharing access to the
    data, this can become challenging.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者还需要`Customer`类的实现，类和序列化器在生产和消费应用程序中需要匹配。在一个拥有许多消费者和生产者共享数据访问权限的大型组织中，这可能会变得具有挑战性。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO11-2)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO11-2)'
- en: We are just reversing the logic of the serializer here—we get the customer ID
    and name out of the byte array and use them to construct the object we need.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里只是颠倒了序列化器的逻辑——我们从字节数组中获取客户ID和名称，并使用它们构造我们需要的对象。
- en: 'The consumer code that uses this deserializer will look similar to this example:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此反序列化器的消费者代码将类似于以下示例：
- en: '[PRE14]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Again, it is important to note that implementing a custom serializer and deserializer
    is not recommended. It tightly couples producers and consumers and is fragile
    and error prone. A better solution would be to use a standard message format,
    such as JSON, Thrift, Protobuf, or Avro. We’ll now see how to use Avro deserializers
    with the Kafka consumer. For background on Apache Avro, its schemas, and schema-compatibility
    capabilities, refer back to [Chapter 3](ch03.html#writing_messages_to_kafka).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，不建议实现自定义序列化器和反序列化器。它会紧密耦合生产者和消费者，并且容易出错。更好的解决方案是使用标准消息格式，如JSON、Thrift、Protobuf或Avro。现在我们将看看如何在Kafka消费者中使用Avro反序列化器。有关Apache
    Avro、其模式和模式兼容性能力的背景，请参阅[第3章](ch03.html#writing_messages_to_kafka)。
- en: Using Avro Deserialization with Kafka Consumer
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Avro反序列化与Kafka消费者
- en: 'Let’s assume we are using the implementation of the `Customer` class in Avro
    that was shown in [Chapter 3](ch03.html#writing_messages_to_kafka). In order to
    consume those objects from Kafka, you want to implement a consuming application
    similar to this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在使用在[第3章](ch03.html#writing_messages_to_kafka)中展示的Avro中的`Customer`类的实现。为了从Kafka中消费这些对象，您需要实现类似于以下的消费应用程序：
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-1)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-1)'
- en: We use `KafkaAvroDeserializer` to deserialize the Avro messages.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`KafkaAvroDeserializer`来反序列化Avro消息。
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-2)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-2)'
- en: '`schema.registry.url` is a new parameter. This simply points to where we store
    the schemas. This way, the consumer can use the schema that was registered by
    the producer to deserialize the message.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-3)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: We specify the generated class, `Customer`, as the type for the record value.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_kafka_consumers__reading_data_from_kafka_CO12-4)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '`record.value()` is a `Customer` instance, and we can use it accordingly.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Standalone Consumer: Why and How to Use a Consumer Without a Group'
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed consumer groups, which are where partitions are assigned
    automatically to consumers and are rebalanced automatically when consumers are
    added or removed from the group. Typically, this behavior is just what you want,
    but in some cases you want something much simpler. Sometimes you know you have
    a single consumer that always needs to read data from all the partitions in a
    topic, or from a specific partition in a topic. In this case, there is no reason
    for groups or rebalances—just assign the consumer-specific topic and/or partitions,
    consume messages, and commit offsets on occasion (although you still need to configure
    `group.id` to commit offsets, without calling *subscribe* the consumer won’t join
    any group).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: When you know exactly which partitions the consumer should read, you don’t *subscribe*
    to a topic—instead, you *assign* yourself a few partitions. A consumer can either
    subscribe to topics (and be part of a consumer group) or assign itself partitions,
    but not both at the same time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how a consumer can assign itself all partitions of a
    specific topic and consume from them:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_kafka_consumers__reading_data_from_kafka_CO13-1)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: We start by asking the cluster for the partitions available in the topic. If
    you only plan on consuming a specific partition, you can skip this part.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_kafka_consumers__reading_data_from_kafka_CO13-2)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Once we know which partitions we want, we call `assign()` with the list.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Other than the lack of rebalances and the need to manually find the partitions,
    everything else is business as usual. Keep in mind that if someone adds new partitions
    to the topic, the consumer will not be notified. You will need to handle this
    by checking `consumer.partitionsFor()` periodically or simply by bouncing the
    application whenever partitions are added.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with an in-depth explanation of Kafka’s consumer groups
    and the way they allow multiple consumers to share the work of reading events
    from topics. We followed the theoretical discussion with a practical example of
    a consumer subscribing to a topic and continuously reading events. We then looked
    into the most important consumer configuration parameters and how they affect
    consumer behavior. We dedicated a large part of the chapter to discussing offsets
    and how consumers keep track of them. Understanding how consumers commit offsets
    is critical when writing reliable consumers, so we took time to explain the different
    ways this can be done. We then discussed additional parts of the Consumer APIs,
    handling rebalances, and closing the consumer.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: We concluded by discussing the deserializers used by consumers to turn bytes
    stored in Kafka into Java objects that the applications can process. We discussed
    Avro deserializers in some detail, even though they are just one type of deserializer
    you can use, because these are most commonly used with Kafka.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.html#idm45351109673920-marker)) Diagrams by Sophie Blee-Goldman,
    from her May 2020 blog post, [“From Eager to Smarter in Apache Kafka Consumer
    Rebalances”](https://oreil.ly/fZzac).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL

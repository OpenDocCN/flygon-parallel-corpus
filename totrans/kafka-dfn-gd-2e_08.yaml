- en: Chapter 6\. Kafka Internals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。Kafka内部
- en: 'It is not strictly necessary to understand Kafka’s internals in order to run
    Kafka in production or write applications that use it. However, knowing how Kafka
    works does provide context when troubleshooting or trying to understand why Kafka
    behaves the way it does. Since covering every single implementation detail and
    design decision is beyond the scope of this book, in this chapter we focus on
    a few topics that are especially relevant to Kafka practitioners:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 要在生产环境中运行Kafka或编写使用Kafka的应用程序并不一定需要严格了解Kafka的内部工作原理。然而，了解Kafka的工作方式确实在故障排除或尝试理解Kafka行为的原因时提供了背景。由于本书的范围无法涵盖每一个实现细节和设计决策，因此在本章中，我们专注于一些对Kafka从业者特别相关的主题：
- en: Kafka controller
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka控制器
- en: How Kafka replication works
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka复制是如何工作的
- en: How Kafka handles requests from producers and consumers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka如何处理来自生产者和消费者的请求
- en: How Kafka handles storage, such as file format and indexes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka如何处理存储，比如文件格式和索引
- en: Understanding these topics in-depth will be especially useful when tuning Kafka—understanding
    the mechanisms that the tuning knobs control goes a long way toward using them
    with precise intent rather than fiddling with them randomly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解这些主题在调整Kafka时将会特别有用——了解调整旋钮控制的机制对于有意识地使用它们而不是随机摆弄它们有很大帮助。
- en: Cluster Membership
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群成员资格
- en: Kafka uses Apache ZooKeeper to maintain the list of brokers that are currently
    members of a cluster. Every broker has a unique identifier that is either set
    in the broker configuration file or automatically generated. Every time a broker
    process starts, it registers itself with its ID in ZooKeeper by creating an [*ephemeral
    node*](http://bit.ly/2s3MYHh). Kafka brokers, the controller, and some of the
    ecosystem tools subscribe to the */brokers/ids* path in ZooKeeper where brokers
    are registered so that they get notified when brokers are added or removed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用Apache ZooKeeper来维护当前集群成员的代理列表。每个代理都有一个唯一的标识符，可以在代理配置文件中设置，也可以自动生成。每次代理进程启动时，它都会通过在ZooKeeper中创建一个[*临时节点*](http://bit.ly/2s3MYHh)来注册自己的ID。Kafka代理、控制器和一些生态系统工具订阅ZooKeeper中的*/brokers/ids*路径，代理在这里注册，以便在代理被添加或移除时得到通知。
- en: If you try to start another broker with the same ID, you will get an error—the
    new broker will try to register but fail because we already have a ZooKeeper node
    for the same broker ID.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试使用相同的ID启动另一个代理，你将会收到一个错误——新代理将尝试注册但失败，因为我们已经有了相同代理ID的ZooKeeper节点。
- en: When a broker loses connectivity to ZooKeeper (usually as a result of the broker
    stopping, but this can also happen as a result of network partition or a long
    garbage-collection pause), the ephemeral node that the broker created when starting
    will be automatically removed from ZooKeeper. Kafka components that are watching
    the list of brokers will be notified that the broker is gone.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理失去与ZooKeeper的连接（通常是由于代理停止，但这也可能是由于网络分区或长时间的垃圾回收暂停导致的），代理在启动时创建的临时节点将自动从ZooKeeper中删除。监视代理列表的Kafka组件将收到通知，表示代理已经离开。
- en: Even though the node representing the broker is gone when the broker is stopped,
    the broker ID still exists in other data structures. For example, the list of
    replicas of each topic (see [“Replication”](#replication)) contains the broker
    IDs for the replica. This way, if you completely lose a broker and start a brand-new
    broker with the ID of the old one, it will immediately join the cluster in place
    of the missing broker with the same partitions and topics assigned to it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 即使代理停止时代表代理的节点消失了，代理ID仍然存在于其他数据结构中。例如，每个主题的副本列表（参见[“复制”](#replication)）包含了副本的代理ID。这样，如果你完全丢失了一个代理并启动一个具有相同ID的全新代理，它将立即加入集群，取代缺失的代理，并分配给它相同的分区和主题。
- en: The Controller
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制器
- en: The controller is one of the Kafka brokers that, in addition to the usual broker
    functionality, is responsible for electing partition leaders. The first broker
    that starts in the cluster becomes the controller by creating an ephemeral node
    in ZooKeeper called `/controller`. When other brokers start, they also try to
    create this node but receive a “node already exists” exception, which causes them
    to “realize” that the controller node already exists and that the cluster already
    has a controller. The brokers create a [*ZooKeeper watch*](http://bit.ly/2sKoTTN)
    on the controller node so they get notified of changes to this node. This way,
    we guarantee that the cluster will only have one controller at a time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器是Kafka代理之一，除了通常的代理功能外，还负责选举分区领导者。在集群中启动的第一个代理成为控制器，通过在ZooKeeper中创建一个名为`/controller`的临时节点来实现。当其他代理启动时，它们也会尝试创建这个节点，但会收到“节点已经存在”的异常，这会导致它们“意识到”控制器节点已经存在，集群已经有了一个控制器。代理在控制器节点上创建一个[*ZooKeeper
    watch*](http://bit.ly/2sKoTTN)，以便在此节点发生变化时得到通知。这样，我们保证集群一次只有一个控制器。
- en: When the controller broker is stopped or loses connectivity to ZooKeeper, the
    ephemeral node will disappear. This includes any scenario in which the ZooKeeper
    client used by the controller stops sending heartbeats to ZooKeeper for longer
    than `zookeeper.session.timeout.ms`. When the ephemeral node disappears, other
    brokers in the cluster will be notified through the ZooKeeper watch that the controller
    is gone and will attempt to create the controller node in ZooKeeper themselves.
    The first node to create the new controller in ZooKeeper becomes the next controller,
    while the other nodes will receive a “node already exists” exception and re-create
    the watch on the new controller node. Each time a controller is elected, it receives
    a new, higher *controller epoch* number through a ZooKeeper conditional increment
    operation. The brokers know the current controller epoch, and if they receive
    a message from a controller with an older number, they know to ignore it. This
    is important because the controller broker can disconnect from ZooKeeper due to
    a long garbage collection pause—during this pause a new controller will be elected.
    When the previous leader resumes operations after the pause, it can continue sending
    messages to brokers without knowing that there is a new controller—in this case,
    the old controller is considered a zombie. The controller epoch in the message,
    which allows brokers to ignore messages from old controllers, is a form of zombie
    fencing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当控制器代理停止或失去与ZooKeeper的连接时，临时节点将消失。这包括任何情况，其中控制器使用的ZooKeeper客户端停止向ZooKeeper发送心跳超过`zookeeper.session.timeout.ms`。当临时节点消失时，集群中的其他代理将通过ZooKeeper监视被通知控制器已经消失，并将尝试在ZooKeeper中自行创建控制器节点。在ZooKeeper中创建新控制器的第一个节点将成为下一个控制器，而其他节点将收到“节点已存在”异常并在新控制器节点上重新创建监视。每次选举控制器时，它都会通过ZooKeeper条件递增操作接收一个新的更高*控制器时代*编号。代理知道当前的控制器时代，如果它们从具有较旧编号的控制器接收到消息，它们会忽略它。这很重要，因为控制器代理可能由于长时间的垃圾回收暂停而断开与ZooKeeper的连接
    - 在此暂停期间，将选举新的控制器。在暂停后，前一领导者恢复操作时，它可以继续向代理发送消息，而不知道有一个新的控制器 - 在这种情况下，旧的控制器被视为僵尸。消息中的控制器时代允许代理忽略来自旧控制器的消息，这是一种僵尸围栏。
- en: When the controller first comes up, it has to read the latest replica state
    map from ZooKeeper before it can start managing the cluster metadata and performing
    leader elections. The loading process uses async APIs, and pipelines the read
    requests to ZooKeeper to hide latencies. But even so, in clusters with large numbers
    of partitions, the loading process can take several seconds—several tests and
    comparisons are described in an [Apache Kafka 1.1.0 blog post](https://oreil.ly/mQpL4).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当控制器首次启动时，必须从ZooKeeper中读取最新的副本状态映射，然后才能开始管理集群元数据并执行领导者选举。加载过程使用异步API，并将读取请求管道化到ZooKeeper以隐藏延迟。但即使如此，在具有大量分区的集群中，加载过程可能需要几秒钟
    - 在[Apache Kafka 1.1.0博客文章](https://oreil.ly/mQpL4)中描述了几个测试和比较。
- en: When the controller notices that a broker left the cluster (by watching the
    relevant ZooKeeper path or because it received a `ControlledShutdownRequest` from
    the broker), it knows that all the partitions that had a leader on that broker
    will need a new leader. It goes over all the partitions that need a new leader
    and determines who the new leader should be (simply the next replica in the replica
    list of that partition). Then it persists the new state to ZooKeeper (again, using
    pipelined async requests to reduce latency) and then sends a `LeaderAndISR` request
    to all the brokers that contain replicas for those partitions. The request contains
    information on the new leader and followers for the partitions. These requests
    are batched for efficiency, so each request includes new leadership information
    for multiple partitions that have a replica on the same broker. Each new leader
    knows that it needs to start serving producer and consumer requests from clients,
    while the followers know that they need to start replicating messages from the
    new leader. Since every broker in the cluster has a `MetadataCache` that includes
    a map of all brokers and all replicas in the cluster, the controller sends all
    brokers information about the leadership change in an `Update​Metadata` request
    so they can update their caches. A similar process repeats when a broker starts
    back up—the main difference is that all replicas in the broker start as followers
    and need to catch up to the leader before they are eligible to be elected as leaders
    themselves.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当控制器注意到代理离开集群（通过监视相关的ZooKeeper路径或因为它收到了来自代理的`ControlledShutdownRequest`），它知道所有在该代理上有领导者的分区都需要新的领导者。它遍历所有需要新领导者的分区，并确定新领导者应该是谁（简单地是该分区副本列表中的下一个副本）。然后它将新状态持久化到ZooKeeper（再次使用管道化的异步请求以减少延迟），然后向包含这些分区副本的所有代理发送`LeaderAndISR`请求。该请求包含有关分区的新领导者和跟随者的信息。这些请求被批处理以提高效率，因此每个请求都包括对同一代理上具有副本的多个分区的新领导信息。每个新领导者都知道它需要开始为来自客户端的生产者和消费者请求提供服务，而跟随者知道它们需要开始复制来自新领导者的消息。由于集群中的每个代理都有一个包含集群中所有代理和所有副本映射的`MetadataCache`，因此控制器向所有代理发送有关领导变更的信息以便它们可以更新它们的缓存。当代理重新启动时，类似的过程重复进行
    - 主要区别在于代理中的所有副本都作为跟随者开始，并且需要在有资格被选举为领导者之前赶上领导者。
- en: To summarize, Kafka uses ZooKeeper’s ephemeral node feature to elect a controller
    and to notify the controller when nodes join and leave the cluster. The controller
    is responsible for electing leaders among the partitions and replicas whenever
    it notices nodes join and leave the cluster. The controller uses the epoch number
    to prevent a “split brain” scenario where two nodes believe each is the current
    controller.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Kafka使用ZooKeeper的临时节点功能来选举控制器，并在节点加入和离开集群时通知控制器。控制器负责在注意到节点加入和离开集群时在分区和副本之间选举领导者。控制器使用时代编号来防止“脑裂”情况，其中两个节点相信彼此是当前控制器。
- en: 'KRaft: Kafka’s New Raft-Based Controller'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KRaft：Kafka的新基于Raft的控制器
- en: 'Starting in 2019, the Apache Kafka community started on an ambitious project:
    moving away from the ZooKeeper-based controller to a Raft-based controller quorum.
    The preview version of the new controller, named KRaft, is part of the Apache
    Kafka 2.8 release. The Apache Kafka 3.0 release, planned for mid 2021, will include
    the first production version of KRaft, and Kafka clusters will be able to run
    with either the traditional ZooKeeper-based controller or KRaft.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从2019年开始，Apache Kafka社区开始了一项雄心勃勃的项目：从基于ZooKeeper的控制器转移到基于Raft的控制器仲裁。新控制器的预览版本名为KRaft，是Apache
    Kafka 2.8版本的一部分。计划于2021年中期发布的Apache Kafka 3.0版本将包括KRaft的首个生产版本，Kafka集群将能够同时运行传统的基于ZooKeeper的控制器或KRaft。
- en: 'Why did the Kafka community decide to replace the controller? Kafka’s existing
    controller already underwent several rewrites, but despite improvements to the
    way it uses ZooKeeper to store the topic, partition, and replica information,
    it became clear that the existing model will not scale to the number of partitions
    we want Kafka to support. Several known concerns motivated the change:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么Kafka社区决定替换控制器？Kafka现有的控制器已经经历了几次重写，但尽管改进了它使用ZooKeeper存储主题、分区和副本信息的方式，但明显地现有模型无法扩展到我们希望Kafka支持的分区数量。几个已知的问题促使了这一变化：
- en: Metadata updates are written to ZooKeeper synchronously but are sent to brokers
    asynchronously. In addition, receiving updates from ZooKeeper is asynchronous.
    All this leads to edge cases where metadata is inconsistent between brokers, controller,
    and ZooKeeper. These cases are challenging to detect.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据更新同步写入ZooKeeper，但异步发送到代理。此外，从ZooKeeper接收更新也是异步的。所有这些导致了元数据在代理、控制器和ZooKeeper之间不一致的边缘情况。这些情况很难检测。
- en: Whenever the controller is restarted, it has to read all the metadata for all
    brokers and partitions from ZooKeeper and then send this metadata to all brokers.
    Despite years of effort, this remains a major bottleneck—as the number of partitions
    and brokers increases, restarting the controller becomes slower.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当控制器重新启动时，它必须从ZooKeeper中读取所有代理和分区的所有元数据，然后将这些元数据发送给所有代理。尽管经过多年的努力，这仍然是一个主要瓶颈——随着分区和代理数量的增加，重新启动控制器变得更慢。
- en: The internal architecture around metadata ownership is not great—some operations
    were done via the controller, others via any broker, and others directly on ZooKeeper.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于元数据所有权的内部架构并不理想——一些操作是通过控制器完成的，另一些是通过任何代理完成的，还有一些是直接在ZooKeeper上完成的。
- en: ZooKeeper is its own distributed system, and, just like Kafka, it requires some
    expertise to operate. Developers who want to use Kafka therefore need to learn
    two distributed systems, not just one.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZooKeeper是自己的分布式系统，就像Kafka一样，需要一些专业知识来操作。想要使用Kafka的开发人员因此需要学习两个分布式系统，而不仅仅是一个。
- en: With all these concerns in mind, the Apache Kafka community chose to replace
    the existing ZooKeeper-based controller.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些问题，Apache Kafka社区决定替换现有的基于ZooKeeper的控制器。
- en: 'In the existing architecture, ZooKeeper has two important functions: it is
    used to elect a controller and to store the cluster metadata—registered brokers,
    configuration, topics, partitions, and replicas. In addition, the controller itself
    manages the metadata—it is used to elect leaders, create and delete topics, and
    reassign replicas. All this functionality will have to be replaced in the new
    controller.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有架构中，ZooKeeper有两个重要功能：用于选举控制器和存储集群元数据——注册的代理、配置、主题、分区和副本。此外，控制器本身管理元数据——用于选举领导者、创建和删除主题以及重新分配副本。所有这些功能都将在新控制器中替换。
- en: The core idea behind the new controller design is that Kafka itself has a log-based
    architecture, where users represent state as a stream of events. The benefits
    of such representation are well understood in the community—multiple consumers
    can quickly catch up to the latest state by replaying events. The log establishes
    a clear ordering between events and ensures that the consumers always move along
    a single timeline. The new controller architecture brings the same benefits to
    the management of Kafka’s metadata.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 新控制器设计的核心思想是，Kafka本身具有基于日志的架构，其中用户将状态表示为事件流。这种表示的好处在社区中得到了充分理解——多个消费者可以通过重放事件快速追上最新状态。日志建立了事件之间的明确顺序，并确保消费者始终沿着单一时间线移动。新的控制器架构为Kafka的元数据管理带来了相同的好处。
- en: In the new architecture, the controller nodes are a Raft quorum that manages
    the log of metadata events. This log contains information about each change to
    the cluster metadata. Everything that is currently stored in ZooKeeper, such as
    topics, partitions, ISRs, configurations, and so on, will be stored in this log.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的架构中，控制器节点是一个Raft仲裁，负责管理元数据事件日志。该日志包含有关集群元数据每个更改的信息。目前存储在ZooKeeper中的所有内容，如主题、分区、ISR、配置等，都将存储在此日志中。
- en: Using the Raft algorithm, the controller nodes will elect a leader from among
    themselves, without relying on any external system. The leader of the metadata
    log is called the *active controller*. The active controller handles all RPCs
    made from the brokers. The follower controllers replicate the data that is written
    to the active controller and serve as hot standbys if the active controller should
    fail. Because the controllers will now all track the latest state, controller
    failover will not require a lengthy reloading period in which we transfer all
    the state to the new controller.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Raft算法，控制器节点将从其中选举出一个领导者，而无需依赖任何外部系统。元数据日志的活跃控制器称为*活跃控制器*。活跃控制器处理来自代理的所有RPC。跟随者控制器复制写入活跃控制器的数据，并在活跃控制器失败时作为热备份。因为控制器现在都跟踪最新状态，控制器故障转移将不需要一个漫长的重新加载期，在此期间我们将所有状态转移到新控制器。
- en: Instead of the controller pushing out updates to the other brokers, those brokers
    will fetch updates from the active controller via a new `MetadataFetch` API. Similar
    to a fetch request, brokers will track the offset of the latest metadata change
    they fetched and will only request newer updates from the controller. Brokers
    will persist the metadata to disk, [which will allow them to start up quickly,
    even with millions of partitions](https://oreil.ly/TsU0w).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器不再推送更新到其他经纪人，而是这些经纪人将通过新的`MetadataFetch` API从活动控制器获取更新。与获取请求类似，经纪人将跟踪其获取的最新元数据更改的偏移量，并且只会从控制器请求更新的元数据。经纪人将将元数据持久化到磁盘，[这将使它们能够快速启动，即使有数百万个分区](https://oreil.ly/TsU0w)。
- en: Brokers will register with the controller quorum and will remain registered
    until unregistered by an admin, so once a broker shuts down, it is offline but
    still registered. Brokers that are online but are not up-to-date with the latest
    metadata will be fenced and will not be able to serve client requests. The new
    fenced state will prevent cases where a client produces events to a broker that
    is no longer a leader but is too out-of-date to be aware that it isn’t a leader.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 经纪人将向控制器仲裁注册，并将保持注册，直到被管理员注销，因此一旦经纪人关闭，它就是离线但仍然注册。在线但未与最新元数据同步的经纪人将被隔离，并且将无法为客户端请求提供服务。新的隔离状态将防止客户端向不再是领导者但过时以至于不知道自己不是领导者的经纪人产生事件的情况。
- en: As part of the migration to the controller quorum, all operations that previously
    involved either clients or brokers communicating directly to ZooKeeper will be
    routed via the controller. This will allow seamless migration by replacing the
    controller without having to change anything on any broker.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为迁移到控制器仲裁的一部分，以前涉及客户端或经纪人直接与ZooKeeper通信的所有操作将通过控制器路由。这将允许通过替换控制器来实现无缝迁移，而无需更改任何经纪人上的任何内容。
- en: Overall design of the new architecture is described in [KIP-500](https://oreil.ly/KAsp9).
    Details on how the Raft protocol was adapted for Kafka is described in [KIP-595](https://oreil.ly/XbI8L).
    Detailed design on the new controller quorum, including controller configuration
    and a new CLI for interacting with cluster metadata, are found in [KIP-631](https://oreil.ly/rpOjK).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 新架构的整体设计在[KIP-500](https://oreil.ly/KAsp9)中有描述。有关如何为Kafka调整Raft协议的详细信息在[KIP-595](https://oreil.ly/XbI8L)中有描述。有关新控制器仲裁的详细设计，包括控制器配置和用于与集群元数据交互的新CLI，可在[KIP-631](https://oreil.ly/rpOjK)中找到。
- en: Replication
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制
- en: Replication is at the heart of Kafka’s architecture. Indeed, Kafka is often
    described as “a distributed, partitioned, replicated commit log service.” Replication
    is critical because it is the way Kafka guarantees availability and durability
    when individual nodes inevitably fail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 复制是Kafka架构的核心。事实上，Kafka经常被描述为“分布式、分区、复制的提交日志服务”。复制是至关重要的，因为这是Kafka在个别节点不可避免地失败时保证可用性和耐久性的方式。
- en: As we’ve already discussed, data in Kafka is organized by topics. Each topic
    is partitioned, and each partition can have multiple replicas. Those replicas
    are stored on brokers, and each broker typically stores hundreds or even thousands
    of replicas belonging to different topics and partitions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的那样，Kafka中的数据是按主题组织的。每个主题都被分区，每个分区可以有多个副本。这些副本存储在经纪人上，每个经纪人通常存储属于不同主题和分区的数百甚至数千个副本。
- en: 'There are two types of replicas:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的副本：
- en: Leader replica
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者副本
- en: Each partition has a single replica designated as the leader. All produce requests
    go through the leader to guarantee consistency. Clients can consume from either
    the lead replica or its followers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区都有一个被指定为领导者的副本。所有生成请求都通过领导者进行以保证一致性。客户端可以从领导副本或其跟随者中消费。
- en: Follower replica
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 追随者副本
- en: All replicas for a partition that are not leaders are called followers. Unless
    configured otherwise, followers don’t serve client requests; their main job is
    to replicate messages from the leader and stay up-to-date with the most recent
    messages the leader has. If a leader replica for a partition crashes, one of the
    follower replicas will be promoted to become the new leader for the partition.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不是领导者的分区的所有副本都被称为跟随者。除非另有配置，否则跟随者不会为客户端请求提供服务；它们的主要工作是从领导者复制消息并保持与领导者最近的消息同步。如果分区的领导者副本崩溃，其中一个跟随者副本将被提升为该分区的新领导者。
- en: Another task the leader is responsible for is knowing which of the follower
    replicas is up-to-date with the leader. Followers attempt to stay up-to-date by
    replicating all the messages from the leader as the messages arrive, but they
    can fail to stay in sync for various reasons, such as when network congestion
    slows down replication or when a broker crashes and all replicas on that broker
    start falling behind until we start the broker and they can start replicating
    again.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者还负责知道哪个跟随者副本与领导者保持同步。跟随者尝试通过复制领导者的所有消息来保持同步，但由于各种原因，例如网络拥塞减慢了复制速度，或者经纪人崩溃并且该经纪人上的所有副本开始落后，直到我们启动经纪人并且它们可以再次开始复制时，它们可能无法保持同步。
- en: To stay in sync with the leader, the replicas send the leader `Fetch` requests,
    the exact same type of requests that consumers send in order to consume messages.
    In response to those requests, the leader sends the messages to the replicas.
    Those `Fetch` requests contain the offset of the message that the replica wants
    to receive next, and will always be in order. This means that the leader can know
    that a replica got all messages up to the last messages that the replica fetched,
    and none of the messages that came after. By looking at the last offset requested
    by each replica, the leader can tell how far behind each replica is. If a replica
    hasn’t requested a message in more than 10 seconds, or if it has requested messages
    but hasn’t caught up to the most recent message in more than 10 seconds, the replica
    is considered *out of sync*. If a replica fails to keep up with the leader, it
    can no longer become the new leader in the event of failure—after all, it does
    not contain all the messages.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与领导者保持同步，副本发送领导者`Fetch`请求，这与消费者为了消费消息发送的请求类型相同。作为对这些请求的响应，领导者将消息发送给副本。这些`Fetch`请求包含副本想要接收的消息的偏移量，并且始终是有序的。这意味着领导者可以知道副本获取了所有消息直到副本获取的最后一条消息，并且没有获取之后的消息。通过查看每个副本请求的最后偏移量，领导者可以知道每个副本落后多少。如果一个副本在10秒内没有请求消息，或者如果它请求了消息但在10秒内没有赶上最新的消息，那么该副本被认为是*不同步*的。如果一个副本无法跟上领导者，它在故障发生时将不再能成为新的领导者——毕竟，它不包含所有的消息。
- en: The inverse of this, replicas that are consistently asking for the latest messages
    are called *in-sync replicas*. Only in-sync replicas are eligible to be elected
    as partition leaders in case the existing leader fails.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个的反义词，一直在请求最新消息的副本被称为*同步副本*。只有同步副本有资格在现有领导者失败的情况下被选举为分区领导者。
- en: The amount of time a follower can be inactive or behind before it is considered
    out of sync is controlled by the `replica.lag.time.max.ms` configuration parameter.
    This allowed lag has implications on client behavior and data retention during
    leader election. We discuss this in depth in [Chapter 7](ch07.html#reliable_data_delivery)
    when we discuss reliability guarantees.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在副本被认为是不同步之前可以不活动或落后的时间由`replica.lag.time.max.ms`配置参数控制。这种允许的滞后对客户端行为和领导者选举期间的数据保留有影响。我们在[第7章](ch07.html#reliable_data_delivery)中深入讨论这个问题，当我们讨论可靠性保证时。
- en: In addition to the current leader, each partition has a *preferred leader*—the
    replica that was the leader when the topic was originally created. It is preferred
    because when partitions are first created, the leaders are balanced among brokers.
    As a result, we expect that when the preferred leader is indeed the leader for
    all partitions in the cluster, load will be evenly balanced between brokers. By
    default, Kafka is configured with `auto.leader.rebalance.enable=true`, which will
    check if the preferred leader replica is not the current leader but is in sync,
    and will trigger leader election to make the preferred leader the current leader.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了当前领导者，每个分区都有一个*首选领导者*——当主题最初创建时是领导者的副本。它是首选的，因为当分区首次创建时，领导者在代理之间是平衡的。因此，我们期望当首选领导者确实是集群中所有分区的领导者时，负载将在代理之间平衡。默认情况下，Kafka配置为`auto.leader.rebalance.enable=true`，它将检查首选领导者副本是否不是当前领导者但是同步的，并将触发领导者选举使首选领导者成为当前领导者。
- en: Finding the Preferred Leaders
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找首选领导者
- en: The best way to identify the current preferred leader is by looking at the list
    of replicas for a partition. (You can see details of partitions and replicas in
    the output of the `kafka-topics.sh` tool. We’ll discuss this and other admin tools
    in [Chapter 13](ch13.html#monitoring_kafka).) The first replica in the list is
    always the preferred leader. This is true no matter who is the current leader
    and even if the replicas were reassigned to different brokers using the replica
    reassignment tool. In fact, if you manually reassign replicas, it is important
    to remember that the replica you specify first will be the preferred replica,
    so make sure you spread those around different brokers to avoid overloading some
    brokers with leaders while other brokers are not handling their fair share of
    the work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 识别当前首选领导者的最佳方法是查看分区的副本列表。（您可以在`kafka-topics.sh`工具的输出中查看分区和副本的详细信息。我们将在[第13章](ch13.html#monitoring_kafka)中讨论这个和其他管理工具。）列表中的第一个副本始终是首选领导者。这一点是正确的，无论当前领导者是谁，甚至如果副本被重新分配到不同的代理使用副本重新分配工具。事实上，如果您手动重新分配副本，重要的是要记住您指定的第一个副本将是首选副本，因此请确保将它们分布在不同的代理上，以避免一些代理负载过重，而其他代理没有处理他们公平份额的工作。
- en: Request Processing
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请求处理
- en: Most of what a Kafka broker does is process requests sent to the partition leaders
    from clients, partition replicas, and the controller. Kafka has a binary protocol
    (over TCP) that specifies the format of the requests and how brokers respond to
    them—both when the request is processed successfully or when the broker encounters
    errors while processing the request.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka代理大部分工作是处理来自客户端、分区副本和控制器的请求发送到分区领导者。Kafka有一个二进制协议（通过TCP）规定了请求的格式以及代理如何响应这些请求——无论是请求成功处理还是代理在处理请求时遇到错误时。
- en: The Apache Kafka project includes Java clients that were implemented and maintained
    by contributors to the Apache Kafka project; there are also clients in other languages,
    such as C, Python, Go, and many others. [You can see the full list on the Apache
    Kafka website](http://bit.ly/2sKvTjx). They all communicate with Kafka brokers
    using this protocol.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka项目包括由Apache Kafka项目的贡献者实现和维护的Java客户端；还有其他语言的客户端，如C、Python、Go等。[您可以在Apache
    Kafka网站上看到完整的列表](http://bit.ly/2sKvTjx)。它们都使用这个协议与Kafka代理进行通信。
- en: Clients always initiate connections and send requests, and the broker processes
    the requests and responds to them. All requests sent to the broker from a specific
    client will be processed in the order in which they were received—this guarantee
    is what allows Kafka to behave as a message queue and provide ordering guarantees
    on the messages it stores.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端始终发起连接并发送请求，代理处理请求并对其做出响应。从特定客户端发送到代理的所有请求将按照接收顺序进行处理，这个保证使得Kafka能够作为消息队列并对其存储的消息提供顺序保证。
- en: 'All requests have a standard header that includes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所有请求都有一个包括以下内容的标准头部：
- en: Request type (also called *API key*)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求类型（也称为*API密钥*）
- en: Request version (so the brokers can handle clients of different versions and
    respond accordingly)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求版本（以便代理可以处理不同版本的客户端并相应地做出响应）
- en: 'Correlation ID: a number that uniquely identifies the request and also appears
    in the response and in the error logs (the ID is used for troubleshooting)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联ID：唯一标识请求的编号，也出现在响应和错误日志中（用于故障排除）
- en: 'Client ID: used to identify the application that sent the request'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端ID：用于标识发送请求的应用程序
- en: We will not describe the protocol here because it is described in significant
    detail in the [Kafka documentation](http://kafka.apache.org/protocol.html). However,
    it is helpful to take a look at how requests are processed by the broker—later,
    when we discuss how to monitor Kafka and the various configuration options, you
    will have context about which queues and threads the metrics and configuration
    parameters refer to.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里描述协议，因为它在[Kafka文档](http://kafka.apache.org/protocol.html)中有详细描述。然而，了解请求是如何由代理处理的是有帮助的——稍后，当我们讨论如何监视Kafka和各种配置选项时，您将了解指标和配置参数所指的队列和线程的上下文。
- en: For each port the broker listens on, the broker runs an *acceptor* thread that
    creates a connection and hands it over to a *processor* thread for handling. The
    number of processor threads (also called *network threads*) is configurable. The
    network threads are responsible for taking requests from client connections, placing
    them in a *request queue*, and picking up responses from a *response queue* and
    sending them back to clients. At times, responses to clients have to be delays—consumers
    only receive responses when data is available, and admin clients receive a response
    to a `DeleteTopic` request after topic deletion is underway. The delayed responses
    are held in a [*purgatory*](https://oreil.ly/2jWos) until they can be completed.
    See [Figure 6-1](#fig-1-request-processing) for a visual of this process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代理监听的每个端口，代理运行一个*接收器*线程来创建连接并将其交给一个*处理器*线程进行处理。处理器线程（也称为*网络线程*）的数量是可配置的。网络线程负责从客户端连接接收请求，将其放入*请求队列*，并从*响应队列*中取出响应并将其发送回客户端。有时，对客户端的响应必须延迟——只有在数据可用时，消费者才会收到响应，而管理员客户端在主题删除进行中时才会收到`DeleteTopic`请求的响应。延迟的响应被保存在[*炼狱*](https://oreil.ly/2jWos)中，直到可以完成。请参阅[图6-1](#fig-1-request-processing)以了解此过程的可视化。
- en: '![kdg2 0601](assets/kdg2_0601.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0601](assets/kdg2_0601.png)'
- en: Figure 6-1\. Request processing inside Apache Kafka
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。Apache Kafka内部的请求处理
- en: 'Once requests are placed on the request queue, *I/O threads* (also called request
    *handler threads*) are responsible for picking them up and processing them. The
    most common types of client requests are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求被放置在请求队列中，*I/O线程*（也称为请求*处理程序线程*）负责接收并处理它们。最常见的客户端请求类型有：
- en: Produce requests
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 生产请求
- en: Sent by producers and contain messages the clients write to Kafka brokers
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由生产者发送，并包含客户端写入Kafka代理的消息
- en: Fetch requests
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 获取请求
- en: Sent by consumers and follower replicas when they read messages from Kafka brokers
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当消费者和追随者副本从Kafka代理读取消息时发送
- en: Admin requests
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员请求
- en: Sent by admin clients when performing metadata operations such as creating and
    deleting topics
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行元数据操作时由管理员客户端发送，例如创建和删除主题
- en: Both produce requests and fetch requests have to be sent to the leader replica
    of a partition. If a broker receives a produce request for a specific partition
    and the leader for this partition is on a different broker, the client that sent
    the produce request will get an error response of “Not a Leader for Partition.”
    The same error will occur if a fetch request for a specific partition arrives
    at a broker that does not have the leader for that partition. Kafka’s clients
    are responsible for sending produce and fetch requests to the broker that contains
    the leader for the relevant partition for the request.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 生产请求和获取请求都必须发送到分区的领导副本。如果代理接收到特定分区的生产请求，而该分区的领导者在另一个代理上，发送生产请求的客户端将收到“不是分区领导者”的错误响应。如果特定分区的获取请求到达一个没有该分区领导者的代理，将发生相同的错误。Kafka的客户端负责将生产和获取请求发送到包含请求相关分区的领导者的代理。
- en: How do the clients know where to send the requests? Kafka clients use another
    request type called a *metadata request*, which includes a list of topics the
    client is interested in. The server response specifies which partitions exist
    in the topics, the replicas for each partition, and which replica is the leader.
    Metadata requests can be sent to any broker because all brokers have a metadata
    cache that contains this information.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端如何知道要发送请求的位置？Kafka客户端使用另一种称为*元数据请求*的请求类型，其中包括客户端感兴趣的主题列表。服务器响应指定了主题中存在的分区，每个分区的副本，以及哪个副本是领导者。元数据请求可以发送到任何代理，因为所有代理都有包含此信息的元数据缓存。
- en: Clients typically cache this information and use it to direct produce and fetch
    requests to the correct broker for each partition. They also need to occasionally
    refresh this information (refresh intervals are controlled by the `metadata.​max.age.ms`
    configuration parameter) by sending another metadata request so they know if the
    topic metadata changed—for example, if a new broker was added or some replicas
    were moved to a new broker ([Figure 6-2](#fig-2-request-routing)). In addition,
    if a client receives the “Not a Leader” error to one of its requests, it will
    refresh its metadata before trying to send the request again, since the error
    indicates that the client is using outdated information and is sending requests
    to the wrong broker.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0602](assets/kdg2_0602.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Client routing requests
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Produce Requests
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [Chapter 3](ch03.html#writing_messages_to_kafka), a configuration
    parameter called `acks` is the number of brokers that need to acknowledge receiving
    the message before it is considered a successful write. Producers can be configured
    to consider messages as “written successfully” when the message was accepted by
    just the leader (`acks=1`), or by all in-sync replicas (`acks=all`), or the moment
    the message was sent without waiting for the broker to accept it at all (`acks=0`).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'When the broker that contains the lead replica for a partition receives a produce
    request for this partition, it will start by running a few validations:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Does the user sending the data have write privileges on the topic?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the number of `acks` specified in the request valid (only 0, 1, and “all”
    are allowed)?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `acks` is set to `all`, are there enough in-sync replicas for safely writing
    the message? (Brokers can be configured to refuse new messages if the number of
    in-sync replicas falls below a configurable number; we will discuss this in more
    detail in [Chapter 7](ch07.html#reliable_data_delivery), when we discuss Kafka’s
    durability and reliability guarantees.)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the broker will write the new messages to local disk. On Linux, the messages
    are written to the filesystem cache, and there is no guarantee about when they
    will be written to disk. Kafka does not wait for the data to get persisted to
    disk—it relies on replication for message durability.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the message is written to the leader of the partition, the broker examines
    the `acks` configuration: if `acks` is set to 0 or 1, the broker will respond
    immediately; if `acks` is set to `all`, the request will be stored in a buffer
    called *purgatory* until the leader observes that the follower replicas replicated
    the message, at which point a response is sent to the client.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Fetch Requests
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brokers process fetch requests in a way that is very similar to how produce
    requests are handled. The client sends a request, asking the broker to send messages
    from a list of topics, partitions, and offsets—something like “Please send me
    messages starting at offset 53 in partition 0 of topic Test and messages starting
    at offset 64 in partition 3 of topic Test.” Clients also specify a limit to how
    much data the broker can return for each partition. The limit is important because
    clients need to allocate memory that will hold the response sent back from the
    broker. Without this limit, brokers could send back replies large enough to cause
    clients to run out of memory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve discussed earlier, the request has to arrive to the leaders of the
    partitions specified in the request, and the client will make the necessary metadata
    requests to make sure it is routing the fetch requests correctly. When the leader
    receives the request, it first checks if the request is valid—does this offset
    even exist for this particular partition? If the client is asking for a message
    that is so old it got deleted from the partition or an offset that does not exist
    yet, the broker will respond with an error.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: If the offset exists, the broker will read messages from the partition, up to
    the limit set by the client in the request, and send the messages to the client.
    Kafka famously uses a `zero-copy` method to send the messages to the clients—this
    means that Kafka sends messages from the file (or more likely, the Linux filesystem
    cache) directly to the network channel without any intermediate buffers. This
    is different than most databases where data is stored in a local cache before
    being sent to clients. This technique removes the overhead of copying bytes and
    managing buffers in memory, and results in much improved performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果偏移存在，经纪人将从分区中读取消息，直到请求中客户端设置的限制，并将消息发送给客户端。Kafka以“零拷贝”方法向客户端发送消息而闻名——这意味着Kafka直接从文件（或更可能是Linux文件系统缓存）将消息发送到网络通道，而无需任何中间缓冲区。这与大多数数据库不同，大多数数据库在发送给客户端之前将数据存储在本地缓存中。这种技术消除了复制字节和管理内存缓冲区的开销，并显著提高了性能。
- en: In addition to setting an upper boundary on the amount of data the broker can
    return, clients can also set a lower boundary on the amount of data returned.
    Setting the lower boundary to 10K, for example, is the client’s way of telling
    the broker, “Only return results once you have at least 10K bytes to send me.”
    This is a great way to reduce CPU and network utilization when clients are reading
    from topics that are not seeing much traffic. Instead of the clients sending requests
    to the brokers every few milliseconds asking for data and getting very few or
    no messages in return, the clients send a request, the broker waits until there
    is a decent amount of data, and returns the data, and only then will the client
    ask for more ([Figure 6-3](#fig-3-delayed-response)). The same amount of data
    is read overall but with much less back-and-forth and therefore less overhead.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设置经纪人可以返回的数据量的上限外，客户端还可以设置返回的数据量的下限。例如，将下限设置为10K是客户端告诉经纪人的方式，“只有在你至少有10K字节要发送给我的时候才返回结果”。这是在客户端从未看到太多流量的主题中减少CPU和网络利用率的好方法。客户端不再每隔几毫秒向经纪人发送请求请求数据，并且几乎没有或没有消息返回，而是客户端发送请求，经纪人等待直到有足够的数据，然后返回数据，然后客户端才会请求更多（[图6-3](#fig-3-delayed-response)）。总体上读取的数据量是相同的，但是往返要少得多，因此开销也更少。
- en: '![kdg2 0603](assets/kdg2_0603.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0603](assets/kdg2_0603.png)'
- en: Figure 6-3\. Broker delaying response until enough data accumulated
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3。 经纪人延迟响应，直到积累足够的数据
- en: Of course, we wouldn’t want clients to wait forever for the broker to have enough
    data. After a while, it makes sense to just take the data that exists and process
    that instead of waiting for more. Therefore, clients can also define a timeout
    to tell the broker, “If you didn’t satisfy the minimum amount of data to send
    within *x* milliseconds, just send what you got.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不希望客户端永远等待经纪人有足够的数据。过了一会儿，最好只是获取已经存在的数据并处理，而不是等待更多。因此，客户端还可以定义超时时间，告诉经纪人，“如果你在*x*毫秒内没有满足发送最小数据量的要求，就发送你得到的数据。”
- en: It is interesting to note that not all the data that exists on the leader of
    the partition is available for clients to read. Most clients can only read messages
    that were written to all in-sync replicas (follower replicas, even though they
    are consumers, are exempt from this—otherwise replication would not work). We
    already discussed that the leader of the partition knows which messages were replicated
    to which replica, and until a message was written to all in-sync replicas, it
    will not be sent to consumers—attempts to fetch those messages will result in
    an empty response rather than an error.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，并非分区领导者上存在的所有数据都可以供客户端读取。大多数客户端只能读取写入所有同步副本的消息（即使它们是消费者，从者副本不受此限制——否则复制将无法工作）。我们已经讨论过，分区的领导者知道哪些消息被复制到了哪个副本，直到消息被写入所有同步副本，它才会被发送给消费者——尝试获取这些消息将导致空响应而不是错误。
- en: The reason for this behavior is that messages not replicated to enough replicas
    yet are considered “unsafe”—if the leader crashes and another replica takes its
    place, these messages will no longer exist in Kafka. If we allowed clients to
    read messages that only exist on the leader, we could see inconsistent behavior.
    For example, if a consumer reads a message and the leader crashed and no other
    broker contained this message, the message is gone. No other consumer will be
    able to read this message, which can cause inconsistency with the consumer who
    did read it. Instead, we wait until all the in-sync replicas get the message and
    only then allow consumers to read it ([Figure 6-4](#fig-4-high-water-mark)). This
    behavior also means that if replication between brokers is slow for some reason,
    it will take longer for new messages to arrive to consumers (since we wait for
    the messages to replicate first). This delay is limited to `replica.lag.time.max.ms`—the
    amount of time a replica can be delayed in replicating new messages while still
    being considered in sync.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为的原因是，尚未复制到足够多副本的消息被视为“不安全”——如果领导者崩溃并且另一个副本取代了它，这些消息将不再存在于Kafka中。如果我们允许客户端读取仅存在于领导者上的消息，我们可能会看到不一致的行为。例如，如果消费者读取了一条消息，领导者崩溃并且没有其他经纪人包含此消息，那么消息就消失了。没有其他消费者将能够读取此消息，这可能会导致与读取它的消费者不一致。相反，我们等到所有同步副本都收到消息，然后才允许消费者读取它（[图6-4](#fig-4-high-water-mark)）。这种行为还意味着，如果经纪人之间的复制因某种原因变慢，新消息到达消费者的时间将更长（因为我们要等待消息首先复制）。此延迟受到`replica.lag.time.max.ms`的限制——即副本在被认为是同步的情况下可以延迟复制新消息的时间量。
- en: '![kdg2 0604](assets/kdg2_0604.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 0604](assets/kdg2_0604.png)'
- en: Figure 6-4\. Consumers only see messages that were replicated to in-sync replicas
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4。 消费者只能看到复制到同步副本的消息
- en: In some cases, a consumer consumes events from a large number of partitions.
    Sending the list of all the partitions it is interested in to the broker with
    every request and having the broker send all its metadata back can be very inefficient—the
    set of partitions rarely changes, their metadata rarely changes, and in many cases
    there isn’t that much data to return. To minimize this overhead, Kafka has `fetch
    session cache`. Consumers can attempt to create a cached session that stores the
    list of partitions they are consuming from and its metadata. Once a session is
    created, consumers no longer need to specify all the partitions in each request
    and can use incremental fetch requests instead. Brokers will only include metadata
    in the response if there were any changes. The session cache has limited space,
    and Kafka prioritizes follower replicas and consumers with a large set of partitions,
    so in some cases a session will not be created or will be evicted. In both these
    cases the broker will return an appropriate error to the client, and the consumer
    will transparently resort to full fetch requests that include all the partition
    metadata.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，消费者从大量分区中消费事件。在每个请求中将其感兴趣的所有分区列表发送给代理，并让代理发送所有其元数据回来可能非常低效 - 分区集合很少改变，它们的元数据很少改变，并且在许多情况下没有太多数据返回。为了最小化这种开销，Kafka有`fetch
    session cache`。消费者可以尝试创建一个缓存会话，存储它们正在消费的分区列表及其元数据。一旦创建了会话，消费者就不再需要在每个请求中指定所有分区，并且可以使用增量获取请求。如果有任何更改，代理将只在响应中包含元数据。会话缓存的空间有限，Kafka优先考虑追随者副本和具有大量分区的消费者，因此在某些情况下可能不会创建或将被驱逐。在这两种情况下，代理将向客户端返回适当的错误，并且消费者将透明地重新使用包含所有分区元数据的完整获取请求。
- en: Other Requests
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他请求
- en: 'We just discussed the most common types of requests used by Kafka clients:
    `Metadata`, `Produce`, and `Fetch`. The Kafka protocol currently handles [61 different
    request types](https://oreil.ly/hBmNc), and more will be added. Consumers alone
    use 15 request types to form groups, coordinate consumption, and allow developers
    to manage the consumer groups. There are also large numbers of requests that are
    related to metadata management and security.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚讨论了Kafka客户端使用的最常见的请求类型：`Metadata`，`Produce`和`Fetch`。Kafka协议目前处理[61种不同的请求类型](https://oreil.ly/hBmNc)，并将添加更多。仅消费者就使用了15种请求类型来形成组，协调消费，并允许开发人员管理消费者组。还有大量与元数据管理和安全性相关的请求。
- en: In addition, the same protocol is used to communicate between the Kafka brokers
    themselves. Those requests are internal and should not be used by clients. For
    example, when the controller announces that a partition has a new leader, it sends
    a `LeaderAndIsr` request to the new leader (so it will know to start accepting
    client requests) and to the followers (so they will know to follow the new leader).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相同的协议用于Kafka代理之间的通信。这些请求是内部的，不应该被客户端使用。例如，当控制器宣布分区有一个新的领导者时，它会向新的领导者发送`LeaderAndIsr`请求（以便它知道开始接受客户端请求），并发送给追随者（以便它们知道跟随新的领导者）。
- en: 'The protocol is ever evolving—as the Kafka community adds more client capabilities,
    the protocol evolves to match. For example, in the past, Kafka consumers used
    Apache ZooKeeper to keep track of the offsets they receive from Kafka. So when
    a consumer is started, it can check ZooKeeper for the last offset that was read
    from its partitions and know where to start processing. For various reasons, the
    community decided to stop using ZooKeeper for this and instead stored those offsets
    in a special Kafka topic. To do this, the contributors had to add several requests
    to the protocol: `OffsetCommitRequest`, `OffsetFetchRequest`, and `ListOffsetsRequest`.
    Now when an application calls the client API to commit consumer offsets, the client
    no longer writes to ZooKeeper; instead, it sends `OffsetCommitRequest` to Kafka.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 协议不断发展演变 - 随着Kafka社区增加更多的客户端功能，协议也在不断演变以匹配。例如，在过去，Kafka消费者使用Apache ZooKeeper来跟踪它们从Kafka接收的偏移量。因此，当消费者启动时，它可以检查ZooKeeper中从其分区读取的最后一个偏移量，并知道从哪里开始处理。出于各种原因，社区决定停止使用ZooKeeper，并将这些偏移量存储在一个特殊的Kafka主题中。为了做到这一点，贡献者们不得不向协议中添加几个请求：`OffsetCommitRequest`，`OffsetFetchRequest`和`ListOffsetsRequest`。现在，当应用程序调用客户端API来提交消费者偏移量时，客户端不再写入ZooKeeper；相反，它将`OffsetCommitRequest`发送到Kafka。
- en: Topic creation used to be handled by command-line tools that directly update
    the list of topics in ZooKeeper. The Kafka community since added a `CreateTopic``Request`,
    and similar requests for managing Kafka’s metadata. Java applications perform
    these metadata operations through Kafka’s `AdminClient`, documented in depth in
    [Chapter 5](ch05.html#admin_client). Since these operations are now part of the
    Kafka protocol, it allows clients in languages that don’t have a ZooKeeper library
    to create topics by asking Kafka brokers directly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 主题创建过去是由命令行工具处理的，这些工具直接更新ZooKeeper中的主题列表。Kafka社区后来添加了`CreateTopic`请求，以及用于管理Kafka元数据的类似请求。Java应用程序通过Kafka的`AdminClient`执行这些元数据操作，在[第5章](ch05.html#admin_client)中有详细记录。由于这些操作现在是Kafka协议的一部分，它允许没有ZooKeeper库的语言的客户端直接向Kafka代理请求创建主题。
- en: In addition to evolving the protocol by adding new request types, Kafka developers
    sometimes choose to modify existing requests to add some capabilities. For example,
    between Kafka 0.9.0 and Kafka 0.10.0, they’ve decided to let clients know who
    the current controller is by adding the information to the `Metadata` response.
    As a result, a new version was added to the `Metadata` request and response. Now,
    0.9.0 clients send `Metadata` requests of version 0 (because version 1 did not
    exist in 0.9.0 clients), and the brokers, whether they are 0.9.0 or 0.10.0, know
    to respond with a version 0 response, which does not have the controller information.
    This is fine, because 0.9.0 clients don’t expect the controller information and
    wouldn’t know how to parse it anyway. If you have the 0.10.0 client, it will send
    a version 1 `Metadata` request, and 0.10.0 brokers will respond with a version
    1 response that contains the controller information, which the 0.10.0 clients
    can use. If a 0.10.0 client sends a version 1 `Metadata` request to a 0.9.0 broker,
    the broker will not know how to handle the newer version of the request and will
    respond with an error. This is the reason we recommend upgrading the brokers before
    upgrading any of the clients—new brokers know how to handle old requests, but
    not vice versa.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过添加新的请求类型来发展协议外，Kafka开发人员有时选择修改现有请求以添加一些功能。例如，在Kafka 0.9.0和Kafka 0.10.0之间，他们决定通过将信息添加到`Metadata`响应来让客户端知道当前的控制器是谁。因此，`Metadata`请求和响应中添加了一个新版本。现在，0.9.0客户端发送版本0的`Metadata`请求（因为0.9.0客户端中不存在版本1），无论是0.9.0还是0.10.0的代理都知道要响应版本0的响应，该响应不包含控制器信息。这没问题，因为0.9.0客户端不期望控制器信息，也不知道如何解析它。如果您有0.10.0客户端，它将发送版本1的`Metadata`请求，而0.10.0代理将响应包含控制器信息的版本1响应，0.10.0客户端可以使用该信息。如果0.10.0客户端向0.9.0代理发送版本1的`Metadata`请求，代理将不知道如何处理较新版本的请求，并将以错误响应。这就是我们建议在升级任何客户端之前先升级代理的原因——新代理知道如何处理旧请求，但反之则不然。
- en: In release 0.10.0, the Kafka community added `ApiVersionRequest`, which allows
    clients to ask the broker which versions of each request are supported and to
    use the correct version accordingly. Clients that use this new capability correctly
    will be able to talk to older brokers by using a version of the protocol that
    is supported by the broker they are connecting to. There is currently ongoing
    work to add APIs that will allow clients to discover which features are supported
    by brokers and to allow brokers to gate features that exist in a specific version.
    This improvement was proposed in [KIP-584](https://oreil.ly/dxg8N), and at this
    time it seems likely to be part of version 3.0.0.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在0.10.0版本中，Kafka社区添加了`ApiVersionRequest`，允许客户端询问代理支持的每个请求的版本，并相应地使用正确的版本。正确使用这种新功能的客户端将能够通过使用代理支持的协议版本与旧代理通信。目前正在进行工作，以添加API，允许客户端发现代理支持哪些功能，并允许代理对存在于特定版本中的功能进行控制。这项改进是在[KIP-584](https://oreil.ly/dxg8N)中提出的，目前看来很可能会成为3.0.0版本的一部分。
- en: Physical Storage
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理存储
- en: The basic storage unit of Kafka is a partition replica. Partitions cannot be
    split between multiple brokers, and not even between multiple disks on the same
    broker. So the size of a partition is limited by the space available on a single
    mount point. (A mount point can be a single disk, if JBOD configuration is used,
    or multiple disks, if RAID is configured. See [Chapter 2](ch02.html#installing_kafka).)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的基本存储单元是分区副本。分区不能在多个代理之间拆分，甚至不能在同一代理的多个磁盘之间拆分。因此，分区的大小受单个挂载点上可用空间的限制。（如果使用JBOD配置，挂载点可以是单个磁盘，如果配置了RAID，则可以是多个磁盘。请参见[第2章](ch02.html#installing_kafka)。）
- en: When configuring Kafka, the administrator defines a list of directories in which
    the partitions will be stored—this is the `log.dirs` parameter (not to be confused
    with the location in which Kafka stores its error log, which is configured in
    the *log4j.properties* file). The usual configuration includes a directory for
    each mount point that Kafka will use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置Kafka时，管理员定义了分区将存储在其中的目录列表——这是`log.dirs`参数（不要与Kafka存储其错误日志的位置混淆，该位置在*log4j.properties*文件中配置）。通常的配置包括Kafka将使用的每个挂载点的目录。
- en: Let’s look at how Kafka uses the available directories to store data. First,
    we want to look at how data is allocated to the brokers in the cluster and the
    directories in the broker. Then we will look at how the broker manages the files—especially
    how the retention guarantees are handled. We will then dive inside the files and
    look at the file and index formats. Finally, we will look at log compaction, an
    advanced feature that allows you to turn Kafka into a long-term data store, and
    describe how it works.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Kafka如何使用可用目录来存储数据。首先，我们要看看数据如何分配给集群中的代理和代理中的目录。然后我们将看看代理如何管理文件——特别是如何处理保留保证。然后，我们将深入文件内部，查看文件和索引格式。最后，我们将看看日志压缩，这是一个高级功能，允许您将Kafka转换为长期数据存储，并描述其工作原理。
- en: Tiered Storage
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层存储
- en: Starting in late 2018, the Apache Kafka community began collaborating on an
    ambitious project to add tiered storage capabilities to Kafka. Work on the project
    is on-going, and it is planned for the 3.0 release.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从2018年底开始，Apache Kafka社区开始合作一个雄心勃勃的项目，为Kafka添加分层存储功能。该项目的工作正在进行中，计划在3.0版本中发布。
- en: 'The motivation is fairly straightforward: Kafka is currently used to store
    large amounts of data, either due to high throughput or long retention periods.
    This introduces the following concerns:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 动机相当简单：Kafka目前用于存储大量数据，要么是由于高吞吐量，要么是由于长期保留期。这引入了以下问题：
- en: You are limited in how much data you can store in a partition. As a result,
    maximum retention and partition counts aren’t simply driven by product requirements
    but also by the limits on physical disk sizes.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在分区中可以存储的数据量是有限的。因此，最大保留和分区计数不仅受产品要求驱动，还受物理磁盘大小的限制。
- en: Your choice of disk and cluster size is driven by storage requirements. Clusters
    often end up larger than they would if latency and throughput were the main considerations,
    which drives up costs.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您对磁盘和集群大小的选择受存储需求驱动。集群通常比如果延迟和吞吐量是主要考虑因素时要大，这会增加成本。
- en: The time it takes to move partitions from one broker to another, for example,
    when expanding or shrinking the cluster, is driven by the size of the partitions.
    Large partitions make the cluster less elastic. These days, architectures are
    designed toward maximum elasticity, taking advantage of flexible cloud deployment
    options.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在扩展或缩小集群时，将分区从一个经纪人移动到另一个经纪人所需的时间取决于分区的大小。大分区会使集群的弹性降低。如今，架构设计朝向最大弹性，利用灵活的云部署选项。
- en: 'In the tiered storage approach, the Kafka cluster is configured with two tiers
    of storage: local and remote. The local tier is the same as the current Kafka
    storage tier—it uses the local disks on the Kafka brokers to store the log segments.
    The new remote tier uses dedicated storage systems, such as HDFS or S3, to store
    the completed log segments.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层存储方法中，Kafka集群配置为具有两个存储层级：本地和远程。本地层级与当前的Kafka存储层级相同——它使用Kafka经纪人上的本地磁盘来存储日志段。新的远程层级使用专用存储系统，例如HDFS或S3，来存储已完成的日志段。
- en: Kafka users can choose to set a separate storage retention policy for each tier.
    Since local storage is typically far more expensive than the remote tier, the
    retention period for the local tier is usually just a few hours or even shorter,
    and the retention period for the remote tier can be much longer—days, or even
    months.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka用户可以选择为每个层级设置单独的存储保留策略。由于本地存储通常比远程层级昂贵得多，因此本地层级的保留期通常只有几个小时甚至更短，而远程层级的保留期可以更长——可以是几天，甚至几个月。
- en: Local storage is significantly lower latency than the remote storage. This works
    well because latency-sensitive applications perform tail reads and are served
    from the local tier, so they benefit from the existing Kafka mechanism of efficiently
    using the page cache to serve the data. Backfill and other applications recovering
    from a failure that needs data older than what is in the local tier are served
    from the remote tier.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本地存储的延迟明显低于远程存储。这很有效，因为对延迟敏感的应用程序执行尾读取，并且从本地层级提供服务，因此它们可以从现有的Kafka机制中受益，该机制可以有效地使用页面缓存来提供数据。从远程层级提供服务的应用程序包括回填和其他需要比本地层级中的数据更旧的数据的应用程序。
- en: The dual-tier architecture used in tiered storage allows scaling storage independent
    of memory and CPUs in a Kafka cluster. This enables Kafka to be a long-term storage
    solution. This also reduces the amount of data stored locally on Kafka brokers,
    and hence the amount of data that needs to be copied during recovery and rebalancing.
    Log segments that are available in the remote tier need not be restored on the
    broker or restored lazily and are served from the remote tier. Since not all data
    is stored on the brokers, increasing the retention period no longer requires scaling
    the Kafka cluster storage and adding new nodes. At the same time, the overall
    data retention can still be much longer, eliminating the need for separate data
    pipelines to copy the data from Kafka to external stores, as done currently in
    many deployments.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 分层存储中使用的双层架构允许在Kafka集群中独立于内存和CPU扩展存储，这使得Kafka成为长期存储解决方案。这也减少了在Kafka经纪人上本地存储的数据量，因此在恢复和重新平衡期间需要复制的数据量也减少了。在远程层级可用的日志段无需在经纪人上恢复，或者可以进行延迟恢复，并且可以从远程层级提供服务。由于并非所有数据都存储在经纪人上，因此增加保留期不再需要扩展Kafka集群存储并添加新节点。同时，整体数据保留仍然可以更长，从而消除了从Kafka复制数据到外部存储的需要，这是当前许多部署中所做的。
- en: The design of tiered storage is documented in detail in [KIP-405](https://oreil.ly/yZP6w),
    including a new component—the `RemoteLogManager` and the interactions with existing
    functionality, such as replicas catching up to the leader and leader elections.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 分层存储的设计在[KIP-405](https://oreil.ly/yZP6w)中有详细记录，其中包括一个新组件——`RemoteLogManager`以及与现有功能的交互，例如副本追赶领导者和领导者选举。
- en: One interesting result that is documented in KIP-405 is the performance implications
    of tiered storage. The team implementing tiered storage measured performance in
    several use cases. The first was using Kafka’s usual high-throughput workload.
    In that case, latency increased a bit (from 21 ms in p99 to 25 ms), since brokers
    also have to ship segments to remote storage. The second use case was when some
    consumers are reading old data. Without tiered storage, consumers reading old
    data have a large impact on latency (21 ms versus 60 ms p99), but with tiered
    storage enabled, the impact is significantly lower (25 ms versus 42 ms p99); this
    is because tiered storage reads are read from HDFS or S3 via a network path. Network
    reads do not compete with local reads on disk I/O or page cache, and leave the
    page cache intact with fresh data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在KIP-405中记录的一个有趣的结果是分层存储的性能影响。实施分层存储的团队在几种用例中测量了性能。第一种是使用Kafka通常的高吞吐量工作负载。在这种情况下，延迟略有增加（从p99的21毫秒到25毫秒），因为经纪人还必须将段发送到远程存储。第二种用例是一些消费者正在读取旧数据。如果没有分层存储，读取旧数据的消费者会对延迟产生很大影响（p99的21毫秒对比60毫秒），但启用分层存储后，影响显著降低（p99的25毫秒对比42毫秒）；这是因为分层存储读取是通过网络路径从HDFS或S3读取的。网络读取不会与磁盘I/O或页面缓存上的本地读取竞争，并且会保持页面缓存完整并具有新鲜数据。
- en: This means that in addition to infinite storage, lower costs, and elasticity,
    tiered storage also delivers isolation between historical reads and real-time
    reads.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，除了无限存储、更低的成本和弹性之外，分层存储还提供了历史读取和实时读取之间的隔离。
- en: Partition Allocation
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区分配
- en: 'When you create a topic, Kafka first decides how to allocate the partitions
    between brokers. Suppose you have 6 brokers and you decide to create a topic with
    10 partitions and a replication factor of 3\. Kafka now has 30 partition replicas
    to allocate to 6 brokers. When doing the allocations, the goals are:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: To spread replicas evenly among brokers—in our example, to make sure we allocate
    five replicas per broker.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make sure that for each partition, each replica is on a different broker.
    If partition 0 has the leader on broker 2, we can place the followers on brokers
    3 and 4, but not on 2 and not both on 3.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the brokers have rack information (available in Kafka release 0.10.0 and
    higher), then assign the replicas for each partition to different racks if possible.
    This ensures that an event that causes downtime for an entire rack does not cause
    complete unavailability for partitions.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do this, we start with a random broker (let’s say 4) and start assigning
    partitions to each broker in a round-robin manner to determine the location for
    the leaders. So partition 0 leader will be on broker 4, partition 1 leader will
    be on broker 5, partition 2 will be on broker 0 (because we only have 6 brokers),
    and so on. Then, for each partition, we place the replicas at increasing offsets
    from the leader. If the leader for partition 0 is on broker 4, the first follower
    will be on broker 5 and the second on broker 0\. The leader for partition 1 is
    on broker 5, so the first replica is on broker 0 and the second on broker 1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: When rack awareness is taken into account, instead of picking brokers in numerical
    order, we prepare a rack-alternating broker list. Suppose that we know that brokers
    0 and 1 are on the same rack, and brokers 2 and 3 are on a separate rack. Instead
    of picking brokers in the order of 0 to 3, we order them as 0, 2, 1, 3—each broker
    is followed by a broker from a different rack ([Figure 6-5](#fig-5-partition-assignment)).
    In this case, if the leader for partition 0 is on broker 2, the first replica
    will be on broker 1, which is on a completely different rack. This is great, because
    if the first rack goes offline, we know that we still have a surviving replica,
    and therefore the partition is still available. This will be true for all our
    replicas, so we have guaranteed availability in the case of rack failure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0605](assets/kdg2_0605.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Partitions and replicas assigned to brokers on different racks
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once we choose the correct brokers for each partition and replica, it is time
    to decide which directory to use for the new partitions. We do this independently
    for each partition, and the rule is very simple: we count the number of partitions
    on each directory and add the new partition to the directory with the fewest partitions.
    This means that if you add a new disk, all the new partitions will be created
    on that disk. This is because, until things balance out, the new disk will always
    have the fewest partitions.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Mind the Disk Space
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that the allocation of partitions to brokers does not take available space
    or existing load into account, and that allocation of partitions to disks takes
    the number of partitions into account but not the size of the partitions. This
    means that if some brokers have more disk space than others (perhaps because the
    cluster is a mix of older and newer servers), some partitions are abnormally large,
    or you have disks of different sizes on the same broker, you need to be careful
    with the partition allocation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: File Management
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retention is an important concept in Kafka—Kafka does not keep data forever,
    nor does it wait for all consumers to read a message before deleting it. Instead,
    the Kafka administrator configures a retention period for each topic—either the
    amount of time to store messages before deleting them or how much data to store
    before older messages are purged.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Because finding the messages that need purging in a large file and then deleting
    a portion of the file is both time-consuming and error prone, we instead split
    each partition into *segments*. By default, each segment contains either 1 GB
    of data or a week of data, whichever is smaller. As a Kafka broker is writing
    to a partition, if the segment limit is reached, it closes the file and starts
    a new one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在大文件中查找需要清除的消息然后删除文件的一部分既耗时又容易出错，我们改为将每个分区分割成*段*。默认情况下，每个段包含1GB的数据或一周的数据，以较小者为准。当Kafka代理写入分区时，如果达到段限制，它将关闭文件并开始新文件。
- en: The segment we are currently writing to is called an *active segment*. The active
    segment is never deleted, so if you set log retention to only store a day of data,
    but each segment contains five days of data, you will really keep data for five
    days because we can’t delete the data before the segment is closed. If you choose
    to store data for a week and roll a new segment every day, you will see that every
    day we will roll a new segment while deleting the oldest segment—so most of the
    time the partition will have seven segments.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前正在写入的段称为*活动段*。活动段永远不会被删除，因此，如果您将日志保留设置为仅存储一天的数据，但每个段包含五天的数据，您实际上将保留五天的数据，因为我们不能在段关闭之前删除数据。如果您选择存储一周的数据并且每天滚动一个新段，您将看到每天我们将滚动一个新段，同时删除最旧的段——因此大部分时间分区将有七个段。
- en: As you learned in [Chapter 2](ch02.html#installing_kafka), a Kafka broker will
    keep an open file handle to every segment in every partition—even inactive segments.
    This leads to an usually high number of open file handles, and the OS must be
    tuned accordingly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第2章](ch02.html#installing_kafka)中学到的，Kafka代理将保持对每个分区中每个段的打开文件句柄，即使是非活动段。这导致打开文件句柄的数量通常很高，操作系统必须相应地进行调整。
- en: File Format
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件格式
- en: Each segment is stored in a single data file. Inside the file, we store Kafka
    messages and their offsets. The format of the data on the disk is identical to
    the format of the messages that we send from the producer to the broker and later
    from the broker to the consumers. Using the same message format on disk and over
    the wire is what allows Kafka to use zero-copy optimization when sending messages
    to consumers, and also avoid decompressing and recompressing messages that the
    producer already compressed. As a result, if we decide to change the message format,
    both the wire protocol and the on-disk format need to change, and Kafka brokers
    need to know how to handle cases in which files contain messages of two formats
    due to upgrades.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个段都存储在单个数据文件中。在文件内部，我们存储Kafka消息及其偏移量。磁盘上的数据格式与我们从生产者发送到代理，以及后来从代理发送到消费者的消息格式相同。在磁盘和网络上传输相同的消息格式是Kafka能够在向消费者发送消息时使用零拷贝优化，并且避免解压和重新压缩生产者已经压缩的消息。因此，如果我们决定更改消息格式，网络协议和磁盘格式都需要更改，Kafka代理需要知道如何处理包含两种格式消息的文件的情况。
- en: Kafka messages consist of user payload and system headers. User payload includes
    an optional key, a value, and an optional collection of headers, where each header
    is its own key/value pair.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka消息由用户有效载荷和系统头组成。用户有效载荷包括可选的键、值和可选的头集合，其中每个头都是其自己的键/值对。
- en: Starting with version 0.11 (and the v2 message format), Kafka producers always
    send messages in batches. If you send a single message, the batching adds a bit
    of overhead. But with two messages or more per batch, the batching saves space,
    which reduces network and disk usage. This is one of the reasons why Kafka performs
    better with `linger.ms=10`—the small delay increases the chance that more messages
    will be sent together. Since Kafka creates a separate batch per partition, producers
    that write to fewer partitions will be more efficient as well. Note that Kafka
    producers can include multiple batches in the same produce request. This means
    that if you are using compression on the producer (recommended!), sending larger
    batches means better compression both over the network and on the broker disks.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本0.11开始（以及v2消息格式），Kafka生产者始终以批处理方式发送消息。如果发送单个消息，批处理会增加一些开销。但是对于每批次两条或更多消息，批处理可以节省空间，从而减少网络和磁盘使用。这是Kafka在`linger.ms=10`时表现更好的原因之一——小延迟增加了更多消息一起发送的机会。由于Kafka为每个分区创建单独的批次，因此写入较少分区的生产者也将更有效。请注意，Kafka生产者可以在同一生产请求中包含多个批次。这意味着，如果您在生产者上使用压缩（建议！），发送更大的批次意味着在网络和代理磁盘上都会获得更好的压缩。
- en: 'Message batch headers include:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 消息批处理头包括：
- en: A magic number indicating the current version of the message format (here we’re
    documenting v2).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指示消息格式当前版本的魔术数字（这里我们正在记录v2）。
- en: The offset of the first message in the batch and the difference from the offset
    of the last message—those are preserved even if the batch is later compacted and
    some messages are removed. The offset of the first message is set to 0 when the
    producer creates and sends the batch. The broker that first persists this batch
    (the partition leader) replaces this with the real offset.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理中第一条消息的偏移量以及与最后一条消息偏移量的差异——即使批处理后来被压缩并删除了一些消息，这些偏移量也会被保留。当生产者创建并发送批处理时，第一条消息的偏移量设置为0。首次持久化此批处理的代理（分区领导者）将其替换为真实偏移量。
- en: The timestamps of the first message and the highest timestamp in the batch.
    The timestamps can be set by the broker if the timestamp type is set to append
    time rather than create time.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一条消息的时间戳和批处理中最高时间戳。如果时间戳类型设置为追加时间而不是创建时间，代理可以设置时间戳。
- en: Size of the batch, in bytes.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理的大小，以字节为单位。
- en: The epoch of the leader that received the batch (this is used when truncating
    messages after leader election; [KIP-101](https://oreil.ly/Ffa4D) and [KIP-279](https://oreil.ly/LO7nx)
    explain the usage in detail).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收批处理的领导者的时代（这在领导者选举后截断消息时使用；[KIP-101](https://oreil.ly/Ffa4D)和[KIP-279](https://oreil.ly/LO7nx)详细解释了使用方法）。
- en: Checksum for validating that the batch is not corrupted.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于验证批处理是否损坏的校验和。
- en: 'Sixteen bits indicating different attributes: compression type, timestamp type
    (timestamp can be set at the client or at the broker), and whether the batch is
    part of a transaction or is a control batch.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 十六位表示不同的属性：压缩类型、时间戳类型（时间戳可以在客户端或经纪人处设置），以及批次是否属于事务或是控制批次。
- en: Producer ID, producer epoch, and the first sequence in the batch—these are all
    used for exactly-once guarantees.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者ID、生产者时代和批次中的第一个序列——这些都用于确保精确一次。
- en: And, of course, the set of messages that are part of the batch.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当然，批次中包含的消息集。
- en: 'As you can see, the batch header includes a lot of information. The records
    themselves also have system headers (not to be confused with headers that can
    be set by users). Each record includes:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，批处理头包含了大量信息。记录本身也有系统头（不要与用户设置的头混淆）。每个记录包括：
- en: Size of the record, in bytes
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录的大小，以字节为单位
- en: Attributes—currently there are no record-level attributes, so this isn’t used
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性——目前没有记录级属性，因此这不会被使用
- en: The difference between the offset of the current record and the first offset
    in the batch
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前记录的偏移量与批次中第一个偏移量之间的差异
- en: The difference, in milliseconds, between the timestamp of this record and the
    first timestamp in the batch
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该记录时间戳与批次中第一个时间戳之间的差异（以毫秒为单位）
- en: 'The user payload: key, value, and headers'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户有效负载：键、值和头
- en: Note that there is very little overhead to each record, and most of the system
    information is at the batch level. Storing the first offset and timestamp of the
    batch in the header and only storing the difference in each record dramatically
    reduces the overhead of each record, making larger batches more efficient.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个记录的开销非常小，大部分系统信息都在批处理级别。在头部存储批处理的第一个偏移量和时间戳，并且仅在每个记录中存储差异，大大减少了每个记录的开销，使得更大的批次更有效。
- en: 'In addition to message batches that contain user data, Kafka also has control
    batches—indicating transactional commits, for instance. Those are handled by the
    consumer and not passed to the user application, and currently they include a
    version and a type indicator: 0 for an aborted transaction, 1 for a commit.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了包含用户数据的消息批次外，Kafka还有控制批次，例如表示事务提交。这些由消费者处理，不会传递给用户应用程序，目前它们包括版本和类型指示器：0表示中止事务，1表示提交。
- en: 'If you wish to see all this for yourself, Kafka brokers ship with the `DumpLogSegment`
    tool, which allows you to look at a partition segment in the filesystem and examine
    its contents. You can run the tool using:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望自己查看所有这些内容，Kafka经纪人附带了`DumpLogSegment`工具，允许您查看文件系统中的分区段并检查其内容。您可以使用以下命令运行该工具：
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you choose the `--deep-iteration` parameter, it will show you information
    about messages compressed inside the wrapper messages.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择`--deep-iteration`参数，它将向您显示包装消息内部压缩的消息的信息。
- en: Message Format Down Conversion
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息格式向下转换
- en: 'The message format documented earlier was introduced in version 0.11\. Since
    Kafka supports upgrading brokers before all the clients are upgraded, it had to
    support any combination of versions between the broker, producer, and consumer.
    Most combinations work with no issues—new brokers will understand the old message
    format from producers, and new producers will know to send old format messages
    to old brokers. But there is a challenging situation when a new producer sends
    v2 messages to new brokers: the message is stored in v2 format, but an old consumer
    that doesn’t support v2 format tries to read it. In this scenario, the broker
    will need to convert the message from v2 format to v1, so the consumer will be
    able to parse it. This conversion uses far more CPU and memory than normal consumption,
    so it is best avoided. [KIP-188](https://oreil.ly/9RwQC) introduced several important
    health metrics, among them `FetchMessageConversionsPerSec` and `MessageConversions​TimeMs`.
    If your organization is still using old clients, we recommend checking the metrics
    and upgrading the clients as soon as possible.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候记录的消息格式是在0.11版本中引入的。由于Kafka支持在所有客户端升级之前升级经纪人，因此它必须支持经纪人、生产者和消费者之间的任何版本组合。大多数组合都可以正常工作——新经纪人将理解生产者的旧消息格式，并且新生产者将知道将旧格式的消息发送到旧经纪人。但是当新生产者向新经纪人发送v2消息时，就会出现一个具有挑战性的情况：消息以v2格式存储，但不支持v2格式的旧消费者尝试读取它。在这种情况下，经纪人将需要将消息从v2格式转换为v1格式，以便消费者能够解析它。这种转换比正常消费使用更多的CPU和内存，因此最好避免。[KIP-188](https://oreil.ly/9RwQC)引入了几个重要的健康指标，其中包括`FetchMessageConversionsPerSec`和`MessageConversions​TimeMs`。如果您的组织仍在使用旧客户端，我们建议检查这些指标，并尽快升级客户端。
- en: Indexes
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: Kafka allows consumers to start fetching messages from any available offset.
    This means that if a consumer asks for 1 MB messages starting at offset 100, the
    broker must be able to quickly locate the message for offset 100 (which can be
    in any of the segments for the partition) and start reading the messages from
    that offset on. In order to help brokers quickly locate the message for a given
    offset, Kafka maintains an index for each partition. The index maps offsets to
    segment files and positions within the file.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka允许消费者从任何可用的偏移量开始获取消息。这意味着，如果消费者要求从偏移量100开始获取1MB的消息，那么经纪人必须能够快速定位偏移量100的消息（可能在分区的任何段中），并从该偏移量开始读取消息。为了帮助经纪人快速定位给定偏移量的消息，Kafka为每个分区维护一个索引。该索引将偏移量映射到段文件和文件内的位置。
- en: Similarly, Kafka has a second index that maps timestamps to message offsets.
    This index is used when searching for messages by timestamp. Kafka Streams uses
    this lookup extensively, and it is also useful in some failover scenarios.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Kafka还有第二个索引，将时间戳映射到消息偏移量。在按时间戳搜索消息时使用此索引。Kafka Streams广泛使用此查找，并且在某些故障转移场景中也很有用。
- en: Indexes are also broken into segments, so we can delete old index entries when
    the messages are purged. Kafka does not attempt to maintain checksums of the index.
    If the index becomes corrupted, it will get regenerated from the matching log
    segment simply by rereading the messages and recording the offsets and locations.
    It is also completely safe (albeit, it can cause a lengthy recovery) for an administrator
    to delete index segments if needed—they will be regenerated automatically.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Compaction
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, Kafka will store messages for a set amount of time and purge messages
    older than the retention period. However, imagine a case where you use Kafka to
    store shipping addresses for your customers. In that case, it makes more sense
    to store the last address for each customer rather than data for just the last
    week or year. This way, you don’t have to worry about old addresses, and you still
    retain the address for customers who haven’t moved in a while. Another use case
    can be an application that uses Kafka to store its current state. Every time the
    state changes, the application writes the new state into Kafka. When recovering
    from a crash, the application reads those messages from Kafka to recover its latest
    state. In this case, it only cares about the latest state before the crash, not
    all the changes that occurred while it was running.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Kafka supports such use cases by allowing the retention policy on a topic to
    be *delete*, which deletes events older than retention time, or to be *compact*,
    which only stores the most recent value for each key in the topic. Obviously,
    setting the policy to compact only makes sense on topics for which applications
    produce events that contain both a key and a value. If the topic contains *null*
    keys, compaction will fail.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Topics can also have a `delete.and.compact` policy that combines compaction
    with a retention period. Messages older than the retention period will be removed
    even if they are the most recent value for a key. This policy prevents compacted
    topics from growing overly large and is also used when the business requires removing
    records after a certain time period.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: How Compaction Works
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each log is viewed as split into two portions (see [Figure 6-6](#fig-7-clean-dirty)):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Clean
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Messages that have been compacted before. This section contains only one value
    for each key, which is the latest value at the time of the previous compaction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Dirty
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Messages that were written after the last compaction.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0606](assets/kdg2_0606.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Partition with clean and dirty portions
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If compaction is enabled when Kafka starts (using the awkwardly named `log.cleaner.enabled`
    configuration), each broker will start a compaction manager thread and a number
    of compaction threads. These are responsible for performing the compaction tasks.
    Each thread chooses the partition with the highest ratio of dirty messages to
    total partition size and cleans this partition.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: To compact a partition, the cleaner thread reads the dirty section of the partition
    and creates an in-memory map. Each map entry is comprised of a 16-byte hash of
    a message key and the 8-byte offset of the previous message that had this same
    key. This means each map entry only uses 24 bytes. If we look at a 1 GB segment
    and assume that each message in the segment takes up 1 KB, the segment will contain
    1 million such messages, and we will only need a 24 MB map to compact the segment
    (we may need a lot less—if the keys repeat themselves, we will reuse the same
    hash entries often and use less memory). This is quite efficient!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: When configuring Kafka, the administrator configures how much memory compaction
    threads can use for this offset map. Even though each thread has its own map,
    the configuration is for total memory across all threads. If you configured 1
    GB for the compaction offset map and you have 5 cleaner threads, each thread will
    get 200 MB for its own offset map. Kafka doesn’t require the entire dirty section
    of the partition to fit into the size allocated for this map, but at least one
    full segment has to fit. If it doesn’t, Kafka will log an error, and the administrator
    will need to either allocate more memory for the offset maps or use fewer cleaner
    threads. If only a few segments fit, Kafka will start by compacting the oldest
    segments that fit into the map. The rest will remain dirty and wait for the next
    compaction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Once the cleaner thread builds the offset map, it will start reading off the
    clean segments, starting with the oldest, and check their contents against the
    offset map. For each message, it checks if the key of the message exists in the
    offset map. If the key does not exist in the map, the value of the message just
    read is still the latest, and the message is copied over to a replacement segment.
    If the key does exist in the map, the message is omitted because there is a message
    with an identical key but newer value later in the partition. Once all the messages
    that still contain the latest value for their key are copied over, the replacement
    segment is swapped for the original and the thread on to the next segment. At
    the end of the process, we are left with one message per key—the one with the
    latest value. See [Figure 6-7](#fig-8-compacted).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0607](assets/kdg2_0607.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Partition segment before and after compaction
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deleted Events
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we always keep the latest message for each key, what do we do when we really
    want to delete all messages for a specific key, such as if a user left our service
    and we are legally obligated to remove all traces of that user from our system?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: To delete a key from the system completely, not even saving the last message,
    the application must produce a message that contains that key and a null value.
    When the cleaner thread finds such a message, it will first do a normal compaction
    and retain only the message with the null value. It will keep this special message
    (known as a *tombstone*) around for a configurable amount of time. During this
    time, consumers will be able to see this message and know that the value is deleted.
    So if a consumer copies data from Kafka to a relational database, it will see
    the tombstone message and know to delete the user from the database. After this
    set amount of time, the cleaner thread will remove the tombstone message, and
    the key will be gone from the partition in Kafka. It is important to give consumers
    enough time to see the tombstone message, because if our consumer was down for
    a few hours and missed the tombstone message, it will simply not see the key when
    consuming and therefore not know that it was deleted from Kafka or that it needs
    to be deleted from the database.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth remembering that Kafka’s admin client also includes a `deleteRecords`
    method. This method deletes all records before a specified offset, and it uses
    a completely different mechanism. When this method is called, Kafka will move
    the low-water mark, its record of the first offset of a partition, to the specified
    offset. This will prevent consumers from consuming the records below the new low-water
    mark and effectively makes these records inaccessible until they get deleted by
    a cleaner thread. This method can be used on topics with a retention policy and
    on compacted topics.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: When Are Topics Compacted?
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the same way that the `delete` policy never deletes the current active segments,
    the `compact` policy never compacts the current segment. Messages are eligible
    for compaction only on inactive segments.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: By default, Kafka will start compacting when 50% of the topic contains dirty
    records. The goal is not to compact too often (since compaction can impact the
    read/write performance on a topic) but also not to leave too many dirty records
    around (since they consume disk space). Wasting 50% of the disk space used by
    a topic on dirty records and then compacting them in one go seems like a reasonable
    trade-off, and it can be tuned by the administrator.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当主题包含脏记录的比例达到50%时，Kafka将开始紧缩。目标不是经常进行紧缩（因为紧缩可能会影响主题的读/写性能），但也不要留下太多脏记录（因为它们会占用磁盘空间）。在主题使用的磁盘空间的50%上浪费脏记录，然后一次性进行紧缩似乎是一个合理的折衷方案，并且可以由管理员进行调整。
- en: 'In addition, administrators can control the timing of compaction with two configuration
    parameters:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，管理员可以通过两个配置参数控制紧缩的时间：
- en: '`min.compaction.lag.ms` can be used to guarantee the minimum length of time
    that must pass after a message is written before it could be compacted.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min.compaction.lag.ms`可用于保证消息写入后必须经过的最短时间，然后才能进行紧缩。'
- en: '`max.compaction.lag.ms` can be used to guarantee the maximum delay between
    the time a message is written and the time the message becomes eligible for compaction.
    This configuration is often used in situations where there is a business reason
    to guarantee compaction within a certain period; for example, GDPR requires that
    certain information will be deleted within 30 days after a request to delete has
    been made.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max.compaction.lag.ms`可用于保证消息写入后到达紧缩资格的最大延迟时间。这种配置通常用于需要在一定时间内保证紧缩的业务场景；例如，GDPR要求在收到删除请求后的30天内删除某些信息。'
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: There is obviously more to Kafka than we could cover in this chapter, but we
    hope this gave you a taste of the kind of design decisions and optimizations the
    Kafka community made when working on the project and perhaps explained some of
    the more obscure behaviors and configurations you’ve run into while using Kafka.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka显然不仅仅是我们在本章中所涵盖的内容，但我们希望这能让您对Kafka社区在开发项目时所做的设计决策和优化有所了解，并且或许解释了您在使用Kafka时遇到的一些更加晦涩的行为和配置。
- en: If you are really interested in Kafka internals, there is no substitute for
    reading the code. The Kafka developer mailing list ([dev@kafka.apache.org](mailto:dev@kafka.apache.org))
    is a very friendly community, and there is always someone willing to answer questions
    regarding how Kafka really works. And while you are reading the code, perhaps
    you can fix a bug or two—open source projects always welcome contributions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您真的对Kafka内部感兴趣，那么没有什么比阅读代码更好的了。Kafka开发者邮件列表（[dev@kafka.apache.org](mailto:dev@kafka.apache.org)）是一个非常友好的社区，总会有人愿意回答关于Kafka实际运行方式的问题。而且在阅读代码的同时，也许您可以修复一个或两个bug——开源项目总是欢迎贡献。

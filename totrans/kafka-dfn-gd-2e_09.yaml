- en: Chapter 7\. Reliable Data Delivery
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。可靠数据传递
- en: Reliability is a property of a system—not of a single component—so when we are
    talking about the reliability guarantees of Apache Kafka, we will need to keep
    the entire system and its use cases in mind. When it comes to reliability, the
    systems that integrate with Kafka are as important as Kafka itself. And because
    reliability is a system concern, it cannot be the responsibility of just one person.
    Everyone—Kafka administrators, Linux administrators, network and storage administrators,
    and the application developers—must work together to build a reliable system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性是系统的属性，而不是单个组件的属性，因此当我们谈论Apache Kafka的可靠性保证时，我们需要牢记整个系统及其用例。在可靠性方面，与Kafka集成的系统和Kafka本身一样重要。由于可靠性是一个系统问题，它不能仅仅是一个人的责任。每个人——Kafka管理员、Linux管理员、网络和存储管理员以及应用程序开发人员——都必须共同努力构建一个可靠的系统。
- en: Apache Kafka is very flexible about reliable data delivery. We understand that
    Kafka has many use cases, from tracking clicks in a website to credit card payments.
    Some of the use cases require utmost reliability, while others prioritize speed
    and simplicity over reliability. Kafka was written to be configurable enough,
    and its client API flexible enough, to allow all kinds of reliability trade-offs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka在可靠数据传递方面非常灵活。我们知道Kafka有许多用例，从跟踪网站上的点击到信用卡支付。一些用例要求最高的可靠性，而其他一些则将速度和简单性置于可靠性之上。Kafka被设计为足够可配置，其客户端API足够灵活，以允许各种可靠性权衡。
- en: Because of its flexibility, it is also easy to accidentally shoot ourselves
    in the foot when using Kafka—believing that our system is reliable when in fact
    it is not. In this chapter, we will start by talking about different kinds of
    reliability and what they mean in the context of Apache Kafka. Then we will talk
    about Kafka’s replication mechanism and how it contributes to the reliability
    of the system. We will then discuss Kafka’s brokers and topics and how they should
    be configured for different use cases. Then we will discuss the clients, producer,
    and consumer, and how they should be used in different reliability scenarios.
    Last, we will discuss the topic of validating the system reliability, because
    it is not enough to believe a system is reliable—the assumption must be thoroughly
    tested.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其灵活性，使用Kafka时也很容易不小心自食其果，认为我们的系统是可靠的，而实际上并非如此。在本章中，我们将首先讨论可靠性的不同类型及其在Apache
    Kafka环境中的含义。然后我们将讨论Kafka的复制机制以及它如何促进系统的可靠性。接下来我们将讨论Kafka的代理和主题以及它们在不同用例中的配置方式。然后我们将讨论客户端、生产者和消费者，以及它们在不同可靠性场景中的使用方式。最后，我们将讨论验证系统可靠性的话题，因为仅仅相信系统是可靠的是不够的——假设必须经过彻底的测试。
- en: Reliability Guarantees
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可靠性保证
- en: When we talk about reliability, we usually talk in terms of *guarantees*, which
    are the behaviors a system is guaranteed to preserve under different circumstances.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论可靠性时，通常是以*保证*的术语来谈论的，这些保证是系统在不同情况下保证保持的行为。
- en: Probably the best-known reliability guarantee is ACID, which is the standard
    reliability guarantee that relational databases universally support. ACID stands
    for *atomicity*, *consistency*, *isolation*, and *durability*. When a vendor explains
    that their database is ACID compliant, it means the database guarantees certain
    behaviors regarding transaction behavior.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最为人熟知的可靠性保证是ACID，这是关系数据库普遍支持的标准可靠性保证。ACID代表*原子性*、*一致性*、*隔离性*和*持久性*。当供应商解释他们的数据库符合ACID时，这意味着数据库保证了关于事务行为的某些行为。
- en: Those guarantees are the reason people trust relational databases with their
    most critical applications—they know exactly what the system promises and how
    it will behave in different conditions. They understand the guarantees and can
    write safe applications by relying on those guarantees.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些保证是人们信任关系数据库的原因，他们知道系统承诺了什么以及在不同条件下它将如何行为。他们理解这些保证，并且可以依靠这些保证编写安全的应用程序。
- en: Understanding the guarantees Kafka provides is critical for those seeking to
    build reliable applications. This understanding allows the developers of the system
    to figure out how it will behave under different failure conditions. So, what
    does Apache Kafka guarantee?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Kafka提供的保证对于那些希望构建可靠应用程序的人来说至关重要。这种理解使系统的开发人员能够弄清在不同的故障条件下它将如何行为。那么，Apache
    Kafka提供了什么样的保证呢？
- en: Kafka provides order guarantee of messages in a partition. If message B was
    written after message A, using the same producer in the same partition, then Kafka
    guarantees that the offset of message B will be higher than message A, and that
    consumers will read message B after message A.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka提供了分区中消息的顺序保证。如果消息B是在消息A之后使用相同的生产者在同一分区中写入的，那么Kafka保证消息B的偏移量将高于消息A，并且消费者将在消息A之后读取消息B。
- en: Produced messages are considered “committed” when they were written to the partition
    on all its in-sync replicas (but not necessarily flushed to disk). Producers can
    choose to receive acknowledgments of sent messages when the message was fully
    committed, when it was written to the leader, or when it was sent over the network.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当消息被写入所有其同步副本的分区时（但不一定刷新到磁盘），产生的消息被视为“已提交”。生产者可以选择在消息完全提交时、在消息被写入领导者时或在消息被发送到网络时接收发送消息的确认。
- en: Messages that are committed will not be lost as long as at least one replica
    remains alive.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要至少有一个副本保持存活，已提交的消息就不会丢失。
- en: Consumers can only read messages that are committed.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者只能读取已提交的消息。
- en: These basic guarantees can be used while building a reliable system, but in
    themselves, they don’t make the system fully reliable. There are trade-offs involved
    in building a reliable system, and Kafka was built to allow administrators and
    developers to decide how much reliability they need by providing configuration
    parameters that allow controlling these trade-offs. The trade-offs usually involve
    how important it is to reliably and consistently store messages versus other important
    considerations, such as availability, high throughput, low latency, and hardware
    costs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本保证可以在构建可靠系统时使用，但它们本身并不能使系统完全可靠。在构建可靠系统时涉及到权衡，Kafka被设计为允许管理员和开发人员通过提供配置参数来控制这些权衡，从而决定他们需要多少可靠性。这些权衡通常涉及到可靠和一致地存储消息的重要性与其他重要考虑因素之间的权衡，比如可用性、高吞吐量、低延迟和硬件成本。
- en: We next review Kafka’s replication mechanism, introduce terminology, and discuss
    how reliability is built into Kafka. After that, we go over the configuration
    parameters we just mentioned.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将回顾Kafka的复制机制，介绍术语，并讨论可靠性是如何内置到Kafka中的。之后，我们将讨论刚才提到的配置参数。
- en: Replication
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制
- en: Kafka’s replication mechanism, with its multiple replicas per partition, is
    at the core of all of Kafka’s reliability guarantees. Having a message written
    in multiple replicas is how Kafka provides durability of messages in the event
    of a crash.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的复制机制，以及每个分区的多个副本，是Kafka所有可靠性保证的核心。在多个副本中写入消息是Kafka在发生崩溃时提供消息持久性的方式。
- en: We explained Kafka’s replication mechanism in depth in [Chapter 6](ch06.html#kafka_internals),
    but let’s recap the highlights here.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](ch06.html#kafka_internals)中深入解释了Kafka的复制机制，但让我们在这里回顾一下要点。
- en: Each Kafka topic is broken down into *partitions*, which are the basic data
    building blocks. A partition is stored on a single disk. Kafka guarantees the
    order of events within a partition, and a partition can be either online (available)
    or offline (unavailable). Each partition can have multiple replicas, one of which
    is a designated leader. All events are produced to the leader replica and are
    usually consumed from the leader replica as well. Other replicas just need to
    stay in sync with the leader and replicate all the recent events on time. If the
    leader becomes unavailable, one of the in-sync replicas becomes the new leader
    (there is an exception to this rule, which we discussed in [Chapter 6](ch06.html#kafka_internals)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Kafka主题都被分解成*分区*，这是基本的数据构建块。一个分区存储在一个磁盘上。Kafka保证分区内事件的顺序，并且一个分区可以是在线的（可用的）或离线的（不可用的）。每个分区可以有多个副本，其中一个是指定的领导者。所有事件都是由领导者副本产生的，并且通常也是从领导者副本消费的。其他副本只需要与领导者保持同步，并及时复制所有最近的事件。如果领导者不可用，一个同步的副本将成为新的领导者（这条规则有一个例外，我们在[第6章](ch06.html#kafka_internals)中讨论过）。
- en: 'A replica is considered in sync if it is the leader for a partition, or if
    it is a follower that:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个副本是分区的领导者，或者是一个从者，它被认为是同步的，如果它在最后10秒内：
- en: Has an active session with ZooKeeper—meaning that it sent a heartbeat to ZooKeeper
    in the last 6 seconds (configurable).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与ZooKeeper有一个活动会话——意味着它在最近6秒内向ZooKeeper发送了心跳（可配置）。
- en: Fetched messages from the leader in the last 10 seconds (configurable).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后10秒内从领导者获取的消息（可配置）。
- en: Fetched the most recent messages from the leader in the last 10 seconds. That
    is, it isn’t enough that the follower is still getting messages from the leader;
    it must have had no lag at least once in the last 10 seconds (configurable).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从领导者那里获取了最近的消息。也就是说，从者仍然从领导者那里获取消息是不够的；它必须在最近的10秒内至少一次没有滞后（可配置）。
- en: If a replica loses connection to ZooKeeper, stops fetching new messages, or
    falls behind and can’t catch up within 10 seconds, the replica is considered out
    of sync. An out-of-sync replica gets back into sync when it connects to ZooKeeper
    again and catches up to the most recent message written to the leader. This usually
    happens quickly after a temporary network glitch is healed but can take a while
    if the broker the replica is stored on was down for a longer period of time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个副本失去与ZooKeeper的连接，停止获取新消息，或者落后并且无法在10秒内赶上，那么该副本被认为是失步的。当失步的副本再次连接到ZooKeeper并赶上最近写入领导者的消息时，它就会重新同步。这通常在暂时的网络故障修复后很快发生，但如果存储副本的代理服务器长时间宕机，可能需要一段时间才能发生。
- en: Out-of-Sync Replicas
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 失步的副本
- en: In older versions of Kafka, it was not uncommon to see one or more replicas
    rapidly flip between in-sync and out-of-sync status. This was a sure sign that
    something was wrong with the cluster. A relatively common cause was a large maximum
    request size and large JVM heap that required tuning to prevent long garbage collection
    pauses that would cause the broker to temporarily disconnect from ZooKeeper. These
    days the problem is very rare, especially when using Apache Kafka release 2.5.0
    and higher with its default configurations for ZooKeeper connection timeout and
    maximum replica lag. The use of JVM version 8 and above (now the minimum version
    supported by Kafka) with [G1 garbage collector](https://oreil.ly/oDL86) helped
    curb this problem, although tuning may still be required for large messages. Generally
    speaking, Kafka’s replication protocol became significantly more reliable in the
    years since the first edition of the book was published. For details on the evolution
    of Kafka’s replication protocol, refer to Jason Gustafson’s excellent talk, [“Hardening
    Apache Kafka Replication”](https://oreil.ly/Z1R1w), and Gwen Shapira’s overview
    of Kafka improvements, [“Please Upgrade Apache Kafka Now”](https://oreil.ly/vKnVl).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧版本的Kafka中，看到一个或多个副本在同步和不同步状态之间迅速切换并不罕见。这是集群出现问题的明显迹象。一个相对常见的原因是较大的最大请求大小和大的JVM堆，需要调整以防止长时间的垃圾回收暂停，导致经纪人暂时断开与ZooKeeper的连接。如今，这个问题非常罕见，特别是在使用Apache
    Kafka 2.5.0及更高版本时，其默认配置用于ZooKeeper连接超时和最大副本滞后。使用JVM 8及以上版本（现在是Kafka支持的最低版本）与[G1垃圾收集器](https://oreil.ly/oDL86)有助于遏制这个问题，尽管对于大消息可能仍需要调整。一般来说，自第一版书籍出版以来，Kafka的复制协议在这些年里变得更加可靠。有关Kafka复制协议演变的详细信息，请参考Jason
    Gustafson的出色演讲[“Hardening Apache Kafka Replication”](https://oreil.ly/Z1R1w)，以及Gwen
    Shapira对Kafka改进的概述[“Please Upgrade Apache Kafka Now”](https://oreil.ly/vKnVl)。
- en: An in-sync replica that is slightly behind can slow down producers and consumers—since
    they wait for all the in-sync replicas to get the message before it is *committed*.
    Once a replica falls out of sync, we no longer wait for it to get messages. It
    is still behind, but now there is no performance impact. The catch is that with
    fewer in-sync replicas, the effective replication factor of the partition is lower,
    and therefore there is a higher risk for downtime or data loss.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 稍有滞后的同步副本会减慢生产者和消费者的速度——因为它们等待所有同步副本收到消息后才会*提交*。一旦副本不再同步，我们就不再等待它接收消息。它仍然滞后，但现在没有性能影响。问题在于，同步副本越少，分区的有效复制因子就越低，因此停机或数据丢失的风险就越高。
- en: In the next section, we will look at what this means in practice.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看看这在实践中意味着什么。
- en: Broker Configuration
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经纪人配置
- en: There are three configuration parameters in the broker that change Kafka’s behavior
    regarding reliable message storage. Like many broker configuration variables,
    these can apply at the broker level, controlling configuration for all topics
    in the system, and at the topic level, controlling behavior for a specific topic.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 经纪人中有三个配置参数，它们改变了Kafka关于可靠消息存储的行为。像许多经纪人配置变量一样，这些可以应用于经纪人级别，控制系统中所有主题的配置，也可以应用于主题级别，控制特定主题的行为。
- en: Being able to control reliability trade-offs at the topic level means that the
    same Kafka cluster can be used to host reliable and nonreliable topics. For example,
    at a bank, the administrator will probably want to set very reliable defaults
    for the entire cluster but make an exception to the topic that stores customer
    complaints where some data loss is acceptable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在主题级别控制可靠性权衡意味着同一个Kafka集群可以用来托管可靠和不可靠的主题。例如，在银行，管理员可能希望为整个集群设置非常可靠的默认值，但对于存储客户投诉的主题做出例外，其中一些数据丢失是可以接受的。
- en: Let’s look at these configuration parameters one by one and see how they affect
    the reliability of message storage in Kafka and the trade-offs involved.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看这些配置参数，看看它们如何影响Kafka中消息存储的可靠性以及涉及的权衡考虑。
- en: Replication Factor
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制因子
- en: The topic-level configuration is `replication.factor`. At the broker level,
    we control the `default.replication.factor` for automatically created topics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 主题级别的配置是`replication.factor`。在经纪人级别，我们控制自动创建主题的`default.replication.factor`。
- en: Until this point in the book, we have assumed that topics had a replication
    factor of three, meaning that each partition is replicated three times on three
    different brokers. This was a reasonable assumption, as this is Kafka’s default,
    but this is a configuration that users can modify. Even after a topic exists,
    we can choose to add or remove replicas and thereby modify the replication factor
    using Kafka’s replica assignment tool.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们假设主题的复制因子为3，这意味着每个分区在三个不同的经纪人上被复制三次。这是一个合理的假设，因为这是Kafka的默认设置，但这是用户可以修改的配置。即使主题存在后，我们也可以选择添加或删除副本，从而使用Kafka的副本分配工具修改复制因子。
- en: A replication factor of *N* allows us to lose *N*-1 brokers while still being
    able to read and write data to the topic. So a higher replication factor leads
    to higher availability, higher reliability, and fewer disasters. On the flip side,
    for a replication factor of *N*, we will need at least *N* brokers and we will
    store *N* copies of the data, meaning we will need *N* times as much disk space.
    We are basically trading availability for hardware.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*的复制因子允许我们失去*N*-1个经纪人，同时仍能够读写数据到主题。因此，更高的复制因子会导致更高的可用性、更高的可靠性和更少的灾难。另一方面，对于*N*的复制因子，我们将需要至少*N*个经纪人，并且我们将存储*N*份数据，这意味着我们将需要*N*倍的磁盘空间。基本上，我们在可用性和硬件之间进行交易。'
- en: 'So how do we determine the right number of replicas for a topic? There are
    a few key considerations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何确定主题的正确副本数量呢？有一些关键考虑因素：
- en: Availability
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性
- en: A partition with just one replica will become unavailable even during a routine
    restart of a single broker. The more replicas we have, the higher availability
    we can expect.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个副本的分区即使在单个经纪人的例行重启期间也将变得不可用。我们拥有的副本越多，我们就可以期望更高的可用性。
- en: Durability
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 耐久性
- en: Each replica is a copy of all the data in a partition. If a partition has a
    single replica and the disk becomes unusable for any reason, we’ve lost all the
    data in the partition. With more copies, especially on different storage devices,
    the probability of losing all of them is reduced.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个副本都是分区中所有数据的副本。如果一个分区只有一个副本，并且由于任何原因磁盘变得无法使用，我们将丢失分区中的所有数据。拥有更多副本，特别是在不同的存储设备上，减少了丢失所有副本的可能性。
- en: Throughput
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量
- en: With each additional replica, we multiply the inter-broker traffic. If we produce
    to a partition at a rate of 10 MBps, then a single replica will not generate any
    replication traffic. If we have 2 replicas, then we’ll have 10 MBps replication
    traffic, with 3 replicas it will be 20 MBps, and with 5 replicas it will be 40
    MBps. We need to take this into account when planning the cluster size and capacity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每增加一个副本，我们就会增加代理之间的流量。如果我们以每秒10MB的速率向分区生成数据，那么单个副本将不会产生任何复制流量。如果我们有2个副本，那么我们将有10MBps的复制流量，有3个副本则为20MBps，有5个副本则为40MBps。在规划集群大小和容量时，我们需要考虑这一点。
- en: End-to-end latency
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端延迟
- en: Each produced record has to be replicated to all in-sync replicas before it
    is available for consumers. In theory, with more replicas, there is higher probability
    that one of these replicas is a bit slow and therefore will slow the consumers
    down. In practice, if one broker becomes slow for any reason, it will slow down
    every client that tries using it, regardless of replication factor.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生成的记录在可供消费者使用之前必须被复制到所有同步的副本中。理论上，拥有更多副本会增加其中一个副本速度较慢的概率，因此会减慢消费者的速度。实际上，如果一个代理因任何原因变慢，它将减慢每个尝试使用它的客户端的速度，无论复制因子如何。
- en: Cost
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: This is the most common reason for using a replication factor lower than 3 for
    noncritical data. The more replicas we have of our data, the higher the storage
    and network costs. Since many storage systems already replicate each block 3 times,
    it sometimes makes sense to reduce costs by configuring Kafka with a replication
    factor of 2\. Note that this will still reduce availability compared to a replication
    factor of 3, but durability will be guaranteed by the storage device.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用非关键数据的复制因子低于3的最常见原因。我们拥有的数据副本越多，存储和网络成本就越高。由于许多存储系统已经将每个块复制3次，因此通过配置Kafka的复制因子为2来降低成本有时是有意义的。请注意，与复制因子为3相比，这仍会降低可用性，但存储设备将保证耐久性。
- en: Placement of replicas is also very important. Kafka will always make sure each
    replica for a partition is on a separate broker. In some cases, this is not safe
    enough. If all replicas for a partition are placed on brokers that are on the
    same rack, and the top-of-rack switch misbehaves, we will lose availability of
    the partition regardless of the replication factor. To protect against rack-level
    misfortune, we recommend placing brokers in multiple racks and using the `broker.rack`
    broker configuration parameter to configure the rack name for each broker. If
    rack names are configured, Kafka will make sure replicas for a partition are spread
    across multiple racks in order to guarantee even higher availability. When running
    Kafka in cloud environments, it is common to consider availability zones as separate
    racks. In [Chapter 6](ch06.html#kafka_internals), we provided details on how Kafka
    places replicas on brokers and racks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 副本的放置也非常重要。Kafka将始终确保分区的每个副本位于不同的代理上。在某些情况下，这还不够安全。如果分区的所有副本都放置在同一机架上的代理上，并且顶部交换机出现故障，我们将失去分区的可用性，无论复制因子如何。为了防止机架级别的不幸，我们建议将代理放置在多个机架上，并使用`broker.rack`代理配置参数为每个代理配置机架名称。如果配置了机架名称，Kafka将确保分区的副本分布在多个机架上，以确保更高的可用性。在云环境中运行Kafka时，通常将可用性区域视为单独的机架。在[第6章](ch06.html#kafka_internals)中，我们提供了有关Kafka如何在代理和机架上放置副本的详细信息。
- en: Unclean Leader Election
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不洁净的领导者选举
- en: This configuration is only available at the broker (and in practice, cluster-wide)
    level. The parameter name is `unclean.leader.election.enable`, and by default
    it is set to `false`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置仅在代理（实际上是在整个集群范围内）级别可用。参数名称为`unclean.leader.election.enable`，默认设置为`false`。
- en: As explained earlier, when the leader for a partition is no longer available,
    one of the in-sync replicas will be chosen as the new leader. This leader election
    is “clean” in the sense that it guarantees no loss of committed data—by definition,
    committed data exists on all in-sync replicas.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当分区的领导者不再可用时，将选择一个同步的副本作为新的领导者。这种领导者选举是“干净的”，因为它保证没有丢失已提交的数据——根据定义，已提交的数据存在于所有同步的副本上。
- en: But what do we do when no in-sync replica exists except for the leader that
    just became unavailable?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当除了刚刚变得不可用的领导者之外，没有同步的副本存在时，我们该怎么办呢？
- en: 'This situation can happen in one of two scenarios:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况可能发生在以下两种情况之一：
- en: The partition had three replicas, and the two followers became unavailable (let’s
    say two brokers crashed). In this situation, as producers continue writing to
    the leader, all the messages are acknowledged and committed (since the leader
    is the one and only in-sync replica). Now let’s say that the leader becomes unavailable
    (oops, another broker crash). In this scenario, if one of the out-of-sync followers
    starts first, we have an out-of-sync replica as the only available replica for
    the partition.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区有三个副本，两个跟随者变得不可用（假设两个代理崩溃）。在这种情况下，当生产者继续写入领导者时，所有消息都被确认和提交（因为领导者是唯一的同步副本）。现在假设领导者不再可用（哎呀，又一个代理崩溃了）。在这种情况下，如果一个不同步的跟随者首先启动，我们将有一个不同步的副本作为分区的唯一可用副本。
- en: The partition had three replicas, and due to network issues, the two followers
    fell behind so that even though they are up and replicating, they are no longer
    in sync. The leader keeps accepting messages as the only in-sync replica. Now
    if the leader becomes unavailable, there are only out-of-sync replicas available
    to become leaders.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区有三个副本，由于网络问题，两个跟随者落后，即使它们已经上线并复制，它们也不再同步。领导者继续接受消息作为唯一的同步副本。现在，如果领导者不可用，只有不同步的副本可以成为领导者。
- en: 'In both these scenarios, we need to make a difficult decision:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们需要做出一个困难的决定：
- en: If we don’t allow the out-of-sync replica to become the new leader, the partition
    will remain offline until we bring the old leader (and the last in-sync replica)
    back online. In some cases (e.g., memory chip needs replacement), this can take
    many hours.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们不允许不同步的副本成为新的领导者，分区将保持离线，直到我们将旧领导者（和最后一个同步副本）重新上线。在某些情况下（例如，内存芯片需要更换），这可能需要很多小时。
- en: If we do allow the out-of-sync replica to become the new leader, we are going
    to lose all messages that were written to the old leader while that replica was
    out of sync and also cause some inconsistencies in consumers. Why? Imagine that
    while replicas 0 and 1 were not available, we wrote messages with offsets 100–200
    to replica 2 (then the leader). Now replica 2 is unavailable and replica 0 is
    back online. Replica 0 only has messages 0–100 but not 100–200\. If we allow replica
    0 to become the new leader, it will allow producers to write new messages and
    allow consumers to read them. So, now the new leader has completely new messages
    100–200\. First, let’s note that some consumers may have read the old messages
    100–200, some consumers got the new 100–200, and some got a mix of both. This
    can lead to pretty bad consequences when looking at things like downstream reports.
    In addition, replica 2 will come back online and become a follower of the new
    leader. At that point, it will delete any messages it got that don’t exist on
    the current leader. Those messages will not be available to any consumer in the
    future.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们允许不同步的副本成为新的领导者，我们将丢失所有在旧领导者不同步时写入的消息，并且还会导致一些消费者的不一致。为什么？想象一下，当副本0和1不可用时，我们将偏移量为100-200的消息写入副本2（然后成为领导者）。现在副本2不可用，副本0重新上线。副本0只有消息0-100，而没有100-200。如果我们允许副本0成为新的领导者，它将允许生产者写入新消息，并允许消费者读取它们。因此，现在新的领导者完全有新的消息100-200。首先，让我们注意到一些消费者可能已经读取了旧消息100-200，一些消费者得到了新的100-200，一些得到了混合的。当涉及到下游报告等事项时，这可能会导致非常糟糕的后果。此外，副本2将重新上线并成为新领导者的跟随者。在那时，它将删除任何它得到的但在当前领导者上不存在的消息。这些消息将不会对未来的任何消费者可用。
- en: In summary, if we allow out-of-sync replicas to become leaders, we risk data
    loss and inconsistencies. If we don’t allow them to become leaders, we face lower
    availability as we must wait for the original leader to become available before
    the partition is back online.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果我们允许不同步的副本成为领导者，我们就面临着数据丢失和不一致的风险。如果我们不允许它们成为领导者，我们将面临较低的可用性，因为我们必须等待原始领导者恢复正常，分区才能重新上线。
- en: By default, `unclean.leader.election.enable` is set to false, which will not
    allow out-of-sync replicas to become leaders. This is the safest option since
    it provides the best guarantees against data loss. It does mean that in the extreme
    unavailability scenarios that we described previously, some partitions will remain
    unavailable until manually recovered. It is always possible for an administrator
    to look at the situation, decide to accept the data loss in order to make the
    partitions available, and switch this configuration to true before starting the
    cluster. Just don’t forget to turn it back to false after the cluster recovered.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`unclean.leader.election.enable`被设置为false，这将不允许不同步的副本成为领导者。这是最安全的选择，因为它提供了最好的保证来防止数据丢失。这意味着在我们之前描述的极端不可用的情况下，一些分区将保持不可用，直到手动恢复。管理员始终可以查看情况，决定接受数据丢失以使分区可用，并在启动集群之前将此配置切换为true。只是不要忘记在集群恢复后将其切换回false。
- en: Minimum In-Sync Replicas
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小同步副本
- en: Both the topic and the broker-level configuration are called `min.insync.replicas`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 主题和经纪人级别的配置都称为`min.insync.replicas`。
- en: As we’ve seen, there are cases where even though we configured a topic to have
    three replicas, we may be left with a single in-sync replica. If this replica
    becomes unavailable, we may have to choose between availability and consistency.
    This is never an easy choice. Note that part of the problem is that, per Kafka
    reliability guarantees, data is considered committed when it is written to all
    in-sync replicas, even when `all` means just one replica and the data could be
    lost if that replica is unavailable.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，有些情况下，即使我们配置了一个主题有三个副本，我们可能只剩下一个同步副本。如果这个副本不可用，我们可能不得不在可用性和一致性之间做出选择。这从来都不是一个容易的选择。请注意，问题的一部分是，根据Kafka的可靠性保证，数据被认为是已提交的，当它被写入所有同步副本时，即使“所有”只意味着一个副本，如果该副本不可用，数据可能会丢失。
- en: When we want to be sure that committed data is written to more than one replica,
    we need to set the minimum number of in-sync replicas to a higher value. If a
    topic has three replicas and we set `min.insync.replicas` to `2`, then producers
    can only write to a partition in the topic if at least two out of the three replicas
    are in sync.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要确保提交的数据被写入多个副本时，我们需要将最小同步副本的数量设置为更高的值。如果一个主题有三个副本，我们将`min.insync.replicas`设置为`2`，那么生产者只能在至少两个副本中有同步的情况下写入主题中的分区。
- en: When all three replicas are in sync, everything proceeds normally. This is also
    true if one of the replicas becomes unavailable. However, if two out of three
    replicas are not available, the brokers will no longer accept produce requests.
    Instead, producers that attempt to send data will receive `NotEnoughReplicasException`.
    Consumers can continue reading existing data. In effect, with this configuration,
    a single in-sync replica becomes read-only. This prevents the undesirable situation
    where data is produced and consumed, only to disappear when unclean election occurs.
    In order to recover from this read-only situation, we must make one of the two
    unavailable partitions available again (maybe restart the broker) and wait for
    it to catch up and get in sync.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当三个副本都同步时，一切都会正常进行。如果其中一个副本不可用，情况也是如此。但是，如果三个副本中有两个不可用，经纪人将不再接受生产请求。相反，试图发送数据的生产者将收到`NotEnoughReplicasException`。消费者可以继续读取现有数据。实际上，使用这种配置，一个单独的同步副本将变为只读。这可以防止产生和消费数据，然后在发生不干净的选举时消失的不良情况。为了从这种只读状态中恢复，我们必须使两个不可用的分区中的一个再次可用（可能重新启动经纪人），并等待它赶上并同步。
- en: Keeping Replicas In Sync
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持副本同步
- en: 'As mentioned earlier, out-of-sync replicas decrease the overall reliability,
    so it is important to avoid these as much as possible. We also explained that
    a replica can become out of sync in one of two ways: either it loses connectivity
    to ZooKeeper or it fails to keep up with the leader and builds up a replication
    lag. Kafka has two broker configurations that control the sensitivity of the cluster
    to these two conditions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，不同步的副本会降低整体可靠性，因此尽量避免这种情况非常重要。我们还解释了副本可能以两种方式之一变得不同步：要么失去与ZooKeeper的连接，要么无法跟上领导者并积累复制延迟。Kafka有两个经纪人配置，用于控制集群对这两种情况的敏感度。
- en: '`zookeeper.session.timeout.ms` is the time interval during which a Kafka broker
    can stop sending heartbeats to ZooKeeper without ZooKeeper considering the broker
    dead and removing it from the cluster. In version 2.5.0, this value was increased
    from 6 seconds to 18 seconds, in order to increase the stability of Kafka clusters
    in cloud environments where network latencies show higher variance. In general,
    we want this time to be high enough to avoid random flapping caused by garbage
    collection or network conditions, but still low enough to make sure brokers that
    are actually frozen will be detected in a timely manner.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`zookeeper.session.timeout.ms`是Kafka经纪人可以在此期间停止向ZooKeeper发送心跳而ZooKeeper不认为经纪人已死并将其从集群中移除的时间间隔。在2.5.0版本中，这个值从6秒增加到18秒，以增加云环境中Kafka集群的稳定性，其中网络延迟显示更高的方差。通常，我们希望这个时间足够长，以避免由垃圾收集或网络条件引起的随机波动，但仍然足够低，以确保实际上被冻结的经纪人将及时被检测到。'
- en: If a replica did not fetch from the leader or did not catch up to the latest
    messages on the leader for longer than `replica.lag.time.max.ms`, it will become
    out of sync. This was increased from 10 seconds to 30 seconds in release 2.5.0
    to improve resilience of the cluster and avoid unnecessary flapping. Note that
    this higher value also impacts maximum latency for the consumer—with the higher
    value it can take up to 30 seconds until a message arrives to all replicas and
    the consumers are allowed to consume it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果副本未从领导者那里获取消息，或者未赶上领导者的最新消息时间超过`replica.lag.time.max.ms`，它将变得不同步。在2.5.0版本中，这个值从10秒增加到30秒，以提高集群的弹性并避免不必要的波动。请注意，这个更高的值也会影响消费者的最大延迟——使用更高的值可能需要长达30秒的时间，直到所有副本都收到消息并允许消费。
- en: Persisting to Disk
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化到磁盘
- en: We’ve mentioned a few times that Kafka will acknowledge messages that were not
    persisted to disk, depending just on the number of replicas that received the
    message. Kafka will flush messages to disk when rotating segments (by default
    1 GB in size) and before restarts but will otherwise rely on Linux page cache
    to flush messages when it becomes full. The idea behind this is that having three
    machines in separate racks or availability zones, each with a copy of the data,
    is safer than writing the messages to disk on the leader, because simultaneous
    failures on two different racks or zones are so unlikely. However, it is possible
    to configure the brokers to persist messages to disk more frequently. The configuration
    parameter `flush.messages` allows us to control the maximum number of messages
    not synced to disk, and `flush.ms` allows us to control the frequency of syncing
    to disk. Before using this feature, it is worth reading [how `fsync` impacts Kafka’s
    throughput and how to mitigate its drawbacks](https://oreil.ly/Ai1hl).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次提到，Kafka将确认未持久化到磁盘的消息，仅取决于接收消息的副本数量。Kafka将在旋转段（默认大小为1GB）之前和重新启动之前将消息刷新到磁盘，但在其他情况下，将依赖于Linux页面缓存在其变满时刷新消息。其背后的想法是，在分开的机架或可用区域中有三台机器，每台机器都有数据的副本，比在领导者上将消息写入磁盘更安全，因为两个不同机架或区域的同时故障是如此不太可能。但是，也可以配置经纪人更频繁地将消息持久化到磁盘。配置参数`flush.messages`允许我们控制未同步到磁盘的最大消息数量，`flush.ms`允许我们控制同步到磁盘的频率。在使用此功能之前，值得阅读[“`fsync`如何影响Kafka的吞吐量以及如何减轻其缺点”](https://oreil.ly/Ai1hl)。
- en: Using Producers in a Reliable System
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在可靠系统中使用生产者
- en: Even if we configure the brokers in the most reliable configuration possible,
    the system as a whole can still potentially lose data if we don’t configure the
    producers to be reliable as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们将经纪人配置为可能的最可靠配置，如果我们不配置生产者也是可靠的，整个系统仍然可能会丢失数据。
- en: 'Here are two example scenarios to demonstrate this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两个示例场景，以演示这一点：
- en: We configured the brokers with three replicas, and unclean leader election is
    disabled. So we should never lose a single message that was committed to the Kafka
    cluster. However, we configured the producer to send messages with `acks=1`. We
    sent a message from the producer, and it was written to the leader but not yet
    to the in-sync replicas. The leader sent back a response to the producer saying,
    “Message was written successfully” and immediately crashes before the data was
    replicated to the other replicas. The other replicas are still considered in sync
    (remember that it takes a while before we declare a replica out of sync), and
    one of them will become the leader. Since the message was not written to the replicas,
    it was lost. But the producing application thinks it was written successfully.
    The system is consistent because no consumer saw the message (it was never committed
    because the replicas never got it), but from the producer perspective, a message
    was lost.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用三个副本配置了代理，并且禁用了不洁净的领导者选举。因此，我们不应该丢失提交到Kafka集群的任何单个消息。然而，我们配置了生产者使用`acks=1`发送消息。我们从生产者发送了一条消息，它被写入了领导者，但尚未被写入同步副本。领导者向生产者发送了一个响应，表示“消息已成功写入”，然后立即在数据被复制到其他副本之前崩溃。其他副本仍然被认为是同步的（记住，在我们宣布副本不同步之前需要一段时间），其中一个将成为领导者。由于消息未被写入副本，因此丢失了。但生产应用程序认为它已成功写入。系统是一致的，因为没有消费者看到消息（因为副本从未收到消息而未提交），但从生产者的角度来看，消息丢失了。
- en: We configured the brokers with three replicas, and unclean leader election is
    disabled. We learned from our mistakes and started producing messages with `acks=all`.
    Suppose that we are attempting to write a message to Kafka, but the leader for
    the partition we are writing to just crashed and a new one is still getting elected.
    Kafka will respond with “Leader not Available.” At this point, if the producer
    doesn’t handle the error correctly and doesn’t retry until the write is successful,
    the message may be lost. Once again, this is not a broker reliability issue because
    the broker never got the message; and it is not a consistency issue because the
    consumers never got the message either. But if producers don’t handle errors correctly,
    they may cause message loss.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用三个副本配置了代理，并且禁用了不洁净的领导者选举。我们从错误中吸取教训，并开始使用`acks=all`来生产消息。假设我们试图向Kafka写入消息，但我们要写入的分区的领导者刚刚崩溃，新的领导者仍在选举中。Kafka将回复“领导者不可用”。此时，如果生产者没有正确处理错误并重试直到写入成功，消息可能会丢失。再次强调，这不是代理可靠性问题，因为代理从未收到消息；也不是一致性问题，因为消费者也从未收到消息。但如果生产者没有正确处理错误，可能会导致消息丢失。
- en: 'As the examples show, there are two important things that everyone who writes
    applications that produce to Kafka must pay attention to:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如示例所示，每个编写生产到Kafka的应用程序的人都必须注意两件重要的事情：
- en: Use the correct `acks` configuration to match reliability requirements
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正确的`acks`配置以满足可靠性要求
- en: Handle errors correctly both in configuration and in code
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在配置和代码中正确处理错误
- en: We discussed producer configuration in depth in [Chapter 3](ch03.html#writing_messages_to_kafka),
    but let’s go over the important points again.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](ch03.html#writing_messages_to_kafka)中深入讨论了生产者配置，但让我们再次重点介绍一些重要的内容。
- en: Send Acknowledgments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发送确认
- en: 'Producers can choose between three different acknowledgment modes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者可以在三种不同的确认模式之间进行选择：
- en: '`acks=0`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=0`'
- en: This means that a message is considered to be written successfully to Kafka
    if the producer managed to send it over the network. We will still get errors
    if the object we are sending cannot be serialized or if the network card failed,
    but we won’t get any error if the partition is offline, a leader election is in
    progress, or even if the entire Kafka cluster is unavailable. Running with `acks=0`
    has low produce latency (which is why we see a lot of benchmarks with this configuration),
    but it will not improve end-to-end latency (remember that consumers will not see
    messages until they are replicated to all available replicas).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果生产者成功将消息发送到网络上，那么消息被认为已成功写入Kafka。如果我们发送的对象无法序列化，或者网络卡失败，我们仍会收到错误，但如果分区脱机，领导者选举正在进行，甚至整个Kafka集群不可用，我们将不会收到任何错误。使用`acks=0`可以降低生产延迟（这就是为什么我们看到很多基准测试使用这种配置），但它不会改善端到端延迟（记住，消费者在所有可用副本中复制之前将看不到消息）。
- en: '`acks=1`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=1`'
- en: This means that the leader will send either an acknowledgment or an error the
    moment it gets the message and writes it to the partition data file (but not necessarily
    synced to disk). We can lose data if the leader shuts down or crashes and some
    messages that were successfully written to the leader and acknowledged were not
    replicated to the followers before the crash. With this configuration, it is also
    possible to write to the leader faster than it can replicate messages and end
    up with under-replicated partitions, since the leader will acknowledge messages
    from the producer before replicating them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着领导者在收到消息并将其写入分区数据文件时（但不一定同步到磁盘），将发送确认或错误。如果领导者关闭或崩溃，并且在崩溃之前成功写入领导者并得到确认的一些消息未被复制到跟随者，我们可能会丢失数据。使用这种配置，还可能会比领导者更快地写入领导者，导致副本不足，因为领导者在复制消息之前将从生产者那里确认消息。
- en: '`acks=all`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`acks=all`'
- en: This means that the leader will wait until all in-sync replicas get the message
    before sending back an acknowledgment or an error. In conjunction with the `min.insync.replicas`
    configuration on the broker, this lets us control how many replicas get the message
    before it is acknowledged. This is the safest option—the producer won’t stop trying
    to send the message before it is fully committed. This is also the option with
    the longest producer latency—the producer waits for all in-sync replicas to get
    all the messages before it can mark the message batch as “done” and carry on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着领导者将等待直到所有同步副本收到消息，然后才发送确认或错误。结合代理上的`min.insync.replicas`配置，这让我们控制在消息被确认之前有多少副本收到消息。这是最安全的选项——生产者在消息完全提交之前不会停止尝试发送消息。这也是生产者延迟最长的选项——生产者等待所有同步副本收到所有消息，然后才能将消息批次标记为“完成”并继续进行。
- en: Configuring Producer Retries
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置生产者重试
- en: 'There are two parts to handling errors in the producer: the errors that the
    producers handle automatically for us and the errors that we, as developers using
    the producer library, must handle.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产者中处理错误有两个部分：生产者自动处理的错误和我们作为使用生产者库的开发人员必须处理的错误。
- en: The producer can handle *retriable* errors. When the producer sends messages
    to a broker, the broker can return either a success or an error code. Those error
    codes belong to two categories—errors that can be resolved after retrying and
    errors that won’t be resolved. For example, if the broker returns the error code
    `LEADER_NOT_AVAILABLE`, the producer can try sending the message again—maybe a
    new broker was elected and the second attempt will succeed. This means that `LEADER_NOT_AVAILABLE`
    is a *retriable* error. On the other hand, if a broker returns an `INVALID_CONFIG`
    exception, trying the same message again will not change the configuration. This
    is an example of a *nonretriable error*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者可以处理*可重试*错误。当生产者向代理发送消息时，代理可以返回成功或错误代码。这些错误代码属于两类——可以在重试后解决的错误和不会解决的错误。例如，如果代理返回错误代码`LEADER_NOT_AVAILABLE`，生产者可以尝试再次发送消息——也许新的代理被选举出来，第二次尝试会成功。这意味着`LEADER_NOT_AVAILABLE`是一个*可重试*错误。另一方面，如果代理返回`INVALID_CONFIG`异常，再次尝试发送相同的消息不会改变配置。这是一个*不可重试*错误的例子。
- en: In general, when our goal is to never lose a message, our best approach is to
    configure the producer to keep trying to send the messages when it encounters
    a retriable error. And the best approach to retries, as recommended in [Chapter 3](ch03.html#writing_messages_to_kafka),
    is to leave the number of retries at its current default (`MAX_INT`, or effectively
    infinite) and use `delivery.timout.ms` to configure the maximum amount of time
    we are willing to wait until giving up on sending a message—the producer will
    retry sending the message as many times as possible within this time interval.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，当我们的目标是永远不丢失消息时，我们最好的方法是配置生产者在遇到可重试错误时继续尝试发送消息。而在[第3章](ch03.html#writing_messages_to_kafka)中推荐的重试最佳方法是将重试次数保持在当前默认值（`MAX_INT`，或者实际上是无限）并使用`delivery.timout.ms`来配置我们愿意等待放弃发送消息的最长时间——生产者将在此时间间隔内尽可能多次地重试发送消息。
- en: Retrying to send a failed message includes a risk that both messages were successfully
    written to the broker, leading to duplicates. Retries and careful error handling
    can guarantee that each message will be stored *at least once*, but not *exactly
    once*. Using `enable.idempotence=true` will cause the producer to include additional
    information in its records, which brokers will use to skip duplicate messages
    caused by retries. In [Chapter 8](ch08.html#exactly_once_semantics), we discuss
    in detail how and when this works.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重试发送失败消息包括一个风险，即两条消息都成功写入代理，导致重复。重试和谨慎的错误处理可以保证每条消息将被存储*至少一次*，但不是*确切一次*。使用`enable.idempotence=true`将导致生产者在其记录中包含额外的信息，代理将使用这些信息来跳过由重试引起的重复消息。在[第8章](ch08.html#exactly_once_semantics)中，我们详细讨论了这是如何工作的。
- en: Additional Error Handling
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外的错误处理
- en: 'Using the built-in producer retries is an easy way to correctly handle a large
    variety of errors without loss of messages, but as developers, we must still be
    able to handle other types of errors. These include:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置的生产者重试是一种正确处理各种错误而不丢失消息的简单方法，但作为开发人员，我们仍然必须能够处理其他类型的错误。这些包括：
- en: Nonretriable broker errors, such as errors regarding message size, authorization
    errors, etc.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可重试的代理错误，例如有关消息大小、授权错误等的错误。
- en: Errors that occur before the message was sent to the broker—for example, serialization
    errors
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在消息发送到代理之前发生的错误，例如序列化错误
- en: Errors that occur when the producer exhausted all retry attempts or when the
    available memory used by the producer is filled to the limit due to using all
    of it to store messages while retrying
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当生产者耗尽所有重试尝试或由于使用所有内存来存储消息而填满生产者可用内存时发生的错误
- en: Timeouts
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超时
- en: In [Chapter 3](ch03.html#writing_messages_to_kafka) we discussed how to write
    error handlers for both sync and async message-sending methods. The content of
    these error handlers is specific to the application and its goals—do we throw
    away “bad messages”? Log errors? Stop reading messages from the source system?
    Apply back pressure to the source system to stop sending messages for a while?
    Store these messages in a directory on the local disk? These decisions depend
    on the architecture and the product requirements. Just note that if all the error
    handler is doing is retrying to send the message, then we’ll be better off relying
    on the producer’s retry functionality.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#writing_messages_to_kafka)中，我们讨论了如何为同步和异步发送消息的方法编写错误处理程序。这些错误处理程序的内容是特定于应用程序及其目标的——我们要丢弃“坏消息”吗？记录错误？停止从源系统读取消息？对源系统施加反压力，暂停发送消息一段时间？将这些消息存储在本地磁盘上的目录中？这些决定取决于架构和产品要求。只需注意，如果错误处理程序所做的只是重试发送消息，那么我们最好依赖生产者的重试功能。
- en: Using Consumers in a Reliable System
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在可靠系统中使用消费者
- en: Now that we have learned how to produce data while taking Kafka’s reliability
    guarantees into account, it is time to see how to consume data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何在考虑Kafka的可靠性保证的情况下生成数据，是时候看看如何消费数据了。
- en: As we saw in the first part of this chapter, data is only available to consumers
    after it has been committed to Kafka—meaning it was written to all in-sync replicas.
    This means that consumers get data that is guaranteed to be consistent. The only
    thing consumers are left to do is make sure they keep track of which messages
    they’ve read and which messages they haven’t. This is key to not losing messages
    while consuming them.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章的第一部分中所看到的，数据只有在提交到Kafka之后才对消费者可用——这意味着它已被写入到所有的同步副本。这意味着消费者获取的数据是保证一致的。消费者唯一需要做的就是确保他们跟踪已经读取的消息和尚未读取的消息。这对于在消费消息时不丢失消息至关重要。
- en: When reading data from a partition, a consumer is fetching a batch of messages,
    checking the last offset in the batch, and then requesting another batch of messages
    starting from the last offset received. This guarantees that a Kafka consumer
    will always get new data in correct order without missing any messages.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当从分区中读取数据时，消费者会获取一批消息，检查批中的最后偏移量，然后请求从上次接收到的偏移量开始的另一批消息。这保证了Kafka消费者始终以正确的顺序获取新数据，而不会错过任何消息。
- en: When a consumer stops, another consumer needs to know where to pick up the work—what
    was the last offset that the previous consumer processed before it stopped? The
    “other” consumer can even be the original one after a restart. It doesn’t really
    matter—some consumer is going to pick up consuming from that partition, and it
    needs to know at which offset to start. This is why consumers need to “commit”
    their offsets. For each partition it is consuming, the consumer stores its current
    location, so it or another consumer will know where to continue after a restart.
    The main way consumers can lose messages is when committing offsets for events
    they’ve read but haven’t completely processed yet. This way, when another consumer
    picks up the work, it will skip those messages and they will never get processed.
    This is why paying careful attention to when and how offsets get committed is
    critical.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个消费者停止时，另一个消费者需要知道从哪里开始工作——前一个消费者在停止之前处理的最后偏移量是多少？“其他”消费者甚至可以是重新启动后的原始消费者。这并不重要——某个消费者将从该分区开始消费，并且需要知道从哪个偏移量开始。这就是为什么消费者需要“提交”它们的偏移量。对于它正在消费的每个分区，消费者都会存储其当前位置，因此它或其他消费者将知道在重新启动后从哪里继续。消费者可能丢失消息的主要方式是在提交已读取但尚未完全处理的事件的偏移量时。这样，当另一个消费者接管工作时，它将跳过这些消息，它们将永远不会被处理。这就是为什么仔细关注偏移量何时以及如何提交是至关重要的。
- en: Committed Messages Versus Committed Offsets
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 已提交的消息与已提交的偏移量
- en: This is different from a *committed message*, which, as discussed previously,
    is a message that was written to all in-sync replicas and is available to consumers.
    *Committed offsets* are offsets the consumer sent to Kafka to acknowledge that
    it received and processed all the messages in a partition up to this specific
    offset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这与*已提交的消息*不同，如之前讨论的，*已提交的消息*是写入所有同步副本并对消费者可用的消息。*已提交的偏移量*是消费者发送给Kafka以确认它已接收并处理了分区中到特定偏移量的所有消息。
- en: In [Chapter 4](ch04.html#reading_data_from_kafka), we discussed the Consumer
    API in detail and covered the many methods for committing offsets. Here we will
    cover some important considerations and choices, but refer back to [Chapter 4](ch04.html#reading_data_from_kafka)
    for details on using the APIs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#reading_data_from_kafka)中，我们详细讨论了消费者API，并涵盖了提交偏移量的许多方法。在这里，我们将介绍一些重要的考虑和选择，但是有关使用API的详细信息，请参考[第4章](ch04.html#reading_data_from_kafka)。
- en: Important Consumer Configuration Properties for Reliable Processing
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠处理的重要消费者配置属性
- en: There are four consumer configuration properties that are important to understand
    in order to configure our consumer for a desired reliability behavior.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个消费者配置属性对于理解如何配置我们的消费者以获得所需的可靠性行为是重要的。
- en: The first is `group.id`, as explained in great detail in [Chapter 4](ch04.html#reading_data_from_kafka).
    The basic idea is that if two consumers have the same group ID and subscribe to
    the same topic, each will be assigned a subset of the partitions in the topic
    and will therefore only read a subset of the messages individually (but all the
    messages will be read by the group as a whole). If we need a consumer to see,
    on its own, every single message in the topics it is subscribed to, it will need
    a unique `group.id`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是`group.id`，如[第4章](ch04.html#reading_data_from_kafka)中详细解释的那样。基本思想是，如果两个消费者具有相同的组ID并订阅相同的主题，每个消费者将被分配主题中分区的一个子集，因此每个消费者将单独读取一部分消息（但整个组将读取所有消息）。如果我们需要一个消费者能够独立地看到其订阅的主题中的每条消息，它将需要一个唯一的`group.id`。
- en: The second relevant configuration is `auto.offset.reset`. This parameter controls
    what the consumer will do when no offsets were committed (e.g., when the consumer
    first starts) or when the consumer asks for offsets that don’t exist in the broker
    ([Chapter 4](ch04.html#reading_data_from_kafka) explains how this can happen).
    There are only two options here. If we choose `earliest`, the consumer will start
    from the beginning of the partition whenever it doesn’t have a valid offset. This
    can lead to the consumer processing a lot of messages twice, but it guarantees
    to minimize data loss. If we choose `latest`, the consumer will start at the end
    of the partition. This minimizes duplicate processing by the consumer but almost
    certainly leads to some messages getting missed by the consumer.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个相关的配置是`auto.offset.reset`。该参数控制当没有提交偏移量时消费者将会做什么（例如，当消费者首次启动时），或者当消费者请求在代理中不存在的偏移量时（[第4章](ch04.html#reading_data_from_kafka)解释了这种情况）。这里只有两个选项。如果我们选择`earliest`，消费者将从分区的开头开始，每当它没有有效的偏移量时。这可能导致消费者处理很多消息两次，但它保证了最小化数据丢失。如果我们选择`latest`，消费者将从分区的末尾开始。这最小化了消费者的重复处理，但几乎肯定会导致一些消息被消费者错过。
- en: 'The third relevant configuration is `enable.auto.commit`. This is a big decision:
    are we going to let the consumer commit offsets for us based on schedule, or are
    we planning on committing offsets manually in our code? The main benefit of automatic
    offset commits is that it’s one less thing to worry about when using consumers
    in our application. When we do all the processing of consumed records within the
    consumer poll loop, then the automatic offset commit guarantees we will never
    accidentally commit an offset that we didn’t process. The main drawbacks of automatic
    offset commits is that we have no control over the number of duplicate records
    the application may process because it was stopped after processing some records
    but before the automated commit kicked in. When the application has more complex
    processing, such as passing records to another thread to process in the background,
    there is no choice but to use manual offset commit since the automatic commit
    may commit offsets for records the consumer has read but perhaps has not processed
    yet.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个相关配置是`enable.auto.commit`。这是一个重大决定：我们是否打算让消费者根据计划为我们提交偏移量，还是打算在我们的代码中手动提交偏移量？自动偏移量提交的主要好处是在我们的应用程序中使用消费者时，这是一件少了的事情需要担心。当我们在消费者轮询循环内处理所有消费记录时，自动偏移量提交可以保证我们永远不会意外提交我们没有处理的偏移量。自动偏移量提交的主要缺点是，我们无法控制应用程序可能处理的重复记录数量，因为它在处理一些记录后停止，但在自动提交生效之前。当应用程序有更复杂的处理，例如将记录传递到另一个线程在后台处理时，除了使用手动偏移量提交外别无选择，因为自动提交可能会提交消费者已读取但可能尚未处理的记录的偏移量。
- en: The fourth relevant configuration, `auto.commit.``interval.ms`, is tied to the
    third. If we choose to commit offsets automatically, this configuration lets us
    configure how frequently they will be committed. The default is every five seconds.
    In general, committing more frequently adds overhead but reduces the number of
    duplicates that can occur when a consumer stops.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个相关配置`auto.commit.``interval.ms`与第三个相关。如果我们选择自动提交偏移量，这个配置让我们配置它们的提交频率。默认值是每五秒一次。一般来说，更频繁地提交会增加开销，但会减少消费者停止时可能发生的重复数量。
- en: While not directly related to reliable data processing, it is difficult to consider
    a consumer reliable if it frequently stops consuming in order to rebalance. [Chapter 4](ch04.html#reading_data_from_kafka)
    includes advice on how to configure consumers to minimize unnecessary rebalancing
    and to minimize pauses while rebalancing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然与可靠的数据处理没有直接关系，但如果消费者经常停止消费以进行重新平衡，很难认为它是可靠的。[第四章](ch04.html#reading_data_from_kafka)包括如何配置消费者以最小化不必要的重新平衡和在重新平衡时最小化暂停的建议。
- en: Explicitly Committing Offsets in Consumers
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在消费者中明确提交偏移量
- en: If we decide we need more control and choose to commit offsets manually, we
    need to be concerned about correctness and performance implications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定需要更多控制并选择手动提交偏移量，我们需要关注正确性和性能影响。
- en: We will not go over the mechanics and APIs involved in committing offsets here,
    since they were covered in great depth in [Chapter 4](ch04.html#reading_data_from_kafka).
    Instead, we will review important considerations when developing a consumer to
    handle data reliably. We’ll start with the simple and perhaps obvious points and
    move on to more complex patterns.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细介绍提交偏移量涉及的机制和API，因为它们在[第四章](ch04.html#reading_data_from_kafka)中已经深入讨论过。相反，我们将回顾在开发消费者处理数据时的重要考虑因素。我们将从简单而明显的观点开始，然后转向更复杂的模式。
- en: Always commit offsets after messages were processed
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在处理消息后始终提交偏移量
- en: If we do all the processing within the poll loop and don’t maintain state between
    poll loops (e.g., for aggregation), this should be easy. We can use the auto-commit
    configuration, commit offset at the end of the poll loop, or commit offset inside
    the loop at a frequency that balances requirements for both overhead and lack
    of duplicate processing. If there are additional threads or stateful processing
    involved, this becomes more complex, especially since the consumer object is not
    thread safe. In [Chapter 4](ch04.html#reading_data_from_kafka), we discussed how
    this can be done and provided references with additional examples.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在轮询循环内进行所有处理，并且在轮询循环之间不保持状态（例如，用于聚合），这应该很容易。我们可以使用自动提交配置，在轮询循环结束时提交偏移量，或者在循环内以平衡开销和缺乏重复处理的要求提交偏移量。如果涉及额外的线程或有状态的处理，这将变得更加复杂，特别是因为消费者对象不是线程安全的。在[第四章](ch04.html#reading_data_from_kafka)中，我们讨论了如何做到这一点，并提供了更多示例的参考。
- en: Commit frequency is a trade-off between performance and number of duplicates
    in the event of a crash
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提交频率是性能和在发生崩溃时重复事件数量之间的权衡。
- en: Even in the simplest case where we do all the processing within the poll loop
    and don’t maintain state between poll loops, we can choose to commit multiple
    times within a loop or choose to only commit every several loops. Committing has
    significant performance overhead. It is similar to produce with `acks=all`, but
    all offset commits of a single consumer group are produced to the same broker,
    which can become overloaded. The commit frequency has to balance requirements
    for performance and lack of duplicates. Committing after every message should
    only ever be done on very low-throughput topics.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在最简单的情况下，我们在轮询循环内进行所有处理，并且在轮询循环之间不保持状态（例如，用于聚合），我们可以选择在循环内多次提交或者选择每隔几个循环才提交一次。提交会带来显著的性能开销。这类似于使用`acks=all`进行生产，但单个消费者组的所有偏移量提交都会发送到同一个代理，这可能会导致负载过重。提交频率必须平衡性能要求和重复处理的要求。在非常低吞吐量的主题上，应该只在每条消息之后才进行提交。
- en: Commit the right offsets at the right time
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在正确的时间提交正确的偏移量
- en: A common pitfall when committing in the middle of the poll loop is accidentally
    committing the last offset read when polling and not the offset after the last
    offset processed. Remember that it is critical to always commit offsets for messages
    after they were processed—committing offsets for messages read but not processed
    can lead to the consumer missing messages. [Chapter 4](ch04.html#reading_data_from_kafka)
    has examples that show how to do just that.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在轮询循环中提交时的一个常见陷阱是在轮询时意外提交了最后读取的偏移量，而不是最后处理的偏移量之后的偏移量。请记住，始终要为处理后的消息提交偏移量——提交读取但未处理的消息的偏移量可能导致消费者丢失消息。[第4章](ch04.html#reading_data_from_kafka)中有示例，展示了如何做到这一点。
- en: Rebalances
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新平衡
- en: When designing an application, we need to remember that consumer rebalances
    will happen, and we need to handle them properly. [Chapter 4](ch04.html#reading_data_from_kafka)
    contains a few examples. This usually involves committing offsets before partitions
    are revoked and cleaning any state the application maintains when it is assigned
    new partitions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计应用程序时，我们需要记住消费者重新平衡会发生，并且需要正确处理它们。[第4章](ch04.html#reading_data_from_kafka)包含一些示例。通常这涉及在分区被撤销之前提交偏移量，并在分配新分区时清理应用程序维护的任何状态。
- en: Consumers may need to retry
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消费者可能需要重试
- en: 'In some cases, after calling poll and processing records, some records are
    not fully processed and will need to be processed later. For example, we may try
    to write records from Kafka to a database but find that the database is not available
    at that moment and we need to retry later. Note that unlike traditional pub/sub
    messaging systems, Kafka consumers commit offsets and do not “ack” individual
    messages. This means that if we failed to process record #30 and succeeded in
    processing record #31, we should not commit offset #31—this would result in marking
    as processed all the records up to #31 including #30, which is usually not what
    we want. Instead, try following one of the following two patterns.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，在调用轮询并处理记录后，一些记录可能没有完全处理，需要稍后处理。例如，我们可能尝试将Kafka中的记录写入数据库，但发现数据库此时不可用，需要稍后重试。请注意，与传统的发布/订阅消息系统不同，Kafka消费者提交偏移量而不是“确认”单个消息。这意味着如果我们未能处理记录＃30并成功处理记录＃31，我们不应该提交偏移量＃31——这将导致标记为已处理所有记录直到＃31，包括＃30，这通常不是我们想要的。相反，尝试遵循以下两种模式之一。
- en: One option when we encounter a retriable error is to commit the last record
    we processed successfully. We’ll then store the records that still need to be
    processed in a buffer (so the next poll won’t override them), use the consumer
    `pause()` method to ensure that additional polls won’t return data, and keep trying
    to process the records.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们遇到可重试错误时的一个选择是提交我们成功处理的最后一条记录。然后，我们将仍需要处理的记录存储在缓冲区中（以便下一次轮询不会覆盖它们），使用消费者的`pause()`方法确保额外的轮询不会返回数据，并继续尝试处理记录。
- en: A second option when encountering a retriable error is to write it to a separate
    topic and continue. A separate consumer group can be used to handle retries from
    the retry topic, or one consumer can subscribe to both the main topic and to the
    retry topic but pause the retry topic between retries. This pattern is similar
    to the dead-letter-queue system used in many messaging systems.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 遇到可重试错误时的第二个选择是将其写入到一个单独的主题中并继续。可以使用单独的消费者组来处理重试主题中的重试，或者一个消费者可以订阅主题和重试主题，但在重试之间暂停重试主题。这种模式类似于许多消息系统中使用的死信队列系统。
- en: Consumers may need to maintain state
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消费者可能需要维护状态
- en: In some applications, we need to maintain state across multiple calls to poll.
    For example, if we want to calculate moving average, we’ll want to update the
    average after every time we poll Kafka for new messages. If our process is restarted,
    we will need to not just start consuming from the last offset, but we’ll also
    need to recover the matching moving average. One way to do this is to write the
    latest accumulated value to a “results” topic at the same time the application
    is committing the offset. This means that when a thread is starting up, it can
    pick up the latest accumulated value when it starts and pick up right where it
    left off. In [Chapter 8](ch08.html#exactly_once_semantics), we discuss how an
    application can write results and commit offsets in a single transaction. In general,
    this is a rather complex problem to solve, and we recommend looking at a library
    like Kafka Streams or Flink, which provides high-level DSL-like APIs for aggregation,
    joins, windows, and other complex analytics.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些应用程序中，我们需要在多次轮询之间保持状态。例如，如果我们想要计算移动平均值，我们将希望在每次轮询Kafka获取新消息后更新平均值。如果我们的进程重新启动，我们不仅需要从最后的偏移量开始消费，还需要恢复匹配的移动平均值。一种方法是在应用程序提交偏移量的同时将最新累积值写入“结果”主题。这意味着当线程启动时，它可以在启动时获取最新的累积值，并从上次离开的地方继续。在[第8章](ch08.html#exactly_once_semantics)中，我们讨论了应用程序如何在单个事务中写入结果和提交偏移量。一般来说，这是一个相当复杂的问题，我们建议查看像Kafka
    Streams或Flink这样的库，它们提供了高级DSL样式的API，用于聚合、连接、窗口和其他复杂的分析。
- en: Validating System Reliability
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证系统可靠性
- en: Once we have gone through the process of figuring out our reliability requirements,
    configuring the brokers, configuring the clients, and using the APIs in the best
    way for our use case, we can just relax and run everything in production, confident
    that no event will ever be missed, right?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们经历了确定我们的可靠性要求、配置代理、配置客户端以及以最佳方式使用API来满足我们的用例的过程，我们就可以放心地在生产环境中运行一切，确信不会错过任何事件，对吗？
- en: 'We recommend doing some validation first and suggest three layers of validation:
    validate the configuration, validate the application, and monitor the application
    in production. Let’s look at each of these steps and see what we need to validate
    and how.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议首先进行一些验证，并建议三层验证：验证配置、验证应用程序，并在生产中监视应用程序。让我们看看每个步骤，并了解我们需要验证什么以及如何验证。
- en: Validating Configuration
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证配置
- en: 'It is easy to test the broker and client configuration in isolation from the
    application logic, and it is recommended to do so for two reasons:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用逻辑中隔离出来测试代理和客户端配置很容易，也建议出于两个原因这样做：
- en: It helps to test if the configuration we’ve chosen can meet our requirements.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试我们选择的配置是否能满足我们的要求是有帮助的。
- en: It is a good exercise to reason through the expected behavior of the system.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理系统的预期行为是一个很好的练习。
- en: Kafka includes two important tools to help with this validation. The `org.apache.kafka.tools`
    package includes `VerifiableProducer` and `VerifiableConsumer` classes. These
    can run as command-line tools or be embedded in an automated testing framework.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka包括两个重要的工具来帮助进行验证。`org.apache.kafka.tools`包括`VerifiableProducer`和`VerifiableConsumer`类。这些可以作为命令行工具运行，也可以嵌入到自动化测试框架中。
- en: The idea is that the verifiable producer produces a sequence of messages containing
    numbers from 1 to a value we choose. We can configure the verifiable producer
    the same way we configure our own producer, setting the right number of `ack`s,
    `retries`, `delivery.timeout.ms`, and rate at which the messages will be produced.
    When we run it, it will print success or error for each message sent to the broker,
    based on the `ack`s received. The verifiable consumer performs the complementary
    check. It consumes events (usually those produced by the verifiable producer)
    and prints out the events it consumed in order. It also prints information regarding
    commits and rebalances.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，可验证的生产者产生一个包含从1到我们选择的值的数字序列的消息。我们可以像配置自己的生产者一样配置可验证的生产者，设置正确数量的`ack`、`retries`、`delivery.timeout.ms`，以及消息产生的速率。当我们运行它时，它将根据接收到的`ack`为每条发送到代理的消息打印成功或错误。可验证的消费者执行补充检查。它消费事件（通常是可验证的生产者产生的事件），并按顺序打印出它消费的事件。它还打印有关提交和重新平衡的信息。
- en: 'It is important to consider which tests we want to run. For example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要考虑我们想要运行哪些测试。例如：
- en: 'Leader election: what happens if we kill the leader? How long does it take
    the producer and consumer to start working as usual again?'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领导者选举：如果我们杀死领导者会发生什么？生产者和消费者需要多长时间才能像往常一样开始工作？
- en: 'Controller election: how long does it take the system to resume after a restart
    of the controller?'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器选举：系统在控制器重启后需要多长时间才能恢复？
- en: 'Rolling restart: can we restart the brokers one by one without losing any messages?'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动重启：我们能否逐个重启代理而不丢失任何消息？
- en: 'Unclean leader election test: what happens when we kill all the replicas for
    a partition one by one (to make sure each goes out of sync) and then start a broker
    that was out of sync? What needs to happen in order to resume operations? Is this
    acceptable?'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非干净的领导者选举测试：当我们逐个杀死一个分区的所有副本（以确保每个副本都不同步），然后启动一个不同步的代理时会发生什么？为了恢复操作需要发生什么？这是可以接受的吗？
- en: Then we pick a scenario, start the verifiable producer, start the verifiable
    consumer, and run through the scenario—for example, kill the leader of the partition
    we are producing data into. If we expected a short pause and then everything to
    resume normally with no message loss, we need to make sure the number of messages
    produced by the producer and the number of messages consumed by the consumer match.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们选择一个场景，启动可验证的生产者，启动可验证的消费者，并运行该场景——例如，杀死我们正在生产数据的分区的领导者。如果我们期望有一个短暂的暂停，然后一切都能正常恢复而没有消息丢失，我们需要确保生产者产生的消息数量和消费者消费的消息数量匹配。
- en: The Apache Kafka source repository includes an [extensive test suite](https://oreil.ly/IjJx8).
    Many of the tests in the suite are based on the same principle and use the verifiable
    producer and consumer to make sure rolling upgrades work.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka源代码库包括一个广泛的测试套件。套件中的许多测试都基于相同的原则，并使用可验证的生产者和消费者来确保滚动升级正常工作。
- en: Validating Applications
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证应用程序
- en: Once we are sure the broker and client configuration meet our requirements,
    it is time to test whether the application provides the guarantees we need. This
    will check things like custom error-handling code, offset commits, and rebalance
    listeners and similar places where the application logic interacts with Kafka’s
    client libraries.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定代理和客户端配置符合我们的要求，就是测试应用程序是否提供我们需要的保证的时候了。这将检查诸如自定义错误处理代码、偏移提交、重新平衡监听器以及应用程序逻辑与Kafka客户端库交互的类似位置。
- en: 'Naturally, because application logic can vary considerably, there is only so
    much guidance we can provide on how to test it. We recommend integration tests
    for the application as part of any development process, and we recommend running
    tests under a variety of failure conditions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，由于应用逻辑可能会有很大的变化，我们只能提供有限的关于如何测试的指导。我们建议将应用程序作为任何开发过程的一部分进行集成测试，并建议在各种故障条件下运行测试：
- en: Clients lose connectivity to one of the brokers
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端失去与其中一个代理的连接
- en: High latency between client and broker
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端和代理之间的高延迟
- en: Disk full
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘已满
- en: Hanging disk (also called “brown out”)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挂起的磁盘（也称为“停电”）
- en: Leader election
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领导者选举
- en: Rolling restart of brokers
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理的滚动重启
- en: Rolling restart of consumers
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者的滚动重启
- en: Rolling restart of producers
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者的滚动重启
- en: There are many tools that can be used to introduce network and disk faults,
    and many are excellent, so we will not attempt to make specific recommendations.
    Apache Kafka itself includes the [Trogdor test framework](https://oreil.ly/P3ai1)
    for fault injection. For each scenario, we will have *expected behavior*, which
    is what we planned on seeing when we developed the application. Then we run the
    test to see what actually happens. For example, when planning for a rolling restart
    of consumers, we planned for a short pause as consumers rebalance and then continue
    consumption with no more than 1,000 duplicate values. Our test will show whether
    the way the application commits offsets and handles rebalances actually works
    this way.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具可用于引入网络和磁盘故障，其中许多都非常出色，因此我们不会尝试提出具体建议。Apache Kafka本身包括[Trogdor测试框架](https://oreil.ly/P3ai1)用于故障注入。对于每种情况，我们将有*预期行为*，这是我们在开发应用程序时计划看到的情况。然后我们运行测试，看看实际发生了什么。例如，当计划对消费者进行滚动重启时，我们计划进行短暂暂停，因为消费者会重新平衡，然后继续消费，最多不超过1,000个重复值。我们的测试将显示应用程序提交偏移量和处理重新平衡的方式是否真的如此运行。
- en: Monitoring Reliability in Production
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在生产环境中监控可靠性
- en: Testing the application is important, but it does not replace the need to continuously
    monitor production systems to make sure data is flowing as expected. [Chapter 12](ch12.html#administering_kafka)
    will cover detailed suggestions on how to monitor the Kafka cluster, but in addition
    to monitoring the health of the cluster, it is important to also monitor the clients
    and the flow of data through the system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 测试应用程序很重要，但这并不能取代持续监控生产系统以确保数据流如预期般顺畅。[第12章](ch12.html#administering_kafka)将详细介绍如何监控Kafka集群，但除了监控集群的健康状况外，还要监控客户端和数据流通过系统的情况。
- en: 'Kafka’s Java clients include JMX metrics that allow monitoring client-side
    status and events. For the producers, the two metrics most important for reliability
    are error-rate and retry-rate per record (aggregated). Keep an eye on those, since
    error or retry rates going up can indicate an issue with the system. Also monitor
    the producer logs for errors that occur while sending events that are logged at
    `WARN` level, and say something along the lines of “Got error produce response
    with correlation id 5689 on topic-partition [topic-1,3], retrying (two attempts
    left). Error: …” When we see events with 0 attempts left, the producer is running
    out of retries. In [Chapter 3](ch03.html#writing_messages_to_kafka) we discussed
    how to configure `delivery.timeout.ms` and `retries` to improve the error handling
    in the producer and avoid running out of retries prematurely. Of course, it is
    always better to solve the problem that caused the errors in the first place.
    `ERROR` level log messages on the producer are likely to indicate that sending
    the message failed completely due to nonretriable error, a retriable error that
    ran out of retries, or a timeout. When applicable, the exact error from the broker
    will be logged as well.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的Java客户端包括JMX指标，允许监控客户端状态和事件。对于生产者来说，可靠性最重要的两个指标是每条记录的错误率和重试率（汇总）。要密切关注这些指标，因为错误率或重试率的上升可能表明系统存在问题。还要监控生产者日志，查看在发送事件时记录为`WARN`级别的错误，内容类似于“在主题-分区
    [topic-1,3] 上使用相关ID 5689产生错误的响应，正在重试（还剩两次尝试）。错误：…”当我们看到剩余尝试次数为0的事件时，表示生产者的重试次数已用尽。在[第3章](ch03.html#writing_messages_to_kafka)中，我们讨论了如何配置`delivery.timeout.ms`和`retries`以改进生产者的错误处理，并避免过早用尽重试次数。当然，解决导致错误的问题才是更好的选择。生产者的`ERROR`级别日志消息可能表明由于不可重试的错误、用尽重试次数的可重试错误或超时而完全发送消息失败。在适用的情况下，经纪人的确切错误也将被记录。
- en: On the consumer side, the most important metric is consumer lag. This metric
    indicates how far the consumer is from the latest message committed to the partition
    on the broker. Ideally, the lag would always be zero and the consumer will always
    read the latest message. In practice, because calling `poll()` returns multiple
    messages and then the consumer spends time processing them before fetching more
    messages, the lag will always fluctuate a bit. What is important is to make sure
    consumers do eventually catch up rather than fall further and further behind.
    Because of the expected fluctuation in consumer lag, setting traditional alerts
    on the metric can be challenging. [Burrow](https://oreil.ly/supY1) is a consumer
    lag checker by LinkedIn and can make this easier.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者方面，最重要的指标是消费者滞后。该指标表示消费者距离经纪人分区上最新提交的消息有多远。理想情况下，滞后应始终为零，消费者将始终读取最新的消息。实际上，因为调用`poll()`会返回多条消息，然后消费者会花时间处理它们，然后再获取更多消息，滞后值会有所波动。重要的是确保消费者最终能够赶上，而不是越来越落后。由于消费者滞后的预期波动，设置传统的警报指标可能会有挑战性。[Burrow](https://oreil.ly/supY1)是LinkedIn开发的消费者滞后检查工具，可以简化这一过程。
- en: 'Monitoring flow of data also means making sure all produced data is consumed
    in a timely manner (“timely manner” is usually based on business requirements).
    In order to make sure data is consumed in a timely manner, we need to know when
    the data was produced. Kafka assists in this: starting with version 0.10.0, all
    messages include a timestamp that indicates when the event was produced (although
    note that this can be overridden either by the application that is sending the
    events or by the brokers themselves if they are configured to do so).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 监控数据流也意味着确保所有生成的数据及时被消费（“及时”通常基于业务需求）。为了确保数据及时被消费，我们需要知道数据是何时生成的。Kafka在这方面提供了帮助：从0.10.0版本开始，所有消息都包括一个时间戳，指示事件生成的时间（尽管请注意，如果应用程序发送事件或经纪人自身配置为这样做，时间戳可以被覆盖）。
- en: To make sure all produced messages are consumed within a reasonable amount of
    time, we will need the application producing the messages to record the number
    of events produced (usually as events per second). The consumers need to record
    the number of events consumed per unit or time, and the lag from the time events
    were produced to the time they were consumed, using the event timestamp. Then
    we will need a system to reconcile the events per second numbers from both the
    producer and the consumer (to make sure no messages were lost on the way) and
    to make sure the interval between produce time and consume time is reasonable.
    This type of end-to-end monitoring systems can be challenging and time-consuming
    to implement. To the best of our knowledge, there is no open source implementation
    of this type of system, but Confluent provides a commercial implementation as
    part of the [Confluent Control Center](https://oreil.ly/KnvVV).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保所有生成的消息在合理的时间内被消耗，我们需要应用程序记录生成的事件数量（通常以每秒事件数的形式）。消费者需要记录每个时间单位消耗的事件数量，以及使用事件时间戳记录事件产生和消耗之间的滞后时间。然后，我们需要一个系统来协调生产者和消费者的每秒事件数量（以确保消息在传输过程中没有丢失），并确保生产时间和消费时间之间的间隔是合理的。这种端到端的监控系统可能具有挑战性，并且实施起来可能耗时。据我们所知，目前没有开源实现这种类型系统的，但Confluent作为[Confluent
    Control Center](https://oreil.ly/KnvVV)的一部分提供了商业实现。
- en: In addition to monitoring clients and the end-to-end flow of data, Kafka brokers
    include metrics that indicate the rate of error responses sent from the brokers
    to clients. We recommend collecting `kafka.server:type=BrokerTopicMetrics,​name=FailedProduceRequestsPerSec`
    and `kafka.server:type=BrokerTopic​Met⁠rics,name=FailedFetchRequestsPerSec`. At
    times, some level of error responses is expected—for example, if we shut down
    a broker for maintenance and new leaders are elected on another broker, it is
    expected that producers will receive a `NOT_LEADER_FOR_PARTITION` error, which
    will cause them to request updated metadata before continuing to produce events
    as usual. Unexplained increases in failed requests should always be investigated.
    To assist in such investigations, the failed requests metrics are tagged with
    the specific error response that the broker sent.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监控客户端和端到端数据流之外，Kafka代理还包括指示从代理发送到客户端的错误响应速率的指标。我们建议收集`kafka.server:type=BrokerTopicMetrics,​name=FailedProduceRequestsPerSec`和`kafka.server:type=BrokerTopic​Met⁠rics,name=FailedFetchRequestsPerSec`。有时，预期会出现一定级别的错误响应，例如，如果我们关闭代理进行维护，并在另一个代理上选举新的领导者，那么预期生产者将收到`NOT_LEADER_FOR_PARTITION`错误，这将导致它们在继续正常生产事件之前请求更新的元数据。无法解释的失败请求增加应该始终进行调查。为了帮助进行此类调查，失败请求指标附带了代理发送的具体错误响应标记。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As we said in the beginning of the chapter, reliability is not just a matter
    of specific Kafka features. We need to build an entire reliable system, including
    the application architecture, the way applications use the producer and Consumer
    APIs, producer and consumer configuration, topic configuration, and broker configuration.
    Making the system more reliable always has trade-offs in application complexity,
    performance, availability, or disk-space usage. By understanding all the options
    and common patterns and understanding requirements for each use case, we can make
    informed decisions regarding how reliable the application and Kafka deployment
    need to be and which trade-offs make sense.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头所说的，可靠性不仅仅是特定Kafka功能的问题。我们需要构建一个完整可靠的系统，包括应用程序架构、应用程序使用生产者和消费者API的方式、生产者和消费者配置、主题配置和代理配置。使系统更加可靠总是会在应用程序复杂性、性能、可用性或磁盘空间使用等方面进行权衡。通过了解所有选项和常见模式，并了解每种用例的要求，我们可以就应用程序和Kafka部署需要多可靠以及哪些权衡是合理的做出明智的决策。

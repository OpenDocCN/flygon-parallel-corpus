- en: Chapter 8\. Exactly-Once Semantics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。精确一次语义
- en: In [Chapter 7](ch07.html#reliable_data_delivery) we discussed the configuration
    parameters and the best practices that allow Kafka users to control Kafka’s reliability
    guarantees. We focused on at-least-once delivery—the guarantee that Kafka will
    not lose messages that it acknowledged as committed. This still leaves open the
    possibility of duplicate messages.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.html#reliable_data_delivery)中，我们讨论了配置参数和最佳实践，使Kafka用户能够控制Kafka的可靠性保证。我们专注于至少一次交付——Kafka不会丢失已确认提交的消息的保证。这仍然存在重复消息的可能性。
- en: In simple systems where messages are produced and then consumed by various applications,
    duplicates are an annoyance that is fairly easy to handle. Most real-world applications
    contain unique identifiers that consuming applications can use to deduplicate
    the messages.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的系统中，消息由各种应用程序生成和消费，重复是一个相当容易处理的烦恼。大多数现实世界的应用程序包含消费应用程序可以使用的唯一标识符来对消息进行去重。
- en: Things become more complicated when we look at stream processing applications
    that aggregate events. When inspecting an application that consumes events, computes
    an average, and produces the results, it is often impossible for those who check
    the results to detect that the average is incorrect because an event was processed
    twice while computing the average. In these cases, it is important to provide
    a stronger guarantee—exactly-once processing semantics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看聚合事件的流处理应用程序时，情况变得更加复杂。当检查一个消费事件、计算平均值并产生结果的应用程序时，往往无法检测到结果不正确，因为在计算平均值时事件被处理了两次。在这些情况下，提供更强的保证——精确一次处理语义是很重要的。
- en: In this chapter, we will discuss how to use Kafka with exactly-once semantics,
    the recommended use cases, and the limitations. As we did with at-least-once guarantees,
    we will dive a bit deeper and provide some insight and intuition into how this
    guarantee is implemented. These details can be skipped when you first read the
    chapter but will be useful to understand before using the feature—it will help
    clarify the meaning of the different configurations and APIs and how best to use
    them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何使用具有精确一次语义的Kafka，推荐的用例以及限制。与至少一次保证一样，我们将深入一点，提供一些洞察力和直觉，以了解此保证是如何实现的。在首次阅读本章时，可以跳过这些细节，但在使用该功能之前理解这些细节将是有用的——它将有助于澄清不同配置和API的含义以及如何最好地使用它们。
- en: 'Exactly-once semantics in Kafka is a combination of two key features: idempotent
    producers, which help avoid duplicates caused by producer retries, and transactional
    semantics, which guarantee exactly-once processing in stream processing applications.
    We will discuss both, starting with the simpler and more generally useful idempotent
    producer.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka中的精确一次语义是两个关键特性的组合：幂等生产者，它有助于避免由生产者重试引起的重复，以及事务语义，它保证流处理应用程序中的精确一次处理。我们将从更简单和更普遍有用的幂等生产者开始讨论这两个特性。
- en: Idempotent Producer
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幂等生产者
- en: A service is called idempotent if performing the same operation multiple times
    has the same result as performing it a single time. In databases it is usually
    demonstrated as the difference between `UPDATE t SET x=x+1 where y=5` and `UPDATE
    t SET x=18 where y=5`. The first example is not idempotent; if we call it three
    times, we’ll end up with a very different result than if we were to call it once.
    The second example is idempotent—no matter how many times we run this statement,
    `x` will be equal to 18.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行相同操作多次的结果与执行一次相同操作的结果相同，则称该服务为幂等。在数据库中，通常可以通过`UPDATE t SET x=x+1 where y=5`和`UPDATE
    t SET x=18 where y=5`之间的差异来演示。第一个例子不是幂等的；如果我们调用它三次，最终的结果将与我们只调用一次时的结果大不相同。第二个例子是幂等的——无论我们运行这个语句多少次，`x`都将等于18。
- en: How is this related to a Kafka producer? If we configure a producer to have
    at-least-once semantics rather than idempotent semantics, it means that in cases
    of uncertainty, the producer will retry sending the message so it will arrive
    at least once. These retries could lead to duplicates.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这与Kafka生产者有什么关系？如果我们将生产者配置为具有至少一次语义而不是幂等语义，这意味着在不确定的情况下，生产者将重试发送消息，以便至少到达一次。这些重试可能导致重复。
- en: The classic case is when a partition leader received a record from the producer,
    replicated it successfully to the followers, and then the broker on which the
    leader resides crashed before it could send a response to the producer. The producer,
    after a certain time without a response, will resend the message. The message
    will arrive at the new leader, who already has a copy of the message from the
    previous attempt—resulting in a duplicate.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 经典案例是分区领导者从生产者接收记录，成功地将其复制到跟随者，然后领导者所在的代理在发送响应给生产者之前崩溃。生产者在一段时间内没有收到响应后，将重新发送消息。消息将到达新的领导者，他已经从先前的尝试中拥有了消息的副本——导致重复。
- en: In some applications duplicates don’t matter much, but in others they can lead
    to inventory miscounts, bad financial statements, or sending someone two umbrellas
    instead of the one they ordered.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，重复并不重要，但在其他应用中，它们可能导致库存错误计数、糟糕的财务报表，或者向某人发送两把雨伞而不是他们订购的一把。
- en: Kafka’s idempotent producer solves this problem by automatically detecting and
    resolving such duplicates.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的幂等生产者通过自动检测和解决这些重复来解决这个问题。
- en: How Does the Idempotent Producer Work?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幂等生产者是如何工作的？
- en: When we enable the idempotent producer, each message will include a unique identified
    producer ID (PID) and a sequence number. These, together with the target topic
    and partition, uniquely identify each message. Brokers use these unique identifiers
    to track the last five messages produced to every partition on the broker. To
    limit the number of previous sequence numbers that have to be tracked for each
    partition, we also require that the producers will use `max.inflight.requests=5`
    or lower (the default is 5).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: When a broker receives a message that it already accepted before, it will reject
    the duplicate with an appropriate error. This error is logged by the producer
    and is reflected in its metrics but does not cause any exception and should not
    cause any alarm. On the producer client, it will be added to the `record-error-rate`
    metric. On the broker, it will be part of the `ErrorsPerSec` metric of the `RequestMetrics`
    type, which includes a separate count for each type of error.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: What if a broker receives a sequence number that is unexpectedly high? The broker
    expects message number 2 to be followed by message number 3; what happens if the
    broker receives message number 27 instead? In such cases the broker will respond
    with an “out of order sequence” error, but if we use an idempotent producer without
    using transactions, this error can be ignored.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the producer will continue normally after encountering an “out of order
    sequence number” exception, this error typically indicates that messages were
    lost between the producer and the broker—if the broker received message number
    2 followed by message number 27, something must have happened to messages 3 to
    26\. When encountering such an error in the logs, it is worth revisiting the producer
    and topic configuration and making sure the producer is configured with recommended
    values for high reliability and to check whether unclean leader election has occurred.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'As is always the case with distributed systems, it is interesting to consider
    the behavior of an idempotent producer under failure conditions. Consider two
    cases: producer restart and broker failure.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Producer restart
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a producer fails, usually a new producer will be created to replace it—whether
    manually by a human rebooting a machine, or using a more sophisticated framework
    like Kubernetes that provides automated failure recovery. The key point is that
    when the producer starts, if the idempotent producer is enabled, the producer
    will initialize and reach out to a Kafka broker to generate a producer ID. Each
    initialization of a producer will result in a completely new ID (assuming that
    we did not enable transactions). This means that if a producer fails and the producer
    that replaces it sends a message that was previously sent by the old producer,
    the broker will not detect the duplicates—the two messages will have different
    producer IDs and different sequence numbers and will be considered as two different
    messages. Note that the same is true if the old producer froze and then came back
    to life after its replacement started—the original producer is not recognized
    as a zombie, because we have two totally different producers with different IDs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Broker failure
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a broker fails, the controller elects new leaders for the partitions that
    had leaders on the failed broker. Say that we have a producer that produced messages
    to topic A, partition 0, which had its lead replica on broker 5 and a follower
    replica on broker 3\. After broker 5 fails, broker 3 becomes the new leader. The
    producer will discover that the new leader is broker 3 via the metadata protocol
    and start producing to it. But how will broker 3 know which sequences were already
    produced in order to reject duplicates?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The leader keeps updating its in-memory producer state with the five last sequence
    IDs every time a new message is produced. Follower replicas update their own in-memory
    buffers every time they replicate new messages from the leader. This means that
    when a follower becomes a leader, it already has the latest sequence numbers in
    memory, and validation of newly produced messages can continue without any issues
    or delays.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: But what happens when the old leader comes back? After a restart, the old in-memory
    producer state will no longer be in memory. To assist in recovery, brokers take
    a snapshot of the producer state to a file when they shut down or every time a
    segment is created. When the broker starts, it reads the latest state from a file.
    The newly restarted broker then keeps updating the producer state as it catches
    up by replicating from the current leader, and it has the most current sequence
    IDs in memory when it is ready to become a leader again.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: What if a broker crashed and the last snapshot is not updated? Producer ID and
    sequence ID are also part of the message format that is written to Kafka’s logs.
    During crash recovery, the producer state will be recovered by reading the older
    snapshot and also messages from the latest segment of each partition. A new snapshot
    will be stored as soon as the recovery process completes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: An interesting question is what happens if there are no messages? Imagine that
    a certain topic has two hours of retention time, but no new messages arrived in
    the last two hours—there will be no messages to use to recover the state if a
    broker crashed. Luckily, no messages also means no duplicates. We will start accepting
    messages immediately (while logging a warning about the lack of state), and create
    the producer state from the new messages that arrive.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of the Idempotent Producer
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka’s idempotent producer only prevents duplicates in case of retries that
    are caused by the producer’s internal logic. Calling `producer.send()` twice with
    the same message will create a duplicate, and the idempotent producer won’t prevent
    it. This is because the producer has no way of knowing that the two records that
    were sent are in fact the same record. It is always a good idea to use the built-in
    retry mechanism of the producer rather than catching producer exceptions and retrying
    from the application itself; the idempotent producer makes this pattern even more
    appealing—it is the easiest way to avoid duplicates when retrying.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: It is also rather common to have applications that have multiple instances or
    even one instance with multiple producers. If two of these producers attempt to
    send identical messages, the idempotent producer will not detect the duplication.
    This scenario is fairly common in applications that get data from a source—a directory
    with files, for instance—and produce it to Kafka. If the application happened
    to have two instances reading the same file and producing records to Kafka, we
    will get multiple copies of the records in that file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The idempotent producer will only prevent duplicates caused by the retry mechanism
    of the producer itself, whether the retry is caused by producer, network, or broker
    errors. But nothing else.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: How Do I Use the Kafka Idempotent Producer?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the easy part. Add `enable.idempotence=true` to the producer configuration.
    If the producer is already configured with `acks=all`, there will be no difference
    in performance. By enabling idempotent producer, the following things will change:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: To retrieve a producer ID, the producer will make one extra API call when starting
    up.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each record batch sent will include the producer ID and the sequence ID for
    the first message in the batch (sequence IDs for each message in the batch are
    derived from the sequence ID of the first message plus a delta). These new fields
    add 96 bits to each record batch (producer ID is a long, and sequence is an integer),
    which is barely any overhead for most workloads.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brokers will validate the sequence numbers from any single producer instance
    and guarantee the lack of duplicate messages.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理将验证来自任何单个生产者实例的序列号，并保证没有重复消息。
- en: The order of messages produced to each partition will be guaranteed, through
    all failure scenarios, even if `max.in.flight.requests.per.connection` is set
    to more than 1 (5 is the default and also the highest value supported by the idempotent
    producer).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将保证每个分区产生的消息顺序，即使`max.in.flight.requests.per.connection`设置为大于1（5是默认值，也是幂等生产者支持的最高值）。
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Idempotent producer logic and error handling improved significantly in version
    2.5 (both on the producer side and the broker side) as a result of KIP-360\. Prior
    to release 2.5, the producer state was not always maintained for long enough,
    which resulted in fatal UNKNOWN_PRODUCER_ID errors in various scenarios (partition
    reassignment had a known edge case where the new replica became the leader before
    any writes happened from a specific producer, meaning that the new leader had
    no state for that partition). In addition, previous versions attempted to rewrite
    the sequence IDs in some error scenarios, which could lead to duplicates. In newer
    versions, if we encounter a fatal error for a record batch, this batch and all
    the batches that are in flight will be rejected. The user who writes the application
    can handle the exception and decide whether to skip those records or retry and
    risk duplicates and reordering.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等生产者逻辑和错误处理在2.5版本中得到了显着改进（生产者端和代理端都是如此），这是KIP-360的结果。在2.5版本发布之前，生产者状态并不总是维持足够长的时间，这导致在各种场景中出现致命的UNKNOWN_PRODUCER_ID错误（分区重新分配存在一个已知的边缘情况，即在特定生产者的任何写操作发生之前，新副本就成为了领导者，这意味着新领导者对该分区没有状态）。此外，以前的版本在某些错误场景下尝试重写序列ID，这可能导致重复。在更新版本中，如果我们遇到记录批次的致命错误，这个批次和所有正在传输的批次都将被拒绝。编写应用程序的用户可以处理异常并决定是跳过这些记录还是重试并冒重复和重新排序的风险。
- en: Transactions
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交易
- en: As we mentioned in the introduction to this chapter, transactions were added
    to Kafka to guarantee the correctness of applications developed using Kafka Streams.
    In order for a stream processing application to generate correct results, each
    input record must be processed exactly one time, and its processing result will
    be reflected exactly one time, even in case of failure. Transactions in Apache
    Kafka allow stream processing applications to generate accurate results. This,
    in turn, enables developers to use stream processing applications in use cases
    where accuracy is a key requirement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章介绍中提到的，Kafka添加了交易以确保使用Kafka Streams开发的应用程序的正确性。为了使流处理应用程序生成正确的结果，每个输入记录必须被精确处理一次，并且其处理结果将被精确反映一次，即使发生故障也是如此。Apache
    Kafka中的交易允许流处理应用程序生成准确的结果。这反过来使开发人员能够在准确性是关键要求的用例中使用流处理应用程序。
- en: It is important to keep in mind that transactions in Kafka were developed specifically
    for stream processing applications. And therefore they were built to work with
    the “consume-process-produce” pattern that forms the basis of stream processing
    applications. Use of transactions can guarantee exactly-once semantics in this
    context—the processing of each input record will be considered complete after
    the application’s internal state has been updated and the results were successfully
    produced to output topics. In [“What Problems Aren’t Solved by Transactions?”](#Limitations),
    we’ll explore a few scenarios where Kafka’s exactly-once guarantees will not apply.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，Kafka中的交易是专门为流处理应用程序开发的。因此，它们被构建为与形成流处理应用程序基础的“消费-处理-生产”模式一起工作。在这种情况下，使用交易可以保证一次性语义——每个输入记录的处理在应用程序的内部状态更新并成功产生到输出主题后被视为完成。在[“交易解决不了哪些问题？”](#Limitations)中，我们将探讨一些Kafka的一次性保证不适用的情况。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Transactions is the name of the underlying mechanism. Exactly-once semantics
    or exactly-once guarantees is the behavior of a stream processing application.
    Kafka Streams uses transactions to implement its exactly-once guarantees. Other
    stream processing frameworks, such as Spark Streaming or Flink, use different
    mechanisms to provide their users with exactly-once semantics.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 交易是底层机制的名称。一次性语义或一次性保证是流处理应用程序的行为。Kafka Streams使用交易来实现其一次性保证。其他流处理框架，如Spark
    Streaming或Flink，使用不同的机制来为其用户提供一次性语义。
- en: Transactions Use Cases
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交易使用案例
- en: Transactions are useful for any stream processing application where accuracy
    is important, and especially where stream processing includes aggregation and/or
    joins. If the stream processing application only performs single record transformation
    and filtering, there is no internal state to update, and even if duplicates were
    introduced in the process, it is fairly straightforward to filter them out of
    the output stream. When the stream processing application aggregates several records
    into one, it is much more difficult to check whether a result record is wrong
    because some input records were counted more than once; it is impossible to correct
    the result without reprocessing the input.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 交易对于任何重视准确性的流处理应用程序都是有用的，特别是当流处理包括聚合和/或连接时。如果流处理应用程序只执行单个记录的转换和过滤，那么没有内部状态需要更新，即使在过程中引入了重复，也很容易将它们从输出流中过滤掉。当流处理应用程序将多个记录聚合为一个记录时，要检查结果记录是否错误要困难得多，因为某些输入记录被计算了多次；在不重新处理输入的情况下无法纠正结果。
- en: Financial applications are typical examples of complex stream processing applications
    where exactly-once capabilities are used to guarantee accurate aggregation. However,
    because it is rather trivial to configure any Kafka Streams application to provide
    exactly-once guarantees, we’ve seen it enabled in more mundane use cases, including,
    for instance, chatbots.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 金融应用程序是复杂流处理应用程序的典型例子，其中使用一次性能力来保证准确的聚合。然而，因为任何Kafka Streams应用程序都可以相当轻松地配置为提供一次性保证，我们已经看到它在更普通的用例中启用，包括例如聊天机器人。
- en: What Problems Do Transactions Solve?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务解决了什么问题？
- en: 'Consider a simple stream processing application: it reads events from a source
    topic, maybe processes them, and writes results to another topic. We want to be
    sure that for each message we process, the results are written exactly once. What
    can possibly go wrong?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的流处理应用程序：它从源主题中读取事件，可能对其进行处理，并将结果写入另一个主题。我们希望确保我们处理的每条消息的结果只被写入一次。可能会发生什么问题？
- en: It turns out that quite a few things could go wrong. Let’s look at two scenarios.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，有很多事情可能会出错。让我们看看两种情况。
- en: Reprocessing caused by application crashes
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 由应用程序崩溃引起的重新处理
- en: 'After consuming a message from the source cluster and processing it, the application
    has to do two things: produce the result to the output topic, and commit the offset
    of the message that we consumed. Suppose that these two separate actions happen
    in this order. What happens if the application crashes after the output was produced
    but before the offset of the input was committed?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从源集群中消费一条消息并处理后，应用程序必须做两件事：将结果生成到输出主题，并提交我们消费的消息的偏移量。假设这两个独立的操作按照这个顺序发生。如果应用程序在生成输出后但在提交输入的偏移量之前崩溃会发生什么？
- en: In [Chapter 4](ch04.html#reading_data_from_kafka), we discussed what happens
    when a consumer crashes. After a few seconds, the lack of heartbeat will trigger
    a rebalance, and the partitions the consumer was consuming from will be reassigned
    to a different consumer. That consumer will begin consuming records from those
    partitions, starting at the last committed offset. This means that all the records
    that were processed by the application between the last committed offset and the
    crash will be processed again, and the results will be written to the output topic
    again—resulting in duplicates.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#reading_data_from_kafka)中，我们讨论了消费者崩溃时会发生什么。几秒钟后，缺少心跳将触发重新平衡，并且消费者正在消费的分区将重新分配给另一个消费者。该消费者将开始从这些分区中消费记录，从上次提交的偏移量开始。这意味着在上次提交的偏移量和崩溃之间应用程序处理的所有记录将被再次处理，并且结果将再次写入输出主题，导致重复。
- en: Reprocessing caused by zombie applications
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 由僵尸应用程序引起的重新处理
- en: What happens if our application just consumed a batch of records from Kafka
    and then froze or lost connectivity to Kafka before doing anything else with this
    batch of records?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的应用程序刚刚从Kafka中消费了一批记录，然后在对这批记录进行任何其他操作之前冻结或失去与Kafka的连接，会发生什么？
- en: Just like in the previous scenario, after several heartbeats are missed, the
    application will be assumed dead and its partitions reassigned to another consumer
    in the consumer group. That consumer will reread that batch of records, process
    it, produce the results to an output topic, and continue on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在先前的情景中一样，如果错过了几次心跳，应用程序将被认为已经死亡，并且其分区将重新分配给消费者组中的另一个消费者。该消费者将重新读取该批记录，处理它，将结果生成到输出主题，并继续进行。
- en: 'Meanwhile, the first instance of the application—the one that froze—may resume
    its activity: process the batch of records it recently consumed, and produce the
    results to the output topic. It can do all that before it polls Kafka for records
    or sends a heartbeat and discovers that it is supposed to be dead and another
    instance now owns those partitions.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，第一个应用程序实例——冻结的那个——可能会恢复其活动：处理它最近消费的一批记录，并将结果生成到输出主题。它可以在轮询Kafka记录或发送心跳并发现自己应该死亡并且另一个实例现在拥有这些分区之前完成所有这些操作。
- en: A consumer that is dead but doesn’t know it is called a zombie. In this scenario,
    we can see that without additional guarantees, zombies can produce data to the
    output topic and cause duplicate results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个死亡但不知道自己已经死亡的消费者被称为僵尸。在这种情况下，我们可以看到，如果没有额外的保证，僵尸可能会向输出主题生成数据，并导致重复的结果。
- en: How Do Transactions Guarantee Exactly-Once?
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务如何保证一次性？
- en: Take our simple stream processing application. It reads data from one topic,
    processes it, and writes the result to another topic. Exactly-once processing
    means that consuming, processing, and producing are done *atomically*. Either
    the offset of the original message is committed and the result is successfully
    produced or neither of these things happen. We need to make sure that partial
    results—where the offset is committed but the result isn’t produced, or vice versa—can’t
    happen.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的简单流处理应用程序为例。它从一个主题中读取数据，处理它，并将结果写入另一个主题。一次性处理意味着消费、处理和生成都是*原子*的。要么原始消息的偏移量被提交，结果被成功生成，要么这两件事都不会发生。我们需要确保部分结果——偏移量已提交但结果未生成，或者反之亦然——不会发生。
- en: To support this behavior, Kafka transactions introduce the idea of *atomic multipartition
    writes*. The idea is that committing offsets and producing results both involve
    writing messages to partitions. However, the results are written to an output
    topic, and offsets are written to the `_consumer_offsets` topic. If we can open
    a transaction, write both messages, and commit if both were written successfully—or
    abort to retry if they were not—we will get the exactly-once semantics that we
    are after.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这种行为，Kafka事务引入了*原子多分区写*的概念。这个想法是，提交偏移量和生成结果都涉及将消息写入分区。然而，结果被写入输出主题，偏移量被写入`_consumer_offsets`主题。如果我们可以开启一个事务，写入这两条消息，并且如果两者都成功写入则提交，或者中止以重试，我们将得到我们想要的一次性语义。
- en: '[Figure 8-1](#atomic-write) illustrates a simple stream processing application,
    performing an atomic multipartition write to two partitions while also committing
    offsets for the event it consumed.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0801](assets/kdg2_0801.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Transactional producer with atomic multipartition write
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To use transactions and perform atomic multipartition writes, we use a *transactional
    producer*. A transactional producer is simply a Kafka producer that is configured
    with a `transactional.id` and has been initialized using `initTransactions()`.
    Unlike `producer.id`, which is generated automatically by Kafka brokers, `transactional.id`
    is part of the producer configuration and is expected to persist between restarts.
    In fact, the main role of the `transactional.id` is to identify the same producer
    across restarts. Kafka brokers maintain `transactional.id` to `producer.id` mapping,
    so if `initTransactions()` is called again with an existing `transactional.id`,
    the producer will also be assigned the same `producer.id` instead of a new random
    number.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Preventing zombie instances of the application from creating duplicates requires
    a mechanism for *zombie fencing*, or preventing zombie instances of the application
    from writing results to the output stream. The usual way of fencing zombies—using
    an epoch—is used here. Kafka increments the epoch number associated with a `transactional.id`
    when `initTransaction()` is invoked to initialize a transactional producer. Send,
    commit, and abort requests from producers with the same `transactional.id` but
    lower epochs will be rejected with the `FencedProducer` error. The older producer
    will not be able to write to the output stream and will be forced to `close()`,
    preventing the zombie from introducing duplicate records. In Apache Kafka 2.5
    and later, there is also an option to add consumer group metadata to the transaction
    metadata. This metadata will also be used for fencing, which will allow producers
    with different transactional IDs to write to the same partitions while still fencing
    against zombie instances.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Transactions are a producer feature for the most part—we create a transactional
    producer, begin the transaction, write records to multiple partitions, produce
    offsets in order to mark records as already processed, and commit or abort the
    transaction. We do all this from the producer. However, this isn’t quite enough—records
    written transactionally, even ones that are part of transactions that were eventually
    aborted, are written to partitions just like any other records. Consumers need
    to be configured with the right isolation guarantees, otherwise we won’t have
    the exactly-once guarantees we expected.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: We control the consumption of messages that were written transactionally by
    setting the `isolation.level` configuration. If set to `read_committed`, calling
    `consumer.poll()` after subscribing to a set of topics will return messages that
    were either part of a successfully committed transaction or that were written
    nontransactionally; it will not return messages that were part of an aborted transaction
    or a transaction that is still open. The default `isolation.level` value, `read_uncommitted`,
    will return all records, including those that belong to open or aborted transactions.
    Configuring `read_committed` mode does not guarantee that the application will
    get all messages that are part of a specific transaction. It is possible to subscribe
    to only a subset of topics that were part of the transaction and therefore get
    a subset of the messages. In addition, the application can’t know when transactions
    begin or end, or which messages are part of which transaction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-2](#read-committed) shows which records are visible to a consumer
    in `read_committed` mode compared to a consumer with the default `read_uncommitted`
    mode.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0802](assets/kdg2_0802.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Consumers in `read_committed` mode will lag behind consumers with
    default configuration
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To guarantee that messages will be read in order, `read_committed` mode will
    not return messages that were produced after the point when the first still-open
    transaction began (known as the Last Stable Offset, or LSO). Those messages will
    be withheld until that transaction is committed or aborted by the producer, or
    until they reach `transaction.timeout.ms` (default of 15 minutes) and are aborted
    by the broker. Holding a transaction open for a long duration will introduce higher
    end-to-end latency by delaying consumers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Our simple stream processing job will have exactly-once guarantees on its output
    even if the input was written nontransactionally. The atomic multipartition produce
    guarantees that if the output records were committed to the output topic, the
    offset of the input records was also committed for that consumer, and as a result
    the input records will not be processed again.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: What Problems Aren’t Solved by Transactions?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As explained earlier, transactions were added to Kafka to provide multipartition
    atomic writes (but not reads) and to fence zombie producers in stream processing
    applications. As a result, they provide exactly-once guarantees when used within
    chains of consume-process-produce stream processing tasks. In other contexts,
    transactions will either straight-out not work or will require additional effort
    in order to achieve the guarantees we want.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The two main mistakes are assuming that exactly-once guarantees apply on actions
    other than producing to Kafka, and that consumers always read entire transactions
    and have information about transaction boundaries.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The following are a few scenarios in which Kafka transactions won’t help achieve
    exactly-once guarantees.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Side effects while stream processing
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s say that the record processing step in our stream processing app includes
    sending email to users. Enabling exactly-once semantics in our app will not guarantee
    that the email will only be sent once. The guarantee only applies to records written
    to Kafka. Using sequence numbers to deduplicate records or using markers to abort
    or to cancel a transaction works within Kafka, but it will not un-send an email.
    The same is true for any action with external effects that is performed within
    the stream processing app: calling a REST API, writing to a file, etc.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Reading from a Kafka topic and writing to a database
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, the application is writing to an external database rather than
    to Kafka. In this scenario, there is no producer involved—records are written
    to the database using a database driver (likely JDBC) and offsets are committed
    to Kafka within the consumer. There is no mechanism that allows writing results
    to an external database and committing offsets to Kafka within a single transaction.
    Instead, we could manage offsets in the database (as explained in [Chapter 4](ch04.html#reading_data_from_kafka))
    and commit both data and offsets to the database in a single transaction—this
    would rely on the database’s transactional guarantees rather than Kafka’s.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Microservices often need to update the database *and* publish a message to Kafka
    within a single atomic transaction, so either both will happen or neither will.
    As we’ve just explained in the last two examples, Kafka transactions will not
    do this.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: A common solution to this common problem is known as the *outbox pattern*. The
    microservice only publishes the message to a Kafka topic (the “outbox”), and a
    separate message relay service reads the event from Kafka and updates the database.
    Because, as we’ve just seen, Kafka won’t guarantee an exactly-once update to the
    database, it is important to make sure the update is idempotent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Using this pattern guarantees that the message will eventually make it to Kafka,
    the topic consumers, and the database—or to none of those.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The inverse pattern—where a database table serves as the outbox and a relay
    service makes sure updates to the table will also arrive to Kafka as messages—is
    also used. This pattern is preferred when built-in RDBMS constraints, such as
    uniqueness and foreign keys, are useful. The Debezium project published an [in-depth
    blog post on the outbox pattern](https://oreil.ly/PB3Vb) with detailed examples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模式——其中数据库表用作发件箱，中继服务确保对表的更新也将作为消息到达Kafka——也被使用。当内置的RDBMS约束，如唯一性和外键，是有用的时，这种模式是首选。Debezium项目发布了一篇[关于发件箱模式的深入博客文章](https://oreil.ly/PB3Vb)，其中包含详细的示例。
- en: Reading data from a database, writing to Kafka, and from there writing to another
    database
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数据库中读取数据，写入Kafka，然后从那里写入另一个数据库
- en: It is very tempting to believe that we can build an app that will read data
    from a database, identify database transactions, write the records to Kafka, and
    from there write records to another database, still maintaining the original transactions
    from the source database.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易相信我们可以构建一个应用程序，从数据库中读取数据，识别数据库事务，将记录写入Kafka，然后从那里将记录写入另一个数据库，仍然保持源数据库的原始事务。
- en: 'Unfortunately, Kafka transactions don’t have the necessary functionality to
    support these kinds of end-to-end guarantees. In addition to the problem with
    committing both records and offsets within the same transaction, there is another
    difficulty: `read_committed` guarantees in Kafka consumers are too weak to preserve
    database transactions. Yes, a consumer will not see records that were not committed.
    But it is not guaranteed to have seen all the records that were committed within
    the transaction because it could be lagging on some topics; it has no information
    to identify transaction boundaries, so it can’t know when a transaction began
    and ended, and whether it has seen some, none, or all of its records.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Kafka事务没有必要的功能来支持这些端到端的保证。除了在同一事务中提交记录和偏移量的问题外，还存在另一个困难：Kafka消费者中的`read_committed`保证对于保留数据库事务来说太弱。是的，消费者不会看到未提交的记录。但它不能保证已经看到了事务中提交的所有记录，因为它可能在某些主题上滞后；它没有信息来识别事务边界，因此无法知道事务何时开始和结束，以及它是否已经看到了一些、没有或所有的记录。
- en: Copying data from one Kafka cluster to another
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从一个Kafka集群复制数据到另一个集群
- en: This one is more subtle—it is possible to support exactly-once guarantees when
    copying data from one Kafka cluster to another. There is a description of how
    this is done in the Kafka improvement proposal for adding [exactly-once capabilities
    in MirrorMaker 2.0](https://oreil.ly/EoM6w). At the time of this writing, the
    proposal is still in draft, but the algorithm is clearly described. This proposal
    includes the guarantee that each record in the source cluster will be copied to
    the destination cluster exactly once.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这更加微妙——在从一个Kafka集群复制数据到另一个集群时，可以支持精确一次性保证。在Kafka改进提案中描述了如何在[镜像制造者2.0中添加精确一次性功能](https://oreil.ly/EoM6w)。在撰写本文时，该提案仍处于草案阶段，但算法已经清楚描述。该提案包括保证源集群中的每个记录将被精确地复制到目标集群中一次。
- en: 'However, this does not guarantee that transactions will be atomic. If an app
    produces several records and offsets transactionally, and then MirrorMaker 2.0
    copies them to another Kafka cluster, the transactional properties and guarantees
    will be lost during the copy process. They are lost for the same reason when copying
    data from Kafka to a relational database: the consumer reading data from Kafka
    can’t know or guarantee that it is getting all the events in a transaction. For
    example, it can replicate part of a transaction if it is only subscribed to a
    subset of the topics.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不保证事务是原子的。如果一个应用程序以事务方式生成多个记录和偏移量，然后MirrorMaker 2.0将它们复制到另一个Kafka集群，事务属性和保证将在复制过程中丢失。当从Kafka复制数据到关系型数据库时，由于消费者从Kafka读取数据时无法知道或保证它是否获取了事务中的所有事件，同样的原因也会丢失。例如，如果只订阅了一部分主题，它可能会复制事务的一部分。
- en: Publish/subscribe pattern
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发布/订阅模式
- en: 'Here’s a slightly more subtle case. We’ve discussed exactly-once in the context
    of the consume-process-produce pattern, but the publish/subscribe pattern is a
    very common use case. Using transactions in a publish/subscribe use case provides
    some guarantees: consumers configured with `read_committed` mode will not see
    records that were published as part of a transaction that was aborted. But those
    guarantees fall short of exactly-once. Consumers may process a message more than
    once, depending on their own offset commit logic.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个稍微微妙的情况。我们已经讨论了在消费-处理-生产模式的上下文中的精确一次性，但发布/订阅模式是一个非常常见的用例。在发布/订阅用例中使用事务提供了一些保证：配置为`read_committed`模式的消费者不会看到作为中止事务的一部分发布的记录。但这些保证还不足以达到精确一次性。消费者可能会根据自己的偏移提交逻辑多次处理消息。
- en: The guarantees Kafka provides in this case are similar to those provided by
    JMS transactions but depend on consumers in `read_committed` mode to guarantee
    that uncommitted transactions will remain invisible. JMS brokers withhold uncommitted
    transactions from all consumers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka在这种情况下提供的保证类似于JMS事务提供的保证，但依赖于`read_committed`模式的消费者来保证未提交的事务将保持不可见。JMS代理会向所有消费者隐藏未提交的事务。
- en: Warning
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: An important pattern to avoid is publishing a message and then waiting for another
    application to respond before committing the transaction. The other application
    will not receive the message until after the transaction was committed, resulting
    in a deadlock.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要避免的一个重要模式是发布消息，然后等待另一个应用程序在提交事务之前做出响应。在事务提交之后，其他应用程序将无法接收到消息，导致死锁。
- en: How Do I Use Transactions?
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我如何使用事务？
- en: Transactions are a broker feature and part of the Kafka protocol, so there are
    multiple clients that support transactions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 事务是代理功能和Kafka协议的一部分，因此有多个客户端支持事务。
- en: The most common and most recommended way to use transactions is to enable exactly-once
    guarantees in Kafka Streams. This way, we will not use transactions directly at
    all, but rather Kafka Streams will use them for us behind the scenes to provide
    the guarantees we need. Transactions were designed with this use case in mind,
    so using them via Kafka Streams is the easiest and most likely to work as expected.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: To enable exactly-once guarantees for a Kafka Streams application, we simply
    set the `processing.guarantee` configuration to either `exactly_once` or `exactly_once_​beta`.
    That’s it.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`exactly_once_beta` is a slightly different method of handling application
    instances that crash or hang with in-flight transactions. This was introduced
    in release 2.5 to Kafka brokers, and in release 2.6 to Kafka Streams. The main
    benefit of this method is the ability to handle many partitions with a single
    transactional producer and therefore create more scalable Kafka Streams applications.
    There is more information about the changes in the [Kafka improvement proposal
    where they were first discussed](https://oreil.ly/O3dSA).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we want exactly-once guarantees without using Kafka Streams? In
    this case we will use transactional APIs directly. Here’s a snippet showing how
    this will work. There is a full example in the Apache Kafka GitHub, which includes
    a [demo driver](https://oreil.ly/45dE4) and a [simple exactly-once processor](https://oreil.ly/CrXHU)
    that runs in separate threads:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_exactly_once_semantics_CO1-1)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a producer with `transactional.id` makes it a transactional producer
    capable of producing atomic multipartition writes. The transactional ID must be
    unique and long-lived. Essentially it defines an instance of the application.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_exactly_once_semantics_CO1-2)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Consumers that are part of the transactions don’t commit their own offsets—the
    producer writes offsets as part of the transaction. So offset commit should be
    disabled.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_exactly_once_semantics_CO1-3)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the consumer reads from an input topic. We will assume that
    the records in the input topic were also written by a transactional producer (just
    for fun—there is no such requirement for the input). To read transactions cleanly
    (i.e., ignore in-flight and aborted transactions), we will set the consumer isolation
    level to `read_committed`. Note that the consumer will still read nontransactional
    writes, in addition to reading committed transactions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_exactly_once_semantics_CO1-4)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The first thing a transactional producer must do is initialize. This registers
    the transactional ID, bumps up the epoch to guarantee that other producers with
    the same ID will be considered zombies, and aborts older in-flight transactions
    from the same transactional ID.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_exactly_once_semantics_CO1-5)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Here we are using the `subscribe` consumer API, which means that partitions
    assigned to this instance of the application can change at any point as a result
    of rebalance. Prior to release 2.5, which introduced API changes from KIP-447,
    this was much more challenging. Transactional producers had to be statically assigned
    a set of partitions, because the transaction fencing mechanism relied on the same
    transactional ID being used for the same partitions (there was no zombie fencing
    protection if the transactional ID changed). KIP-447 added new APIs, used in this
    example, that attach consumer-group information to the transaction, and this information
    is used for fencing. When using this method, it also makes sense to commit transactions
    whenever the related partitions are revoked.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_exactly_once_semantics_CO1-6)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: We consumed records, and now we want to process them and produce results. This
    method guarantees that everything that is produced from the time it was called,
    until the transaction is either committed or aborted, is part of a single atomic
    transaction.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_exactly_once_semantics_CO1-7)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: This is where we process the records—all our business logic goes here.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_exactly_once_semantics_CO1-8)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: As we explained earlier in the chapter, it is important to commit the offsets
    as part of the transaction. This guarantees that if we fail to produce results,
    we won’t commit the offsets for records that were not, in fact, processed. This
    method commits offsets as part of the transaction. Note that it is important not
    to commit offsets in any other way—disable offset auto-commit, and don’t call
    any of the consumer commit APIs. Committing offsets by any other method does not
    provide transactional guarantees.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_exactly_once_semantics_CO1-9)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: We produced everything we needed, we committed offsets as part of the transaction,
    and it is time to commit the transaction and seal the deal. Once this method returns
    successfully, the entire transaction has made it through, and we can continue
    to read and process the next batch of events.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](assets/10.png)](#co_exactly_once_semantics_CO1-10)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: If we got this exception, it means we are the zombie. Somehow our application
    froze or disconnected, and there is a newer instance of the app with our transactional
    ID running. Most likely the transaction we started has already been aborted and
    someone else is processing those records. Nothing to do but die gracefully.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[![11](assets/11.png)](#co_exactly_once_semantics_CO1-11)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: If we got an error while writing a transaction, we can abort the transaction,
    set the consumer position back, and try again.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Transactional IDs and Fencing
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the transactional ID for producers is important and a bit more challenging
    than it seems. Assigning the transactional ID incorrectly can lead to either application
    errors or loss of exactly-once guarantees. The key requirements are that the transactional
    ID will be consistent for the same instance of the application between restarts
    and is different for different instances of the application, otherwise the brokers
    will not be able to fence off zombie instances.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Until release 2.5, the only way to guarantee fencing was to statically map the
    transactional ID to partitions. This guaranteed that each partition will always
    be consumed with the same transactional ID. If a producer with transactional ID
    A processed messages from topic T and lost connectivity, and the new producer
    that replaces it has transactional ID B, and later producer A comes back as a
    zombie, zombie A will not be fenced because the ID doesn’t match that of the new
    producer B. We want producer A to always be replaced by producer A, and the new
    producer A will have a higher epoch number and zombie A will be properly fenced
    away. In those releases, the previous example would be incorrect—transactional
    IDs are assigned randomly to threads without making sure the same transactional
    ID is always used to write to the same partition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In Apache Kafka 2.5, KIP-447 introduced a second method of fencing based on
    consumer group metadata for fencing in addition to transactional IDs. We use the
    producer offset commit method and pass as an argument the consumer group metadata
    rather than just the consumer group ID.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we have topic T1 with two partitions, t-0 and t-1\. Each is consumed
    by a separate consumer in the same group; each consumer passes records to a matching
    transactional producer—one with transactional ID A and the other with transactional
    ID B; and they are writing output to topic T2 partitions 0 and 1, respectively.
    [Figure 8-3](#transaction-processor) illustrates this scenario.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0803](assets/kdg2_0803.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Transactional record processor
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As illustrated in [Figure 8-4](#transaction-processor-2), if the application
    instance with consumer A and producer A becomes a zombie, consumer B will start
    processing records from both partitions. If we want to guarantee that no zombies
    write to partition 0, consumer B can’t just start reading from partition 0 and
    writing to partition 0 with transactional ID B. Instead the application will need
    to instantiate a new producer, with transactional ID A, to safely write to partition
    0 and fence the old transactional ID A. This is wasteful. Instead, we include
    the consumer group information in the transactions. Transactions from producer
    B will show that they are from a newer generation of the consumer group, and therefore
    they will go through, while transactions from the now-zombie producer A will show
    an old generation of the consumer group and will be fenced.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 0804](assets/kdg2_0804.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Transactional record processor after a rebalance
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How Transactions Work
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use transactions by calling the APIs without understanding how they work.
    But having some mental model of what is going on under the hood will help us troubleshoot
    applications that do not behave as expected.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic algorithm for transactions in Kafka was inspired by Chandy-Lamport
    snapshots, in which “marker” control messages are sent into communication channels,
    and consistent state is determined based on the arrival of the marker. Kafka transactions
    use marker messages to indicate that transactions are committed or aborted across
    multiple partitions—when the producer decides to commit a transaction, it sends
    a “commit” message to the transaction coordinator, which then writes commit markers
    to all partitions involved in a transaction. But what happens if the producer
    crashes after only writing commit messages to a subset of the partitions? Kafka
    transactions solve this by using two-phase commit and a transaction log. At a
    high level, the algorithm will:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Log the existence of an ongoing transaction, including the partitions involved
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log the intent to commit or abort—once this is logged, we are doomed to commit
    or abort eventually
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write all the transaction markers to all the partitions
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log the completion of the transaction
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To implement this basic algorithm, Kafka needs a transaction log. We use an
    internal topic called `__transaction_state`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how this algorithm works in practice by going through the inner workings
    of the transactional API calls we’ve used in the preceding code snippet.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin the first transaction, producers need to register as transactional
    by calling `initTransaction()`. This request is sent to a broker that will be
    the *transaction coordinator* for this transactional producer. Each broker is
    the transactional coordinator for a subset of the producers, just like each broker
    is the consumer group coordinator for a subset of the consumer groups. The transaction
    coordinator for each transactional ID is the leader of the partition of the transaction
    log the transactional ID is mapped to.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The `initTransaction()` API registers a new transactional ID with the coordinator,
    or increments the epoch of an existing transactional ID in order to fence off
    previous producers that may have become zombies. When the epoch is incremented,
    pending transactions will be aborted.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The next step for the producer is to call `beginTransaction()`. This API call
    isn’t part of the protocol—it simply tells the producer that there is now a transaction
    in progress. The transaction coordinator on the broker side is still unaware that
    the transaction began. However, once the producer starts sending records, each
    time the producer detects that it is sending records to a new partition, it will
    also send `Add​Par⁠titionsToTxnRequest` to the broker informing it that there
    is a transaction in progress for this producer, and that additional partitions
    are part of the transaction. This information will be recorded in the transaction
    log.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: When we are done producing results and are ready to commit, we start by committing
    offsets for the records we’ve processed in this transaction. Committing offsets
    can be done at any time but must be done before the transaction is committed.
    Calling `sendOffsetsToTransaction()` will send a request to the transaction coordinator
    that includes the offsets and also the consumer group ID. The transaction coordinator
    will use the consumer group ID to find the group coordinator and commit the offsets
    as a consumer group normally would.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Now it is time to commit—or abort. Calling `commitTransaction()` or `abort​Transac⁠tion()`
    will send an `EndTransactionRequest` to the transaction coordinator. The transaction
    coordinator will log the commit or abort intention to the transaction log. Once
    this step is successful, it is the transaction coordinator’s responsibility to
    complete the commit (or abort) process. It writes a commit marker to all the partitions
    involved in the transaction, then writes to the transaction log that the commit
    completed successfully. Note that if the transaction coordinator shuts down or
    crashes after logging the intention to commit and before completing the process,
    a new transaction coordinator will be elected, pick up the intent to commit from
    the transaction log, and complete the process.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: If a transaction is not committed or aborted within `transaction.timeout.ms`,
    the transaction coordinator will abort it automatically.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each broker that receives records from transactional or idempotent producers
    will store the producer/transactional IDs in memory, together with related state
    for each of the last five batches sent by the producer: sequence numbers, offsets,
    and such. This state is stored for `transactional.id.expiration.ms` milliseconds
    after the producer stopped being active (seven days by default). This allows the
    producer to resume activity without running into `UNKNOWN_PRODUCER_ID` errors.
    It is possible to cause something similar to a memory leak in the broker by creating
    new idempotent producers or new transactional IDs at a very high rate but never
    reusing them. Three new idempotent producers per second, accumulated over the
    course of a week, will result in 1.8 million producer state entries with a total
    of 9 million batch metadata stored, using around 5 GB RAM. This can cause out-of-memory
    or severe garbage collection issues on the broker. We recommend architecting the
    application to initialize a few long-lived producers when the application starts
    up, and then reuse them for the lifetime of the application. If this isn’t possible
    (Function as a Service makes this difficult), we recommend lowering `transactional.id.​expira⁠tion.ms`
    so the IDs will expire faster, and therefore old state that will never be reused
    won’t take up a significant part of the broker memory.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Performance of Transactions
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transactions add moderate overhead to the producer. The request to register
    transactional ID occurs once in the producer lifecycle. Additional calls to register
    partitions as part of a transaction happen at most one per partition for each
    transaction, then each transaction sends a commit request, which causes an extra
    commit marker to be written on each partition. The transactional initialization
    and transaction commit requests are synchronous, so no data will be sent until
    they complete successfully, fail, or time out, which further increases the overhead.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Note that the overhead of transactions on the producer is independent of the
    number of messages in a transaction. So a larger number of messages per transaction
    will both reduce the relative overhead and reduce the number of synchronous stops,
    resulting in higher throughput overall.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: On the consumer side, there is some overhead involved in reading commit markers.
    The key impact that transactions have on consumer performance is introduced by
    the fact that consumers in `read_committed` mode will not return records that
    are part of an open transaction. Long intervals between transaction commits mean
    that the consumer will need to wait longer before returning messages, and as a
    result, end-to-end latency will increase.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that the consumer does not need to buffer messages that belong
    to open transactions. The broker will not return those in response to fetch requests
    from the consumer. Since there is no extra work for the consumer when reading
    transactions, there is no decrease in throughput either.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Exactly-once semantics in Kafka is the opposite of chess: it is challenging
    to understand but easy to use.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covered the two key mechanisms that provide exactly-once guarantees
    in Kafka: idempotent producer, which avoids duplicates that are caused by the
    retry mechanism, and transactions, which form the basis of exactly-once semantics
    in Kafka Streams.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Both can be enabled in a single configuration and allow us to use Kafka for
    applications that require fewer duplicates and stronger correctness guarantees.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: We discussed in depth specific scenarios and use cases to show the expected
    behavior, and even looked at some of the implementation details. Those details
    are important when troubleshooting applications or when using transactional APIs
    directly.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: By understanding what Kafka’s exactly-once semantics guarantee in which use
    case, we can design applications that will use exactly-once when necessary. Application
    behavior should not be surprising, and the information in this chapter will help
    us avoid surprises.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL

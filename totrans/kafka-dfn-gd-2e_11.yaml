- en: Chapter 9\. Building Data Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。构建数据管道
- en: When people discuss building data pipelines using Apache Kafka, they are usually
    referring to a couple of use cases. The first is building a data pipeline where
    Apache Kafka is one of the two end points—for example, getting data from Kafka
    to S3 or getting data from MongoDB into Kafka. The second use case involves building
    a pipeline between two different systems but using Kafka as an intermediary. An
    example of this is getting data from Twitter to Elasticsearch by sending the data
    first from Twitter to Kafka and then from Kafka to Elasticsearch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们讨论使用Apache Kafka构建数据管道时，他们通常指的是一些用例。第一种是构建一个数据管道，其中Apache Kafka是两个端点之一，例如，将数据从Kafka传输到S3，或者将数据从MongoDB传输到Kafka。第二种用例涉及在两个不同系统之间构建管道，但使用Kafka作为中间件。一个例子是通过首先将数据从Twitter发送到Kafka，然后从Kafka发送到Elasticsearch，从而将数据从Twitter传输到Elasticsearch。
- en: When we added Kafka Connect to Apache Kafka in version 0.9, it was after we
    saw Kafka used in both use cases at LinkedIn and other large organizations. We
    noticed that there were specific challenges in integrating Kafka into data pipelines
    that every organization had to solve, and decided to add APIs to Kafka that solve
    some of those challenges rather than force every organization to figure them out
    from scratch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Apache Kafka的0.9版本中添加Kafka Connect时，是因为我们看到Kafka在LinkedIn和其他大型组织中都被用于这两种用例。我们注意到将Kafka集成到数据管道中存在特定的挑战，每个组织都必须解决这些挑战，因此决定向Kafka添加API，以解决其中一些挑战，而不是强迫每个组织从头开始解决这些挑战。
- en: The main value Kafka provides to data pipelines is its ability to serve as a
    very large, reliable buffer between various stages in the pipeline. This effectively
    decouples producers and consumers of data within the pipeline and allows use of
    the same data from the source in multiple target applications and systems, all
    with different timeliness and availability requirements. This decoupling, combined
    with reliability, security, and efficiency, makes Kafka a good fit for most data
    pipelines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka为数据管道提供的主要价值在于其作为管道中各个阶段之间的非常大的、可靠的缓冲区。这有效地解耦了管道内的数据生产者和消费者，并允许在多个目标应用程序和系统中使用相同数据源的数据，这些目标应用程序和系统具有不同的及时性和可用性要求。这种解耦，加上可靠性、安全性和效率，使Kafka非常适合大多数数据管道。
- en: Putting Data Integration in Context
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据集成放入背景中
- en: Some organizations think of Kafka as an *end point* of a pipeline. They look
    at questions such as “How do I get data from Kafka to Elastic?” This is a valid
    question to ask—especially if there is data you need in Elastic and it is currently
    in Kafka—and we will look at ways to do exactly this. But we are going to start
    the discussion by looking at the use of Kafka within a larger context that includes
    at least two (and possibly many more) end points that are not Kafka itself. We
    encourage anyone faced with a data-integration problem to consider the bigger
    picture and not focus only on the immediate end points. Focusing on short-term
    integrations is how you end up with a complex and expensive-to-maintain data integration
    mess.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一些组织认为Kafka是管道的*端点*。他们关注的问题是“我如何将数据从Kafka传输到Elastic？”这是一个值得问的问题，特别是如果你需要Elastic中的数据，而它目前在Kafka中，我们将看看如何做到这一点。但我们将从在至少包括两个（可能更多）不是Kafka本身的端点的更大背景中开始讨论Kafka的使用。我们鼓励面临数据集成问题的任何人考虑更大的背景，而不仅仅关注于即时端点。专注于短期集成是导致复杂且昂贵的数据集成混乱的原因。
- en: In this chapter, we’ll discuss some of the common issues that you need to take
    into account when building data pipelines. Those challenges are not specific to
    Kafka but are general data integration problems. Nonetheless, we will show why
    Kafka is a good fit for data integration use cases and how it addresses many of
    those challenges. We will discuss how the Kafka Connect API is different from
    the normal producer and consumer clients, and when each client type should be
    used. Then we’ll jump into some details of Kafka Connect. While a full discussion
    of Kafka Connect is outside the scope of this chapter, we will show examples of
    basic usage to get you started and give you pointers on where to learn more. Finally,
    we’ll discuss other data integration systems and how they integrate with Kafka.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论构建数据管道时需要考虑的一些常见问题。这些挑战并不特定于Kafka，而是一般的数据集成问题。尽管如此，我们将展示为什么Kafka非常适合数据集成用例，以及它如何解决许多这些挑战。我们将讨论Kafka
    Connect API与普通的生产者和消费者客户端的不同之处，以及何时应该使用每种客户端类型。然后我们将深入讨论Kafka Connect的一些细节。虽然本章不涉及Kafka
    Connect的全面讨论，但我们将展示基本用法的示例，以帮助您入门，并指导您在哪里学习更多。最后，我们将讨论其他数据集成系统以及它们如何与Kafka集成。
- en: Considerations When Building Data Pipelines
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据管道时的考虑因素
- en: While we won’t get into all the details on building data pipelines here, we
    would like to highlight some of the most important things to take into account
    when designing software architectures with the intent of integrating multiple
    systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会在这里详细讨论构建数据管道的所有细节，但我们想强调在设计软件架构时需要考虑的一些最重要的事情，目的是集成多个系统。
- en: Timeliness
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 及时性
- en: 'Some systems expect their data to arrive in large bulks once a day; others
    expect the data to arrive a few milliseconds after it is generated. Most data
    pipelines fit somewhere in between these two extremes. Good data integration systems
    can support different timeliness requirements for different pipelines and also
    make the migration between different timetables easier as business requirements
    change. Kafka, being a streaming data platform with scalable and reliable storage,
    can be used to support anything from near-real-time pipelines to daily batches.
    Producers can write to Kafka as frequently and infrequently as needed, and consumers
    can also read and deliver the latest events as they arrive. Or consumers can work
    in batches: run every hour, connect to Kafka, and read the events that accumulated
    during the previous hour.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统期望它们的数据一天一次以大批量到达；其他系统期望数据在生成后的几毫秒内到达。大多数数据管道都处于这两个极端之间的某个位置。良好的数据集成系统可以支持不同管道的不同及时性要求，并且在业务需求变化时也可以更容易地迁移不同的时间表。Kafka作为一个具有可扩展和可靠存储的流数据平台，可以用于支持从几乎实时的管道到每日批处理的任何需求。生产者可以根据需要频繁或不频繁地写入Kafka，消费者也可以在事件到达时读取和传递最新的事件。或者消费者可以批量处理：每小时运行一次，连接到Kafka，并读取前一个小时积累的事件。
- en: A useful way to look at Kafka in this context is that it acts as a giant buffer
    that decouples the time-sensitivity requirements between producers and consumers.
    Producers can write events in real time, while consumers process batches of events,
    or vice versa. This also makes it trivial to apply back pressure—Kafka itself
    applies back pressure on producers (by delaying acks when needed) since consumption
    rate is driven entirely by the consumers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，看待Kafka的一个有用方式是它充当了一个巨大的缓冲区，解耦了生产者和消费者之间的时间敏感性要求。生产者可以实时写入事件，而消费者可以处理事件批次，反之亦然。这也使得施加反压变得微不足道——Kafka本身对生产者施加反压（在需要时延迟确认），因为消费速率完全由消费者驱动。
- en: Reliability
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性
- en: We want to avoid single points of failure and allow for fast and automatic recovery
    from all sorts of failure events. Data pipelines are often the way data arrives
    to business-critical systems; failure for more than a few seconds can be hugely
    disruptive, especially when the timeliness requirement is closer to the few milliseconds
    end of the spectrum. Another important consideration for reliability is delivery
    guarantees—some systems can afford to lose data, but most of the time there is
    a requirement for *at-least-once* delivery, which means every event from the source
    system will reach its destination, but sometimes retries will cause duplicates.
    Often, there is even a requirement for *exactly-once* delivery—every event from
    the source system will reach the destination with no possibility for loss or duplication.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望避免单点故障，并允许从各种故障事件中快速且自动地恢复。数据管道通常是数据到达业务关键系统的方式；故障超过几秒钟可能会带来巨大的破坏，特别是当时效性要求更接近光谱的几毫秒端时。可靠性的另一个重要考虑因素是交付保证——一些系统可以承受数据丢失，但大多数情况下都需要至少一次交付，这意味着源系统的每个事件都将到达其目的地，但有时重试会导致重复。通常，甚至需要恰好一次的交付——源系统的每个事件都将无法丢失或重复地到达目的地。
- en: We discussed Kafka’s availability and reliability guarantees in depth in [Chapter 7](ch07.html#reliable_data_delivery).
    As we discussed, Kafka can provide at-least-once on its own, and exactly-once
    when combined with an external data store that has a transactional model or unique
    keys. Since many of the end points are data stores that provide the right semantics
    for exactly-once delivery, a Kafka-based pipeline can often be implemented as
    exactly-once. It is worth highlighting that Kafka’s Connect API makes it easier
    for connectors to build an end-to-end exactly-once pipeline by providing an API
    for integrating with the external systems when handling offsets. Indeed, many
    of the available open source connectors support exactly-once delivery.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第7章](ch07.html#reliable_data_delivery)中深入讨论了Kafka的可用性和可靠性保证。正如我们所讨论的，Kafka可以单独提供至少一次的保证，并且当与具有事务模型或唯一键的外部数据存储结合时，可以提供恰好一次的保证。由于许多终点是提供恰好一次交付语义的数据存储，基于Kafka的管道通常可以实现恰好一次的交付。值得强调的是，Kafka的Connect
    API通过提供与处理偏移量时与外部系统集成的API，使连接器更容易构建端到端的恰好一次管道。事实上，许多可用的开源连接器支持恰好一次的交付。
- en: High and Varying Throughput
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高吞吐量和变化吞吐量
- en: The data pipelines we are building should be able to scale to very high throughputs,
    as is often required in modern data systems. Even more importantly, they should
    be able to adapt if throughput suddenly increases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的数据管道应该能够扩展到非常高的吞吐量，这在现代数据系统中经常需要。更重要的是，如果吞吐量突然增加，它们应该能够适应。
- en: With Kafka acting as a buffer between producers and consumers, we no longer
    need to couple consumer throughput to the producer throughput. We no longer need
    to implement a complex back-pressure mechanism because if producer throughput
    exceeds that of the consumer, data will accumulate in Kafka until the consumer
    can catch up. Kafka’s ability to scale by adding consumers or producers independently
    allows us to scale either side of the pipeline dynamically and independently to
    match the changing requirements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有了Kafka作为生产者和消费者之间的缓冲区，我们不再需要将消费者的吞吐量与生产者的吞吐量耦合在一起。我们也不再需要实现复杂的反压机制，因为如果生产者的吞吐量超过消费者的吞吐量，数据将在Kafka中积累，直到消费者赶上。Kafka通过独立添加消费者或生产者来扩展的能力，使我们能够动态地独立地扩展管道的任一侧，以满足不断变化的需求。
- en: Kafka is a high-throughput distributed system—capable of processing hundreds
    of megabytes per second on even modest clusters—so there is no concern that our
    pipeline will not scale as demand grows. In addition, the Kafka Connect API focuses
    on parallelizing the work and can do this on a single node as well as by scaling
    out, depending on system requirements. We’ll describe in the following sections
    how the platform allows data sources and sinks to split the work among multiple
    threads of execution and use the available CPU resources even when running on
    a single machine.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个高吞吐量的分布式系统，即使在较小的集群上，也能每秒处理数百兆字节的数据，因此我们不必担心我们的管道在需求增长时无法扩展。此外，Kafka
    Connect API专注于并行化工作，可以根据系统要求在单个节点上进行工作，也可以进行扩展。我们将在接下来的章节中描述平台如何允许数据源和目标在多个执行线程中分割工作，并在单台机器上运行时利用可用的CPU资源。
- en: Kafka also supports several types of compression, allowing users and admins
    to control the use of network and storage resources as the throughput requirements
    increase.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka还支持多种类型的压缩，允许用户和管理员在吞吐量要求增加时控制网络和存储资源的使用。
- en: Data Formats
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据格式
- en: One of the most important considerations in a data pipeline is reconciling different
    data formats and data types. The data types supported vary among different databases
    and other storage systems. You may be loading XMLs and relational data into Kafka,
    using Avro within Kafka, and then need to convert data to JSON when writing it
    to Elasticsearch, to Parquet when writing to HDFS, and to CSV when writing to
    S3.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道中最重要的考虑之一是协调不同的数据格式和数据类型。不同数据库和其他存储系统支持的数据类型各不相同。您可能会将XML和关系数据加载到Kafka中，在Kafka中使用Avro，然后在将数据写入Elasticsearch时需要将数据转换为JSON，在将数据写入HDFS时需要将数据转换为Parquet，在将数据写入S3时需要将数据转换为CSV。
- en: Kafka itself and the Connect API are completely agnostic when it comes to data
    formats. As we’ve seen in previous chapters, producers and consumers can use any
    serializer to represent data in any format that works for you. Kafka Connect has
    its own in-memory objects that include data types and schemas, but as we’ll soon
    discuss, it allows for pluggable converters to allow storing these records in
    any format. This means that no matter which data format you use for Kafka, it
    does not restrict your choice of connectors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka本身和Connect API在数据格式方面完全是不可知的。正如我们在前几章中看到的，生产者和消费者可以使用任何序列化器来表示适合您的任何格式的数据。Kafka
    Connect有自己的内存对象，包括数据类型和模式，但正如我们将很快讨论的那样，它允许可插拔的转换器来允许以任何格式存储这些记录。这意味着无论您使用Kafka的哪种数据格式，它都不会限制您选择的连接器。
- en: Many sources and sinks have a schema; we can read the schema from the source
    with the data, store it, and use it to validate compatibility or even update the
    schema in the sink database. A classic example is a data pipeline from MySQL to
    Snowflake. If someone added a column in MySQL, a great pipeline will make sure
    the column gets added to Snowflake too as we are loading new data into it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多来源和目标都有模式；我们可以从数据源中读取模式，存储它，并用它来验证兼容性，甚至在目标数据库中更新模式。一个经典的例子是从MySQL到Snowflake的数据管道。如果有人在MySQL中添加了一列，一个优秀的管道将确保该列也被添加到Snowflake中，因为我们正在向其中加载新数据。
- en: In addition, when writing data from Kafka to external systems, sink connectors
    are responsible for the format in which the data is written to the external system.
    Some connectors choose to make this format pluggable. For example, the S3 connector
    allows a choice between Avro and Parquet formats.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在将数据从Kafka写入外部系统时，接收连接器负责将数据写入外部系统的格式。一些连接器选择使此格式可插拔。例如，S3连接器允许在Avro和Parquet格式之间进行选择。
- en: It is not enough to support different types of data. A generic data integration
    framework should also handle differences in behavior between various sources and
    sinks. For example, Syslog is a source that pushes data, while relational databases
    require the framework to pull data out. HDFS is append-only and we can only write
    data to it, while most systems allow us to both append data and update existing
    records.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 支持不同类型的数据是不够的。通用数据集成框架还应处理各种来源和目标之间的行为差异。例如，Syslog是一个推送数据的来源，而关系数据库需要框架从中提取数据。HDFS是追加写入的，我们只能向其写入数据，而大多数系统允许我们追加数据和更新现有记录。
- en: Transformations
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换
- en: 'Transformations are more controversial than other requirements. There are generally
    two approaches to building data pipelines: ETL and ELT. ETL, which stands for
    *Extract-Transform-Load*, means that the data pipeline is responsible for making
    modifications to the data as it passes through. It has the perceived benefit of
    saving time and storage because you don’t need to store the data, modify it, and
    store it again. Depending on the transformations, this benefit is sometimes real,
    but sometimes it shifts the burden of computation and storage to the data pipeline
    itself, which may or may not be desirable. The main drawback of this approach
    is that the transformations that happen to the data in the pipeline may tie the
    hands of those who wish to process the data further down the pipe. If the person
    who built the pipeline between MongoDB and MySQL decided to filter certain events
    or remove fields from records, all the users and applications who access the data
    in MySQL will only have access to partial data. If they require access to the
    missing fields, the pipeline needs to be rebuilt, and historical data will require
    reprocessing (assuming it is available).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 转换比其他要求更有争议。通常有两种构建数据管道的方法：ETL和ELT。ETL代表*提取-转换-加载*，意味着数据管道负责在数据通过时对数据进行修改。它被认为可以节省时间和存储空间，因为您不需要存储数据，修改数据，然后再次存储数据。根据转换的不同，这种好处有时是真实的，但有时会将计算和存储的负担转移到数据管道本身，这可能是不可取的。这种方法的主要缺点是，在管道中对数据进行的转换可能会限制希望在管道下游进一步处理数据的人的手段。如果在MongoDB和MySQL之间构建管道的人决定过滤某些事件或从记录中删除字段，那么所有访问MySQL中数据的用户和应用程序只能访问部分数据。如果他们需要访问缺失的字段，就需要重建管道，并且历史数据需要重新处理（假设可用）。
- en: ELT stands for *Extract-Load-Transform* and means that the data pipeline does
    only minimal transformation (mostly around data type conversion), with the goal
    of making sure the data that arrives at the target is as similar as possible to
    the source data. In these systems, the target system collects “raw data” and all
    required processing is done at the target system. The benefit here is that the
    system provides maximum flexibility to users of the target system, since they
    have access to all the data. These systems also tend to be easier to troubleshoot
    since all data processing is limited to one system rather than split between the
    pipeline and additional applications. The drawback is that the transformations
    take CPU and storage resources at the target system. In some cases, these systems
    are expensive and there is strong motivation to move computation off those systems
    when possible.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ELT代表*提取-加载-转换*，意味着数据管道只进行最小的转换（主要是数据类型转换），目标是确保到达目标系统的数据尽可能与源数据相似。在这些系统中，目标系统收集“原始数据”，所有必需的处理都在目标系统中完成。这里的好处是系统为目标系统的用户提供了最大的灵活性，因为他们可以访问所有数据。这些系统也往往更容易进行故障排除，因为所有数据处理都限制在一个系统中，而不是在管道和其他应用程序之间分开。缺点是转换会在目标系统消耗CPU和存储资源。在某些情况下，这些系统是昂贵的，因此有强烈的动机尽可能将计算从这些系统中移出。
- en: Kafka Connect includes the Single Message Transformation feature, which transforms
    records while they are being copied from a source to Kafka, or from Kafka to a
    target. This includes routing messages to different topics, filtering messages,
    changing data types, redacting specific fields, and more. More complex transformations
    that involve joins and aggregations are typically done using Kafka Streams, and
    we will explore those in detail in a separate chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect包括单条消息转换功能，它在从源到Kafka或从Kafka到目标系统的复制过程中转换记录。这包括将消息路由到不同的主题，过滤消息，更改数据类型，删除特定字段等。通常使用Kafka
    Streams进行涉及连接和聚合的更复杂的转换，我们将在单独的章节中详细探讨这些内容。
- en: Warning
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When building an ETL system with Kafka, keep in mind that Kafka allows you to
    build one-to-many pipelines, where the source data is written to Kafka once and
    then consumed by multiple applications and written to multiple target systems.
    Some preprocessing and cleanup is expected, such as standardizing timestamps and
    data types, adding lineage, and perhaps removing personal information—transformations
    that will benefit all consumers of the data. But don’t prematurely clean and optimize
    the data on ingest because it might be needed less refined elsewhere.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Kafka构建ETL系统时，请记住，Kafka允许您构建一对多的管道，其中源数据只写入Kafka一次，然后被多个应用程序消费，并写入多个目标系统。预期会进行一些预处理和清理，例如标准化时间戳和数据类型，添加谱系，以及可能删除个人信息
    - 这些转换将使所有数据的消费者受益。但是不要在摄取时过早地清理和优化数据，因为它可能在其他地方需要不太精细的数据。
- en: Security
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全
- en: 'Security should always be a concern. In terms of data pipelines, the main security
    concerns are usually:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 安全始终应该是一个关注点。在数据管道方面，主要的安全问题通常是：
- en: Who has access to the data that is ingested into Kafka?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁可以访问被摄入到Kafka中的数据？
- en: Can we make sure the data going through the pipe is encrypted? This is mainly
    a concern for data pipelines that cross datacenter boundaries.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能确保通过管道传输的数据是加密的吗？这主要是对跨数据中心边界的数据管道的关注。
- en: Who is allowed to make modifications to the pipelines?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁有权对管道进行修改？
- en: If the data pipeline needs to read or write from access-controlled locations,
    can it authenticate properly?
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据管道需要从受访控制的位置读取或写入，它能够正确进行身份验证吗？
- en: Is our PII (Personally Identifiable Information) handling compliant with laws
    and regulations regarding its storage, access and use?
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们处理的个人可识别信息（PII）是否符合有关其存储、访问和使用的法律和法规？
- en: Kafka allows encrypting data on the wire, as it is piped from sources to Kafka
    and from Kafka to sinks. It also supports authentication (via SASL) and authorization—so
    you can be sure that if a topic contains sensitive information, it can’t be piped
    into less secured systems by someone unauthorized. Kafka also provides an audit
    log to track access—unauthorized and authorized. With some extra coding, it is
    also possible to track where the events in each topic came from and who modified
    them, so you can provide the entire lineage for each record.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka允许在数据传输过程中对数据进行加密，从数据源到Kafka，再从Kafka到目标系统。它还支持身份验证（通过SASL）和授权，因此您可以确保敏感信息不会被未经授权的人传输到不太安全的系统中。Kafka还提供审计日志以跟踪访问
    - 无权和有权的访问。通过一些额外的编码，还可以跟踪每个主题中事件的来源和修改者，以便为每条记录提供完整的谱系。
- en: Kafka security is discussed in detail in [Chapter 11](ch11.html#securing_kafka).
    However, Kafka Connect and its connectors need to be able to connect to, and authenticate
    with, external data systems, and configuration of connectors will include credentials
    for authenticating with external data systems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka安全性在[第11章](ch11.html#securing_kafka)中有详细讨论。但是，Kafka Connect及其连接器需要能够连接到外部数据系统，并对外部数据系统的连接进行身份验证，连接器的配置将包括用于与外部数据系统进行身份验证的凭据。
- en: These days it is not recommended to store credentials in configuration files,
    since this means that the configuration files have to be handled with extra care
    and have restricted access. A common solution is to use an external secret management
    system such as [HashiCorp Vault](https://www.vaultproject.io). Kafka Connect includes
    support for [external secret configuration](https://oreil.ly/5eVRU). Apache Kafka
    only includes the framework that allows introduction of pluggable external config
    providers, an example provider that reads configuration from a file, and there
    are [community-developed external config providers](https://oreil.ly/ovntG) that
    integrate with Vault, AWS, and Azure.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Failure Handling
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assuming that all data will be perfect all the time is dangerous. It is important
    to plan for failure handling in advance. Can we prevent faulty records from ever
    making it into the pipeline? Can we recover from records that cannot be parsed?
    Can bad records get fixed (perhaps by a human) and reprocessed? What if the bad
    event looks exactly like a normal event and you only discover the problem a few
    days later?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Because Kafka can be configured to store all events for long periods of time,
    it is possible to go back in time and recover from errors when needed. This also
    allows replaying the events stored in Kafka to the target system if they were
    lost.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Coupling and Agility
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A desirable characteristic of data pipeline implementation is to decouple the
    data sources and data targets. There are multiple ways accidental coupling can
    happen:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Ad hoc pipelines
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Some companies end up building a custom pipeline for each pair of applications
    they want to connect. For example, they use Logstash to dump logs to Elasticsearch,
    Flume to dump logs to HDFS, Oracle GoldenGate to get data from Oracle to HDFS,
    Informatica to get data from MySQL and XML to Oracle, and so on. This tightly
    couples the data pipeline to the specific end points and creates a mess of integration
    points that requires significant effort to deploy, maintain, and monitor. It also
    means that every new system the company adopts will require building additional
    pipelines, increasing the cost of adopting new technology, and inhibiting innovation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Loss of metadata
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: If the data pipeline doesn’t preserve schema metadata and does not allow for
    schema evolution, you end up tightly coupling the software producing the data
    at the source and the software that uses it at the destination. Without schema
    information, both software products need to include information on how to parse
    the data and interpret it. If data flows from Oracle to HDFS and a DBA added a
    new field in Oracle without preserving schema information and allowing schema
    evolution, either every app that reads data from HDFS will break or all the developers
    will need to upgrade their applications at the same time. Neither option is agile.
    With support for schema evolution in the pipeline, each team can modify their
    applications at their own pace without worrying that things will break down the
    line.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Extreme processing
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned when discussing data transformations, some processing of data
    is inherent to data pipelines. After all, we are moving data between different
    systems where different data formats make sense and different use cases are supported.
    However, too much processing ties all the downstream systems to decisions made
    when building the pipelines about which fields to preserve, how to aggregate data,
    etc. This often leads to constant changes to the pipeline as requirements of downstream
    applications change, which isn’t agile, efficient, or safe. The more agile way
    is to preserve as much of the raw data as possible and allow downstream apps,
    including Kafka Streams apps, to make their own decisions regarding data processing
    and aggregation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Kafka Connect Versus Producer and Consumer
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing to Kafka or reading from Kafka, you have the choice between using
    traditional producer and consumer clients, as described in Chapters [3](ch03.html#writing_messages_to_kafka)
    and [4](ch04.html#reading_data_from_kafka), or using the Kafka Connect API and
    the connectors, as we’ll describe in the following sections. Before we start diving
    into the details of Kafka Connect, you may already be wondering, “When do I use
    which?”
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入Kafka或从Kafka读取时，您可以选择使用传统的生产者和消费者客户端，如第3章和第4章中所述，或者使用Kafka Connect API和连接器，我们将在接下来的章节中描述。在我们开始深入了解Kafka
    Connect的细节之前，您可能已经在想，“我应该在什么时候使用哪个呢？”
- en: As we’ve seen, Kafka clients are clients embedded in your own application. It
    allows your application to write data to Kafka or to read data from Kafka. Use
    Kafka clients when you can modify the code of the application that you want to
    connect an application to and when you want to either push data into Kafka or
    pull data from Kafka.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Kafka客户端是嵌入在您自己的应用程序中的客户端。它允许您的应用程序将数据写入Kafka或从Kafka读取数据。当您可以修改要将应用程序连接到的应用程序的代码，并且希望将数据推送到Kafka或从Kafka拉取数据时，请使用Kafka客户端。
- en: You will use Connect to connect Kafka to datastores that you did not write and
    whose code or APIs you cannot or will not modify. Connect will be used to pull
    data from the external datastore into Kafka or push data from Kafka to an external
    store. To use Kafka Connect, you need a connector for the datastore to which you
    want to connect, and nowadays these connectors are plentiful. This means that
    in practice, users of Kafka Connect only need to write configuration files.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用Connect将Kafka连接到您没有编写并且无法或不会修改其代码或API的数据存储。Connect将用于将数据从外部数据存储拉取到Kafka，或者将数据从Kafka推送到外部存储。要使用Kafka
    Connect，您需要连接到要连接的数据存储的连接器，如今这些连接器已经很丰富。这意味着在实践中，Kafka Connect的用户只需要编写配置文件。
- en: If you need to connect Kafka to a datastore and a connector does not exist yet,
    you can choose between writing an app using the Kafka clients or the Connect API.
    Connect is recommended because it provides out-of-the-box features like configuration
    management, offset storage, parallelization, error handling, support for different
    data types, and standard management REST APIs. Writing a small app that connects
    Kafka to a datastore sounds simple, but there are many little details you will
    need to handle concerning data types and configuration that make the task nontrivial.
    What’s more, you will need to maintain this pipeline app and document it, and
    your teammates will need to learn how to use it. Kafka Connect is a standard part
    of the Kafka ecosystem, and it handles most of this for you, allowing you to focus
    on transporting data to and from the external stores.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要将Kafka连接到数据存储，并且尚不存在连接器，您可以选择使用Kafka客户端或Connect API编写应用程序。推荐使用Connect，因为它提供了诸如配置管理、偏移存储、并行化、错误处理、支持不同数据类型和标准管理REST
    API等开箱即用的功能。编写一个将Kafka连接到数据存储的小应用听起来很简单，但实际上需要处理许多关于数据类型和配置的细节，使任务变得复杂。此外，您还需要维护这个管道应用并对其进行文档化，您的团队成员需要学习如何使用它。Kafka
    Connect是Kafka生态系统的标准部分，它为您处理了大部分工作，使您能够专注于在外部存储之间传输数据。
- en: Kafka Connect
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect
- en: Kafka Connect is a part of Apache Kafka and provides a scalable and reliable
    way to copy data between Kafka and other datastores. It provides APIs and a runtime
    to develop and run *connector plug-ins*—libraries that Kafka Connect executes
    and that are responsible for moving the data. Kafka Connect runs as a cluster
    of *worker processes*. You install the connector plug-ins on the workers and then
    use a REST API to configure and manage *connectors*, which run with a specific
    configuration. *Connectors* start additional *tasks* to move large amounts of
    data in parallel and use the available resources on the worker nodes more efficiently.
    Source connector tasks just need to read data from the source system and provide
    Connect data objects to the worker processes. Sink connector tasks get connector
    data objects from the workers and are responsible for writing them to the target
    data system. Kafka Connect uses *convertors* to support storing those data objects
    in Kafka in different formats—JSON format support is part of Apache Kafka, and
    the Confluent Schema Registry provides Avro, Protobuf, and JSON Schema converters.
    This allows users to choose the format in which data is stored in Kafka independent
    of the connectors they use, as well as how the schema of the data is handled (if
    at all).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect是Apache Kafka的一部分，提供了一种可扩展和可靠的方式在Kafka和其他数据存储之间复制数据。它提供了API和运行时来开发和运行*连接器插件*——Kafka
    Connect执行的库，负责移动数据。Kafka Connect作为一组*worker进程*运行。您可以在工作节点上安装连接器插件，然后使用REST API配置和管理*连接器*，这些连接器使用特定配置运行。*连接器*启动额外的*任务*以并行移动大量数据，并更有效地利用工作节点上的可用资源。源连接器任务只需要从源系统读取数据并向工作进程提供Connect数据对象。汇连接器任务从工作进程获取连接器数据对象，并负责将其写入目标数据系统。Kafka
    Connect使用*转换器*来支持以不同格式存储这些数据对象——JSON格式支持是Apache Kafka的一部分，Confluent Schema Registry提供了Avro、Protobuf和JSON
    Schema转换器。这使用户可以选择在Kafka中存储数据的格式，而不受其使用的连接器的影响，以及如何处理数据的模式（如果有）。
- en: This chapter cannot possibly get into all the details of Kafka Connect and its
    many connectors. This could fill an entire book on its own. We will, however,
    give an overview of Kafka Connect and how to use it, and point to additional resources
    for reference.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本章无法涵盖Kafka Connect及其众多连接器的所有细节。这本身就可以填满一整本书。然而，我们将概述Kafka Connect及其使用方法，并指向其他参考资源。
- en: Running Kafka Connect
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行Kafka Connect
- en: Kafka Connect ships with Apache Kafka, so there is no need to install it separately.
    For production use, especially if you are planning to use Connect to move large
    amounts of data or run many connectors, you should run Connect on separate servers
    from your Kafka brokers. In this case, install Apache Kafka on all the machines,
    and simply start the brokers on some servers and start Connect on other servers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect随Apache Kafka一起提供，因此无需单独安装。对于生产使用，特别是如果您计划使用Connect来移动大量数据或运行许多连接器，您应该将Connect运行在与Kafka代理不同的服务器上。在这种情况下，在所有机器上安装Apache
    Kafka，并在一些服务器上启动代理，然后在其他服务器上启动Connect。
- en: 'Starting a Connect worker is very similar to starting a broker—you call the
    start script with a properties file:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Connect worker与启动代理非常相似-您使用属性文件调用启动脚本：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are a few key configurations for Connect workers:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Connect worker的一些关键配置：
- en: '`bootstrap.servers`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap.servers`'
- en: A list of Kafka brokers that Connect will work with. `Connectors` will pipe
    their data either to or from those brokers. You don’t need to specify every broker
    in the cluster, but it’s recommended to specify at least three.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect将与之一起工作的Kafka代理的列表。`连接器`将把它们的数据传输到这些代理中的一个或多个。您不需要指定集群中的每个代理，但建议至少指定三个。
- en: '`group.id`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`group.id`'
- en: All workers with the same group ID are part of the same Connect cluster. A connector
    started on the cluster will run on any worker, and so will its tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 具有相同组ID的所有worker都属于同一个Connect集群。在集群上启动的连接器将在任何worker上运行，其任务也将在任何worker上运行。
- en: '`plugin.path`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`plugin.path`'
- en: Kafka Connect uses a pluggable architecture where connectors, converters, transformations,
    and secret providers can be downloaded and added to the platform. In order to
    do this, Kafka Connect has to be able to find and load those plug-ins.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect使用可插拔架构，其中连接器、转换器、转换和秘密提供者可以被下载并添加到平台上。为了做到这一点，Kafka Connect必须能够找到并加载这些插件。
- en: We can configure one or more directories as locations where connectors and their
    dependencies can be found. For example, we can configure `plugin.path=/opt/connectors,/home/gwenshap/connectors`.
    Inside one of these directories, we will typically create a subdirectory for each
    connector, so in the previous example, we’ll create `/opt/connectors/jdbc` and
    `/opt/​con⁠nec⁠tors/elastic`. Inside each subdirectory, we’ll place the connector
    jar itself and all its dependencies. If the connector ships as an `uberJar` and
    has no dependencies, it can be placed directly in `plugin.path` and doesn’t require
    a subdirectory. But note that placing dependencies in the top-level path will
    not work.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以配置一个或多个目录作为连接器及其依赖项的位置。例如，我们可以配置`plugin.path=/opt/connectors,/home/gwenshap/connectors`。在这些目录中的一个中，我们通常会为每个连接器创建一个子目录，因此在前面的示例中，我们将创建`/opt/connectors/jdbc`和`/opt/​con⁠nec⁠tors/elastic`。在每个子目录中，我们将放置连接器jar本身及其所有依赖项。如果连接器作为`uberJar`发货并且没有依赖项，它可以直接放置在`plugin.path`中，不需要子目录。但请注意，将依赖项放在顶级路径将不起作用。
- en: An alternative is to add the connectors and all their dependencies to the Kafka
    Connect classpath, but this is not recommended and can introduce errors if you
    use a connector that brings a dependency that conflicts with one of Kafka’s dependencies.
    The recommended approach is to use `plugin.path` configuration.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将连接器及其所有依赖项添加到Kafka Connect类路径中，但这并不推荐，如果您使用一个带有与Kafka的依赖项冲突的依赖项的连接器，可能会引入错误。推荐的方法是使用`plugin.path`配置。
- en: '`key.converter` and `value.converter`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`key.converter`和`value.converter`'
- en: Connect can handle multiple data formats stored in Kafka. The two configurations
    set the converter for the key and value part of the message that will be stored
    in Kafka. The default is JSON format using the `JSONConverter` included in Apache
    Kafka. These configurations can also be set to `AvroConverter`, `ProtobufConverter`,
    or `JscoSchemaConverter`, which are part of the Confluent Schema Registry.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Connect可以处理存储在Kafka中的多种数据格式。这两个配置设置了将存储在Kafka中的消息的键和值部分的转换器。默认值是使用Apache Kafka中包含的`JSONConverter`的JSON格式。这些配置也可以设置为`AvroConverter`、`ProtobufConverter`或`JscoSchemaConverter`，这些都是Confluent
    Schema Registry的一部分。
- en: Some converters include converter-specific configuration parameters. You need
    to prefix these parameters with `key.converter.` or `value.converter.`, depending
    on whether you want to apply them to the key or value converter. For example,
    JSON messages can include a schema or be schema-less. To support either, you can
    set `key.converter.schemas.enable=true` or `false`, respectively. The same configuration
    can be used for the value converter by setting `value.converter.schemas.enable`
    to `true` or `false`. Avro messages also contain a schema, but you need to configure
    the location of the Schema Registry using `key.converter.schema.registry.url`
    and `value.converter.schema.​regis⁠try.url`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些转换器包括特定于转换器的配置参数。您需要使用`key.converter.`或`value.converter.`作为前缀来设置这些参数，具体取决于您是要将其应用于键还是值转换器。例如，JSON消息可以包含模式或无模式。为了支持任一种情况，您可以分别设置`key.converter.schemas.enable=true`或`false`。相同的配置也可以用于值转换器，通过将`value.converter.schemas.enable`设置为`true`或`false`。Avro消息也包含模式，但您需要使用`key.converter.schema.registry.url`和`value.converter.schema.​regis⁠try.url`来配置模式注册表的位置。
- en: '`rest.host.name` and `rest.port`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`rest.host.name`和`rest.port`'
- en: Connectors are typically configured and monitored through the REST API of Kafka
    Connect. You can configure the specific port for the REST API.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 连接器通常通过Kafka Connect的REST API进行配置和监视。您可以为REST API配置特定的端口。
- en: 'Once the workers are up and you have a cluster, make sure it is up and running
    by checking the REST API:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦工作人员上岗并且您有一个集群，请通过检查REST API确保其正常运行：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Accessing the base REST URI should return the current version you are running.
    We are running a snapshot of Kafka 3.0.0 (prerelease). We can also check which
    connector plug-ins are available:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 访问基本REST URI应返回您正在运行的当前版本。我们正在运行Kafka 3.0.0的快照（预发布）。我们还可以检查可用的连接器插件：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are running plain Apache Kafka, so the only available connector plug-ins
    are the file source, file sink, and the connectors that are part of MirrorMaker
    2.0.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在运行纯粹的Apache Kafka，因此唯一可用的连接器插件是文件源、文件接收器，以及MirrorMaker 2.0的一部分连接器。
- en: Let’s see how to configure and use these example connectors, and then we’ll
    dive into more advanced examples that require setting up external data systems
    to connect to.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何配置和使用这些示例连接器，然后我们将深入更高级的示例，这些示例需要设置外部数据系统来连接。
- en: Standalone Mode
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立模式
- en: Take note that Kafka Connect also has a standalone mode. It is similar to distributed
    mode—you just run `bin/connect-standalone.sh` instead of `bin/connect-distributed.sh`.
    You can also pass in a connector configuration file on the command line instead
    of through the REST API. In this mode, all the connectors and tasks run on the
    one standalone worker. It is used in cases where connectors and tasks need to
    run on a specific machine (e.g., the `syslog` connector listens on a port, so
    you need to know which machines it is running on).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Kafka Connect还有一个独立模式。它类似于分布式模式——你只需要运行`bin/connect-standalone.sh`而不是`bin/connect-distributed.sh`。您还可以通过命令行传递连接器配置文件，而不是通过REST
    API。在这种模式下，所有的连接器和任务都在一个独立的工作节点上运行。它用于需要在特定机器上运行连接器和任务的情况（例如，`syslog`连接器监听一个端口，所以你需要知道它在哪些机器上运行）。
- en: 'Connector Example: File Source and File Sink'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接器示例：文件源和文件接收器
- en: This example will use the file connectors and JSON converter that are part of
    Apache Kafka. To follow along, make sure you have ZooKeeper and Kafka up and running.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将使用Apache Kafka的文件连接器和JSON转换器。要跟着做，请确保您的ZooKeeper和Kafka已经运行起来。
- en: 'To start, let’s run a distributed Connect worker. In a real production environment,
    you’ll want at least two or three of these running to provide high availability.
    In this example, we’ll only start one:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们运行一个分布式的Connect工作节点。在真实的生产环境中，您至少需要运行两到三个这样的节点，以提供高可用性。在这个例子中，我们只会启动一个：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now it’s time to start a file source. As an example, we will configure it to
    read the Kafka configuration file—basically piping Kafka’s configuration into
    a Kafka topic:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候启动一个文件源了。举个例子，我们将配置它来读取Kafka配置文件——基本上是将Kafka的配置导入到一个Kafka主题中：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To create a connector, we wrote a JSON that includes a connector name, `load-kafka-config`,
    and a connector configuration map, which includes the connector class, the file
    we want to load, and the topic we want to load the file into.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个连接器，我们编写了一个JSON，其中包括一个连接器名称`load-kafka-config`，以及一个连接器配置映射，其中包括连接器类、我们要加载的文件和我们要将文件加载到的主题。
- en: 'Let’s use the Kafka Console consumer to check that we have loaded the configuration
    into a topic:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Kafka控制台消费者来检查我们是否已经将配置加载到一个主题中：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If all went well, you should see something along the lines of:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会看到类似以下的内容：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is literally the contents of the *config/server.properties* file, as it
    was converted to JSON line by line and placed in `kafka-config-topic` by our connector.
    Note that by default, the JSON converter places a schema in each record. In this
    specific case, the schema is very simple—there is only a single column, named
    `payload` of type `string`, and it contains a single line from the file for each
    record.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是*config/server.properties*文件的内容，因为它被逐行转换为JSON并放置在`kafka-config-topic`中。请注意，默认情况下，JSON转换器在每条记录中放置一个模式。在这种特定情况下，模式非常简单——只有一个名为`payload`的列，类型为`string`，每条记录中包含一个文件的单行。
- en: 'Now let’s use the file sink converter to dump the contents of that topic into
    a file. The resulting file should be completely identical to the original *server.properties*
    file, as the JSON converter will convert the JSON records back into simple text
    lines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用文件接收器转换器将该主题的内容转储到一个文件中。生成的文件应该与原始的*server.properties*文件完全相同，因为JSON转换器将JSON记录转换回简单的文本行：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note the changes from the source configuration: the class we are using is now
    `FileStreamSink` rather than `FileStreamSource`. We still have a file property,
    but now it refers to the destination file rather than the source of the records,
    and instead of specifying a *topic*, you specify *topics*. Note the plurality—you
    can write multiple topics into one file with the sink, while the source only allows
    writing into one topic.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意源配置的变化：我们现在使用的类是`FileStreamSink`而不是`FileStreamSource`。我们仍然有一个文件属性，但现在它指的是目标文件而不是记录的源，而且不再指定*topic*，而是指定*topics*。注意复数形式——你可以用sink将多个主题写入一个文件，而源只允许写入一个主题。
- en: If all went well, you should have a file named *copy-of-server-properties*,
    which is completely identical to the *config/server.properties* we used to populate
    `kafka-config-topic`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该有一个名为*copy-of-server-properties*的文件，它与我们用来填充`kafka-config-topic`的*config/server.properties*完全相同。
- en: 'To delete a connector, you can run:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除一个连接器，您可以运行：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Warning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'This example uses FileStream connectors because they are simple and built into
    Kafka, allowing you to create your first pipeline without installing anything
    except Kafka. These should not be used for actual production pipelines, as they
    have many limitations and no reliability guarantees. There are several alternatives
    you can use if you want to ingest data from files: [FilePulse Connector](https://oreil.ly/VLCf2),
    [FileSystem Connector](https://oreil.ly/Fcryw), or [SpoolDir](https://oreil.ly/qgsI4).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用了FileStream连接器，因为它们简单且内置于Kafka中，允许您在安装Kafka之外创建您的第一个管道。这些不应该用于实际的生产管道，因为它们有许多限制和没有可靠性保证。如果您想从文件中摄取数据，可以使用几种替代方案：[FilePulse
    Connector](https://oreil.ly/VLCf2), [FileSystem Connector](https://oreil.ly/Fcryw),
    或 [SpoolDir](https://oreil.ly/qgsI4)。
- en: 'Connector Example: MySQL to Elasticsearch'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接器示例：MySQL到Elasticsearch
- en: Now that we have a simple example working, let’s do something more useful. Let’s
    take a MySQL table, stream it to a Kafka topic, and from there load it to Elasticsearch
    and index its content.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个简单的例子正在运行，让我们做一些更有用的事情。让我们将一个MySQL表流式传输到一个Kafka主题，然后从那里将其加载到Elasticsearch并索引其内容。
- en: 'We are running tests on a MacBook. To install MySQL and Elasticsearch, simply
    run:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next step is to make sure you have the connectors. There are a few options:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Download and install using [Confluent Hub client](https://oreil.ly/c7S5z).
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download from the [Confluent Hub](https://www.confluent.io/hub) website (or
    from any other website where the connector you are interested in is hosted).
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build from source code. To do this, you’ll need to:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the connector source:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Run `mvn install -DskipTests` to build the project.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat with [the JDBC connector](https://oreil.ly/yXg0S).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we need to load these connectors. Create a directory, such as */opt/connectors*
    and update *config/connect-distributed.properties* to include `plugin.path=/opt/​con⁠nec⁠tors`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Then take the jars that were created under the `target` directory where you
    built each connector and copy each one, plus their dependencies, to the appropriate
    subdirectories of `plugin.path`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In addition, since we need to connect not just to any database but specifically
    to MySQL, you’ll need to download and install a MySQL JDBC driver. The driver
    doesn’t ship with the connector for license reasons. You can download the driver
    from the [MySQL website](https://oreil.ly/KZCPw) and then place the jar in */opt/connectors/jdbc*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Restart the Kafka Connect workers and check that the new connector plug-ins
    are listed:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see that we now have additional connector plug-ins available in our `Connect`
    cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create a table in MySQL that we can stream into Kafka using
    our JDBC connector:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, we created a database and a table, and inserted a few rows as
    an example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to configure our JDBC source connector. We can find out which
    configuration options are available by looking at the documentation, but we can
    also use the REST API to find the available configuration options:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We asked the REST API to validate configuration for a connector and sent it
    a configuration with just the class name (this is the bare minimum configuration
    necessary). As a response, we got the JSON definition of all available configurations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'With this information in mind, it’s time to create and configure our JDBC connector:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s make sure it worked by reading data from the `mysql.login` topic:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you get errors saying the topic doesn’t exist or you see no data, check
    the Connect worker logs for errors such as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Other issues can involve the existence of the driver in the classpath or permissions
    to read the table.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Once the connector is running, if you insert additional rows in the `login`
    table, you should immediately see them reflected in the `mysql.login` topic.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Change Data Capture and Debezium Project
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The JDBC connector that we are using uses JDBC and SQL to scan database tables
    for new records. It detects new records by using timestamp fields or an incrementing
    primary key. This is a relatively inefficient and at times inaccurate process.
    All relational databases have a transaction log (also called redo log, binlog,
    or write-ahead log) as part of their implementation, and many allow external systems
    to read data directly from their transaction log—a far more accurate and efficient
    process known as `change data` `capture`. Most modern ETL systems depend on change
    data capture as a data source. The [Debezium Project](https://debezium.io) provides
    a collection of high-quality, open source, change capture connectors for a variety
    of databases. If you are planning on streaming data from a relational database
    to Kafka, we highly recommend using a Debezium change capture connector if one
    exists for your database. In addition, the Debezium documentation is one of the
    best we’ve seen—in addition to documenting the connectors themselves, it covers
    useful design patterns and use cases related to change data capture, especially
    in the context of microservices.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Getting MySQL data to Kafka is useful in itself, but let’s make things more
    fun by writing the data to Elasticsearch.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start Elasticsearch and verify it is up by accessing its local port:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now create and start the connector:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There are a few configurations we need to explain here. The `connection.url`
    is simply the URL of the local Elasticsearch server we configured earlier. Each
    topic in Kafka will become, by default, a separate Elasticsearch index, with the
    same name as the topic. The only topic we are writing to Elasticsearch is `mysql.login`.
    The JDBC connector does not populate the message key. As a result, the events
    in Kafka have null keys. Because the events in Kafka lack keys, we need to tell
    the Elasticsearch connector to use the topic name, partition ID, and offset as
    the key for each event. This is done by setting `key.ignore` configuration to
    `true`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check that the index with `mysql.login` data was created:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If the index isn’t there, look for errors in the Connect worker log. Missing
    configurations or libraries are common causes for errors. If all is well, we can
    search the index for our records:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you add new records to the table in MySQL, they will automatically appear
    in the `mysql.login` topic in Kafka and in the corresponding Elasticsearch index.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how to build and install the JDBC source and Elasticsearch
    sink, we can build and use any pair of connectors that suits our use case. Confluent
    maintains a set of their own prebuilt connectors, as well as some from across
    the community and other vendors, at [Confluent Hub](https://www.confluent.io/hub).
    You can pick any connector on the list that you wish to try out, download it,
    configure it—either based on the documentation or by pulling the configuration
    from the REST API—and run it on your Connect worker cluster.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Build Your Own Connectors
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Connector API is public and anyone can create a new connector. So if the
    datastore you wish to integrate with does not have an existing connector, we encourage
    you to write your own. You can then contribute it to Confluent Hub so others can
    discover and use it. It is beyond the scope of this chapter to discuss all the
    details involved in building a connector, but there are multiple blog posts that
    [explain how to do so](https://oreil.ly/WUqlZ), and good talks from [Kafka Summit
    NY 2019](https://oreil.ly/rV9RH), [Kafka Summit London 2018](https://oreil.ly/Jz7XV),
    and [ApacheCon](https://oreil.ly/8QsOL). We also recommend looking at the existing
    connectors as a starting point and perhaps jump-starting using an [Apache Maven
    archtype](http://bit.ly/2sc9E9q). We always encourage you to ask for assistance
    or show off your latest connectors on the Apache Kafka community mailing list
    ([users@kafka.apache.org](mailto:users@kafka.apache.org)) or submit them to Confluent
    Hub so they can be easily found.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Single Message Transformations
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Copying records from MySQL to Kafka and from there to Elastic is rather useful
    on its own, but ETL pipelines typically involve a transformation step. In the
    Kafka ecosystem we separate transformations to single message transformations
    (SMTs), which are stateless, and stream processing, which can be stateful. SMTs
    can be done within Kafka Connect transforming messages while they are being copied,
    often without writing any code. More complex transformations, which typically
    involve joins or aggregation, will require the stateful Kafka Streams framework.
    We’ll discuss Kafka Streams in a later chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Kafka includes the following SMTs:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Cast
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Change data type of a field.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: MaskField
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Replace the contents of a field with null. This is useful for removing sensitive
    or personally identifying data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Filter
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Drop or include all messages that match a specific condition. Built-in conditions
    include matching on a topic name, a particular header, or whether the message
    is a tombstone (that is, has a null value).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Flatten
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Transform a nested data structure to a flat one. This is done by concatenating
    all the names of all fields in the path to a specific value.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: HeaderFrom
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Move or copy fields from the message into the header.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将消息中的字段移动或复制到标头中。
- en: InsertHeader
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 插入标头
- en: Add a static string to the header of each message.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 向每条消息的标头添加静态字符串。
- en: InsertField
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 插入字段
- en: Add a new field to a message, either using values from its metadata such as
    offset, or with a static value.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 向消息添加一个新字段，可以使用其偏移等元数据的值，也可以使用静态值。
- en: RegexRouter
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 正则路由器
- en: Change the destination topic using a regular expression and a replacement string.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式和替换字符串更改目标主题。
- en: ReplaceField
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 替换字段
- en: Remove or rename a field in the message.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 删除或重命名消息中的字段。
- en: TimestampConverter
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳转换器
- en: Modify the time format of a field—for example, from Unix Epoch to a String.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 修改字段的时间格式 - 例如，从Unix Epoch到字符串。
- en: TimestampRouter
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳路由器
- en: Modify the topic based on the message timestamp. This is mostly useful in sink
    connectors when we want to copy messages to specific table partitions based on
    their timestamp and the topic field is used to find an equivalent dataset in the
    destination system.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据消息时间戳修改主题。这在接收连接器中非常有用，当我们想要根据时间戳将消息复制到特定的表分区时，主题字段用于在目标系统中找到等效的数据集。
- en: In addition, transformations are available from contributors outside the main
    Apache Kafka code base. Those can be found on GitHub ([Lenses.io](https://oreil.ly/fWAyh),
    [Aiven](https://oreil.ly/oQRG5), and [Jeremy Custenborder](https://oreil.ly/OdPHW)
    have useful collections) or on [Confluent Hub](https://oreil.ly/Up8dM).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，转换是从主要Apache Kafka代码库之外的贡献者那里获得的。这些可以在GitHub（[Lenses.io](https://oreil.ly/fWAyh)、[Aiven](https://oreil.ly/oQRG5)和[Jeremy
    Custenborder](https://oreil.ly/OdPHW)有有用的集合）或[Confluent Hub](https://oreil.ly/Up8dM)上找到。
- en: To learn more about Kafka Connect SMTs, you can read detailed examples of many
    transformations in the [“Twelve Days of SMT”](https://oreil.ly/QnpQV) blog series.
    In addition, you can learn how to write your own transformations by following
    a [tutorial and deep dive](https://oreil.ly/rw4CU).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Kafka Connect SMT的信息，您可以阅读[“SMT十二天”](https://oreil.ly/QnpQV)博客系列中的详细示例。此外，您还可以通过[教程和深入探讨](https://oreil.ly/rw4CU)来学习如何编写自己的转换。
- en: As an example, let’s say that we want to add a [record header](https://oreil.ly/ISiWs)
    to each record produced by the MySQL connector we created previously. The header
    will indicate that the record was created by this MySQL connector, which is useful
    in case auditors want to examine the lineage of these records.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要为之前创建的MySQL连接器生成的每条记录添加[记录标头](https://oreil.ly/ISiWs)。标头将指示该记录是由此MySQL连接器创建的，这在审计人员想要检查这些记录的渊源时非常有用。
- en: 'To do this, we’ll replace the previous MySQL connector configuration with the
    following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将用以下内容替换以前的MySQL连接器配置：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, if you insert a few more records into the MySQL table that we created
    in the previous example, you’ll be able to see that the new messages in the `mysql.login`
    topic have headers (note that you’ll need Apache Kafka 2.7 or higher to print
    headers in the console consumer):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您向我们在上一个示例中创建的MySQL表中插入几条记录，您将能够看到`mysql.login`主题中的新消息具有标头（请注意，您需要Apache
    Kafka 2.7或更高版本才能在控制台消费者中打印标头）：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, the old records show `NO_HEADERS`, but the new records show
    `MessageSource:mysql-login-connector`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，旧记录显示“NO_HEADERS”，而新记录显示“MessageSource:mysql-login-connector”。
- en: Error Handling and Dead Letter Queues
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误处理和死信队列
- en: Transforms is an example of a connector config that isn’t specific to one connector
    but can be used in the configuration of any connector. Another very useful connector
    configuration that can be used in any sink connector is `error.tolerance`—you
    can configure any connector to silently drop corrupt messages, or to route them
    to a special topic called a “dead letter queue.” You can find more details in
    the [“Kafka Connect Deep Dive—Error Handling and Dead Letter Queues” blog post](https://oreil.ly/935hH).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是一个连接器配置的示例，它不特定于一个连接器，但可以在任何连接器的配置中使用。另一个非常有用的连接器配置是`error.tolerance` - 您可以配置任何连接器静默丢弃损坏的消息，或将它们路由到一个名为“死信队列”的特殊主题。您可以在[“Kafka
    Connect深入探讨-错误处理和死信队列”博客文章](https://oreil.ly/935hH)中找到更多详细信息。
- en: A Deeper Look at Kafka Connect
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解Kafka Connect
- en: To understand how Kafka Connect works, you need to understand three basic concepts
    and how they interact. As we explained earlier and demonstrated with examples,
    to use Kafka Connect, you need to run a cluster of workers and create/remove connectors.
    An additional detail we did not dive into before is the handling of data by converters—these
    are the components that convert MySQL rows to JSON records, which the connector
    wrote into Kafka.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解Kafka Connect的工作原理，您需要了解三个基本概念以及它们之间的相互作用。正如我们之前解释过并通过示例演示的那样，要使用Kafka Connect，您需要运行一个工作节点集群并创建/删除连接器。我们之前没有深入探讨的一个额外细节是转换器处理数据的方式
    - 这些是将MySQL行转换为JSON记录的组件，连接器将其写入Kafka。
- en: Let’s look a bit deeper into each system and how they interact with one another.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解每个系统以及它们之间的相互作用。
- en: Connectors and tasks
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接器和任务
- en: 'Connector plug-ins implement the Connector API, which includes two parts:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 连接器插件实现了连接器API，其中包括两个部分：
- en: Connectors
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 连接器
- en: 'The connector is responsible for three important things:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 连接器负责三件重要的事情：
- en: Determining how many tasks will run for the connector
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定连接器将运行多少任务
- en: Deciding how to split the data-copying work between the tasks
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定如何在任务之间分配数据复制工作
- en: Getting configurations for the tasks from the workers and passing them along
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从工作节点获取任务的配置并将其传递
- en: For example, the JDBC source connector will connect to the database, discover
    the existing tables to copy, and based on that decide how many tasks are needed—choosing
    the lower of `tasks.max` configuration and the number of tables. Once it decides
    how many tasks will run, it will generate a configuration for each task—using
    both the connector configuration (e.g., `connection.url`) and a list of tables
    it assigns for each task to copy. The `taskConfigs()` method returns a list of
    maps (i.e., a configuration for each task we want to run). The workers are then
    responsible for starting the tasks and giving each one its own unique configuration
    so that it will copy a unique subset of tables from the database. Note that when
    you start the connector via the REST API, it may start on any node, and subsequently
    the tasks it starts may also execute on any node.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Tasks
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Tasks are responsible for actually getting the data in and out of Kafka. All
    tasks are initialized by receiving a context from the worker. Source context includes
    an object that allows the source task to store the offsets of source records (e.g.,
    in the file connector, the offsets are positions in the file; in the JDBC source
    connector, the offsets can be a timestamp column in a table). Context for the
    sink connector includes methods that allow the connector to control the records
    it receives from Kafka—this is used for things like applying back pressure and
    retrying and storing offsets externally for exactly-once delivery. After tasks
    are initialized, they are started with a `Properties` object that contains the
    configuration the `Connector` created for the task. Once tasks are started, source
    tasks poll an external system and return lists of records that the worker sends
    to Kafka brokers. Sink tasks receive records from Kafka through the worker and
    are responsible for writing the records to an external system.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Workers
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kafka Connect’s worker processes are the “container” processes that execute
    the connectors and tasks. They are responsible for handling the HTTP requests
    that define connectors and their configuration, as well as for storing the connector
    configuration in an internal Kafka topic, starting the connectors and their tasks,
    and passing the appropriate configurations along. If a worker process is stopped
    or crashes, other workers in a Connect cluster will recognize that (using the
    heartbeats in Kafka’s consumer protocol) and reassign the connectors and tasks
    that ran on that worker to the remaining workers. If a new worker joins a Connect
    cluster, other workers will notice that and assign connectors or tasks to it to
    make sure load is balanced among all workers fairly. Workers are also responsible
    for automatically committing offsets for both source and sink connectors into
    internal Kafka topics and for handling retries when tasks throw errors.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand workers is to realize that connectors and tasks are
    responsible for the “moving data” part of data integration, while the workers
    are responsible for the REST API, configuration management, reliability, high
    availability, scaling, and load balancing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: This separation of concerns is the main benefit of using the Connect API versus
    the classic consumer/producer APIs. Experienced developers know that writing code
    that reads data from Kafka and inserts it into a database takes maybe a day or
    two, but if you need to handle configuration, errors, REST APIs, monitoring, deployment,
    scaling up and down, and handling failures, it can take a few months to get everything
    right. And most data integration pipelines involve more than just the one source
    or target. So now consider that effort spent on bespoke code for just a database
    integration, repeated many times for other technologies. If you implement data
    copying with a connector, your connector plugs into workers that handle a bunch
    of complicated operational issues that you don’t need to worry about.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Converters and Connect’s data model
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last piece of the Connect API puzzle is the connector data model and the
    converters. Kafka’s Connect API includes a data API, which includes both data
    objects and a schema that describes that data. For example, the JDBC source reads
    a column from a database and constructs a `Connect Schema` object based on the
    data types of the columns returned by the database. It then uses the schema to
    construct a `Struct` that contains all the fields in the database record. For
    each column, we store the column name and the value in that column. Every source
    connector does something similar—read an event from the source system and generate
    a `Schema` and `Value` pair. Sink connectors do the opposite—get a `Schema` and
    `Value` pair and use the `Schema` to parse the values and insert them into the
    target system.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Though source connectors know how to generate objects based on the Data API,
    there is still a question of how `Connect` workers store these objects in Kafka.
    This is where the converters come in. When users configure the worker (or the
    connector), they choose which converter they want to use to store data in Kafka.
    At the moment, the available choices are primitive types, byte arrays, strings,
    Avro, JSON, JSON schemas, or Protobufs. The JSON converter can be configured to
    either include a schema in the result record or not include one—so we can support
    both structured and semistructured data. When the connector returns a Data API
    record to the worker, the worker then uses the configured converter to convert
    the record to an Avro object, a JSON object, or a string, and the result is then
    stored into Kafka.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The opposite process happens for sink connectors. When the Connect worker reads
    a record from Kafka, it uses the configured converter to convert the record from
    the format in Kafka (i.e., primitive types, byte arrays, strings, Avro, JSON,
    JSON schema, or Protobufs) to the Connect Data API record and then passes it to
    the sink connector, which inserts it into the destination system.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: This allows the Connect API to support different types of data stored in Kafka,
    independent of the connector implementation (i.e., any connector can be used with
    any record type, as long as a converter is available).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Offset management
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Offset management is one of the convenient services the workers perform for
    the connectors (in addition to deployment and configuration management via the
    REST API). The idea is that connectors need to know which data they have already
    processed, and they can use APIs provided by Kafka to maintain information on
    which events were already processed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: For source connectors, this means that the records the connector returns to
    the Connect workers include a logical partition and a logical offset. Those are
    not Kafka partitions and Kafka offsets but rather partitions and offsets as needed
    in the source system. For example, in the file source, a partition can be a file
    and an offset can be a line number or character number in the file. In a JDBC
    source, a partition can be a database table and the offset can be an ID or timestamp
    of a record in the table. One of the most important design decisions involved
    in writing a source connector is deciding on a good way to partition the data
    in the source system and to track offsets—this will impact the level of parallelism
    the connector can achieve and whether it can deliver at-least-once or exactly-once
    semantics.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: When the source connector returns a list of records, which includes the source
    partition and offset for each record, the worker sends the records to Kafka brokers.
    If the brokers successfully acknowledge the records, the worker then stores the
    offsets of the records it sent to Kafka. This allows connectors to start processing
    events from the most recently stored offset after a restart or a crash. The storage
    mechanism is pluggable and is usually a Kafka topic; you can control the topic
    name with the `offset.storage.topic` configuration. In addition, Connect uses
    Kafka topics to store the configuration of all the connectors we’ve created and
    the status of each connector—these use names configured by `config.storage.topic`
    and `status.​stor⁠age.topic`, respectively.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Sink connectors have an opposite but similar workflow: they read Kafka records,
    which already have a topic, partition, and offset identifiers. Then they call
    the `connector` `put()` method that should store those records in the destination
    system. If the connector reports success, they commit the offsets they’ve given
    to the connector back to Kafka, using the usual consumer commit methods.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Offset tracking provided by the framework itself should make it easier for developers
    to write connectors and guarantee some level of consistent behavior when using
    different connectors.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Alternatives to Kafka Connect
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we’ve looked at Kafka’s Connect API in great detail. While we love the
    convenience and reliability the Connect API provides, it is not the only method
    for getting data in and out of Kafka. Let’s look at other alternatives and when
    they are commonly used.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Ingest Frameworks for Other Datastores
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we like to think that Kafka is the center of the universe, some people
    disagree. Some people build most of their data architectures around systems like
    Hadoop or Elasticsearch. Those systems have their own data ingestion tools—Flume
    for Hadoop, and Logstash or Fluentd for Elasticsearch. We recommend Kafka’s Connect
    API when Kafka is an integral part of the architecture and when the goal is to
    connect large numbers of sources and sinks. If you are actually building a Hadoop-centric
    or Elastic-centric system and Kafka is just one of many inputs into that system,
    then using Flume or Logstash makes sense.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: GUI-Based ETL Tools
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Old-school systems like Informatica, open source alternatives like Talend and
    Pentaho, and even newer alternatives such as Apache NiFi and StreamSets, support
    Apache Kafka as both a data source and a destination. If you are already using
    these systems—if you already do everything using Pentaho, for example—you may
    not be interested in adding another data integration system just for Kafka. They
    also make sense if you are using a GUI-based approach to building ETL pipelines.
    The main drawback of these systems is that they are usually built for involved
    workflows and will be a somewhat heavy and involved solution if all you want to
    do is get data in and out of Kafka. We believe that data integration should focus
    on faithful delivery of messages under all conditions, while most ETL tools add
    unnecessary complexity.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We do encourage you to look at Kafka as a platform that can handle data integration
    (with Connect), application integration (with producers and consumers), and stream
    processing. Kafka could be a viable replacement for an ETL tool that only integrates
    data stores.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Stream Processing Frameworks
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost all stream processing frameworks include the ability to read events from
    Kafka and write them to a few other systems. If your destination system is supported
    and you already intend to use that stream processing framework to process events
    from Kafka, it seems reasonable to use the same framework for data integration
    as well. This often saves a step in the stream processing workflow (no need to
    store processed events in Kafka—just read them out and write them to another system),
    with the drawback that it can be more difficult to troubleshoot things like lost
    and corrupted messages.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we discussed the use of Kafka for data integration. Starting
    with reasons to use Kafka for data integration, we covered general considerations
    for data integration solutions. We showed why we think Kafka and its Connect API
    are a good fit. We then gave several examples of how to use Kafka Connect in different
    scenarios, spent some time looking at how Connect works, and then discussed a
    few alternatives to Kafka Connect.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Whatever data integration solution you eventually land on, the most important
    feature will always be its ability to deliver all messages under all failure conditions.
    We believe that Kafka Connect is extremely reliable—based on its integration with
    Kafka’s tried-and-true reliability features—but it is important that you test
    the system of your choice, just like we do. Make sure your data integration system
    of choice can survive stopped processes, crashed machines, network delays, and
    high loads without missing a message. After all, at their heart, data integration
    systems only have one job—delivering those messages.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Of course, while reliability is usually the most important requirement when
    integrating data systems, it is only one requirement. When choosing a data system,
    it is important to first review your requirements (refer to [“Considerations When
    Building Data Pipelines”](#considerations) for examples) and then make sure your
    system of choice satisfies them. But this isn’t enough—you must also learn your
    data integration solution well enough to be certain that you are using it in a
    way that supports your requirements. It isn’t enough that Kafka supports at-least-once
    semantics; you must be sure you aren’t accidentally configuring it in a way that
    may end up with less than complete reliability.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL

- en: Chapter 10\. Cross-Cluster Data Mirroring
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 跨集群数据镜像
- en: For most of the book we discuss the setup, maintenance, and use of a single
    Kafka cluster. There are, however, a few scenarios in which an architecture may
    need more than one cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在大部分的书中，我们讨论了单个Kafka集群的设置、维护和使用。然而，有一些情况下，架构可能需要不止一个集群。
- en: In some cases, the clusters are completely separated. They belong to different
    departments or different use cases, and there is no reason to copy data from one
    cluster to another. Sometimes, different SLAs or workloads make it difficult to
    tune a single cluster to serve multiple use cases. Other times, there are different
    security requirements. Those use cases are fairly easy—managing multiple distinct
    clusters is the same as running a single cluster multiple times.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，集群是完全分离的。它们属于不同的部门或不同的用例，没有理由将数据从一个集群复制到另一个集群。有时，不同的SLA或工作负载使得很难调整单个集群以服务多个用例。其他时候，存在不同的安全要求。这些用例相当容易——管理多个不同的集群与多次运行单个集群是一样的。
- en: In other use cases, the different clusters are interdependent, and the administrators
    need to continuously copy data between the clusters. In most databases, continuously
    copying data between database servers is called *replication*. Since we’ve used
    replication to describe movement of data between Kafka nodes that are part of
    the same cluster, we’ll call copying of data between Kafka clusters *mirroring*.
    Apache Kafka’s built-in cross-cluster replicator is called *MirrorMaker*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他用例中，不同的集群是相互依存的，管理员需要在集群之间持续复制数据。在大多数数据库中，不断地在数据库服务器之间复制数据被称为*复制*。由于我们已经使用复制来描述Kafka节点之间的数据移动，我们将称在Kafka集群之间复制数据为*镜像*。Apache
    Kafka内置的跨集群复制器称为*MirrorMaker*。
- en: In this chapter, we will discuss cross-cluster mirroring of all or part of the
    data. We’ll start by discussing some of the common use cases for cross-cluster
    mirroring. Then we’ll show a few architectures that are used to implement these
    use cases and discuss the pros and cons of each architecture pattern. We’ll then
    discuss MirrorMaker itself and how to use it. We’ll share operational tips, including
    deployment and performance tuning. We’ll finish by discussing a few alternatives
    to MirrorMaker.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论所有或部分数据的跨集群镜像。我们将首先讨论一些跨集群镜像的常见用例。然后，我们将展示用于实现这些用例的一些架构，并讨论每种架构模式的优缺点。然后我们将讨论MirrorMaker本身以及如何使用它。我们将分享操作提示，包括部署和性能调优。最后，我们将讨论一些MirrorMaker的替代方案。
- en: Use Cases of Cross-Cluster Mirroring
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨集群镜像的用例
- en: 'The following is a list of examples of when cross-cluster mirroring would be
    used:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是跨集群镜像将被使用的一些示例列表：
- en: Regional and central clusters
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 区域和中央集群
- en: In some cases, the company has one or more datacenters in different geographical
    regions, cities, or continents. Each datacenter has its own Kafka cluster. Some
    applications can work just by communicating with the local cluster, but some applications
    require data from multiple datacenters (otherwise, you wouldn’t be looking at
    cross-datacenter replication solutions). There are many cases when this is a requirement,
    but the classic example is a company that modifies prices based on supply and
    demand. This company can have a datacenter in each city in which it has a presence,
    collects information about local supply and demand, and adjusts prices accordingly.
    All this information will then be mirrored to a central cluster where business
    analysts can run company-wide reports on its revenue.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，公司在不同的地理区域、城市或大陆拥有一个或多个数据中心。每个数据中心都有自己的Kafka集群。一些应用程序可以通过与本地集群通信来工作，但有些应用程序需要来自多个数据中心的数据（否则，您不会寻找跨数据中心复制解决方案）。有许多情况下这是一个要求，但经典的例子是一家根据供需情况调整价格的公司。这家公司可以在其存在的每个城市都有一个数据中心，收集有关当地供需情况的信息，并相应地调整价格。然后所有这些信息将被镜像到一个中央集群，业务分析师可以在其收入上运行公司范围的报告。
- en: High availability (HA) and disaster recovery (DR)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性（HA）和灾难恢复（DR）
- en: The applications run on just one Kafka cluster and don’t need data from other
    locations, but you are concerned about the possibility of the entire cluster becoming
    unavailable for some reason. For redundancy, you’d like to have a second Kafka
    cluster with all the data that exists in the first cluster, so in case of emergency
    you can direct your applications to the second cluster and continue as usual.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序仅在一个Kafka集群上运行，不需要来自其他位置的数据，但您担心由于某种原因整个集群可能变得不可用。为了冗余，您希望有第二个Kafka集群，其中包含第一个集群中存在的所有数据，以便在紧急情况下，您可以将应用程序指向第二个集群并继续正常运行。
- en: Regulatory compliance
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监管合规性
- en: Companies operating in different countries may need to use different configurations
    and policies to conform to legal and regulatory requirements in each country.
    For instance, some datasets may be stored in separate clusters with strict access
    control, with subsets of data replicated to other clusters with wider access.
    To comply with regulatory policies that govern retention period in each region,
    datasets may be stored in clusters in different regions with different configurations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同国家运营的公司可能需要使用不同的配置和政策，以符合每个国家的法律和监管要求。例如，一些数据集可能存储在具有严格访问控制的单独集群中，其中数据子集被复制到其他具有更广泛访问权限的集群。为了遵守各个地区规定的保留期限的监管政策，数据集可能存储在具有不同配置的不同地区的集群中。
- en: Cloud migrations
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 云迁移
- en: Many companies these days run their business in both an on-premises datacenter
    and a cloud provider. Often, applications run on multiple regions of the cloud
    provider for redundancy, and sometimes multiple cloud providers are used. In these
    cases, there is often at least one Kafka cluster in each on-premises datacenter
    and each cloud region. Those Kafka clusters are used by applications in each datacenter
    and region to transfer data efficiently between the datacenters. For example,
    if a new application is deployed in the cloud but requires some data that is updated
    by applications running in the on-premises datacenter and stored in an on-premises
    database, you can use Kafka Connect to capture database changes to the local Kafka
    cluster and then mirror these changes to the cloud Kafka cluster where the new
    application can use them. This helps control the costs of cross-datacenter traffic
    as well as improve governance and security of the traffic.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多公司在自己的数据中心和云服务提供商中运行业务。通常，应用程序在云服务提供商的多个区域运行，以实现冗余，有时会使用多个云服务提供商。在这些情况下，通常每个自有数据中心和每个云区域至少有一个Kafka集群。这些Kafka集群被每个数据中心和区域的应用程序用来在数据中心之间高效地传输数据。例如，如果在云中部署了一个新应用程序，但需要一些由在自有数据中心运行并存储在自有数据库中的应用程序更新的数据，您可以使用Kafka
    Connect捕获数据库更改并将这些更改镜像到云Kafka集群，新应用程序可以使用这些更改。这有助于控制跨数据中心流量的成本，以及改善流量的治理和安全性。
- en: Aggregation of data from edge clusters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘集群数据聚合
- en: Several industries, including retail, telecommunications, transportation, and
    healthcare, generate data from small devices with limited connectivity. An aggregate
    cluster with high availability can be used to support analytics and other use
    cases for data from a large number of edge clusters. This reduces connectivity,
    availability, and durability requirements on low-footprint edge clusters, for
    example, in IoT use cases. A highly available aggregate cluster provides business
    continuity even when edge clusters are offline and simplifies the development
    of applications that don’t have to directly deal with a large number of edge clusters
    with unstable networks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 包括零售、电信、交通运输和医疗保健在内的几个行业从具有有限连接性的小型设备生成数据。可以使用具有高可用性的聚合集群来支持来自大量边缘集群的数据的分析和其他用例。这减少了对低占地面积的边缘集群的连接性、可用性和耐久性要求，例如在物联网用例中。高可用的聚合集群即使在边缘集群离线时也提供业务连续性，并简化了不必直接处理大量具有不稳定网络的边缘集群的应用程序的开发。
- en: Multicluster Architectures
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多集群架构
- en: Now that we’ve seen a few use cases that require multiple Kafka clusters, let’s
    look at some common architectural patterns that we’ve successfully used when implementing
    these use cases. Before we go into the architectures, we’ll give a brief overview
    of the realities of cross-datacenter communications. The solutions we’ll discuss
    may seem overly complicated without understanding that they represent trade-offs
    in the face of specific network conditions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了一些需要多个Kafka集群的用例，让我们来看一些我们在实现这些用例时成功使用的常见架构模式。在我们讨论架构之前，我们将简要概述跨数据中心通信的现实情况。我们将讨论的解决方案可能在特定网络条件下代表了权衡，因此可能看起来过于复杂。
- en: Some Realities of Cross-Datacenter Communication
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨数据中心通信的一些现实情况
- en: 'The following is a list of some things to consider when it comes to cross-datacenter
    communication:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在跨数据中心通信时需要考虑的一些事项：
- en: High latencies
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高延迟
- en: Latency of communication between two Kafka clusters increases as the distance
    and the number of network hops between the two clusters increase.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 两个Kafka集群之间的通信延迟随着两个集群之间的距离和网络跳数的增加而增加。
- en: Limited bandwidth
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有限的带宽
- en: Wide area networks (WANs) typically have far lower available bandwidth than
    what you’ll see inside a single datacenter, and the available bandwidth can vary
    from minute to minute. In addition, higher latencies make it more challenging
    to utilize all the available bandwidth.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 广域网（WAN）通常比单个数据中心内的可用带宽要低得多，可用带宽可能会每分钟都有所变化。此外，更高的延迟使得更具挑战性地利用所有可用带宽。
- en: Higher costs
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的成本
- en: Regardless of whether you are running Kafka on premise or in the cloud, there
    are higher costs to communicate between clusters. This is partly because the bandwidth
    is limited and adding bandwidth can be prohibitively expensive, and also because
    of the prices vendors charge for transferring data among datacenters, regions,
    and clouds.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是在本地运行Kafka还是在云中运行Kafka，跨集群通信的成本都更高。这部分是因为带宽有限，增加带宽可能成本过高，另一部分是因为供应商对数据中心、区域和云之间的数据传输收费。
- en: Apache Kafka’s brokers and clients were designed, developed, tested, and tuned,
    all within a single datacenter. We assumed low latency and high bandwidth between
    brokers and clients. This is apparent in the default timeouts and sizing of various
    buffers. For this reason, it is not recommended (except in specific cases, which
    we’ll discuss later) to install some Kafka brokers in one datacenter and others
    in another datacenter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka的代理和客户端是在单个数据中心内设计、开发、测试和调整的。我们假设代理和客户端之间的延迟低，带宽高。这在默认超时和各种缓冲区的大小中是显而易见的。因此，不建议（除非在特定情况下，我们稍后会讨论）在一个数据中心安装一些Kafka代理，而在另一个数据中心安装其他Kafka代理。
- en: In most cases, it’s best to avoid producing data to a remote datacenter, and
    when you do, you need to account for higher latency and the potential for more
    network errors. You can handle the errors by increasing the number of producer
    retries, and handle the higher latency by increasing the size of the buffers that
    hold records between attempts to send them.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，最好避免向远程数据中心生成数据，如果必须这样做，需要考虑更高的延迟和更多网络错误的可能性。您可以通过增加生产者重试的次数来处理错误，并通过增加保存记录的缓冲区的大小来处理更高的延迟，以便在尝试发送记录时进行处理。
- en: If we need any kind of replication between clusters, and we ruled out inter-broker
    communication and producer-broker communication, then we must allow for broker-consumer
    communication. Indeed, this is the safest form of cross-cluster communication
    because in the event of network partition that prevents a consumer from reading
    data, the records remain safe inside the Kafka brokers until communications resume
    and consumers can read them. There is no risk of accidental data loss due to network
    partitions. Still, because bandwidth is limited, if there are multiple applications
    in one datacenter that need to read data from Kafka brokers in another datacenter,
    we prefer to install a Kafka cluster in each datacenter and mirror the necessary
    data between them once rather than have multiple applications consume the same
    data across the WAN.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在集群之间进行任何形式的复制，并且排除了经纪人之间的通信和生产者-经纪人之间的通信，那么我们必须允许经纪人-消费者之间的通信。事实上，这是跨集群通信中最安全的形式，因为在阻止消费者读取数据的网络分区事件发生时，记录仍然安全地保存在Kafka经纪人中，直到通信恢复并且消费者可以读取它们。由于网络带宽有限，如果一个数据中心中有多个应用程序需要从另一个数据中心的Kafka经纪人读取数据，我们更倾向于在每个数据中心安装一个Kafka集群，并在它们之间镜像必要的数据，而不是让多个应用程序通过广域网消费相同的数据。
- en: 'We’ll talk more about tuning Kafka for cross-datacenter communication, but
    the following principles will guide most of the architectures we’ll discuss next:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更多地讨论调整Kafka以进行跨数据中心通信，但以下原则将指导我们将讨论的大多数架构：
- en: No less than one cluster per datacenter.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据中心至少有一个集群。
- en: Replicate each event exactly once (barring retries due to errors) between each
    pair of datacenters.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每对数据中心之间精确复制每个事件一次（除了由于错误而重试）。
- en: When possible, consume from a remote datacenter rather than produce to a remote
    datacenter.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下，从远程数据中心消费，而不是向远程数据中心生产。
- en: Hub-and-Spoke Architecture
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集线器和辐射架构
- en: This architecture is intended for the case where there are multiple local Kafka
    clusters and one central Kafka cluster. See [Figure 10-1](#fig0801).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构适用于存在多个本地Kafka集群和一个中央Kafka集群的情况。参见[图10-1](#fig0801)。
- en: '![kdg2 1001](assets/kdg2_1001.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1001](assets/kdg2_1001.png)'
- en: Figure 10-1\. The hub-and-spoke architecture
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。集线器和辐射架构
- en: 'There is also a simpler variation of this architecture with just two clusters:
    a leader and a follower. See [Figure 10-2](#fig0802).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种更简单的变体，只有两个集群：一个领导者和一个追随者。参见[图10-2](#fig0802)。
- en: '![kdg2 1002](assets/kdg2_1002.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1002](assets/kdg2_1002.png)'
- en: Figure 10-2\. A simpler version of the hub-and-spoke architecture
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。集线器和辐射架构的简化版本
- en: This architecture is used when data is produced in multiple datacenters and
    some consumers need access to the entire dataset. The architecture also allows
    for applications in each datacenter to only process data local to that specific
    datacenter. But it does not give access to the entire dataset from every datacenter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据在多个数据中心产生并且一些消费者需要访问整个数据集时，使用这种架构。该架构还允许每个数据中心的应用程序仅处理特定数据中心的本地数据。但它不允许每个数据中心从整个数据集中获取数据。
- en: The main benefit of this architecture is that data is always produced to the
    local datacenter and events from each datacenter are only mirrored once—to the
    central datacenter. Applications that process data from a single datacenter can
    be located at that datacenter. Applications that need to process data from multiple
    datacenters will be located at the central datacenter where all the events are
    mirrored. Because replication always goes in one direction and because each consumer
    always reads from the same cluster, this architecture is simple to deploy, configure,
    and monitor.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的主要好处是数据始终产生在本地数据中心，并且每个数据中心的事件只被镜像一次 - 到中央数据中心。从单个数据中心处理数据的应用程序可以位于该数据中心。需要从多个数据中心处理数据的应用程序将位于所有事件都被镜像的中央数据中心。由于复制始终是单向的，并且每个消费者始终从同一集群中读取，因此这种架构易于部署、配置和监视。
- en: The main drawback of this architecture is the direct result of its benefits
    and simplicity. Processors in one regional datacenter can’t access data in another.
    To understand better why this is a limitation, let’s look at an example of this
    architecture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的主要缺点直接源于其好处和简单性。一个区域数据中心的处理器无法访问另一个数据中心的数据。为了更好地理解这是一个限制，让我们看一个这种架构的例子。
- en: Suppose that we are a large bank and have branches in multiple cities. Let’s
    say that we decide to store user profiles and their account history in a Kafka
    cluster in each city. We replicate all this information to a central cluster that
    is used to run the bank’s business analytics. When users connect to the bank website
    or visit their local branch, they are routed to send events to their local cluster
    and read events from the same local cluster. However, suppose that a user visits
    a branch in a different city. Because the user information doesn’t exist in the
    city they are visiting, the branch will be forced to interact with a remote cluster
    (not recommended) or have no way to access the user’s information (really embarrassing).
    For this reason, use of this pattern is usually limited to only parts of the dataset
    that can be completely separated between regional datacenters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是一家大型银行，在多个城市设有分支机构。假设我们决定将用户配置文件和其账户历史存储在每个城市的Kafka集群中。我们将所有这些信息复制到一个用于运行银行业务分析的中央集群。当用户连接到银行网站或访问他们的本地分支机构时，他们被路由到发送事件到他们的本地集群并从同一本地集群读取事件。然而，假设用户访问另一个城市的分支机构。因为用户信息不存在于他们正在访问的城市，分支机构将被迫与远程集群交互（不建议）或无法访问用户的信息（非常尴尬）。因此，通常仅将此模式用于可以在区域数据中心之间完全分离的数据集的部分。
- en: When implementing this architecture, for each regional datacenter you need at
    least one mirroring process on the central datacenter. This process will consume
    data from each remote regional cluster and produce it to the central cluster.
    If the same topic exists in multiple datacenters, you can write all the events
    from this topic to one topic with the same name in the central cluster, or write
    events from each datacenter to a separate topic.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施这种架构时，对于每个区域数据中心，您至少需要一个在中央数据中心的镜像过程。此过程将从每个远程区域集群中获取数据并将其生成到中央集群。如果在多个数据中心中存在相同的主题，您可以将此主题的所有事件写入到中央集群中具有相同名称的一个主题中，或者将每个数据中心的事件写入到一个单独的主题中。
- en: Active-Active Architecture
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动-主动架构
- en: This architecture is used when two or more datacenters share some or all of
    the data, and each datacenter is able to both produce and consume events. See
    [Figure 10-3](#fig0803).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个或更多个数据中心共享部分或全部数据，并且每个数据中心都能够生成和消费事件时，将使用此架构。参见[图10-3](#fig0803)。
- en: '![kdg2 1003](assets/kdg2_1003.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1003](assets/kdg2_1003.png)'
- en: Figure 10-3\. The active-active architecture model
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3。主动-主动架构模型
- en: The main benefit of this architecture is the ability to serve users from a nearby
    datacenter, which typically has performance benefits, without sacrificing functionality
    due to limited availability of data (as we’ve seen happen in the hub-and-spoke
    architecture). A secondary benefit is redundancy and resilience. Since every datacenter
    has all the functionality, if one datacenter is unavailable, you can direct users
    to a remaining datacenter. This type of failover only requires network redirects
    of users, typically the easiest and most transparent type of failover.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的主要优点是能够从附近的数据中心为用户提供服务，这通常具有性能优势，而不会因数据的有限可用性（正如我们在集线器和辐射架构中看到的那样）而牺牲功能。第二个好处是冗余和弹性。由于每个数据中心都具有所有功能，因此如果一个数据中心不可用，您可以将用户引导到剩余的数据中心。这种故障转移只需要对用户进行网络重定向，通常是最简单和最透明的故障转移类型。
- en: 'The main drawback of this architecture is the challenge in avoiding conflicts
    when data is read and updated asynchronously in multiple locations. This includes
    technical challenges in mirroring events—for example, how do we make sure the
    same event isn’t mirrored back and forth endlessly? But more importantly, maintaining
    data consistency between the two datacenters will be difficult. Here are few examples
    of the difficulties you will encounter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的主要缺点是在数据在多个位置异步读取和更新时避免冲突的挑战。这包括在镜像事件方面的技术挑战，例如，我们如何确保相同的事件不会无休止地来回镜像？但更重要的是，在两个数据中心之间保持数据一致性将是困难的。以下是您将遇到的一些困难的几个例子：
- en: If a user sends an event to one datacenter and reads events from another datacenter,
    it is possible that the event they wrote hasn’t arrived at the second datacenter
    yet. To the user, it will look like they just added a book to their wish list
    and clicked on the wish list, but the book isn’t there. For this reason, when
    this architecture is used, developers usually find a way to “stick” each user
    to a specific datacenter and make sure they use the same cluster most of the time
    (unless they connect from a remote location or the datacenter becomes unavailable).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户将事件发送到一个数据中心，并从另一个数据中心读取事件，那么他们写入的事件可能尚未到达第二个数据中心。对用户来说，看起来就像他们刚刚将一本书添加到愿望清单并单击了愿望清单，但书并不在那里。因此，当使用这种架构时，开发人员通常会找到一种方法来将每个用户“粘附”到特定的数据中心，并确保他们大部分时间使用相同的集群（除非他们从远程位置连接或数据中心不可用）。
- en: An event from one datacenter says the user ordered book A, and an event from
    more or less the same time at a second datacenter says that the same user ordered
    book B. After mirroring, both datacenters have both events and thus we can say
    that each datacenter has two conflicting events. Applications on both datacenters
    need to know how to deal with this situation. Do we pick one event as the “correct”
    one? If so, we need consistent rules on how to pick one event so applications
    on both datacenters will arrive at the same conclusion. Do we decide that both
    are true and simply send the user two books and have another department deal with
    returns? Amazon used to resolve conflicts that way, but organizations dealing
    with stock trades, for example, can’t. The specific method for minimizing conflicts
    and handling them when they occur is specific to each use case. It is important
    to keep in mind that if you use this architecture, you *will* have conflicts and
    will need to deal with them.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据中心的事件表示用户订购了书A，第二个数据中心在更多或更少相同的时间内的事件表示同一用户订购了书B。在镜像之后，两个数据中心都有了这两个事件，因此我们可以说每个数据中心都有两个冲突的事件。两个数据中心上的应用程序需要知道如何处理这种情况。我们选择一个事件作为“正确”的事件吗？如果是这样，我们需要一致的规则来选择一个事件，这样两个数据中心上的应用程序将得出相同的结论。我们决定两者都是真实的，只是给用户发送两本书，并让另一个部门处理退货？亚马逊过去是以这种方式解决冲突的，但是处理股票交易等问题的组织不能这样做。最小化冲突和处理冲突发生时的具体方法是特定于每个用例的。重要的是要记住，如果您使用这种架构，您*将*会有冲突，并且需要处理它们。
- en: If you find ways to handle the challenges of asynchronous reads and writes to
    the same dataset from multiple locations, then this architecture is highly recommended.
    It is the most scalable, resilient, flexible, and cost-effective option we are
    aware of. So, it is well worth the effort to figure out solutions for avoiding
    replication cycles, keeping users mostly in the same datacenter, and handling
    conflicts when they occur.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您找到了处理从多个位置异步读取和写入相同数据集的挑战的方法，那么强烈推荐使用这种架构。这是我们所知道的最具可扩展性、有弹性、灵活和经济有效的选择。因此，值得努力找出避免复制周期、使用户大部分时间保持在同一个数据中心，并在发生冲突时处理冲突的解决方案。
- en: Part of the challenge of active-active mirroring, especially with more than
    two datacenters, is that you will need mirroring tasks for each pair of datacenters
    and each direction. Many mirroring tools these days can share processes, for example,
    using the same process for all mirroring to a destination cluster.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 主动-主备镜像的挑战之一，特别是在有两个以上的数据中心时，是您将需要为每对数据中心和每个方向设置镜像任务。如今许多镜像工具可以共享进程，例如使用相同的进程进行所有镜像到目标集群。
- en: In addition, you will want to avoid loops in which the same event is mirrored
    back and forth endlessly. You can do this by giving each “logical topic” a separate
    topic for each datacenter and making sure to avoid replicating topics that originated
    in remote datacenters. For example, logical topic *users* will be topic *SF.users*
    in one datacenter and *NYC.users* in another datacenter. The mirroring processes
    will mirror topic *SF.users* from SF to NYC and topic *NYC.users* from NYC to
    SF. As a result, each event will only be mirrored once, but each datacenter will
    contain both *SF.users* and *NYC.users*, which means each datacenter will have
    information for all the users. Consumers will need to consume events from **.users*
    if they wish to consume all user events. Another way to think of this setup is
    to see it as a separate namespace for each datacenter that contains all the topics
    for the specific datacenter. In our example, we’ll have the NYC and the SF namespaces.
    Some mirroring tools like MirrorMaker prevent replication cycles using a similar
    naming convention.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您将希望避免同一事件在不同数据中心之间无休止地来回镜像。您可以通过为每个“逻辑主题”在每个数据中心分配一个单独的主题，并确保避免复制源自远程数据中心的主题来实现这一点。例如，逻辑主题*用户*在一个数据中心将是*SF.users*，在另一个数据中心将是*NYC.users*。镜像过程将从SF到NYC镜像主题*SF.users*，从NYC到SF镜像主题*NYC.users*。因此，每个事件只会被镜像一次，但每个数据中心将包含*SF.users*和*NYC.users*，这意味着每个数据中心都将包含所有用户的信息。消费者需要从**.users*中消费事件，如果他们希望消费所有用户事件。另一种思考这种设置的方式是将其视为每个数据中心的单独命名空间，其中包含特定数据中心的所有主题。在我们的例子中，我们将有NYC和SF命名空间。一些镜像工具如MirrorMaker使用类似的命名约定来防止复制循环。
- en: Record headers introduced in Apache Kafka in version 0.11.0 enable events to
    be tagged with their originating datacenter. Header information may also be used
    to avoid endless mirroring loops and to allow processing events from different
    datacenters separately. You can also implement this feature by using a structured
    data format for the record values (Avro is our favorite example) and use this
    to include tags and headers in the event itself. However, this does require extra
    effort when mirroring, since none of the existing mirroring tools will support
    your specific header format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 0.11.0版本引入的记录头使事件可以被标记其来源数据中心。头信息也可以用于避免无休止的镜像循环，并允许分别处理来自不同数据中心的事件。您还可以通过使用结构化数据格式（Avro是我们最喜欢的例子）来实现此功能，并将标签和头信息包含在事件本身中。然而，这需要在镜像时额外的工作，因为没有现有的镜像工具会支持您的特定头格式。
- en: Active-Standby Architecture
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主备架构
- en: In some cases, the only requirement for multiple clusters is to support some
    kind of disaster scenario. Perhaps you have two clusters in the same datacenter.
    You use one cluster for all the applications, but you want a second cluster that
    contains (almost) all the events in the original cluster that you can use if the
    original cluster is completely unavailable. Or perhaps you need geographic resiliency.
    Your entire business is running from a datacenter in California, but you need
    a second datacenter in Texas that usually doesn’t do much and that you can use
    in case of an earthquake. The Texas datacenter will probably have an inactive
    (“cold”) copy of all the applications that admins can start up in case of emergency
    and that will use the second cluster ([Figure 10-4](#fig0804)). This is often
    a legal requirement rather than something that the business is actually planning
    on doing—but you still need to be ready.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，多个集群的唯一要求是支持某种灾难情景。也许您在同一个数据中心中有两个集群。您使用一个集群来运行所有应用程序，但您希望有第二个集群包含（几乎）原始集群中的所有事件，以便在原始集群完全不可用时使用。或者您可能需要地理弹性。您的整个业务都是从加利福尼亚州的一个数据中心运行的，但您需要德克萨斯州的第二个数据中心，通常不做太多事情，以防发生地震时使用。德克萨斯数据中心可能会有所有应用程序的非活动（“冷”）副本，管理员可以在紧急情况下启动，并且将使用第二个集群（[图10-4](#fig0804)）。这通常是法律要求，而不是业务实际计划要做的事情，但您仍然需要做好准备。
- en: '![kdg2 1004](assets/kdg2_1004.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1004](assets/kdg2_1004.png)'
- en: Figure 10-4\. The active-standby architecture
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. 主备架构
- en: The benefits of this setup are simplicity in setup and the fact that it can
    be used in pretty much any use case. You simply install a second cluster and set
    up a mirroring process that streams all the events from one cluster to another.
    No need to worry about access to data, handling conflicts, and other architectural
    complexities.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置的好处是设置简单，并且几乎可以用于任何用例。您只需安装第二个集群，并设置一个镜像过程，将所有事件从一个集群流式传输到另一个集群。无需担心数据访问、处理冲突和其他架构复杂性。
- en: The disadvantages are waste of a good cluster and the fact that failover between
    Kafka clusters is, in fact, much harder than it looks. The bottom line is that
    it is currently not possible to perform cluster failover in Kafka without either
    losing data or having duplicate events. Often both. You can minimize them but
    never fully eliminate them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是浪费一个好的集群，并且Kafka集群之间的故障转移实际上比看起来要困难得多。最重要的是，目前在Kafka中执行集群故障转移时要么会丢失数据，要么会有重复事件。通常两者都有。您可以将它们最小化，但永远无法完全消除。
- en: It should be obvious that a cluster that does nothing except wait around for
    a disaster is a waste of resources. Since disasters are (or should be) rare, most
    of the time we are looking at a cluster of machines that does nothing at all.
    Some organizations try to fight this issue by having a DR (disaster recovery)
    cluster that is much smaller than the production cluster. But this is a risky
    decision because you can’t be sure that this minimally sized cluster will hold
    up during an emergency. Other organizations prefer to make the cluster useful
    during nondisasters by shifting some read-only workloads to run on the DR cluster,
    which means they are really running a small version of a hub-and-spoke architecture
    with a single spoke.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的是，一个除了等待灾难之外什么也不做的集群是资源的浪费。由于灾难是（或应该是）罕见的，大部分时间我们看到的是一组机器的集群根本什么也不做。一些组织试图通过拥有比生产集群小得多的DR（灾难恢复）集群来解决这个问题。但这是一个冒险的决定，因为您无法确定这个尺寸最小的集群在紧急情况下是否能够支撑住。其他组织更喜欢在非灾难期间使集群有用，通过将一些只读工作负载转移到DR集群上运行，这意味着他们实际上正在运行一个带有单个辐条的小型版本的集线器和辐条架构。
- en: The more serious issue is, how do you failover to a DR cluster in Apache Kafka?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更严重的问题是，如何在Apache Kafka中切换到DR集群？
- en: First, it should go without saying that whichever failover method you choose,
    your SRE team must practice it on a regular basis. A plan that works today may
    stop working after an upgrade, or perhaps new use cases make the existing tooling
    obsolete. Once a quarter is usually the bare minimum for failover practices. Strong
    SRE teams practice far more frequently. Netflix’s famous Chaos Monkey, a service
    that randomly causes disasters, is the extreme—any day may become failover practice
    day.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，毋庸置疑，无论选择哪种故障切换方法，您的SRE团队都必须定期进行实践。今天有效的计划可能在升级后停止工作，或者新的用例使现有的工具过时。每季度通常是故障切换实践的最低要求。强大的SRE团队经常进行实践。Netflix著名的混沌猴（Chaos
    Monkey）是极端的例子 - 任何一天都可能成为故障切换实践的日子。
- en: Now, let’s take a look at what is involved in a failover.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看故障切换涉及哪些内容。
- en: Disaster recovery planning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 灾难恢复规划
- en: When planning for disaster recovery, it is important to consider two key metrics.
    Recovery time objective (RTO) defines the maximum amount of time before all services
    must resume after a disaster. Recovery point objective (RPO) defines the maximum
    amount of time for which data may be lost as a result of a disaster. The lower
    the RTO, the more important it is to avoid manual processes and application restarts,
    since very low RTO can be achieved only with automated failover. Low RPO requires
    real-time mirroring with low latencies, and `RPO=0` requires synchronous replication.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在灾难恢复规划时，重要的是考虑两个关键指标。恢复时间目标（RTO）定义了灾难后所有服务必须恢复的最长时间。恢复点目标（RPO）定义了可能丢失数据的最长时间。RTO越低，避免手动流程和应用程序重新启动就越重要，因为只有通过自动故障切换才能实现非常低的RTO。低RPO需要低延迟的实时镜像，`RPO=0`需要同步复制。
- en: Data loss and inconsistencies in unplanned failover
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未经计划的故障切换中的数据丢失和不一致性
- en: Because Kafka’s various mirroring solutions are all asynchronous (we’ll discuss
    a synchronous solution in the next section), the DR cluster will not have the
    latest messages from the primary cluster. You should always monitor how far behind
    the DR cluster is and never let it fall too far behind. But in a busy system you
    should expect the DR cluster to be a few hundred or even a few thousand messages
    behind the primary. If your Kafka cluster handles 1 million messages a second
    and the lag between the primary and the DR cluster is 5 milliseconds, your DR
    cluster will be 5,000 messages behind the primary in the best-case scenario. So,
    prepare for unplanned failover to include some data loss. In planned failover,
    you can stop the primary cluster and wait for the mirroring process to mirror
    the remaining messages before failing over applications to the DR cluster, thus
    avoiding this data loss. When unplanned failover occurs and you lose a few thousand
    messages, note that mirroring solutions currently don’t support transactions,
    which means that if some events in multiple topics are related to each other (e.g.,
    sales and line items), you can have some events arrive to the DR site in time
    for the failover and others that don’t. Your applications will need to be able
    to handle a line item without a corresponding sale after you failover to the DR
    cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Kafka的各种镜像解决方案都是异步的（我们将在下一节讨论同步解决方案），DR集群将不会拥有来自主要集群的最新消息。您应始终监视DR集群的落后程度，并且永远不要让它落后太远。但在繁忙的系统中，您应该预期DR集群落后主要集群几百甚至几千条消息。如果您的Kafka集群每秒处理100万条消息，并且主要集群和DR集群之间的滞后时间为5毫秒，那么在最佳情况下，您的DR集群将落后于主要集群5,000条消息。因此，准备好未经计划的故障切换包括一些数据丢失。在计划的故障切换中，您可以停止主要集群，并等待镜像过程在故障切换应用程序到DR集群之前镜像剩余的消息，从而避免这种数据丢失。当发生未经计划的故障切换并丢失了几千条消息时，请注意，镜像解决方案目前不支持事务，这意味着如果多个主题中的某些事件相关（例如销售和行项目），您可以在故障切换时有一些事件及时到达DR站点，而其他事件则没有。您的应用程序需要能够处理故障切换到DR集群后没有相应销售的行项目。
- en: Start offset for applications after failover
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障切换后应用程序的起始偏移
- en: 'One of the challenging tasks in failing over to another cluster is making sure
    applications know where to start consuming data. There are several common approaches.
    Some are simple but can cause additional data loss or duplicate processing; others
    are more involved but minimize additional data loss and reprocessing. Let’s take
    a look at a few:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在切换到另一个集群时，一个具有挑战性的任务是确保应用程序知道从何处开始消费数据。有几种常见的方法。有些方法简单，但可能会导致额外的数据丢失或重复处理；其他方法更复杂，但可以最小化额外的数据丢失和重新处理。让我们来看看其中的一些：
- en: Auto offset reset
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自动偏移重置
- en: Apache Kafka consumers have a configuration for how to behave when they don’t
    have a previously committed offset—they either start reading from the beginning
    of the partition or from the end of the partition. If you are not somehow mirroring
    these offsets as part of the DR plan, you need to choose one of these options.
    Either start reading from the beginning of available data and handle large amounts
    of duplicates or skip to the end and miss an unknown (and hopefully small) number
    of events. If your application handles duplicates with no issues, or missing some
    data is no big deal, this option is by far the easiest. Simply skipping to the
    end of the topic on failover is a popular failover method due to its simplicity.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Replicate offsets topic
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Kafka consumers from version 0.9.0 and later, the consumers
    will commit their offsets to a special topic: `__consumer_offsets`. If you mirror
    this topic to your DR cluster, when consumers start consuming from the DR cluster,
    they will be able to pick up their old offsets and continue from where they left
    off. It is simple, but there is a long list of caveats involved.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: First, there is no guarantee that offsets in the primary cluster will match
    those in the secondary cluster. Suppose you only store data in the primary cluster
    for three days and you start mirroring a topic a week after it was created. In
    this case, the first offset available in the primary cluster may be offset 57,000,000
    (older events were from the first 4 days and were removed already), but the first
    offset in the DR cluster will be 0\. So, a consumer that tries to read offset
    57,000,003 (because that’s its next event to read) from the DR cluster will fail
    to do this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Second, even if you started mirroring immediately when the topic was first created
    and both the primary and the DR topics start with 0, producer retries can cause
    offsets to diverge. We discuss an alternative mirroring solution that preserves
    offsets between primary and DR clusters at the end of this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Third, even if the offsets were perfectly preserved, because of the lag between
    primary and DR clusters and because mirroring solutions currently don’t support
    transactions, an offset committed by a Kafka consumer may arrive ahead or behind
    the record with this offset. A consumer that fails over may find committed offsets
    without matching records. Or it may find that the latest committed offset in the
    DR site is older than the latest committed offset in the primary site. See [Figure 10-5](#fig0805).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1005](assets/kdg2_1005.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. A failover causes committed offsets without matching records
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In these cases, you need to accept some duplicates if the latest committed offset
    in the DR site is older than the one committed on the primary or if the offsets
    in the records in the DR site are ahead of the primary due to retries. You will
    also need to figure out how to handle cases where the latest committed offset
    in the DR site doesn’t have a matching record—do you start processing from the
    beginning of the topic or skip to the end?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this approach has its limitations. Still, this option lets you
    failover to another DR with a reduced number of duplicated or missing events compared
    to other approaches while still being simple to implement.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Time-based failover
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'From version 0.10.0 onward, each message includes a timestamp indicating the
    time the message was sent to Kafka. From 0.10.1.0 onward, brokers include an index
    and an API for looking up offsets by the timestamp. So, if you failover to the
    DR cluster and you know that your trouble started at 4:05 a.m., you can tell consumers
    to start processing data from 4:03 a.m. There will be some duplicates from those
    two minutes, but it is probably better than other alternatives and the behavior
    is much easier to explain to everyone in the company—“We failed back to 4:03 a.m.”
    sounds better than “We failed back to what may or may not be the latest committed
    offsets.” So, this is often a good compromise. The only question is: how do we
    tell consumers to start processing data from 4:03 a.m.?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从 0.10.0 版本开始，每条消息都包括一个时间戳，指示消息发送到 Kafka 的时间。从 0.10.1.0 版本开始，代理包括一个索引和一个用于按时间戳查找偏移的
    API。因此，如果您切换到 DR 集群，并且知道您的问题是在凌晨 4:05 开始的，您可以告诉消费者从凌晨 4:03 开始处理数据。这两分钟内会有一些重复，但这可能比其他选择更好，并且行为更容易向公司中的每个人解释——“我们回滚到了凌晨
    4:03”听起来比“我们回滚到可能是最新提交的偏移”好。因此，这通常是一个很好的折衷方案。唯一的问题是：我们如何告诉消费者从凌晨 4:03 开始处理数据？
- en: One option is to bake it right into your app. Have a user-configurable option
    to specify the start time for the app. If this is configured, the app can use
    the new APIs to fetch offset by time, seek to that time, and start consuming from
    the right point, committing offsets as usual.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是将其直接嵌入到您的应用程序中。具有用户可配置选项来指定应用程序的启动时间。如果配置了这个选项，应用程序可以使用新的 API 来按时间获取偏移，寻找到那个时间，并从正确的位置开始消费，像往常一样提交偏移。
- en: 'This option is great if you wrote all your applications this way in advance.
    But what if you didn’t? Apache Kafka provides the `kafka-consumer-groups` tool
    to reset offsets based on a range of options, including timestamp-based reset
    added in 0.11.0\. The consumer group should be stopped while running this type
    of tool and started immediately after. For example, the following command resets
    consumer offsets for all topics belonging to a particular group to a specific
    time:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您事先没有编写所有应用程序，这个选项就很好。Apache Kafka 提供了 `kafka-consumer-groups` 工具，根据一系列选项重置偏移，包括在
    0.11.0 版本中添加的基于时间戳的重置。在运行此类型的工具时，应该停止消费者组，并在之后立即启动。例如，以下命令将为属于特定组的所有主题重置消费者偏移到特定时间：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This option is recommended in deployments that need to guarantee a level of
    certainty in their failover.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选项建议在需要保证故障切换的确定性的部署中使用。
- en: Offset translation
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移翻译
- en: When discussing mirroring the offsets topic, one of the biggest challenges is
    the fact that offsets in primary and DR clusters can diverge. In the past, some
    organizations chose to use an external data store, such as Apache Cassandra, to
    store mapping of offsets from one cluster to another. Whenever an event is produced
    to the DR cluster, both offsets are sent to the external data store by the mirroring
    tool when offsets diverge. These days, mirroring solutions, including MirrorMaker,
    use a Kafka topic for storing offset translation metadata. Offsets are stored
    whenever the difference between the two offsets changes. For example, if offset
    495 on the primary mapped to offset 500 on the DR cluster, we’ll record (495,500)
    in the external store or offset translation topic. If the difference changes later
    due to duplicates and offset 596 is mapped to 600, then we’ll record the new mapping
    (596,600). There is no need to store all the offset mappings between 495 and 596;
    we just assume that the difference remains the same and so offset 550 in the primary
    cluster will map to 555 in the DR. Then when failover occurs, instead of mapping
    timestamps (which are always a bit inaccurate) to offsets, we map primary offsets
    to DR offsets and use those. One of the two techniques listed previously can be
    used to force consumers to start using the new offsets from the mapping. This
    still has an issue with offset commits that arrived ahead of the records themselves
    and offset commits that didn’t get mirrored to the DR on time, but it covers some
    cases.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论镜像偏移主题时，最大的挑战之一是主要和 DR 集群中的偏移可能会发散。过去，一些组织选择使用外部数据存储，如 Apache Cassandra，来存储从一个集群到另一个集群的偏移映射。每当事件被生产到
    DR 集群时，当偏移发散时，镜像工具会将两个偏移发送到外部数据存储。如今，包括 MirrorMaker 在内的镜像解决方案使用 Kafka 主题来存储偏移翻译元数据。偏移存储在两个偏移之间的差异发生变化时。例如，如果主要偏移495映射到
    DR 集群的偏移500，我们将在外部存储或偏移翻译主题中记录(495,500)。如果由于重复而导致差异发生变化，偏移596映射到600，那么我们将记录新的映射(596,600)。我们不需要存储495到596之间的所有偏移映射；我们只是假设差异保持不变，因此主要集群中的偏移550将映射到
    DR 中的偏移555。然后当发生故障切换时，我们不是将时间戳（始终有点不准确）映射到偏移，而是将主要偏移映射到 DR 偏移并使用它们。之前列出的两种技术之一可以用于强制消费者开始使用映射的新偏移。这仍然存在偏移提交的问题，这些提交在记录本身之前到达，并且偏移提交未及时镜像到
    DR，但它涵盖了一些情况。
- en: After the failover
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障切换后
- en: Let’s say that failover was successful. Everything is working just fine on the
    DR cluster. Now we need to do something with the primary cluster. Perhaps turn
    it into a DR.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 假设故障切换成功了。 DR 集群上的一切都运行正常。现在我们需要对主要集群做些什么。也许把它变成一个 DR。
- en: 'It is tempting to simply modify the mirroring processes to reverse their direction
    and simply start mirroring from the new primary to the old one. However, this
    leads to two important questions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地修改镜像进程以颠倒其方向并从新的主要集群开始镜像到旧的主要集群是很诱人的。然而，这引发了两个重要问题：
- en: How do we know where to start mirroring? We need to solve the same problem we
    have for all our consumers for the mirroring application itself. And remember
    that all our solutions have cases where they either cause duplicates or miss data—sometimes
    both.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何知道从哪里开始镜像？我们需要解决与镜像应用程序本身的所有消费者相同的问题。请记住，我们所有的解决方案都有可能导致重复或丢失数据的情况——有时两者都有。
- en: In addition, for reasons we discussed previously, it is likely that your original
    primary will have events that the DR cluster does not. If you just start mirroring
    new data back, the extra history will remain and the two clusters will be inconsistent.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this reason, for scenarios where consistency and ordering guarantees are
    critical, the simplest solution is to first scrape the original cluster—delete
    all the data and committed offsets—and then start mirroring from the new primary
    back to what is now the new DR cluster. This gives you a clean slate that is identical
    to the new primary.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: A few words on cluster discovery
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the important points to consider when planning a standby cluster is that
    in the event of failover, your applications will need to know how to start communicating
    with the failover cluster. If you hardcoded the hostnames of your primary cluster
    brokers in the producer and consumer properties, this will be challenging. Most
    organizations keep it simple and create a DNS name that usually points to the
    primary brokers. In case of an emergency, the DNS name can be pointed to the standby
    cluster. The discovery service (DNS or other) doesn’t need to include all the
    brokers—Kafka clients only need to access a single broker successfully in order
    to get metadata about the cluster and discover the other brokers. So, including
    just three brokers is usually fine. Regardless of the discovery method, most failover
    scenarios do require bouncing consumer applications after failover so they can
    find the new offsets from which they need to start consuming. For automated failover
    without application restart to achieve very low RTO, failover logic should be
    built into client applications.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Stretch Clusters
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Active-standby architectures are used to protect the business against the failure
    of a Kafka cluster by moving applications to communicate with another cluster
    in case of cluster failure. Stretch clusters are intended to protect the Kafka
    cluster from failure during a datacenter outage. This is achieved by installing
    a single Kafka cluster across multiple datacenters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Stretch clusters are fundamentally different from other multidatacenter scenarios.
    To start with, they are not multicluster—it is just one cluster. As a result,
    we don’t need a mirroring process to keep two clusters in sync. Kafka’s normal
    replication mechanism is used, as usual, to keep all brokers in the cluster in
    sync. This setup can include synchronous replication. Producers normally receive
    an acknowledgment from a Kafka broker after the message was successfully written
    to Kafka. In the stretch cluster case, we can configure things so the acknowledgment
    will be sent after the message is written successfully to Kafka brokers in two
    datacenters. This involves using rack definitions to make sure each partition
    has replicas in multiple datacenters, and the use of `min.insync.replicas` and
    `acks=all` to ensure that every write is acknowledged from at least two datacenters.
    From 2.4.0 onward, brokers can also be configured to enable consumers to fetch
    from the closest replica using rack definitions. Brokers match their rack with
    that of the consumer to find the local replica that is most up-to-date, falling
    back to the leader if a suitable local replica is not available. Consumers fetching
    from followers in their local datacenter achieve higher throughput, lower latency,
    and lower cost by reducing cross-datacenter traffic.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of this architecture are in the synchronous replication—some
    types of business simply require that their DR site is always 100% synchronized
    with the primary site. This is often a legal requirement and is applied to any
    data store across the company—Kafka included. The other advantage is that both
    datacenters and all brokers in the cluster are used. There is no waste like we
    saw in active-standby architectures.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This architecture is limited in the type of disasters it protects against. It
    only protects from datacenter failures, not any kind of application or Kafka failures.
    The operational complexity is also limited. This architecture demands physical
    infrastructure that not all companies can provide.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构在其保护的灾难类型方面存在局限性。它只能保护数据中心的故障，而不能保护任何类型的应用程序或Kafka故障。操作复杂性也受到限制。这种架构需要物理基础设施，而并非所有公司都能提供。
- en: This architecture is feasible if you can install Kafka (and ZooKeeper) in at
    least three datacenters with high bandwidth and low latency between them. This
    can be done if your company owns three buildings on the same street, or—more commonly—by
    using three availability zones inside one region of your cloud provider.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的公司拥有三栋建筑位于同一条街上，或者更常见的是，在云服务提供商的一个区域内使用三个可用区，那么在至少三个数据中心中安装Kafka（和ZooKeeper）是可行的，且它们之间具有高带宽和低延迟。
- en: The reason three datacenters are important is because ZooKeeper requires an
    uneven number of nodes in a cluster and will remain available if a majority of
    the nodes are available. With two datacenters and an uneven number of nodes, one
    datacenter will always contain a majority, which means that if this datacenter
    is unavailable, ZooKeeper is unavailable, and Kafka is unavailable. With three
    datacenters, you can easily allocate nodes so no single datacenter has a majority.
    So, if one datacenter is unavailable, a majority of nodes exist in the other two
    datacenters, and the ZooKeeper cluster will remain available. Therefore, so will
    the Kafka cluster.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 三个数据中心之所以重要，是因为ZooKeeper在集群中需要一个不均匀数量的节点，并且如果大多数节点可用，它将保持可用。有了两个数据中心和不均匀数量的节点，一个数据中心将始终包含大多数节点，这意味着如果这个数据中心不可用，ZooKeeper就不可用，Kafka也不可用。有了三个数据中心，您可以轻松地分配节点，以便没有一个单一的数据中心拥有大多数节点。因此，如果一个数据中心不可用，大多数节点存在于其他两个数据中心中，ZooKeeper集群将保持可用。因此，Kafka集群也将保持可用。
- en: 2.5 DC Architecture
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.5 DC架构
- en: A popular model for stretch clusters is a 2.5 DC (datacenter) architecture with
    both Kafka and ZooKeeper running in two datacenters, and a third “0.5” datacenter
    with one ZooKeeper node to provide quorum if a datacenter fails.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的拉伸集群模型是2.5 DC（数据中心）架构，其中Kafka和ZooKeeper都在两个数据中心运行，并且第三个“0.5”数据中心有一个ZooKeeper节点，以提供如果一个数据中心失败的话，提供法定人数。
- en: It is possible to run ZooKeeper and Kafka in two datacenters using a ZooKeeper
    group configuration that allows for manual failover between two datacenters. However,
    this setup is uncommon.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在两个数据中心中运行ZooKeeper和Kafka，使用ZooKeeper组配置允许在两个数据中心之间进行手动故障转移。然而，这种设置并不常见。
- en: Apache Kafka’s MirrorMaker
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Kafka的MirrorMaker
- en: Apache Kafka contains a tool called MirrorMaker for mirroring data between two
    datacenters. Early versions of MirrorMaker used a collection of consumers that
    were members of a consumer group to read data from a set of source topics and
    a shared Kafka producer in each MirrorMaker process to send those events to the
    destination cluster. While this was sufficient to mirror data across clusters
    in some scenarios, it had several issues, particularly latency spikes as configuration
    changes and addition of new topics resulted in stop-the-world rebalances. MirrorMaker
    2.0 is the next-generation multicluster mirroring solution for Apache Kafka that
    is based on the Kafka Connect framework, overcoming many of the shortcomings of
    its predecessor. Complex topologies can be easily configured to support a wide
    range of use cases like disaster recovery, backup, migration, and data aggregation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka包含一个名为MirrorMaker的工具，用于在两个数据中心之间镜像数据。早期版本的MirrorMaker使用了一个消费者组的集合，这些消费者是消费者组的成员，用于从一组源主题中读取数据，并且每个MirrorMaker进程中都有一个共享的Kafka生产者，用于将这些事件发送到目标集群。虽然这足以在某些情况下在集群之间镜像数据，但它存在一些问题，特别是随着配置更改和新主题的添加导致的延迟峰值，会导致停止世界的重新平衡。MirrorMaker
    2.0是基于Kafka Connect框架的下一代多集群镜像解决方案，克服了其前身的许多缺点。可以轻松配置复杂的拓扑结构，以支持灾难恢复、备份、迁移和数据聚合等各种用例。
- en: More about MirrorMaker
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于MirrorMaker的更多信息
- en: MirrorMaker sounds very simple, but because we were trying to be very efficient
    and get very close to exactly-once delivery, it turned out to be tricky to implement
    correctly. MirrorMaker has been rewritten multiple times. The description here
    and the details in the following sections apply to MirrorMaker 2.0, which was
    introduced in 2.4.0.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MirrorMaker听起来很简单，但因为我们试图非常高效并且非常接近精确一次交付，所以正确实现它变得棘手。MirrorMaker已经被多次重写。这里的描述和以下部分的细节适用于MirrorMaker
    2.0，该版本在2.4.0中引入。
- en: MirrorMaker uses a source connector to consume data from another Kafka cluster
    rather than from a database. Use of the Kafka Connect framework minimizes administration
    overhead for busy enterprise IT departments. If you recall the Kafka Connect architecture
    from [Chapter 9](ch09.html#building_data_pipelines), you remember that each connector
    divides the work among a configurable number of tasks. In MirrorMaker, each task
    is a consumer and a producer pair. The Connect framework assigns those tasks to
    different Connect worker nodes as needed—so you may have multiple tasks on one
    server or have the tasks spread out to multiple servers. This replaces the manual
    work of figuring out how many MirrorMaker streams should run per instance and
    how many instances per machine. Connect also has a REST API to centrally manage
    the configuration for the connectors and tasks. If we assume that most Kafka deployments
    include Kafka Connect for other reasons (sending database change events into Kafka
    is a very popular use case), then by running MirrorMaker inside Connect, we can
    cut down on the number of clusters we need to manage.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: MirrorMaker allocates partitions to tasks evenly without using Kafka’s consumer
    group-management protocol to avoid latency spikes due to rebalances when new topics
    or partitions are added. Events from each partition in the source cluster are
    mirrored to the same partition in the target cluster, preserving semantic partitioning
    and maintaining ordering of events for each partition. If new partitions are added
    to source topics, they are automatically created in the target topic. In addition
    to data replication, MirrorMaker also supports migration of consumer offsets,
    topic configuration, and topic ACLs, making it a complete mirroring solution for
    multicluster deployments. A replication flow defines the configuration of a directional
    flow from a source cluster to a target cluster. Multiple replication flows can
    be defined for MirrorMaker to define complex topologies, including the architectural
    patterns we discussed earlier like hub-and-spoke, active-standby, and active-active
    architectures. [Figure 10-6](#fig0806) shows the use of MirrorMaker in an active-standby
    architecture.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1006](assets/kdg2_1006.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. The MirrorMaker process in Kafka
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Configuring MirrorMaker
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MirrorMaker is highly configurable. In addition to the cluster settings to define
    the topology, Kafka Connect, and connector settings, every configuration property
    of the underlying producer, consumers, and admin client used by MirrorMaker can
    be customized. We will show a few examples here and highlight some of the important
    configuration options, but exhaustive documentation of MirrorMaker is outside
    our scope.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, let’s take a look at a MirrorMaker example. The following
    command starts MirrorMaker with the configuration options specified in the properties
    file:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s look at some of MirrorMaker’s configuration options:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Replication flow
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the configuration options for setting up an active-standby
    replication flow between two datacenters in New York and London:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_cross_cluster_data_mirroring_CO1-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Define aliases for the clusters used in replication flows.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cross_cluster_data_mirroring_CO1-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Configure bootstrap for each cluster, using the cluster alias as the prefix.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_cross_cluster_data_mirroring_CO1-3)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Enable replication flow between a pair of clusters using the prefix `source​-⁠>target`.
    All configuration options for this flow use the same prefix.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_cross_cluster_data_mirroring_CO1-4)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Configure the topics to be mirrored for this replication flow.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Mirror topics
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the example, for each replication flow, a regular expression may
    be specified for the topic names that will be mirrored. In this example, we chose
    to replicate every topic, but it is often good practice to use something like
    *prod.** and avoid replicating test topics. A separate topic exclusion list containing
    topic names or patterns like *test.** may also be specified to exclude topics
    that don’t require mirroring. Target topic names are automatically prefixed with
    the source cluster alias by default. For example, in active-active architecture,
    MirrorMaker replicating topics from an NYC datacenter to a LON datacenter will
    mirror the topic *orders* from NYC to the topic *NYC.orders* in LON. This default
    naming strategy prevents replication cycles resulting in events being endlessly
    mirrored between the two clusters in active-active mode if topics are mirrored
    from NYC to LON as well as LON to NYC. The distinction between local and remote
    topics also supports aggregation use cases since consumers may choose subscription
    patterns to consume data produced from just the local region or subscribe to topics
    from all regions to get the complete dataset.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: MirrorMaker periodically checks for new topics in the source cluster and starts
    mirroring these topics automatically if they match the configured patterns. If
    more partitions are added to the source topic, the same number of partitions is
    automatically added to the target topic, ensuring that events in the source topic
    appear in the same partitions in the same order in the target topic.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Consumer offset migration
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: MirrorMaker contains a utility class `RemoteClusterUtils` to enable consumers
    to seek to the last checkpointed offset in a DR cluster with offset translation
    when failing over from a primary cluster. Support for periodic migration of consumer
    offsets was added in 2.7.0 to automatically commit translated offsets to the target
    `__consumer_offsets` topic so that consumers switching to a DR cluster can restart
    from where they left off in the primary cluster with no data loss and minimal
    duplicate processing. Consumer groups for which offsets are migrated can be customized,
    and for added protection, MirrorMaker does not overwrite offsets if consumers
    on the target cluster are actively using the target consumer group, thus avoiding
    any accidental conflicts.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Topic configuration and ACL migration
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In addition to mirroring data records, MirrorMaker may be configured to mirror
    topic configuration and access control lists (ACLs) of the topics to retain the
    same behavior for the mirrored topic. The default configuration enables this migration
    with reasonable periodic refresh intervals that may be sufficient in most cases.
    Most of the topic configuration settings from the source are applied to the target
    topic, but a few like `min.insync.replicas` are not applied by default. The list
    of excluded configs can be customized.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Only literal topic ACLs that match topics being mirrored are migrated, so if
    you are using prefixed or wildcard ACLs or alternative authorization mechanisms,
    you will need to configure those on the target cluster explicitly. ACLs for `Topic:Write`
    are not migrated to ensure that only MirrorMaker is allowed to write to the target
    topic. Appropriate access must be explicitly granted at the time of failover to
    ensure that applications work with the secondary cluster.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Connector tasks
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The configuration option `tasks.max` limits the maximum number of tasks that
    the connector associated with MirrorMaker may use. The default is 1, but a minimum
    of 2 is recommended. When replicating a lot of topic partitions, higher values
    should be used if possible to increase parallelism.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Configuration prefixes
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'MirrorMaker supports customization of configuration options for all its components,
    including connectors, producers, consumers, and admin clients. Kafka Connect and
    connector configs can be specified without any prefix. But since MirrorMaker configuration
    can include configuration for multiple clusters, prefixes can be used to specify
    cluster-specific configs or configs for a particular replication flow. As we saw
    in the example earlier, clusters are identified using aliases that are used as
    a configuration prefix for options related to that cluster. Prefixes can be used
    to build a hierarchical configuration, with the more specific prefixed configuration
    having higher precedence than the less specific or nonprefixed configuration.
    MirrorMaker uses the following prefixes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '{cluster}.{connector_config}'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{cluster}.admin.{admin_config}'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{source_cluster}.consumer.{consumer_config}'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{target_cluster}.producer.{producer_config}'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{source_cluster}->{target_cluster}.{replication_flow_config}'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicluster Replication Topology
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen an example configuration for a simple active-standby replication
    flow for MirrorMaker. Now let’s look at extending the configuration to support
    other common architectural patterns.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Active-active topology between New York and London can be configured by enabling
    replication flow in both directions. In this case, even though all topics from
    NYC are mirrored to LON and vice versa, MirrorMaker ensures that the same event
    isn’t constantly mirrored back and forth between the pair of clusters since remote
    topics use the cluster alias as the prefix. It is good practice to use the same
    configuration file that contains the full replication topology for different MirrorMaker
    processes since it avoids conflicts when configs are shared using the internal
    configs topic in the target datacenter. MirrorMaker processes can be started in
    the target datacenter using the shared configuration file by specifying the target
    cluster when starting the MirrorMaker process using the `--clusters` option:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_cross_cluster_data_mirroring_CO2-1)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Enable replication from New York to London.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cross_cluster_data_mirroring_CO2-2)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Specify topics that are replicated from New York to London.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_cross_cluster_data_mirroring_CO2-3)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Enable replication from London to New York.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_cross_cluster_data_mirroring_CO2-4)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Specify topics that are replicated from London to New York.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'More replication flows with additional source or target clusters can also be
    added to the topology. For example, we can extend the configuration to support
    the fan out from NYC to SF and LON by adding a new replication flow for SF:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Securing MirrorMaker
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For production clusters, it is important to ensure that all cross-datacenter
    traffic is secure. Options for securing Kafka clusters are described in [Chapter 11](ch11.html#securing_kafka).
    MirrorMaker must be configured to use a secure broker listener in both source
    and target clusters, and client-side security options for each cluster must be
    configured for MirrorMaker to enable it to establish authenticated connections.
    SSL should be used to encrypt all cross-datacenter traffic. For example, the following
    configuration may be used to configure credentials for MirrorMaker:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_cross_cluster_data_mirroring_CO3-1)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Security protocol should match that of the broker listener corresponding to
    the bootstrap servers specified for the cluster. `SSL` or `SASL_SSL` is recommended.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cross_cluster_data_mirroring_CO3-2)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Credentials for MirrorMaker are specified here using JAAS configuration since
    SASL is used. For SSL, keystores should be specified if mutual client authentication
    is enabled.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The principal associated with MirrorMaker must also be granted appropriate
    permissions on the source and target clusters if authorization is enabled on the
    clusters. ACLs must be granted for the MirrorMaker process for:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '`Topic:Read` on the source cluster to consume from source topics; `Topic:Create`
    and `Topic:Write` on the target cluster to create and produce to target topics.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Topic:DescribeConfigs` on the source cluster to obtain source topic configuration;
    `Topic:AlterConfigs` on the target cluster to update target topic configuration.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Topic:Alter` on the target cluster to add partitions if new source partitions
    are detected.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Group:Describe` on the source cluster to obtain source consumer group metadata,
    including offsets; `Group:Read` on the target cluster to commit offsets for those
    groups in the target cluster.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cluster:Describe` on the source cluster to obtain source topic ACLs; `Cluster:Alter`
    on the target cluster to update the target topic ACLs.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Topic:Create` and `Topic:Write` permissions for internal MirrorMaker topics
    in the source and target clusters.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying MirrorMaker in Production
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we started MirrorMaker in dedicated mode on the command
    line. You can start any number of these processes to form a dedicated MirrorMaker
    cluster that is scalable and fault-tolerant. The processes mirroring to the same
    cluster will find each other and balance load between them automatically. Usually
    when running MirrorMaker in a production environment, you will want to run MirrorMaker
    as a service, running in the background with `nohup` and redirecting its console
    output to a log file. The tool also has `-daemon` as a command-line option that
    should do that for you. Most companies that use MirrorMaker have their own startup
    scripts that also include the configuration parameters they use. Production deployment
    systems like Ansible, Puppet, Chef, and Salt are often used to automate deployment
    and manage the many configuration options. MirrorMaker may also be run inside
    a Docker container. MirrorMaker is completely stateless and doesn’t require any
    disk storage (all the data and state are stored in Kafka itself).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Since MirrorMaker is based on Kafka Connect, all deployment modes of Connect
    can be used with MirrorMaker. Standalone mode may be used for development and
    testing where MirrorMaker runs as a standalone Connect worker on a single machine.
    MirrorMaker may also be run as a connector in an existing distributed Connect
    cluster by explicitly configuring the connectors. For production use, we recommend
    running MirrorMaker in distributed mode either as a dedicated MirrorMaker cluster
    or in a shared distributed Connect cluster.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: If at all possible, run MirrorMaker at the target datacenter. So, if you are
    sending data from NYC to SF, MirrorMaker should run in SF and consume data across
    the US from NYC. The reason for this is that long-distance networks can be a bit
    less reliable than those inside a datacenter. If there is a network partition
    and you lose connectivity between the datacenters, having a consumer that is unable
    to connect to a cluster is much safer than a producer that can’t connect. If the
    consumer can’t connect, it simply won’t be able to read events, but the events
    will still be stored in the source Kafka cluster and can remain there for a long
    time. There is no risk of losing events. On the other hand, if the events were
    already consumed and MirrorMaker can’t produce them due to network partition,
    there is always a risk that these events will accidentally get lost by MirrorMaker.
    So, remote consuming is safer than remote producing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: When do you have to consume locally and produce remotely? The answer is when
    you need to encrypt the data while it is transferred between the datacenters but
    you don’t need to encrypt the data inside the datacenter. Consumers take a significant
    performance hit when connecting to Kafka with SSL encryption—much more so than
    producers. This is because use of SSL requires copying data for encryption, which
    means consumers no longer enjoy the performance benefits of the usual zero-copy
    optimization. And this performance hit also affects the Kafka brokers themselves.
    If your cross datacenter traffic requires encryption, but local traffic does not,
    then you may be better off placing MirrorMaker at the source datacenter, having
    it consume unencrypted data locally, and then producing it to the remote datacenter
    through an SSL encrypted connection. This way, the producer connects to Kafka
    with SSL but not the consumer, which doesn’t impact performance as much. If you
    use this consume locally and produce remotely approach, make sure MirrorMaker’s
    Connect producer is configured to never lose events by configuring it with `acks=all`
    and a sufficient number of retries. Also, configure MirrorMaker to fail fast using
    `errors.tolerance=none` when it fails to send events, which is typically safer
    to do than to continue and risk data loss. Note that newer versions of Java have
    significantly increased SSL performance, so producing locally and consuming remotely
    may be a viable option even with encryption.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Another case where we may need to produce remotely and consume locally is a
    hybrid scenario when mirroring from an on-premises cluster to a cloud cluster.
    Secure on-premises clusters are likely to be behind a firewall that doesn’t allow
    incoming connections from the cloud. Running MirrorMaker on premise allows all
    connections to be from on premises to the cloud.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'When deploying MirrorMaker in production, it is important to remember to monitor
    it as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect monitoring
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect provides a wide range of metrics to monitor different aspects
    like connector metrics to monitor connector status, source connector metrics to
    monitor throughout, and worker metrics to monitor rebalance delays. Connect also
    provides a REST API to view and manage connectors.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: MirrorMaker metrics monitoring
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In addition to metrics from Connect, MirrorMaker adds metrics to monitor mirroring
    throughput and replication latency. The replication latency metric `replication-latency-ms`
    shows the time interval between the record timestamp and the time at which the
    record was successfully produced to the target cluster. This is useful to detect
    if the target is not keeping up with the source in a timely manner. Increased
    latency during peak hours may be OK if there is sufficient capacity to catch up
    later, but sustained increase in latency may indicate insufficient capacity. Other
    metrics like `record-age-ms`, which shows the age of records at the time of replication,
    `byte-rate`, which shows replication throughout, and `checkpoint-latency-ms`,
    which shows offset migration latency, can also be very useful. MirrorMaker also
    emits periodic heartbeats by default, which can be used to monitor its health.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Lag monitoring
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: You will definitely want to know if the target cluster is falling behind the
    source. The lag is the difference in offsets between the latest message in the
    source Kafka cluster and the latest message in the target cluster. See [Figure 10-7](#fig0807).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1007](assets/kdg2_1007.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Monitoring the lag difference in offsets
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Figure 10-7](#fig0807), the last offset in the source cluster is 7, and
    the last offset in the target is 5—meaning there is a lag of 2 messages.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to track this lag, and neither is perfect:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Check the latest offset committed by MirrorMaker to the source Kafka cluster.
    You can use the `kafka-consumer-groups` tool to check for each partition MirrorMaker
    is reading— the offset of the last event in the partition, the last offset MirrorMaker
    committed, and the lag between them. This indicator is not 100% accurate because
    MirrorMaker doesn’t commit offsets all the time. It commits offsets every minute
    by default, so you will see the lag grow for a minute and then suddenly drop.
    In the diagram, the real lag is 2, but the `kafka-consumer-groups` tool will report
    a lag of 5 because MirrorMaker hasn’t committed offsets for more recent messages
    yet. LinkedIn’s Burrow monitors the same information but has a more sophisticated
    method to determine whether the lag represents a real problem, so you won’t get
    false alerts.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the latest offset read by MirrorMaker (even if it isn’t committed). The
    consumers embedded in MirrorMaker publish key metrics in JMX. One of them is the
    consumer maximum lag (over all the partitions it is consuming). This lag is also
    not 100% accurate because it is updated based on what the consumer read but doesn’t
    take into account whether the producer managed to send those messages to the destination
    Kafka cluster and whether they were acknowledged successfully. In this example,
    the MirrorMaker consumer will report a lag of 1 message rather than 2, because
    it already read message 6—even though the message wasn’t produced to the destination
    yet.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if MirrorMaker skips or drops messages, neither method will detect
    an issue because they just track the latest offset. [Confluent Control Center](https://oreil.ly/KnvVV)
    is a commercial tool that monitors message counts and checksums and closes this
    monitoring gap.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Producer and consumer metrics monitoring
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kafka Connect framework used by MirrorMaker contains a producer and a consumer.
    Both have many available metrics, and we recommend collecting and tracking them.
    The [Kafka documentation](http://bit.ly/2sMfZWf) lists all the available metrics.
    Here are a few metrics that are useful in tuning MirrorMaker performance:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Consumer
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch-size-avg`, `fetch-size-max`, `fetch-rate`, `fetch-throttle-time-avg`,
    and `fetch-throttle-time-max`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Producer
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '`batch-size-avg`, `batch-size-max`, `requests-in-flight`, and `record-retry-rate`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Both
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '`io-ratio` and `io-wait-ratio`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Canary
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: If you monitor everything else, a canary isn’t strictly necessary, but we like
    to add it in for multiple layers of monitoring. It provides a process that, every
    minute, sends an event to a special topic in the source cluster and tries to read
    the event from the destination cluster. It also alerts you if the event takes
    more than an acceptable amount of time to arrive. This can mean that MirrorMaker
    is lagging or that it isn’t available at all.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Tuning MirrorMaker
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MirrorMaker is horizontally scalable. Sizing of the MirrorMaker cluster depends
    on the throughput you need and the lag you can tolerate. If you can’t tolerate
    any lag, you have to size MirrorMaker with enough capacity to keep up with your
    top throughput. If you can tolerate some lag, you can size MirrorMaker to be 75–80%
    utilized 95–99% of the time. Then, expect some lag to develop when you are at
    peak throughput. Because MirrorMaker has spare capacity most of the time, it will
    catch up once the peak is over.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Then you want to measure the throughput you get from MirrorMaker with a different
    number of connector tasks—configured with the `tasks.max` parameter. This depends
    a lot on your hardware, datacenter, or cloud provider, so you will want to run
    your own tests. Kafka ships with the `kafka-performance-producer` tool. Use it
    to generate load on a source cluster and then connect MirrorMaker and start mirroring
    this load. Test MirrorMaker with 1, 2, 4, 8, 16, 24, and 32 tasks. Watch where
    performance tapers off and set `tasks.max` just below this point. If you are consuming
    or producing compressed events (recommended, since bandwidth is the main bottleneck
    for cross-datacenter mirroring), MirrorMaker will have to decompress and recompress
    the events. This uses a lot of CPU, so keep an eye on CPU utilization as you increase
    the number of tasks. Using this process, you will find the maximum throughput
    you can get with a single MirrorMaker worker. If it is not enough, you will want
    to experiment with additional workers. If you are running MirrorMaker on an existing
    Connect cluster with other connectors, make sure you also take the load from those
    connectors into account when sizing the cluster.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you may want to separate sensitive topics—those that absolutely
    require low latency and where the mirror must be as close to the source as possible—to
    a separate MirrorMaker cluster. This will prevent a bloated topic or an out-of-control
    producer from slowing down your most sensitive data pipeline.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty much all the tuning you can do to MirrorMaker itself. However,
    you can still increase the throughput of each task and each MirrorMaker worker.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running MirrorMaker across datacenters, tuning the TCP stack can
    help to increase the effective bandwidth. In Chapters [3](ch03.html#writing_messages_to_kafka)
    and [4](ch04.html#reading_data_from_kafka), we saw that TCP buffer sizes can be
    configured for producers and consumers using `send.buffer.bytes` and `receive.buffer.bytes`.
    Similarly, broker-side buffer sizes can be configured using `socket.send.buffer.bytes`
    and `socket.receive.buffer.bytes` on brokers. These configuration options should
    be combined with optimization of the network configuration in Linux, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Increase the TCP buffer size (`net.core.rmem_default`, `net.core.rmem_max`,
    `net.core.wmem_default`, `net.core.wmem_max`, and `net.core.optmem_max`)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable automatic window scaling (`sysctl –w net.ipv4.tcp_window_scaling=1 or
    add net.ipv4.tcp_window_scaling=1 to /etc/sysctl.conf`)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the TCP slow start time (set `/proc/sys/net/ipv4/tcp_slow_​start_after_idle`
    to `0`)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that tuning the Linux network is a large and complex topic. To understand
    more about these parameters and others, we recommend reading a network tuning
    guide such as *Performance Tuning for Linux Servers* by Sandra K. Johnson et al.
    (IBM Press).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you may want to tune the underlying producers and consumers of
    MirrorMaker. First, you will want decide whether the producer or the consumer
    is the bottleneck—is the producer waiting for the consumer to bring more data
    or the other way around? One way to decide is to look at the producer and consumer
    metrics you are monitoring. If one process is idle while the other is fully utilized,
    you know which one needs tuning. Another method is to do several thread dumps
    (using jstack) and see if the MirrorMaker threads are spending most of the time
    in poll or in send—more time spent polling usually means the consumer is the bottleneck,
    while more time spent sending shift points to the producer.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to tune the producer, the following configuration settings can
    be useful:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '`linger.ms` and `batch.size`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: If your monitoring shows that the producer consistently sends partially empty
    batches (i.e., `batch-size-avg` and `batch-size-max` metrics are lower than configured
    `batch.size`), you can increase throughput by introducing a bit of latency. Increase
    `linger.ms` and the producer will wait a few milliseconds for the batches to fill
    up before sending them. If you are sending full batches and have memory to spare,
    you can increase `batch.size` and send larger batches.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '`max.in.flight.requests.per.connection`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the number of in-flight requests to 1 is currently the only way for
    MirrorMaker to guarantee that message ordering is preserved if some messages require
    multiple retries before they are successfully acknowledged. But this means every
    request that was sent by the producer has to be acknowledged by the target cluster
    before the next message is sent. This can limit throughput, especially if there
    is significant latency before the brokers acknowledge the messages. If message
    order is not critical for your use case, using the default value of 5 for `max.in.flight.requests.per.connection`
    can significantly increase your throughput.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The following consumer configurations can increase throughput for the consumer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch.max.bytes`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: If the metrics you are collecting show that `fetch-size-avg` and `fetch-size-max`
    are close to the `fetch.max.bytes` configuration, the consumer is reading as much
    data from the broker as it is allowed. If you have available memory, try increasing
    `fetch.max.bytes` to allow the consumer to read more data in each request.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch.min.bytes` and `fetch.max.wait.ms`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: If you see in the consumer metrics that `fetch-rate` is high, the consumer is
    sending too many requests to the brokers and not receiving enough data in each
    request. Try increasing both `fetch.min.bytes` and `fetch.max.wait.ms` so the
    consumer will receive more data in each request and the broker will wait until
    enough data is available before responding to the consumer request.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Other Cross-Cluster Mirroring Solutions
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We looked in depth at MirrorMaker because this mirroring software arrives as
    part of Apache Kafka. However, MirrorMaker also has some limitations when used
    in practice. It is worthwhile to look at some of the alternatives to MirrorMaker
    and the ways they address MirrorMaker limitations and complexities. We describe
    a couple of open source solutions from Uber and LinkedIn and commercial solutions
    from Confluent.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Uber uReplicator
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uber ran legacy MirrorMaker at very large scale, and as the number of topics
    and partitions grew and the cluster throughput increased, it started running into
    several problems. As we saw earlier, the legacy MirrorMaker used consumers that
    were members of a single consumer group to consume from source topics. Adding
    MirrorMaker threads, adding MirrorMaker instances, bouncing MirrorMaker instances,
    or even adding new topics that match the regular expression used in the inclusion
    filter all caused consumers to rebalance. As we saw in [Chapter 4](ch04.html#reading_data_from_kafka),
    rebalancing stops all the consumers until new partitions can be assigned to each
    consumer. With a very large number of topics and partitions, this can take a while.
    This is especially true when using old consumers like Uber did. In some cases,
    this caused 5–10 minutes of inactivity, causing mirroring to fall behind and accumulate
    a large backlog of events to mirror, which can take a long time to recover from.
    This caused very high latency for consumers reading events from the destination
    cluster. To avoid rebalances when someone added a topic matching the topic inclusion
    filter, Uber decided to maintain a list of exact topic names to mirror instead
    of using a regular expression filter. But this was hard to maintain as all MirrorMaker
    instances had to be reconfigured and bounced to add a new topic. If not done correctly,
    this could result in endless rebalances as the consumers won’t be able to agree
    on the topics they subscribe to.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Given these issues, Uber decided to write its own MirrorMaker clone, called
    `uReplicator`. Uber decided to use Apache Helix as a central (but highly available)
    controller to manage the topic list and the partitions assigned to each uReplicator
    instance. Administrators use a REST API to add new topics to the list in Helix,
    and uReplicator is responsible for assigning partitions to the different consumers.
    To achieve this, Uber replaced the Kafka consumers used in MirrorMaker with a
    Kafka consumer Uber engineers wrote called Helix consumer. This consumer takes
    its partition assignment from the Apache Helix controller rather than as a result
    of an agreement between the consumers (see [Chapter 4](ch04.html#reading_data_from_kafka)
    for details on how this is done in Kafka). As a result, the Helix consumer can
    avoid rebalances and instead listen to changes in the assigned partitions that
    arrive from Helix.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Uber engineering wrote a [blog post](https://oreil.ly/SGItx) describing the
    architecture in more detail and showing the improvements they experienced. uReplicator’s
    dependency on Apache Helix introduces a new component to learn and manage, adding
    complexity to any deployment. As we saw earlier, MirrorMaker 2.0 solves many of
    these scalability and fault-tolerance issues of legacy MirrorMaker without any
    external dependencies.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: LinkedIn Brooklin
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like Uber, LinkedIn was also using legacy MirrorMaker for transferring data
    between Kafka clusters. As the scale of the data grew, it also ran into similar
    scalability issues and operational challenges. So LinkedIn built a mirroring solution
    on top of its data streaming system called Brooklin. Brooklin is a distributed
    service that can stream data between different heterogeneous data source and target
    systems, including Kafka. As a generic data ingestion framework that can be used
    to build data pipelines, Brooklin supports multiple use cases:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Data bridge to feed data into stream processing systems from different data
    sources
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stream change data capture (CDC) events from different data stores
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-cluster mirroring solution for Kafka
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brooklin is a scalable distributed system designed for high reliability and
    has been tested with Kafka at scale. It is used to mirror trillions of messages
    a day and has been optimized for stability, performance, and operability. Brooklin
    comes with a REST API for management operations. It is a shared service that can
    process a large number of data pipelines, enabling the same service to mirror
    data across multiple Kafka clusters.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Confluent Cross-Datacenter Mirroring Solutions
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the same time that Uber developed its uReplicator, Confluent independently
    developed Confluent Replicator. Despite the similarities in names, the projects
    have almost nothing in common—they are different solutions to two different sets
    of MirrorMaker problems. Like MirrorMaker 2.0, which came later, Confluent’s Replicator
    is based on the Kafka Connect framework and was developed to address issues its
    enterprise customers encountered when using legacy MirrorMaker to manage their
    multicluster deployments.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: For customers who use stretch clusters for their operational simplicity and
    low RTO and RPO, Confluent added Multi-Region Cluster (MRC) as a built-in feature
    of Confluent Server, which is a commercial component of the Confluent Platform.
    MRC extends Kafka’s support for stretch clusters using asynchronous replicas to
    limit impact on latency and throughput. Like stretch clusters, this is suitable
    for replication between availability zones or regions with latencies less than
    50 ms and benefits from transparent client failover. For distant clusters with
    less reliable networks, a new built-in feature called Cluster Linking was added
    to Confluent Server more recently. Cluster Linking extends Kafka’s offset-preserving
    intra-cluster replication protocol to mirror data between clusters.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the features supported by each of these solutions:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Confluent Replicator
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Confluent Replicator is a mirroring tool similar to MirrorMaker that relies
    on the Kafka Connect framework for cluster management and can run on existing
    Connect clusters. Both support data replication for different topologies as well
    as migration of consumer offsets and topic configuration. There are some differences
    in features between the two. For example, MirrorMaker supports ACL migration and
    offset translation for any client, but Replicator doesn’t migrate ACLs and supports
    offset translation (using timestamp interceptor) only for Java clients. Replicator
    doesn’t have the concept of local and remote topics like MirrorMaker, but it supports
    aggregate topics. Like MirrorMaker, Replicator also avoids replication cycles
    but does so using provenance headers. Replicator provides a range of metrics,
    like replication lag, and can be monitored using its REST API or Control Center
    UI. It also supports schema migration between clusters and can perform schema
    translation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Region Clusters (MRC)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We saw earlier that stretch clusters provide simple transparent failover and
    failback for clients without the need for offset translation or client restarts.
    But stretch clusters require datacenters to be close to each other and provide
    a stable low-latency network to enable synchronous replication between datacenters.
    MRC is also suitable only for datacenters within a 50 ms latency, but it uses
    a combination of synchronous and asynchronous replication to limit impact on producer
    performance and provide higher network tolerance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, Apache Kafka supports fetching from followers to enable clients
    to fetch from their closest brokers based on rack ID, thereby reducing cross-datacenter
    traffic. Confluent Server also adds the concept of *observers*, which are asynchronous
    replicas that do not join the ISR and hence have no impact on producers using
    `acks=all` but are able to deliver records to consumers. Operators can configure
    synchronous replication within a region and asynchronous replication between regions
    to benefit from both low latency and high durability at the same time. Replica
    placement constraints in Confluent Server allow you to specify a minimum number
    of replicas per region using rack IDs to ensure that replicas are spread across
    regions to guarantee durability. Confluent Platform 6.1 also adds automatic observer
    promotion with configurable criteria, enabling fast failover without data loss
    automatically. When `min.insync.replicas` falls below a configured minimum number
    of synchronous replicas, observers that have caught up are automatically promoted
    to allow them to join ISRs, bringing the number of ISRs back up to the required
    minimum. The promoted observers use synchronous replication and may impact throughput,
    but the cluster remains operational throughout without data loss even if a region
    fails. When the failed region recovers, observers are automatically demoted, getting
    the cluster back to normal performance levels.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Linking
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Linking, introduced as a preview feature in Confluent Platform 6.0,
    builds inter-cluster replication directly into the Confluent Server. By using
    the same protocol as inter-broker replication within a cluster, Cluster Linking
    performs offset-preserving replication across clusters, enabling seamless migration
    of clients without any need for offset translation. Topic configuration, partitions,
    consumer offsets, and ACLs are all kept synchronized between the two clusters
    to enable failover with low RTO if a disaster occurs. A cluster link defines the
    configuration of a directional flow from a source cluster to a destination cluster.
    Leader brokers of mirror partitions in the destination cluster fetch partition
    data from the corresponding source leaders, while followers in the destination
    replicate from their local leader using the standard replication mechanism in
    Kafka. Mirror topics are marked as read-only in the destination to prevent any
    local produce to these topics, ensuring that mirror topics are logically identical
    to their source topic.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Linking provides operational simplicity without the need for separate
    clusters like Connect clusters and is more performant than external tools since
    it avoids decompression and recompression during mirroring. Unlike MRC, there
    is no option for synchronous replication, and client failover is a manual process
    that requires client restart. But Cluster Linking may be used with distant datacenters
    with unreliable high-latency networks and reduces cross-datacenter traffic by
    replicating only once between datacenters. It is suitable for cluster migration
    and topic sharing use cases.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by describing the reasons you may need to manage more
    than a single Kafka cluster and then proceeded to describe several common multicluster
    architectures, ranging from the simple to the very complex. We went into the details
    of implementing failover architecture for Kafka and compared the different options
    currently available. Then we proceeded to discuss the available tools. Starting
    with Apache Kafka’s MirrorMaker, we went into many details of using it in production.
    We finished by reviewing alternative options that solve some of the issues you
    might encounter with MirrorMaker.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Whichever architecture and tools you end up using, remember that multicluster
    configuration and mirroring pipelines should be monitored and tested just like
    everything else you take into production. Because multicluster management in Kafka
    can be easier than it is with relational databases, some organizations treat it
    as an afterthought and neglect to apply proper design, planning, testing, deployment
    automation, monitoring, and maintenance. By taking multicluster management seriously,
    preferably as part of a holistic disaster or geodiversity plan for the entire
    organization that involves multiple applications and data stores, you will greatly
    increase the chances of successfully managing multiple Kafka clusters.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL

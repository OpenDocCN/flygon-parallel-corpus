- en: Chapter 11\. Securing Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is used for a variety of use cases ranging from website activity tracking
    and metrics pipelines to patient record management and online payments. Each use
    case has different requirements in terms of security, performance, reliability,
    and availability. While it is always preferable to use the strongest and latest
    security features available, trade-offs are often necessary since increased security
    impacts performance, cost, and user experience. Kafka supports several standard
    security technologies with a range of configuration options to tailor security
    to each use case.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Like performance and reliability, security is an aspect of the system that must
    be addressed for the system as a whole, rather than component by component. The
    security of a system is only as strong as the weakest link, and security processes
    and policies must be enforced across the system, including the underlying platform.
    The customizable security features in Kafka enable integration with existing security
    infrastructure to build a consistent security model that applies to the entire
    system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the security features in Kafka and see how
    they address different aspects of security and contribute toward the overall security
    of the Kafka installation. Throughout the chapter, we will share best practices,
    potential threats, and techniques to mitigate these threats. We will also review
    additional measures that can be adopted to secure ZooKeeper and the rest of the
    platform.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Locking Down Kafka
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka uses a range of security procedures to establish and maintain confidentiality,
    integrity, and availability of data:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Authentication establishes your identity and determines *who* you are.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization determines *what* you are allowed to do.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption protects your data from eavesdropping and tampering.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditing tracks what you have done or have attempted to do.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quotas control how much resources you can utilize.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand how to lock down a Kafka deployment, let’s first look at how data
    flows through a Kafka cluster. [Figure 11-1](#fig-1-secure-flow) shows the main
    steps in an example data flow. In this chapter, we will use this example flow
    to examine the different ways in which Kafka can be configured to protect data
    at every step to guarantee security of the entire deployment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1101](assets/kdg2_1101.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Data flow in a Kafka cluster
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Alice produces a customer order record to a partition of the topic named `customerOrders`.
    The record is sent to the leader of the partition.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The leader broker writes the record to its local log file.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A follower broker fetches the message from the leader and writes to its local
    replica log file.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The leader broker updates the partition state in ZooKeeper to update in-sync
    replicas, if required.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bob consumes customer order records from the topic `customerOrders`. Bob receives
    the record produced by Alice.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An internal application processes all messages arriving in `customerOrders`
    to produce real-time metrics on popular products.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A secure deployment must guarantee:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Client authenticity
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: When Alice establishes a client connection to the broker, the broker should
    authenticate the client to ensure that the message is really coming from Alice.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Server authenticity
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Before sending a message to the leader broker, Alice’s client should verify
    that the connection is to the real broker.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: All connections where the message flows, as well as all disks where messages
    are stored, should be encrypted or physically secured to prevent eavesdroppers
    from reading the data and to ensure that data cannot be stolen.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Message digests should be included for data transmitted over insecure networks
    to detect tampering.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Access control
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Before writing the message to the log, the leader broker should verify that
    Alice is authorized to write to `customerOrders`. Before returning messages to
    Bob’s consumer, the broker should verify that Bob is authorized to read from the
    topic. If Bob’s consumer uses group management, the broker should also verify
    that Bob has access to the consumer group.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在将消息写入日志之前，领导经纪人应验证Alice是否有权写入`customerOrders`。在将消息返回给Bob的消费者之前，经纪人应验证Bob是否有权从主题中读取消息。如果Bob的消费者使用组管理，则经纪人还应验证Bob是否有权访问消费者组。
- en: Auditability
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 审计性
- en: An audit trail that shows all operations that were performed by brokers, Alice,
    Bob, and other clients should be logged.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 应记录经纪人、Alice、Bob和其他客户执行的所有操作的审计跟踪。
- en: Availability
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性
- en: Brokers should apply quotas and limits to avoid some users hogging all the available
    bandwidth or overwhelming the broker with denial-of-service attacks. ZooKeeper
    should be locked down to ensure availability of the Kafka cluster since broker
    availability is dependent on ZooKeeper availability and the integrity of metadata
    stored in ZooKeeper.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 经纪人应该应用配额和限制，以避免一些用户占用所有可用带宽或用拒绝服务攻击压倒经纪人。应该锁定ZooKeeper以确保Kafka集群的可用性，因为经纪人的可用性取决于ZooKeeper的可用性和ZooKeeper中存储的元数据的完整性。
- en: In the following sections, we explore the Kafka security features that can be
    used to provide these guarantees. We first introduce the Kafka connection model
    and the security protocols associated with connections from clients to Kafka brokers.
    We then look at each security protocol in detail and examine the authentication
    capabilities of each protocol to ascertain client authenticity and server authenticity.
    We review options for encryption at different stages, including built-in encryption
    of data in transit in some security protocols to address data privacy and data
    integrity. Then, we explore customizable authorization in Kafka to manage access
    control and the main logs that contribute to auditability. Finally, we review
    security for the rest of the system, including ZooKeeper and the platform, which
    is necessary to maintain availability. For details on quotas that contribute to
    service availability through fair allocation of resources among users, refer to
    [Chapter 3](ch03.html#writing_messages_to_kafka).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的章节中，我们将探讨Kafka安全功能，这些功能可用于提供这些保证。我们首先介绍Kafka连接模型以及与客户端到Kafka经纪人的连接相关的安全协议。然后，我们详细查看每个安全协议，并检查每个协议的身份验证能力，以确定客户端真实性和服务器真实性。我们审查了不同阶段的加密选项，包括某些安全协议中数据在传输过程中的内置加密，以解决数据隐私和数据完整性问题。然后，我们探讨了Kafka中可定制的授权，以管理访问控制和有助于审计的主要日志。最后，我们审查了系统的其他安全性，包括ZooKeeper和必须维护可用性的平台。有关配额的详细信息，配额有助于通过在用户之间公平分配资源来提供服务的可用性，请参阅[第3章](ch03.html#writing_messages_to_kafka)。 '
- en: Security Protocols
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全协议
- en: Kafka brokers are configured with listeners on one or more endpoints and accept
    client connections on these listeners. Each listener can be configured with its
    own security settings. Security requirements on a private internal listener that
    is physically protected and only accessible to authorized personnel may be different
    from the security requirements of an external listener accessible over the public
    internet. The choice of security protocol determines the level of authentication
    and encryption of data in transit.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka经纪人配置了一个或多个端点的侦听器，并在这些侦听器上接受客户端连接。每个侦听器可以配置自己的安全设置。在物理上受保护并且只对授权人员可访问的私有内部侦听器的安全要求可能与可通过公共互联网访问的外部侦听器的安全要求不同。安全协议的选择确定了数据在传输过程中的身份验证和加密级别。
- en: 'Kafka supports four security protocols using two standard technologies, TLS
    and SASL. Transport Layer Security (TLS), commonly referred to by the name of
    its predecessor, Secure Sockets Layer (SSL), supports encryption as well as client
    and server authentication. Simple Authentication and Security Layer (SASL) is
    a framework for providing authentication using different mechanisms in connection-oriented
    protocols. Each Kafka security protocol combines a transport layer (PLAINTEXT
    or SSL) with an optional authentication layer (SSL or SASL):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用两种标准技术支持四种安全协议，即TLS和SASL。传输层安全性（TLS），通常称为其前身安全套接字层（SSL），支持加密以及客户端和服务器身份验证。简单认证和安全层（SASL）是提供使用不同机制进行身份验证的框架，用于连接导向的协议。每个Kafka安全协议都将传输层（PLAINTEXT或SSL）与可选的身份验证层（SSL或SASL）结合在一起：
- en: PLAINTEXT
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PLAINTEXT
- en: PLAINTEXT transport layer with no authentication. Is suitable only for use within
    private networks for processing data that is not sensitive since no authentication
    or encryption is used.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PLAINTEXT传输层，无身份验证。仅适用于私有网络中处理非敏感数据，因为没有使用身份验证或加密。
- en: SSL
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: SSL
- en: SSL transport layer with optional SSL client authentication. Is suitable for
    use in insecure networks since client and server authentication as well as encryption
    are supported.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SSL传输层，带有可选的SSL客户端身份验证。适用于在不安全的网络中使用，因为支持客户端和服务器身份验证以及加密。
- en: SASL_PLAINTEXT
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: SASL_PLAINTEXT
- en: PLAINTEXT transport layer with SASL client authentication. Some SASL mechanisms
    also support server authentication. Does not support encryption and hence is suitable
    only for use within private networks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: PLAINTEXT传输层，带有SASL客户端身份验证。一些SASL机制也支持服务器身份验证。不支持加密，因此仅适用于私有网络中使用。
- en: SASL_SSL
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SASL_SSL
- en: SSL transport layer with SASL authentication. Is suitable for use in insecure
    networks since client and server authentication as well as encryption are supported.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SSL传输层，带有SASL身份验证。适用于在不安全的网络中使用，因为支持客户端和服务器身份验证以及加密。
- en: TLS/SSL
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TLS/SSL
- en: TLS is one of the most widely used cryptographic protocols on the public internet.
    Application protocols like HTTP, SMTP, and FTP rely on TLS to provide privacy
    and integrity of data in transit. TLS relies on a Public Key Infrastructure (PKI)
    to create, manage, and distribute digital certificates that can be used for asymmetric
    encryption, avoiding the need for distributing shared secrets between servers
    and clients. Session keys generated during the TLS handshake enable symmetric
    encryption with higher performance for subsequent data transfer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'The listener used for inter-broker communication can be selected by configuring
    `inter.broker.listener.name` or `security.inter.broker.protocol`. Both server-side
    and client-side configuration options must be provided in the broker configuration
    for the security protocol used for inter-broker communication. This is because
    brokers need to establish client connections for that listener. The following
    example configures SSL for the inter-broker and internal listeners, and SASL_SSL
    for the external listener:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Clients are configured with a security protocol and bootstrap servers that
    determine the broker listener. Metadata returned to clients contains only the
    endpoints corresponding to the same listener as the bootstrap servers:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the next section on authentication, we review the protocol-specific configuration
    options for brokers and clients for each security protocol.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authentication is the process of establishing the identity of the client and
    server to verify client authenticity and server authenticity. When Alice’s client
    connects to the leader broker to produce a customer order record, server authentication
    enables the client to establish that the server that the client is talking to
    is the actual broker. Client authentication verifies Alice’s identity by validating
    Alice’s credentials, like a password or digital certificate, to determine that
    the connection is from Alice and not an impersonator. Once authenticated, Alice’s
    identity is associated with the connection throughout the lifetime of the connection.
    Kafka uses an instance of `KafkaPrincipal` to represent client identity and uses
    this principal to grant access to resources and allocate quotas for connections
    with that client identity. The `KafkaPrincipal` for each connection is established
    during authentication based on the authentication protocol. For example, the principal
    `User:Alice` may be used for Alice based on the username provided for password-based
    authentication. `KafkaPrincipal` may be customized by configuring `principal.builder.class`
    for brokers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Anonymous Connections
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The principal `User:ANONYMOUS` is used for unauthenticated connections. This
    includes clients on PLAINTEXT listeners as well as unauthenticated clients on
    SSL listeners.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: SSL
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When Kafka is configured with SSL or SASL_SSL as the security protocol for a
    listener, TLS is used as the secure transport layer for connections on that listener.
    When a connection is established over TLS, the TLS handshake process performs
    authentication, negotiates cryptographic parameters, and generates shared keys
    for encryption. The server’s digital certificate is verified by the client to
    establish the identity of the server. If client authentication using SSL is enabled,
    the server also verifies the client’s digital certificate to establish the identity
    of the client. All traffic over SSL is encrypted, making it suitable for use in
    insecure networks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: SSL Performance
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSL channels are encrypted and hence introduce a noticeable overhead in terms
    of CPU usage. Zero-copy transfer is currently not supported for SSL. Depending
    on the traffic pattern, the overhead may be up to 20–30%.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Configuring TLS
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When TLS is enabled for a broker listener using SSL or SASL_SSL, brokers should
    be configured with a key store containing the broker’s private key and certificate,
    and clients should be configured with a trust store containing the broker certificate
    or the certificate of the certificate authority (CA) that signed the broker certificate.
    Broker certificates should contain the broker hostname as a Subject Alternative
    Name (SAN) extension or as the Common Name (CN) to enable clients to verify the
    server hostname. Wildcard certificates can be used to simplify administration
    by using the same key store for all brokers in a domain.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Server Hostname verification
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Kafka clients verify that the hostname of the server stored in the
    server certificate matches the host that the client is connecting to. The connection
    hostname may be a bootstrap server that the client is configured with or an advertised
    listener hostname that was returned by a broker in a metadata response. Hostname
    verification is a critical part of server authentication that protects against
    man-in-the-middle attacks and hence should not be disabled in production systems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Brokers can be configured to authenticate clients connecting over listeners
    using SSL as the security protocol by setting the broker configuration option
    `ssl.​cli⁠ent.auth=required`. Clients should be configured with a key store, and
    brokers should be configured with a trust store containing client certificates
    or the certificate of the CAs that signed the client certificates. If SSL is used
    for inter-broker communication, broker trust stores should include the CA of the
    broker certificates as well as the CA of the client certificates. By default,
    the distinguished name (DN) of the client certificate is used as the `KafkaPrincipal`
    for authorization and quotas. The configuration option `ssl.principal.mapping.rules`
    can be used to provide a list of rules to customize the principal. Listeners using
    SASL_SSL disable TLS client authentication and rely on SASL authentication and
    the `KafkaPrincipal` established by SASL.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: SSL Client Authentication
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSL client authentication may be made optional by setting `ssl.​cli⁠ent.auth=requested`.
    Clients that are not configured with key stores will complete the TLS handshake
    in this case, but will be assigned the principal `User:ANONYMOUS`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The following examples show how to create key stores and trust stores for server
    and client authentication using a self-signed CA.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate self-signed CA key-pair for brokers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO1-1)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Create a key-pair for the CA and store it in a PKCS12 file server.ca.p12\. We
    use this for signing certificates.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO1-2)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Export the CA’s public certificate to server.ca.crt. This will be included in
    trust stores and certificate chains.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Create key stores for brokers with a certificate signed by the self-signed
    CA. If using wildcard hostnames, the same key store can be used for all brokers.
    Otherwise, create a key store for each broker with its fully qualified domain
    name (FQDN):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO2-1)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Generate a private key for a broker and store it in the PKCS12 file server.ks.p12.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO2-2)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Generate a certificate signing request.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO2-3)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Use the CA key store to sign the broker’s certificate. The signed certificate
    is stored in server.crt.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO2-4)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Import the broker’s certificate chain into the broker’s key store.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'If TLS is used for inter-broker communication, create a trust store for brokers
    with the broker’s CA certificate to enable brokers to authenticate one another:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Generate a trust store for clients with the broker’s CA certificate:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If TLS client authentication is enabled, clients must be configured with a
    key store. The following script generates a self-signed CA for clients and creates
    a key store for clients with a certificate signed by the client CA. The client
    CA is added to the broker trust store so that brokers can verify client authenticity:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO3-1)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: We create a new CA for clients in this example.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO3-2)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Clients authenticating with this certificate use `User:CN=Metrics ⁠App,​O=Con⁠flu⁠ent,C=GB`
    as the principal, by default.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO3-3)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: We add the client certificate chain to the client key store.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO3-4)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The broker’s trust store should contain the CAs of all clients.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the key and trust stores, we can configure TLS for brokers. Brokers
    require a trust store only if TLS is used for inter-broker communication or if
    client authentication is enabled:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Clients are configured with the generated trust store. The key store should
    be configured for clients if client authentication is required.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Trust Stores
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trust store configuration can be omitted in brokers as well as clients when
    using certificates signed by well-known trusted authorities. The default trust
    stores in the Java installation will be sufficient to establish trust in this
    case. Installation steps are described in [Chapter 2](ch02.html#installing_kafka).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Key stores and trust stores must be updated periodically before certificates
    expire to avoid TLS handshake failures. Broker SSL stores can be dynamically updated
    by modifying the same file or setting the configuration option to a new versioned
    file. In both cases, the Admin API or the Kafka configs tool can be used to trigger
    the update. The following example updates the key store for the external listener
    of a broker with broker id `0` using the configs tool:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Security considerations
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TLS is widely used to provide transport layer security for several protocols,
    including HTTPS. As with any security protocol, it is important to understand
    the potential threats and mitigation strategies when adopting a protocol for mission-critical
    applications. Kafka enables only the newer protocols TLSv1.2 and TLSv1.3 by default,
    since older protocols like TLSv1.1 have known vulnerabilities. Due to issues with
    insecure renegotiation, Kafka does not support renegotiation for TLS connections.
    Hostname verification is enabled by default to prevent man-in-the-middle attacks.
    Security can be tightened further by restricting cipher suites. Strong ciphers
    with at least a 256-bit encryption key size protect against cryptographic attacks
    and ensure data integrity when transporting data over an insecure network. Some
    organizations require TLS protocol and ciphers to be restricted to comply with
    security standards like FIPS 140-2.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Since key stores containing private keys are stored on the filesystem by default,
    it is vital to limit access to key store files using filesystem permissions. Standard
    Java TLS features can be used to enable certificate revocation if a private key
    is compromised. Short-lived keys can be used to reduce exposure in this case.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: TLS handshakes are expensive and utilize a significant amount of time on network
    threads in brokers. Listeners using TLS on insecure networks should be protected
    against denial-of-service attacks using connection quotas and limits to protect
    availability of brokers. The broker configuration option `connection.failed.​aut⁠hen⁠tication.delay.ms`
    can be used to delay failed response on authentication failures to reduce the
    rate at which authentication failures are retried by clients.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: SASL
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kafka protocol supports authentication using SASL and has built-in support
    for several commonly used SASL mechanisms. SASL can be combined with TLS as the
    transport layer to provide a secure channel with authentication and encryption.
    SASL authentication is performed through a sequence of server challenges and client
    responses where the SASL mechanism defines the sequence and wire format of challenges
    and responses. Kafka brokers support the following SASL mechanisms out of the
    box with customizable callbacks to integrate with existing security infrastructure:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka协议支持使用SASL进行身份验证，并内置支持几种常用的SASL机制。SASL可以与TLS结合使用作为传输层，以提供具有身份验证和加密的安全通道。SASL身份验证通过服务器挑战和客户端响应的序列执行，其中SASL机制定义了挑战和响应的序列和线路格式。Kafka经纪人直接支持以下SASL机制，并具有可定制的回调，以与现有安全基础设施集成：
- en: GSSAPI
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GSSAPI
- en: Kerberos authentication is supported using SASL/GSSAPI and can be used to integrate
    with Kerberos servers like Active Directory or OpenLDAP.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Kerberos身份验证使用SASL/GSSAPI进行支持，并可用于与Active Directory或OpenLDAP等Kerberos服务器集成。
- en: PLAIN
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: PLAIN
- en: Username/password authentication that is typically used with a custom server-side
    callback to verify passwords from an external password store.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义服务器端回调来验证来自外部密码存储的密码的用户名/密码身份验证。
- en: SCRAM-SHA-256 and SCRAM-SHA-512
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: SCRAM-SHA-256和SCRAM-SHA-512
- en: Username/password authentication available out of the box with Kafka without
    the need for additional password stores.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka可以直接使用用户名/密码进行身份验证，无需额外的密码存储。
- en: OAUTHBEARER
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: OAUTHBEARER
- en: Authentication using OAuth bearer tokens that is typically used with custom
    callbacks to acquire and validate tokens granted by standard OAuth servers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OAuth令牌进行身份验证，通常与自定义回调一起使用，以获取和验证标准OAuth服务器授予的令牌。
- en: One or more SASL mechanisms may be enabled on each SASL-enabled listener in
    the broker by configuring `sasl.enabled.mechanisms` for that listener. Clients
    may choose any of the enabled mechanisms by configuring `sasl.mechanism`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个启用SASL的监听器上可以通过为该监听器配置`sasl.enabled.mechanisms`来启用一个或多个SASL机制。客户端可以通过配置`sasl.mechanism`选择任何已启用的机制。
- en: Kafka uses the Java Authentication and Authorization Service (JAAS) for configuring
    SASL. The configuration option `sasl.jaas.config` contains a single JAAS configuration
    entry that specifies a login module and its options. Brokers use the `listener`
    and `mechanism` prefixes when configuring `sasl.jaas.config`. For example, `listener.name.external.gssapi.sasl.jaas.config`
    configures the JAAS configuration entry for SASL/GSSAPI on the listener named
    `EXTERNAL`. The login process on brokers and clients uses the JAAS configuration
    to determine the public and private credentials used for authentication.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用Java身份验证和授权服务（JAAS）来配置SASL。配置选项`sasl.jaas.config`包含一个单个JAAS配置条目，指定登录模块及其选项。在配置`sasl.jaas.config`时，经纪人使用`listener`和`mechanism`前缀。例如，`listener.name.external.gssapi.sasl.jaas.config`配置了名为`EXTERNAL`的监听器上SASL/GSSAPI的JAAS配置条目。经纪人和客户端上的登录过程使用JAAS配置来确定用于身份验证的公共和私有凭据。
- en: JAAS Configuration File
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JAAS配置文件
- en: JAAS configuration may also be specified in configuration files using the Java
    system property `java.security.auth.login.​con⁠fig`. However, the Kafka option
    `sasl.jaas.config` is recommended since it supports password protection and separate
    configuration for each SASL mechanism when multiple mechanisms are enabled on
    a listener.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用Java系统属性`java.security.auth.login.​con⁠fig`在配置文件中指定JAAS配置。但是，建议使用Kafka选项`sasl.jaas.config`，因为它支持密码保护，并且在监听器上启用多个机制时为每个SASL机制单独配置。
- en: SASL mechanisms supported by Kafka can be customized to integrate with third-party
    authentication servers using callback handlers. A login callback handler may be
    provided for brokers or clients to customize the login process, for example, to
    acquire credentials to be used for authentication. A server callback handler may
    be provided to perform authentication of client credentials, for example, to verify
    passwords using an external password server. A client callback handler may be
    provided to inject client credentials instead of including them in the JAAS configuration.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka支持的SASL机制可以定制，以与第三方身份验证服务器集成，使用回调处理程序。可以为经纪人或客户端提供登录回调处理程序，以自定义登录过程，例如获取用于身份验证的凭据。可以提供服务器回调处理程序来执行客户端凭据的身份验证，例如使用外部密码服务器验证密码。可以提供客户端回调处理程序来注入客户端凭据，而不是将它们包含在JAAS配置中。
- en: In the following subsections, we explore the SASL mechanisms supported by Kafka
    in more detail.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将更详细地探讨Kafka支持的SASL机制。
- en: SASL/GSSAPI
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SASL/GSSAPI
- en: Kerberos is a widely used network authentication protocol that uses strong cryptography
    to support secure mutual authentication over an insecure network. Generic Security
    Service Application Program Interface (GSS-API) is a framework for providing security
    services to applications using different authentication mechanisms. [RFC-4752](https://oreil.ly/wxTZt)
    introduces the SASL mechanism GSSAPI for authentication using GSS-API’s Kerberos
    V5 mechanism. The availability of open source as well as enterprise-grade commercial
    implementations of Kerberos servers has made Kerberos a popular choice for authentication
    across many sectors with strict security requirements. Kafka supports Kerberos
    authentication using SASL/GSSAPI.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Kerberos是一种广泛使用的网络身份验证协议，使用强加密来支持在不安全网络上进行安全的相互身份验证。通用安全服务应用程序接口（GSS-API）是一个框架，用于为使用不同身份验证机制的应用程序提供安全服务。[RFC-4752](https://oreil.ly/wxTZt)介绍了使用GSS-API的Kerberos
    V5机制进行身份验证的SASL机制GSSAPI。开源和企业级商业实现的Kerberos服务器的可用性使Kerberos成为许多具有严格安全要求的部门身份验证的流行选择。Kafka支持使用SASL/GSSAPI进行Kerberos身份验证。
- en: Configuring SASL/GSSAPI
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置SASL/GSSAPI
- en: 'Kafka uses GSSAPI security providers included in the Java runtime environment
    to support secure authentication using Kerberos. JAAS configuration for GSSAPI
    includes the path of a keytab file that contains the mapping of principals to
    their long-term keys in encrypted form. To configure GSSAPI for brokers, create
    a keytab for each broker with a principal that includes the broker’s hostname.
    Broker hostnames are verified by clients to ensure server authenticity and prevent
    man-in-the-middle attacks. Kerberos requires a secure DNS service for host name
    lookup during authentication. In deployments where forward and reverse lookup
    do not match, the Kerberos configuration file *krb5.conf* on clients can be configured
    to set `rdns=false` to disable reverse lookup. JAAS configuration for each broker
    should include the Kerberos V5 login module from the Java runtime, the pathname
    of the keytab file, and the full broker principal:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO4-1)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: We use `sasl.jaas.config` prefixed with the listener prefix, which contains
    the listener name and SASL mechanism in lowercase.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO4-2)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Keytab files must be readable by the broker process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO4-3)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Service principal for brokers should include the broker hostname.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'If SASL/GSSAPI is used for inter-broker communication, inter-broker SASL mechanism
    and the Kerberos service name should also be configured for brokers:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Clients should be configured with their own keytab and principal in the JAAS
    configuration and `sasl.kerberos.service.name` to indicate the name of the service
    they are connecting to:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO5-1)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The service name for the Kafka service should be specified for clients.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO5-2)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Clients may use principals without hostname.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The short name of the principal is used as the client identity by default. For
    example, `User:Alice` is the client principal and `User:kafka` is the broker principal
    in the example. The broker configuration `sasl.kerberos.principal.to.local.rules`
    can be used to apply a list of rules to transform the fully qualified principal
    to a custom principal.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use of SASL_SSL is recommended in production deployments using Kerberos to protect
    the authentication flow as well as data traffic on the connection after authentication.
    If TLS is not used to provide a secure transport layer, eavesdroppers on the network
    may gain enough information to mount a dictionary attack or brute-force attack
    to steal client credentials. It is safer to use randomly generated keys for brokers
    instead of keys generated from passwords that are easier to crack. Weak encryption
    algorithms like DES-MD5 should be avoided in favor of stronger algorithms. Access
    to keytab files must be restricted using filesystem permissions since any user
    in possession of the file may impersonate the user.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: SASL/GSSAPI requires a secure DNS service for server authentication. Because
    denial-of-service attacks against the KDC or DNS service can result in authentication
    failures in clients, it is necessary to monitor the availability of these services.
    Kerberos also relies on loosely synchronized clocks with configurable variability
    to detect replay attacks. It is important to ensure that clock synchronization
    is secure.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: SASL/PLAIN
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RFC-4616](https://oreil.ly/wZrxB) defines a simple username/password authentication
    mechanism that can be used with TLS to provide secure authentication. During authentication,
    the client sends a username and password to the server, and the server verifies
    the password using its password store. Kafka has built-in SASL/PLAIN support that
    can be integrated with a secure external password database using a custom callback
    handler.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/PLAIN
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The default implementation of SASL/PLAIN uses the broker’s JAAS configuration
    as the password store. All client usernames and passwords are included as login
    options, and the broker verifies that the password provided by a client during
    authentication matches one of these entries. A broker username and password are
    required only if SASL/PLAIN is used for inter-broker communication:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO6-1)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The username and password used for inter-broker connections initiated by the
    broker.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO6-2)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: When Alice’s client connects to the broker, the password provided by Alice is
    validated against this password in the broker’s config.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients must be configured with username and password for authentication:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The built-in implementation that stores all passwords in every broker’s JAAS
    configuration is insecure and not very flexible since all brokers will need to
    be restarted to add or remove a user. When using SASL/PLAIN in production, a custom
    server callback handler can be used to integrate brokers with a secure third-party
    password server. Custom callback handlers can also be used to support password
    rotation. On the server side, a server callback handler should support both old
    and new passwords for an overlapping period until all clients switch to the new
    password. The following example shows a callback handler that verifies encrypted
    passwords from files generated using the Apache tool `htpasswd`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO7-1)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: We use multiple password files so that we can support password rotation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO7-2)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: We pass pathnames of password files as a JAAS option in the broker configuration.
    Custom broker configuration options may also be used.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO7-3)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We check if the password matches in any of the files, allowing both old and
    new passwords to be used for a period of time.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO7-4)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: We use `htpasswd` for simplicity. A secure database can be used for production
    deployments.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Brokers are configured with the password validation callback handler and its
    options:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'On the client side, a client callback handler that implements `org.apache.kafka.​com⁠mon.security.auth.AuthenticateCallbackHandler`
    can be used to load passwords dynamically at runtime when a connection is established
    instead of loading statically from the JAAS configuration during startup. Passwords
    may be loaded from encrypted files or using an external secure server to improve
    security. The following example loads passwords dynamically from a file using
    configuration classes in Kafka:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO8-1)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: We load the config file within the callback to ensure we use the latest password
    to support password rotation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO8-2)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The underlying configuration library returns the actual password value even
    if the password is externalized.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO8-3)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: We define password configs with the `PASSWORD` type to ensure that passwords
    are not included in log entries.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients as well as brokers that use SASL/PLAIN for inter-broker communication
    can be configured with the client-side callback:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Security considerations
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since SASL/PLAIN transmits clear-text passwords over the wire, the PLAIN mechanism
    should be enabled only with encryption using SASL_SSL to provide a secure transport
    layer. Passwords stored in clear text in the JAAS configuration of brokers and
    clients are not secure, so consider encrypting or externalizing these passwords
    in a secure password store. Instead of using the built-in password store that
    stores all client passwords in the broker JAAS configuration, use a secure external
    password server that stores passwords securely and enforces strong password policies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Clear-Text Passwords
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Avoid clear-text passwords in configuration files even if the files can be protected
    using filesystem permissions. Consider externalizing or encrypting passwords to
    ensure that passwords are not inadvertently exposed. Kafka’s password protection
    feature is described later in this chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: SASL/SCRAM
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RFC-5802](https://oreil.ly/dXe3y) introduces a secure username/password authentication
    mechanism that addresses the security concerns with password authentication mechanisms
    like SASL/PLAIN, which send passwords over the wire. The Salted Challenge Response
    Authentication Mechanism (SCRAM) avoids transmitting clear-text passwords and
    stores passwords in a format that makes it impractical to impersonate clients.
    Salting combines passwords with some random data before applying a one-way cryptographic
    hash function to store passwords securely. Kafka has a built-in SCRAM provider
    that can be used in deployments with secure ZooKeeper without the need for additional
    password servers. The SCRAM mechanisms `SCRAM-SHA-256` and `SCRAM-SHA-512` are
    supported by the Kafka provider.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/SCRAM
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An initial set of users can be created after starting ZooKeeper prior to starting
    brokers. Brokers load SCRAM user metadata into an in-memory cache during startup,
    ensuring that all users, including the broker user for inter-broker communication,
    can authenticate successfully. Users can be added or deleted at any time. Brokers
    keep the cache up-to-date using notifications based on a ZooKeeper watcher. In
    this example, we create a user with the principal `User:Alice` and password `Alice-password`
    for SASL mechanism `SCRAM-SHA-512`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'One or more SCRAM mechanisms can be enabled on a listener by configuring the
    mechanisms on the broker. A username and password are required for brokers only
    if the listener is used for inter-broker communication:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO9-1)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Username and password for inter-broker connections initiated by the broker.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients must be configured to use one of the SASL mechanisms enabled in the
    broker, and the client JAAS configuration must include a username and password:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can add new SCRAM users using `--add-config` and delete users using the
    `--delete-config` option of the configs tool. When an existing user is deleted,
    new connections cannot be established for that user, but existing connections
    of the user will continue to work. A reauthentication interval can be configured
    for the broker to limit the amount of time existing connections may continue to
    operate after a user is deleted. The following example deletes the `SCRAM-SHA-512`
    config for `Alice` to remove Alice’s credentials for that mechanism:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Security considerations
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SCRAM applies a one-way cryptographic hash function on the password combined
    with a random salt to avoid the actual password being transmitted over the wire
    or stored in a database. However, any password-based system is only as secure
    as the passwords. Strong password policies must be enforced to protect the system
    from brute-force or dictionary attacks. Kafka provides safeguards by supporting
    only the strong hashing algorithms SHA-256 and SHA-512 and avoiding weaker algorithms
    like SHA-1\. This is combined with a high default iteration count of 4,096 and
    unique random salts for every stored key to limit the impact if ZooKeeper security
    is compromised.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: You should take additional precautions to protect the keys transmitted during
    handshake and the keys stored in ZooKeeper to protect against brute-force attacks.
    SCRAM must be used with `SASL_SSL` as the security protocol to avoid eavesdroppers
    from gaining access to hashed keys during authentication. ZooKeeper must also
    be SSL-enabled, and ZooKeeper data must be protected using disk encryption to
    ensure that stored keys cannot be retrieved even if the store is compromised.
    In deployments without a secure ZooKeeper, SCRAM callbacks can be used to integrate
    with a secure external credential store.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: SASL/OAUTHBEARER
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OAuth is an authorization framework that enables applications to obtain limited
    access to HTTP services. [RFC-7628](https://oreil.ly/sPBfv) defines the OAUTHBEARER
    SASL mechanism that enables credentials obtained using OAuth 2.0 to access protected
    resources in non-HTTP protocols. OAUTHBEARER avoids security vulnerabilities in
    mechanisms that use long-term passwords by using OAuth 2.0 bearer tokens with
    a shorter lifetime and limited resource access. Kafka supports SASL/OAUTHBEARER
    for client authentication, enabling integration with third-party OAuth servers.
    The built-in implementation of OAUTHBEARER uses unsecured JSON Web Tokens (JWTs)
    and is not suitable for production use. Custom callbacks can be added to integrate
    with standard OAuth servers to provide secure authentication using the OAUTHBEARER
    mechanism in production deployments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/OAUTHBEARER
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The built-in implementation of SASL/OAUTHBEARER in Kafka does not validate
    tokens and hence only requires the login module to be specified in the JAAS configuration.
    If the listener is used for inter-broker communication, details of the token used
    for client connections initiated by brokers must also be provided. The option
    `unsecuredLoginStringClaim_sub` is the subject claim that determines the `KafkaPrincipal`
    for the connection by default:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO10-1)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Subject claim for the token used for inter-broker connections.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients must be configured with the subject claim option `unsecuredLoginStringClaim_sub`.
    Other claims and token lifetime may also be configured:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO11-1)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '`User:Alice` is the default `KafkaPrincipal` for connections using this configuration.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate Kafka with third-party OAuth servers for using bearer tokens in
    production, Kafka clients must be configured with `sasl.login.callback.handler.class`
    to acquire tokens from the OAuth server using the long-term password or a refresh
    token. If OAUTHBEARER is used for inter-broker communication, brokers must also
    be configured with a login callback handler to acquire tokens for client connections
    created by the broker for inter-broker communication:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO12-1)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Clients must acquire a token from the OAuth server and set a valid token on
    the callback.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO12-2)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The client may also include optional extensions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Brokers must also be configured with a server callback handler using `listener.name.<listener-name>.oauthbearer.sasl.server.callback.handler.​class`
    for validating tokens provided by the client:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO13-1)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '`OAuthBearerValidatorCallback` contains the token from the client. Brokers
    validate this token.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO13-2)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Brokers validate any optional extensions from the client.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since SASL/OAUTHBEARER clients send OAuth 2.0 bearer tokens over the network
    and these tokens may be used to impersonate clients, TLS must be enabled to encrypt
    authentication traffic. Short-lived tokens can be used to limit exposure if tokens
    are compromised. Reauthentication may be enabled for brokers to prevent connections
    outliving the tokens used for authentication. A reauthentication interval configured
    on brokers, combined with token revocation support, limit the amount of time an
    existing connection may continue to use a token after revocation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Delegation tokens
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delegation tokens are shared secrets between Kafka brokers and clients that
    provide a lightweight configuration mechanism without the requirement to distribute
    SSL key stores or Kerberos keytabs to client applications. Delegation tokens can
    be used to reduce the load on authentication servers, like the Kerberos Key Distribution
    Center (KDC). Frameworks like Kafka Connect can use delegation tokens to simplify
    security configuration for workers. A client that has authenticated with Kafka
    brokers can create delegation tokens for the same user principal and distribute
    these tokens to workers, which can then authenticate directly with Kafka brokers.
    Each delegation token consists of a token identifier and a hash-based message
    authentication code (HMAC) used as a shared secret. Client authentication with
    delegation tokens is performed using SASL/SCRAM with the token identifier as username
    and HMAC as the password.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Delegation tokens can be created or renewed using the Kafka Admin API or the
    `delegation-tokens` command. To create delegation tokens for the principal `User:Alice`,
    the client must be authenticated using Alice’s credentials for any authentication
    protocol other than delegation tokens. Clients authenticated using delegation
    tokens cannot create other delegation tokens:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO14-1)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: If Alice runs this command, the generated token can be used to impersonate Alice.
    The owner of this token is `User:Alice`. We also configure `User:Bob` as a token
    renewer.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO14-2)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The renewal command can be run by the token owner (Alice) or the token renewer
    (Bob).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Configuring delegation tokens
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To create and validate delegation tokens, all brokers must be configured with
    the same master key using the configuration option `delegation.token.master.key`.
    This key can only be rotated by restarting all brokers. All existing tokens should
    be deleted before updating the master key since they can no longer be used, and
    new tokens should be created after the key is updated on all brokers.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'At least one of the SASL/SCRAM mechanisms must be enabled on brokers to support
    authentication using delegation tokens. Clients should be configured to use SCRAM
    with a token identifier as username and token HMAC as the password. The `Kafka​P⁠rincipal`
    for the connections using this configuration will be the original principal associated
    with the token, e.g., `User:Alice`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO15-1)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: SCRAM configuration with `tokenauth` is used to configure delegation tokens.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like the built-in SCRAM implementation, delegation tokens are suitable for production
    use only in deployments where ZooKeeper is secure. All the security considerations
    described under SCRAM also apply to delegation tokens.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The master key used by brokers for generating tokens must be protected using
    encryption or by externalizing the key in a secure password store. Short-lived
    delegation tokens can be used to limit exposure if a token is compromised. Reauthentication
    can be enabled in brokers to prevent connections operating with expired tokens
    and to limit the amount of time existing connections may continue to operate after
    token deletion.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Reauthentication
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw earlier, Kafka brokers perform client authentication when a connection
    is established by the client. Client credentials are verified by the brokers,
    and the connection authenticates successfully if the credentials are valid at
    that time. Some security mechanisms like Kerberos and OAuth use credentials with
    a limited lifetime. Kafka uses a background login thread to acquire new credentials
    before the old ones expire, but the new credentials are used only to authenticate
    new connections by default. Existing connections that were authenticated with
    old credentials continue to process requests until disconnection occurs due to
    a request timeout, an idle timeout, or network errors. Long-lived connections
    may continue to process requests long after the credentials used to authenticate
    the connections expire. Kafka brokers support reauthentication for connections
    authenticated using SASL using the configuration option `connections.max.reauth.ms`.
    When this option is set to a positive integer, Kafka brokers determine the session
    lifetime for SASL connections and inform clients of this lifetime during the SASL
    handshake. Session lifetime is the lower of the remaining lifetime of the credential
    or `connections.max.reauth.ms`. Any connection that doesn’t reauthenticate within
    this interval is terminated by the broker. Clients perform reauthentication using
    the latest credentials acquired by the background login thread or injected using
    custom callbacks. Reauthentication can be used to tighten security in several
    scenarios:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: For SASL mechanisms like GSSAPI and OAUTHBEARER that use credentials with a
    limited lifetime, reauthentication guarantees that all active connections are
    associated with valid credentials. Short-lived credentials limit exposure in case
    credentials that are compromised.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Password-based SASL mechanisms like PLAIN and SCRAM can support password rotation
    by adding periodic login. Reauthentication limits the amount of time requests
    are processed on connections authenticated with the old password. Custom server
    callback that allows both old and new passwords for a period of time can be used
    to avoid outages until all clients migrate to the new password.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connections.max.reauth.ms` forces reauthentication in all SASL mechanisms,
    including those with nonexpiring credentials. This limits the amount of time a
    credential may be associated with an active connection after it has been revoked.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connections from clients without SASL reauthentication support are terminated
    on session expiry, forcing the clients to reconnect and authenticate again, thus
    providing the same security guarantees for expired or revoked credentials.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compromised Users
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a user is compromised, action must be taken to remove the user from the system
    as soon as possible. All new connections will fail to authenticate with Kafka
    brokers once the user is removed from the authentication server. Existing connections
    will continue to process requests until the next reauthentication timeout. If
    `connections.max.reauth.ms` is not configured, no timeout is applied and existing
    connections may continue to use the compromised user’s identity for a long time.
    Kafka does not support SSL renegotiation due to known vulnerabilities during renegotiation
    in older SSL protocols. Newer protocols like TLSv1.3 do not support renegotiation.
    So, existing SSL connections may continue to use revoked or expired certificates.
    `Deny` ACLs for the user principal can be used to prevent these connections from
    performing any operation. Since ACL changes are applied with very small latencies
    across all brokers, this is the quickest way to disable access for compromised
    users.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Security Updates Without Downtime
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka deployments need regular maintenance to rotate secrets, apply security
    fixes, and update to the latest security protocols. Many of these maintenance
    tasks are performed using rolling updates where one by one, brokers are shut down
    and restarted with an updated configuration. Some tasks like updating SSL key
    stores and trust stores can be performed using dynamic config updates without
    restarting brokers.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'When adding a new security protocol to an existing deployment, a new listener
    can be added to brokers with the new protocol while retaining the old listener
    with the old protocol to ensure that client applications can continue to function
    using the old listener during the update. For example, the following sequence
    can be used to switch from PLAINTEXT to SASL_SSL in an existing deployment:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Add a new listener on a new port to each broker using the Kafka configs tool.
    Use a single config update command to update `listeners` and `advertised.​lis⁠teners`
    to include the old listener as well as the new listener, and provide all the configuration
    options for the new SASL_SSL listener with the listener prefix.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify all client applications to use the new SASL_SSL listener.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If inter-broker communication is being updated to use the new SASL_SSL listener,
    perform a rolling update of brokers with the new `inter.broker.​lis⁠tener.name`.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the configs tool to remove the old listener from `listeners` and `advertised.listeners`
    and to remove any unused configuration options of the old listener.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SASL mechanisms can be added or removed from existing SASL listeners without
    downtime using rolling updates on the same listener port. The following sequence
    switches the mechanism from PLAIN to SCRAM-SHA-256:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Add all existing users to the SCRAM store using the Kafka configs tool.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `sasl.enabled.mechanisms=PLAIN,SCRAM-SHA-256`, configure `list⁠ener.​name.<_listener-name_>.scram-sha-256.sasl.jaas.config`
    for the listener, and perform a rolling update of brokers.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify all client applications to use `sasl.mechanism=SCRAM-SHA-256` and update
    `sasl.jaas.config` to use SCRAM.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the listener is used for inter-broker communication, use a rolling update
    of brokers to set `sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256`.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform another rolling update of brokers to remove the PLAIN mechanism. Set
    `sasl.enabled.mechanisms=SCRAM-SHA-256` and remove `listener.name.​<listener-name>.plain.sasl.jaas.config`
    and any other configuration options for PLAIN.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encryption
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Encryption is used to preserve data privacy and data integrity. As we discussed
    earlier, Kafka listeners using SSL and SASL_SSL security protocols use TLS as
    the transport layer, providing secure encrypted channels that protect data transmitted
    over an insecure network. TLS cipher suites can be restricted to strengthen security
    and adhere to security requirements like the Federal Information Processing Standard
    (FIPS).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Additional measures must be taken to protect data at rest to ensure that sensitive
    data cannot be retrieved even by users with physical access to the disk that stores
    Kafka logs. To avoid security breaches even if the disk is stolen, physical storage
    can be encrypted using whole disk encryption or volume encryption.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: While encryption of transport layer and data storage may provide adequate protection
    in many deployments, additional protection may be required to avoid granting automatic
    data access to platform administrators. Unencrypted data present in broker memory
    may appear in heap dumps, and administrators with direct access to the disk will
    be able to access these, as well as Kafka logs containing potentially sensitive
    data. In deployments with highly sensitive data or Personally Identifiable Information
    (PII), extra measures are required to preserve data privacy. To comply with regulatory
    requirements, especially in cloud deployments, it is necessary to guarantee that
    confidential data cannot be accessed by platform administrators or cloud providers
    by any means. Custom encryption providers can be plugged into Kafka clients to
    implement end-to-end encryption that guarantees that the entire data flow is encrypted.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Encryption
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#writing_messages_to_kafka) on Kafka producers, we saw
    that *serializers* are used to convert messages into the byte array stored in
    Kafka logs, and in [Chapter 4](ch04.html#reading_data_from_kafka) on Kafka consumers,
    we saw that *deserializers* converted the byte array back to the message. Serializers
    and deserializers can be integrated with an encryption library to perform encryption
    of the message during serialization, and decryption during deserialization. Message
    encryption is typically performed using symmetric encryption algorithms like AES.
    A shared encryption key stored in a key management system (KMS) enables producers
    to encrypt the message and consumers to decrypt the message. Brokers do not require
    access to the encryption key and never see the unencrypted contents of the message,
    making this approach safe to use in cloud environments. Encryption parameters
    that are required to decrypt the message may be stored in message headers or in
    the message payload if older consumers without header support need access to the
    message. A digital signature may also be included in message headers to verify
    message integrity.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-2](#fig-2-end-to-end-encryption) shows a Kafka data flow with end-to-end
    encryption.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1102](assets/kdg2_1102.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. End-to-end encryption
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We send a message using a Kafka producer.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The producer uses an encryption key from KMS to encrypt the message.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encrypted message is sent to the broker. The broker stores the encrypted
    message in the partition logs.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The broker sends the encrypted message to consumers.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The consumer uses the encryption key from KMS to decrypt the message.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Producers and consumers must be configured with credentials to obtain shared
    keys from KMS. Periodic key rotation is recommended to harden security, since
    frequent rotation limits the number of compromised messages in case of a breach
    and also protects against brute-force attacks. Consumption must be supported with
    both old and new keys during the retention period of messages encrypted with the
    old key. Many KMS systems support graceful key rotation out of the box for symmetric
    encryption without requiring any special handling in Kafka clients. For compacted
    topics, messages encrypted with old keys may be retained for a long time, and
    it may be necessary to re-encrypt old messages. To avoid interference with newer
    messages, producers and consumers must be offline during this process.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Compression of Encrypted Messages
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compressing messages after encryption is unlikely to provide any benefit in
    terms of space reduction compared to compressing prior to encryption. Serializers
    may be configured to perform compression before encrypting the message, or applications
    may be configured to perform compression prior to producing messages. In either
    case, it is better to disable compression in Kafka since it adds overhead without
    providing any additional benefit. For messages transmitted over an insecure transport
    layer, known security exploits of compressed encrypted messages must also be taken
    into account.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: In many environments, especially when TLS is used as the transport layer, message
    keys do not require encryption since they typically do not contain sensitive data
    like message payloads. But in some cases, clear-text keys may not comply with
    regulatory requirements. Since message keys are used for partitioning and compaction,
    transformation of keys must preserve the required hash equivalence to ensure that
    a key retains the same hash value even if encryption parameters are altered. One
    approach would be to store a secure hash of the original key as the message key
    and store the encrypted message key in the message payload or in a header. Since
    Kafka serializes message key and value independently, a producer interceptor can
    be used to perform this transformation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Authorization
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorization is the process that determines what operations you are allowed
    to perform on which resources. Kafka brokers manage access control using a customizable
    authorizer. We saw earlier that whenever connections are established from a client
    to a broker, the broker authenticates the client and associates a `KafkaPrincipal`
    that represents the client identity with the connection. When a request is processed,
    the broker verifies that the principal associated with the connection is authorized
    to perform that request. For example, when Alice’s producer attempts to write
    a new customer order record to the topic `customerOrders`, the broker verifies
    that `User:Alice` is authorized to write to that topic.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka has a built-in authorizer, `AclAuthorizer`, that can be enabled by configuring
    the authorizer class name as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: SimpleAclAuthorizer
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`AclAuthorizer` was introduced in Apache Kafka 2.3\. Older versions from 0.9.0.0
    onward had a built-in authorizer, `kafka.security.auth.SimpleAclAuthorizer`, which
    has been deprecated but is still supported.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: AclAuthorizer
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`AclAuthorizer` supports fine-grained access control for Kafka resources using
    access control lists (ACLs). ACLs are stored in ZooKeeper and cached in memory
    by every broker to enable high-performance lookup for authorizing requests. ACLs
    are loaded into the cache when the broker starts up, and the cache is kept up-to-date
    using notifications based on a ZooKeeper watcher. Every Kafka request is authorized
    by verifying that the `KafkaPrincipal` associated with the connection has permissions
    to perform the requested operation on the requested resources.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Each ACL binding consists of:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Resource type: `Cluster|Topic|Group|TransactionalId|DelegationToken`'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattern type: `Literal|Prefixed`'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resource name: Name of the resource or prefix, or the wildcard `*`'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operation: `Describe|Create|Delete|Alter|Read|Write|DescribeConfigs|AlterConfigs`'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Permission type: `Allow|Deny`; `Deny` has higher precedence.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principal: Kafka principal represented as <principalType>:<principalName>,
    e.g., `User:Bob` or `Group:Sales`. ACLs may use `User:*` to grant access to all
    users.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Host: Source IP address of the client connection or `*` if all hosts are authorized.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, an ACL may specify:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`AclAuthorizer` authorizes an action if there are no `Deny` ACLs that match
    the action and there is at least one `Allow` ACL that matches the action. `Describe`
    permission is implicitly granted if `Read`, `Write`, `Alter`, or `Delete` permission
    is granted. `DescribeConfigs` permission is implicitly granted if `AlterConfigs`
    permission is granted.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Wildcard ACLs
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ACLs with pattern type `Literal` and resource name `*` are used as wildcard
    ACLs that match all resource names of a resource type.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Brokers must be granted `Cluster:ClusterAction` access in order to authorize
    controller requests and replica fetch requests. Producers require `Topic:Write`
    for producing to a topic. For idempotent produce without transactions, producers
    must also be granted `Cluster:IdempotentWrite`. Transactional producers require
    `TransactionalId:Write` access to the transaction IS and `Group:Read` for consumer
    groups to commit offsets. Consumers require `Topic:Read` to consume from a topic
    and `Group:Read` for the consumer group if using group management or offset management.
    Administrative operations require appropriate `Create`, `Delete`, `Describe`,
    `Alter`, `DescribeConfigs`, or `AlterConfigs` access. [Table 11-1](#table-1-acls)
    shows the Kafka requests to which each ACL is applied.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-1\. Access granted for each Kafka ACL
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '| ACL | Kafka requests | Notes |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:ClusterAction` | Inter-broker requests, including controller requests
    and follower fetch requests for replication | Should only be granted to brokers.
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:Create` | `CreateTopics` and auto-topic creation | Use `Topic:Create`
    for fine-grained access control to create specific topics. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:Alter` | `CreateAcls`, `DeleteAcls`, `AlterReplicaLogDirs`, `ElectReplicaLeader`,
    `Alter​PartitionReassignments` |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:AlterConfigs` | `AlterConfigs` and `IncrementalAlterConfigs` for
    broker and broker logger, `AlterClientQuotas` |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:Describe` | `DescribeAcls`, `DescribeLogDirs`, `ListGroups`, `ListPartitionReassignments`,
    describing authorized operations for cluster in Metadata request | Use `Group:Describe`
    for fine-grained access control for ListGroups. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:DescribeConfigs` | `DescribeConfigs` for broker and broker logger,
    `DescribeClientQuotas` |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:IdempotentWrite` | Idempotent `InitProducerId` and `Produce` requests
    | Only required for nontransactional idempotent producers. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Create` | `CreateTopics` and auto-topic creation |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Delete` | `DeleteTopics`, `DeleteRecords` |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Alter` | `CreatePartitions` |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| `Topic:AlterConfigs` | `AlterConfigs` and `IncrementalAlterConfigs` for topics
    |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Describe` | Metadata request for topic, `OffsetForLeaderEpoch`, `ListOffset`,
    `OffsetFetch` |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| `Topic:DescribeConfigs` | `DescribeConfigs` for topics, for returning configs
    in `CreateTopics` response |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Read` | `Consumer Fetch`, `OffsetCommit`, `TxnOffsetCommit`, `OffsetDelete`
    | Should be granted to consumers. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Write` | `Produce`, `AddPartitionToTxn` | Should be granted to producers.
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| `Group:Read` | `JoinGroup`, `SyncGroup`, `LeaveGroup`, `Heartbeat`, `OffsetCommit`,
    `AddOffsetsToTxn`, `TxnOffsetCommit` | Required for consumers using consumer group
    management or Kafka-based offset management. Also required for transactional producers
    to commit offsets within a transaction. |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| `Group:Describe` | `FindCoordinator`, `DescribeGroup`, `ListGroups`, `OffsetFetch`
    |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| `Group:Delete` | `DeleteGroups`, `OffsetDelete` |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| `TransactionalId:Write` | `Produce` and `InitProducerId` with transactions,
    `AddPartitionToTxn`, `AddOffsetsToTxn`, `TxnOffsetCommit`, `EndTxn` | Required
    for transactional producers. |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| `TransactionalId:​Describe` | `FindCoordinator` for transaction coordinator
    |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| `DelegationToken:​Describe` | `DescribeTokens` |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: 'Kafka provides a tool for managing ACLs using the authorizer configured in
    brokers. ACLs can be created directly in ZooKeeper as well. This is useful to
    create broker ACLs prior to starting brokers:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO16-1)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ACLs for broker user are created directly in ZooKeeper.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO16-2)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: By default, the ACLs command grants literal ACLs. `User:Alice` is granted access
    to write to the topic `customerOrders`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO16-3)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The prefixed ACL grants permission for Bob to read all topics starting with
    `customer`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '`AclAuthorizer` has two configuration options to grant broad access to resources
    or principals in order to simplify management of ACLs, especially when adding
    authorization to existing clusters for the first time:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Super users are granted access for all operations on all resources without any
    restrictions and cannot be denied access using `Deny` ACLs. If Carol’s credentials
    are compromised, Carol must be removed from `super.users`, and brokers must be
    restarted to apply the changes. It is safer to grant specific access using ACLs
    to users in production systems to ensure access can be revoked easily, if required.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Super User Separator
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike other list configurations in Kafka that are comma-separated, `super.users`
    are separated by a semicolon since user principals such as distinguished names
    from SSL certificates often contain commas.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: If `allow.everyone.if.no.acl.found` is enabled, all users are granted access
    to resources without any ACLs. This option may be useful when enabling authorization
    for the first time in a cluster or during development, but is not suitable for
    production use since access may be granted unintentionally to new resources. Access
    may also be unexpectedly removed when ACLs for a matching prefix or wildcard are
    added if the condition for `no.acl.found` no longer applies.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Authorization
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authorization can be customized in Kafka to implement additional restrictions
    or add new types of access control, like role-based access control.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'The following custom authorizer restricts usage of some requests to the internal
    listener alone. For simplicity, the requests and listener name are hard-coded
    here, but they can be configured using custom authorizer properties instead for
    flexibility:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO17-1)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Authorizers are given the request context with metadata that includes listener
    names, security protocol, request types, etc., enabling custom authorizers to
    add or remove restrictions based on the context.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO17-2)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: We reuse functionality from the built-in Kafka authorizer using the public API.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka authorizer can also be integrated with external systems to support group-based
    access control or role-based access control. Different principal types can be
    used to create ACLs for group principals or role principals. For instance, roles
    and groups from an LDAP server can be used to periodically populate `groups` and
    `roles` in the Scala class below to support `Allow` ACLs at different levels:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO18-1)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Groups to which each user belongs, populated from an external source like LDAP.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO18-2)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Roles associated with each user, populated from an external source like LDAP.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO18-3)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: We perform authorization for the user as well as for all the groups and roles
    of the user.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO18-4)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: If any of the contexts are authorized, we return `ALLOWED`. Note that this example
    doesn’t support `Deny` ACLs for groups or roles.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_securing_kafka_CO18-5)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: We create an authorization context for each principal with the same metadata
    as the original context.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'ACLs can be assigned for the group `Sales` or the role `Operator` using the
    standard Kafka ACL tool:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO19-1)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: We use the principal `Group:Sales` with the custom principal type `Group` to
    create an ACL that applies to users belonging to the group `Sales`.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO19-2)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: We use the principal `Role:Operator` with the custom principal type `Role` to
    create an ACL that applies to users with the role `Operator`.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Security Considerations
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since `AclAuthorizer` stores ACLs in ZooKeeper, access to ZooKeeper should be
    restricted. Deployments without a secure ZooKeeper can implement custom authorizers
    to store ACLs in a secure external database.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: In large organizations with a large number of users, managing ACLs for individual
    resources may become very cumbersome. Reserving different resource prefixes for
    different departments enables the use of prefixed ACLs that minimize the number
    of ACLs required. This can be combined with group- or role-based ACLs, as shown
    in the example earlier, to further simplify access control in large deployments.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Restricting user access using the principle of least privilege can limit exposure
    if a user is compromised. This means granting access only to the resources necessary
    for each user principal to perform their operations, and removing ACLs when they
    are no longer required. ACLs should be removed immediately when a user principal
    is no longer in use, for instance, when a person leaves the organization. Long-running
    applications can be configured with service credentials rather than credentials
    associated with a specific user to avoid any disruption when employees leave the
    organization. Since long-lived connections with a user principal may continue
    to process requests even after the user has been removed from the system, `Deny`
    ACLs can be used to ensure that the principal is not unintentionally granted access
    through ACLs with wildcard principals. Reuse of principals must be avoided if
    possible to prevent access from being granted to connections using the older version
    of a principal.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Auditing
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka brokers can be configured to generate comprehensive *log4j* logs for auditing
    and debugging. The logging level as well as the appenders used for logging and
    their configuration options can be specified in *log4j.properties*. The logger
    instances `kafka.authorizer.logger` used for authorization logging and kafka.request.​log⁠ger
    used for request logging can be configured independently to customize the log
    level and retention for audit logging. Production systems can use frameworks like
    the Elastic Stack to analyze and visualize these logs.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Authorizers generate `INFO`-level log entries for every attempted operation
    for which access was denied, and log entries at the `DEBUG` level for every operation
    for which access was granted. For example:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Request logging generated at the `DEBUG` level also includes details of the
    user principal and client host. Full details of the request are included if the
    request logger is configured to log at the `TRACE` level. For example:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Authorizer and request logs can be analyzed to detect suspicious activities.
    Metrics that track authentication failures, as well as authorization failure logs,
    can be extremely useful for auditing and provide valuable information in the event
    of an attack or unauthorized access. For end-to-end auditability and traceability
    of messages, audit metadata can be included in message headers when messages are
    produced. End-to-end encryption can be used to protect the integrity of this metadata.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Securing ZooKeeper
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZooKeeper stores Kafka metadata that is critical for maintaining the availability
    of Kafka clusters, and hence it is vital to secure ZooKeeper in addition to securing
    Kafka. ZooKeeper supports authentication using SASL/GSSAPI for Kerberos authentication
    and SASL/DIGEST-MD5 for username/password authentication. ZooKeeper also added
    TLS support in 3.5.0, enabling mutual authentication as well as encryption of
    data in transit. Note that SASL/DIGEST-MD5 should only be used with TLS encryption
    and is not suitable for production use due to known security vulnerabilities.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: SASL
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SASL configuration for ZooKeeper is provided using the Java system property
    `java.security.auth.login.config`. The property must be set to a JAAS configuration
    file that contains a login section with the appropriate login module and its options
    for the ZooKeeper server. Kafka brokers must be configured with the client-side
    login section for ZooKeeper clients to talk to SASL-enabled ZooKeeper servers.
    The `Server` section that follows provides the JAAS configuration for the ZooKeeper
    server to enable Kerberos authentication:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To enable SASL authentication on ZooKeeper servers, configure authentication
    providers in the ZooKeeper configuration file:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Broker Principal
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, ZooKeeper uses the full Kerberos principal, e.g., `kafka/broker1.example.com@EXAMPLE.COM`,
    as the client identity. When ACLs are enabled for ZooKeeper authorization, ZooKeeper
    servers should be configured with `kerberos.removeHostFromPrincipal=​true` and
    `kerberos.removeRealmFromPrincipal=true` to ensure that all brokers have the same
    principal.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka brokers must be configured to authenticate to ZooKeeper using SASL with
    a JAAS configuration file that provides client credentials for the broker:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: SSL
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SSL may be enabled on any ZooKeeper endpoint, including those that use SASL
    authentication. Like Kafka, SSL may be configured to enable client authentication,
    but unlike Kafka, connections with both SASL and SSL client authentication authenticate
    using both protocols and associate multiple principals with the connection. ZooKeeper
    authorizer grants access to a resource if any of the principals associated with
    the connection have access.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure SSL on a ZooKeeper server, a key store with the hostname of the
    server or a wildcarded host should be configured. If client authentication is
    enabled, a trust store to validate client certificates is also required:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To configure SSL for Kafka connections to ZooKeeper, brokers should be configured
    with a trust store to validate ZooKeeper certificates. If client authentication
    is enabled, a key store is also required:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Authorization
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authorization can be enabled for ZooKeeper nodes by setting ACLs for the path.
    When brokers are configured with `zookeeper.set.acl=true`, the broker sets ACLs
    for ZooKeeper nodes when creating the node. By default, metadata nodes are readable
    by everyone but modifiable only by brokers. Additional ACLs may be added if required
    for internal admin users who may need to update metadata directly in ZooKeeper.
    Sensitive paths, like nodes containing SCRAM credentials, are not world-readable
    by default.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Securing the Platform
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we discussed the options for locking down access to
    Kafka and ZooKeeper in order to safeguard Kafka deployments. Security design for
    a production system should use a threat model that addresses security threats
    not just for individual components but also for the system as a whole. Threat
    models build an abstraction of the system and identify potential threats and the
    associated risks. Once the threats are evaluated, documented, and prioritized
    based on risks, mitigation strategies must be implemented for each potential threat
    to ensure that the whole system is protected. When assessing potential threats,
    it is important to consider external threats as well as insider threats. For systems
    that store Personally Identifiable Information (PII) or other sensitive data,
    additional measures to comply with regulatory policies must also be implemented.
    An in-depth discussion of standard threat modeling techniques is outside the scope
    of this chapter.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: In addition to protecting data in Kafka and metadata in ZooKeeper using secure
    authentication, authorization, and encryption, extra steps must be taken to ensure
    that the platform is secure. Defenses may include network firewall solutions to
    protect the network and encryption to protect physical storage. Key stores, trust
    stores, and Kerberos keytab files that contain credentials used for authentication
    must be protected using filesystem permissions. Access to configuration files
    containing security-critical information like credentials must be restricted.
    Since passwords stored in clear-text in configuration files are insecure even
    if access is restricted, Kafka supports externalizing passwords in a secure store.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Password Protection
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customizable configuration providers can be configured for Kafka brokers and
    clients to retrieve passwords from a secure third-party password store. Passwords
    may also be stored in encrypted form in configuration files with custom configuration
    providers that perform decryption.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'The custom configuration provider that follows uses the tool `gpg` to decrypt
    broker or client properties stored in a file:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[![1](assets/1.png)](#co_securing_kafka_CO20-1)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: We provide the passphrase for decoding passwords to the process in the environment
    variable `PASSPHRASE`.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO20-2)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: We decrypt the configs using `gpg`. The return value contains the full set of
    decrypted configs.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO20-3)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: We parse the configs in `data` as Java properties.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO20-4)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: We fail fast with a `RuntimeException` if an error is encountered.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_securing_kafka_CO20-5)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Caller may request a subset of keys from the path; here we get all values and
    return the requested subset.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall that in the section on SASL/PLAIN, we used standard Kafka configuration
    classes to load credentials from an external file. We can now encrypt that file
    using `gpg`:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We now add indirect configs and config provider options to the original properties
    file so that Kafka clients load their credentials from the encrypted file:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Sensitive broker configuration options can also be stored encrypted in ZooKeeper
    using the Kafka configs tool without using custom providers. The following command
    can be executed before starting brokers to store encrypted SSL key store passwords
    for brokers in ZooKeeper. The password encoder secret must be configured in each
    broker’s configuration file to decrypt the value:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Summary
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The frequency and scale of data breaches have been increasing over the last
    decade as cyberattacks have become increasingly sophisticated. In addition to
    the significant cost of isolating and resolving breaches and the cost of outages
    until security fixes have been applied, data breaches may also result in regulatory
    penalties and long-term damage to brand reputation. In this chapter, we explored
    the vast array of options available to guarantee the confidentiality, integrity,
    and availability of data stored in Kafka.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the example data flow at the start of this chapter, we reviewed
    the options available for different aspects of security throughout the flow:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Client authenticity
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: When Alice’s client establishes connection to a Kafka broker, a listener using
    SASL or SSL with client authentication can verify that the connection is really
    from Alice and not an imposter. Reauthentication can configured to limit exposure
    in case a user is compromised.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Server authenticity
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Alice’s client can verify that its connection is to the genuine broker using
    SSL with hostname validation or using SASL mechanisms with mutual authentication,
    like Kerberos or SCRAM.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Use of SSL to encrypt data in transit protects data from eavesdroppers. Disk
    or volume encryption protects data at rest even if the disk is stolen. For highly
    sensitive data, end-to-end encryption provides fine-grained data access control
    and ensures that cloud providers and platform administrators with physical access
    to network and disks cannot access the data.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: SSL can be used to detect tampering of data over an insecure network. Digital
    signatures can be included in messages to verify integrity when using end-to-end
    encryption.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Access control
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Every operation performed by Alice, Bob, and even brokers is authorized using
    a customizable authorizer. Kafka has a built-in authorizer that enables fine-grained
    access control using ACLs.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Auditability
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Authorizer logs and request logs can be used to track operations and attempted
    operations for auditing and anomaly detection.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: A combination of quotas and configuration options to manage connections can
    be used to protect brokers from denial-of-service attacks. ZooKeeper can be secured
    using SSL, SASL, and ACLs to ensure that the metadata needed to ensure the availability
    of Kafka brokers is secure.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: With the wide choice of options available for security, choosing the appropriate
    options for each use case can be a daunting task. We reviewed the security concerns
    to consider for each security mechanism, and the controls and policies that can
    be adopted to limit the potential attack surface. We also reviewed the additional
    measures required to lock down ZooKeeper and the rest of the platform. The standard
    security technologies supported by Kafka and the various extension points to integrate
    with the existing security infrastructure in your organization enable you to build
    consistent security solutions to protect the whole platform.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL

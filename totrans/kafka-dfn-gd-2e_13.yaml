- en: Chapter 11\. Securing Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is used for a variety of use cases ranging from website activity tracking
    and metrics pipelines to patient record management and online payments. Each use
    case has different requirements in terms of security, performance, reliability,
    and availability. While it is always preferable to use the strongest and latest
    security features available, trade-offs are often necessary since increased security
    impacts performance, cost, and user experience. Kafka supports several standard
    security technologies with a range of configuration options to tailor security
    to each use case.
  prefs: []
  type: TYPE_NORMAL
- en: Like performance and reliability, security is an aspect of the system that must
    be addressed for the system as a whole, rather than component by component. The
    security of a system is only as strong as the weakest link, and security processes
    and policies must be enforced across the system, including the underlying platform.
    The customizable security features in Kafka enable integration with existing security
    infrastructure to build a consistent security model that applies to the entire
    system.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the security features in Kafka and see how
    they address different aspects of security and contribute toward the overall security
    of the Kafka installation. Throughout the chapter, we will share best practices,
    potential threats, and techniques to mitigate these threats. We will also review
    additional measures that can be adopted to secure ZooKeeper and the rest of the
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Locking Down Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka uses a range of security procedures to establish and maintain confidentiality,
    integrity, and availability of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Authentication establishes your identity and determines *who* you are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization determines *what* you are allowed to do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption protects your data from eavesdropping and tampering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditing tracks what you have done or have attempted to do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quotas control how much resources you can utilize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand how to lock down a Kafka deployment, let’s first look at how data
    flows through a Kafka cluster. [Figure 11-1](#fig-1-secure-flow) shows the main
    steps in an example data flow. In this chapter, we will use this example flow
    to examine the different ways in which Kafka can be configured to protect data
    at every step to guarantee security of the entire deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1101](assets/kdg2_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Data flow in a Kafka cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Alice produces a customer order record to a partition of the topic named `customerOrders`.
    The record is sent to the leader of the partition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The leader broker writes the record to its local log file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A follower broker fetches the message from the leader and writes to its local
    replica log file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The leader broker updates the partition state in ZooKeeper to update in-sync
    replicas, if required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bob consumes customer order records from the topic `customerOrders`. Bob receives
    the record produced by Alice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An internal application processes all messages arriving in `customerOrders`
    to produce real-time metrics on popular products.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A secure deployment must guarantee:'
  prefs: []
  type: TYPE_NORMAL
- en: Client authenticity
  prefs: []
  type: TYPE_NORMAL
- en: When Alice establishes a client connection to the broker, the broker should
    authenticate the client to ensure that the message is really coming from Alice.
  prefs: []
  type: TYPE_NORMAL
- en: Server authenticity
  prefs: []
  type: TYPE_NORMAL
- en: Before sending a message to the leader broker, Alice’s client should verify
    that the connection is to the real broker.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  prefs: []
  type: TYPE_NORMAL
- en: All connections where the message flows, as well as all disks where messages
    are stored, should be encrypted or physically secured to prevent eavesdroppers
    from reading the data and to ensure that data cannot be stolen.
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  prefs: []
  type: TYPE_NORMAL
- en: Message digests should be included for data transmitted over insecure networks
    to detect tampering.
  prefs: []
  type: TYPE_NORMAL
- en: Access control
  prefs: []
  type: TYPE_NORMAL
- en: Before writing the message to the log, the leader broker should verify that
    Alice is authorized to write to `customerOrders`. Before returning messages to
    Bob’s consumer, the broker should verify that Bob is authorized to read from the
    topic. If Bob’s consumer uses group management, the broker should also verify
    that Bob has access to the consumer group.
  prefs: []
  type: TYPE_NORMAL
- en: Auditability
  prefs: []
  type: TYPE_NORMAL
- en: An audit trail that shows all operations that were performed by brokers, Alice,
    Bob, and other clients should be logged.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs: []
  type: TYPE_NORMAL
- en: Brokers should apply quotas and limits to avoid some users hogging all the available
    bandwidth or overwhelming the broker with denial-of-service attacks. ZooKeeper
    should be locked down to ensure availability of the Kafka cluster since broker
    availability is dependent on ZooKeeper availability and the integrity of metadata
    stored in ZooKeeper.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we explore the Kafka security features that can be
    used to provide these guarantees. We first introduce the Kafka connection model
    and the security protocols associated with connections from clients to Kafka brokers.
    We then look at each security protocol in detail and examine the authentication
    capabilities of each protocol to ascertain client authenticity and server authenticity.
    We review options for encryption at different stages, including built-in encryption
    of data in transit in some security protocols to address data privacy and data
    integrity. Then, we explore customizable authorization in Kafka to manage access
    control and the main logs that contribute to auditability. Finally, we review
    security for the rest of the system, including ZooKeeper and the platform, which
    is necessary to maintain availability. For details on quotas that contribute to
    service availability through fair allocation of resources among users, refer to
    [Chapter 3](ch03.html#writing_messages_to_kafka).
  prefs: []
  type: TYPE_NORMAL
- en: Security Protocols
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka brokers are configured with listeners on one or more endpoints and accept
    client connections on these listeners. Each listener can be configured with its
    own security settings. Security requirements on a private internal listener that
    is physically protected and only accessible to authorized personnel may be different
    from the security requirements of an external listener accessible over the public
    internet. The choice of security protocol determines the level of authentication
    and encryption of data in transit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka supports four security protocols using two standard technologies, TLS
    and SASL. Transport Layer Security (TLS), commonly referred to by the name of
    its predecessor, Secure Sockets Layer (SSL), supports encryption as well as client
    and server authentication. Simple Authentication and Security Layer (SASL) is
    a framework for providing authentication using different mechanisms in connection-oriented
    protocols. Each Kafka security protocol combines a transport layer (PLAINTEXT
    or SSL) with an optional authentication layer (SSL or SASL):'
  prefs: []
  type: TYPE_NORMAL
- en: PLAINTEXT
  prefs: []
  type: TYPE_NORMAL
- en: PLAINTEXT transport layer with no authentication. Is suitable only for use within
    private networks for processing data that is not sensitive since no authentication
    or encryption is used.
  prefs: []
  type: TYPE_NORMAL
- en: SSL
  prefs: []
  type: TYPE_NORMAL
- en: SSL transport layer with optional SSL client authentication. Is suitable for
    use in insecure networks since client and server authentication as well as encryption
    are supported.
  prefs: []
  type: TYPE_NORMAL
- en: SASL_PLAINTEXT
  prefs: []
  type: TYPE_NORMAL
- en: PLAINTEXT transport layer with SASL client authentication. Some SASL mechanisms
    also support server authentication. Does not support encryption and hence is suitable
    only for use within private networks.
  prefs: []
  type: TYPE_NORMAL
- en: SASL_SSL
  prefs: []
  type: TYPE_NORMAL
- en: SSL transport layer with SASL authentication. Is suitable for use in insecure
    networks since client and server authentication as well as encryption are supported.
  prefs: []
  type: TYPE_NORMAL
- en: TLS/SSL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TLS is one of the most widely used cryptographic protocols on the public internet.
    Application protocols like HTTP, SMTP, and FTP rely on TLS to provide privacy
    and integrity of data in transit. TLS relies on a Public Key Infrastructure (PKI)
    to create, manage, and distribute digital certificates that can be used for asymmetric
    encryption, avoiding the need for distributing shared secrets between servers
    and clients. Session keys generated during the TLS handshake enable symmetric
    encryption with higher performance for subsequent data transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The listener used for inter-broker communication can be selected by configuring
    `inter.broker.listener.name` or `security.inter.broker.protocol`. Both server-side
    and client-side configuration options must be provided in the broker configuration
    for the security protocol used for inter-broker communication. This is because
    brokers need to establish client connections for that listener. The following
    example configures SSL for the inter-broker and internal listeners, and SASL_SSL
    for the external listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Clients are configured with a security protocol and bootstrap servers that
    determine the broker listener. Metadata returned to clients contains only the
    endpoints corresponding to the same listener as the bootstrap servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the next section on authentication, we review the protocol-specific configuration
    options for brokers and clients for each security protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authentication is the process of establishing the identity of the client and
    server to verify client authenticity and server authenticity. When Alice’s client
    connects to the leader broker to produce a customer order record, server authentication
    enables the client to establish that the server that the client is talking to
    is the actual broker. Client authentication verifies Alice’s identity by validating
    Alice’s credentials, like a password or digital certificate, to determine that
    the connection is from Alice and not an impersonator. Once authenticated, Alice’s
    identity is associated with the connection throughout the lifetime of the connection.
    Kafka uses an instance of `KafkaPrincipal` to represent client identity and uses
    this principal to grant access to resources and allocate quotas for connections
    with that client identity. The `KafkaPrincipal` for each connection is established
    during authentication based on the authentication protocol. For example, the principal
    `User:Alice` may be used for Alice based on the username provided for password-based
    authentication. `KafkaPrincipal` may be customized by configuring `principal.builder.class`
    for brokers.
  prefs: []
  type: TYPE_NORMAL
- en: Anonymous Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The principal `User:ANONYMOUS` is used for unauthenticated connections. This
    includes clients on PLAINTEXT listeners as well as unauthenticated clients on
    SSL listeners.
  prefs: []
  type: TYPE_NORMAL
- en: SSL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When Kafka is configured with SSL or SASL_SSL as the security protocol for a
    listener, TLS is used as the secure transport layer for connections on that listener.
    When a connection is established over TLS, the TLS handshake process performs
    authentication, negotiates cryptographic parameters, and generates shared keys
    for encryption. The server’s digital certificate is verified by the client to
    establish the identity of the server. If client authentication using SSL is enabled,
    the server also verifies the client’s digital certificate to establish the identity
    of the client. All traffic over SSL is encrypted, making it suitable for use in
    insecure networks.
  prefs: []
  type: TYPE_NORMAL
- en: SSL Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSL channels are encrypted and hence introduce a noticeable overhead in terms
    of CPU usage. Zero-copy transfer is currently not supported for SSL. Depending
    on the traffic pattern, the overhead may be up to 20–30%.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring TLS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When TLS is enabled for a broker listener using SSL or SASL_SSL, brokers should
    be configured with a key store containing the broker’s private key and certificate,
    and clients should be configured with a trust store containing the broker certificate
    or the certificate of the certificate authority (CA) that signed the broker certificate.
    Broker certificates should contain the broker hostname as a Subject Alternative
    Name (SAN) extension or as the Common Name (CN) to enable clients to verify the
    server hostname. Wildcard certificates can be used to simplify administration
    by using the same key store for all brokers in a domain.
  prefs: []
  type: TYPE_NORMAL
- en: Server Hostname verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Kafka clients verify that the hostname of the server stored in the
    server certificate matches the host that the client is connecting to. The connection
    hostname may be a bootstrap server that the client is configured with or an advertised
    listener hostname that was returned by a broker in a metadata response. Hostname
    verification is a critical part of server authentication that protects against
    man-in-the-middle attacks and hence should not be disabled in production systems.
  prefs: []
  type: TYPE_NORMAL
- en: Brokers can be configured to authenticate clients connecting over listeners
    using SSL as the security protocol by setting the broker configuration option
    `ssl.​cli⁠ent.auth=required`. Clients should be configured with a key store, and
    brokers should be configured with a trust store containing client certificates
    or the certificate of the CAs that signed the client certificates. If SSL is used
    for inter-broker communication, broker trust stores should include the CA of the
    broker certificates as well as the CA of the client certificates. By default,
    the distinguished name (DN) of the client certificate is used as the `KafkaPrincipal`
    for authorization and quotas. The configuration option `ssl.principal.mapping.rules`
    can be used to provide a list of rules to customize the principal. Listeners using
    SASL_SSL disable TLS client authentication and rely on SASL authentication and
    the `KafkaPrincipal` established by SASL.
  prefs: []
  type: TYPE_NORMAL
- en: SSL Client Authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSL client authentication may be made optional by setting `ssl.​cli⁠ent.auth=requested`.
    Clients that are not configured with key stores will complete the TLS handshake
    in this case, but will be assigned the principal `User:ANONYMOUS`.
  prefs: []
  type: TYPE_NORMAL
- en: The following examples show how to create key stores and trust stores for server
    and client authentication using a self-signed CA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate self-signed CA key-pair for brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a key-pair for the CA and store it in a PKCS12 file server.ca.p12\. We
    use this for signing certificates.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Export the CA’s public certificate to server.ca.crt. This will be included in
    trust stores and certificate chains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create key stores for brokers with a certificate signed by the self-signed
    CA. If using wildcard hostnames, the same key store can be used for all brokers.
    Otherwise, create a key store for each broker with its fully qualified domain
    name (FQDN):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a private key for a broker and store it in the PKCS12 file server.ks.p12.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a certificate signing request.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the CA key store to sign the broker’s certificate. The signed certificate
    is stored in server.crt.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Import the broker’s certificate chain into the broker’s key store.
  prefs: []
  type: TYPE_NORMAL
- en: 'If TLS is used for inter-broker communication, create a trust store for brokers
    with the broker’s CA certificate to enable brokers to authenticate one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a trust store for clients with the broker’s CA certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If TLS client authentication is enabled, clients must be configured with a
    key store. The following script generates a self-signed CA for clients and creates
    a key store for clients with a certificate signed by the client CA. The client
    CA is added to the broker trust store so that brokers can verify client authenticity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We create a new CA for clients in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Clients authenticating with this certificate use `User:CN=Metrics ⁠App,​O=Con⁠flu⁠ent,C=GB`
    as the principal, by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We add the client certificate chain to the client key store.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The broker’s trust store should contain the CAs of all clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the key and trust stores, we can configure TLS for brokers. Brokers
    require a trust store only if TLS is used for inter-broker communication or if
    client authentication is enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Clients are configured with the generated trust store. The key store should
    be configured for clients if client authentication is required.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Trust Stores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trust store configuration can be omitted in brokers as well as clients when
    using certificates signed by well-known trusted authorities. The default trust
    stores in the Java installation will be sufficient to establish trust in this
    case. Installation steps are described in [Chapter 2](ch02.html#installing_kafka).
  prefs: []
  type: TYPE_NORMAL
- en: 'Key stores and trust stores must be updated periodically before certificates
    expire to avoid TLS handshake failures. Broker SSL stores can be dynamically updated
    by modifying the same file or setting the configuration option to a new versioned
    file. In both cases, the Admin API or the Kafka configs tool can be used to trigger
    the update. The following example updates the key store for the external listener
    of a broker with broker id `0` using the configs tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Security considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TLS is widely used to provide transport layer security for several protocols,
    including HTTPS. As with any security protocol, it is important to understand
    the potential threats and mitigation strategies when adopting a protocol for mission-critical
    applications. Kafka enables only the newer protocols TLSv1.2 and TLSv1.3 by default,
    since older protocols like TLSv1.1 have known vulnerabilities. Due to issues with
    insecure renegotiation, Kafka does not support renegotiation for TLS connections.
    Hostname verification is enabled by default to prevent man-in-the-middle attacks.
    Security can be tightened further by restricting cipher suites. Strong ciphers
    with at least a 256-bit encryption key size protect against cryptographic attacks
    and ensure data integrity when transporting data over an insecure network. Some
    organizations require TLS protocol and ciphers to be restricted to comply with
    security standards like FIPS 140-2.
  prefs: []
  type: TYPE_NORMAL
- en: Since key stores containing private keys are stored on the filesystem by default,
    it is vital to limit access to key store files using filesystem permissions. Standard
    Java TLS features can be used to enable certificate revocation if a private key
    is compromised. Short-lived keys can be used to reduce exposure in this case.
  prefs: []
  type: TYPE_NORMAL
- en: TLS handshakes are expensive and utilize a significant amount of time on network
    threads in brokers. Listeners using TLS on insecure networks should be protected
    against denial-of-service attacks using connection quotas and limits to protect
    availability of brokers. The broker configuration option `connection.failed.​aut⁠hen⁠tication.delay.ms`
    can be used to delay failed response on authentication failures to reduce the
    rate at which authentication failures are retried by clients.
  prefs: []
  type: TYPE_NORMAL
- en: SASL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kafka protocol supports authentication using SASL and has built-in support
    for several commonly used SASL mechanisms. SASL can be combined with TLS as the
    transport layer to provide a secure channel with authentication and encryption.
    SASL authentication is performed through a sequence of server challenges and client
    responses where the SASL mechanism defines the sequence and wire format of challenges
    and responses. Kafka brokers support the following SASL mechanisms out of the
    box with customizable callbacks to integrate with existing security infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: GSSAPI
  prefs: []
  type: TYPE_NORMAL
- en: Kerberos authentication is supported using SASL/GSSAPI and can be used to integrate
    with Kerberos servers like Active Directory or OpenLDAP.
  prefs: []
  type: TYPE_NORMAL
- en: PLAIN
  prefs: []
  type: TYPE_NORMAL
- en: Username/password authentication that is typically used with a custom server-side
    callback to verify passwords from an external password store.
  prefs: []
  type: TYPE_NORMAL
- en: SCRAM-SHA-256 and SCRAM-SHA-512
  prefs: []
  type: TYPE_NORMAL
- en: Username/password authentication available out of the box with Kafka without
    the need for additional password stores.
  prefs: []
  type: TYPE_NORMAL
- en: OAUTHBEARER
  prefs: []
  type: TYPE_NORMAL
- en: Authentication using OAuth bearer tokens that is typically used with custom
    callbacks to acquire and validate tokens granted by standard OAuth servers.
  prefs: []
  type: TYPE_NORMAL
- en: One or more SASL mechanisms may be enabled on each SASL-enabled listener in
    the broker by configuring `sasl.enabled.mechanisms` for that listener. Clients
    may choose any of the enabled mechanisms by configuring `sasl.mechanism`.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka uses the Java Authentication and Authorization Service (JAAS) for configuring
    SASL. The configuration option `sasl.jaas.config` contains a single JAAS configuration
    entry that specifies a login module and its options. Brokers use the `listener`
    and `mechanism` prefixes when configuring `sasl.jaas.config`. For example, `listener.name.external.gssapi.sasl.jaas.config`
    configures the JAAS configuration entry for SASL/GSSAPI on the listener named
    `EXTERNAL`. The login process on brokers and clients uses the JAAS configuration
    to determine the public and private credentials used for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: JAAS Configuration File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAAS configuration may also be specified in configuration files using the Java
    system property `java.security.auth.login.​con⁠fig`. However, the Kafka option
    `sasl.jaas.config` is recommended since it supports password protection and separate
    configuration for each SASL mechanism when multiple mechanisms are enabled on
    a listener.
  prefs: []
  type: TYPE_NORMAL
- en: SASL mechanisms supported by Kafka can be customized to integrate with third-party
    authentication servers using callback handlers. A login callback handler may be
    provided for brokers or clients to customize the login process, for example, to
    acquire credentials to be used for authentication. A server callback handler may
    be provided to perform authentication of client credentials, for example, to verify
    passwords using an external password server. A client callback handler may be
    provided to inject client credentials instead of including them in the JAAS configuration.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we explore the SASL mechanisms supported by Kafka
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: SASL/GSSAPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kerberos is a widely used network authentication protocol that uses strong cryptography
    to support secure mutual authentication over an insecure network. Generic Security
    Service Application Program Interface (GSS-API) is a framework for providing security
    services to applications using different authentication mechanisms. [RFC-4752](https://oreil.ly/wxTZt)
    introduces the SASL mechanism GSSAPI for authentication using GSS-API’s Kerberos
    V5 mechanism. The availability of open source as well as enterprise-grade commercial
    implementations of Kerberos servers has made Kerberos a popular choice for authentication
    across many sectors with strict security requirements. Kafka supports Kerberos
    authentication using SASL/GSSAPI.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/GSSAPI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Kafka uses GSSAPI security providers included in the Java runtime environment
    to support secure authentication using Kerberos. JAAS configuration for GSSAPI
    includes the path of a keytab file that contains the mapping of principals to
    their long-term keys in encrypted form. To configure GSSAPI for brokers, create
    a keytab for each broker with a principal that includes the broker’s hostname.
    Broker hostnames are verified by clients to ensure server authenticity and prevent
    man-in-the-middle attacks. Kerberos requires a secure DNS service for host name
    lookup during authentication. In deployments where forward and reverse lookup
    do not match, the Kerberos configuration file *krb5.conf* on clients can be configured
    to set `rdns=false` to disable reverse lookup. JAAS configuration for each broker
    should include the Kerberos V5 login module from the Java runtime, the pathname
    of the keytab file, and the full broker principal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use `sasl.jaas.config` prefixed with the listener prefix, which contains
    the listener name and SASL mechanism in lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Keytab files must be readable by the broker process.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Service principal for brokers should include the broker hostname.
  prefs: []
  type: TYPE_NORMAL
- en: 'If SASL/GSSAPI is used for inter-broker communication, inter-broker SASL mechanism
    and the Kerberos service name should also be configured for brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Clients should be configured with their own keytab and principal in the JAAS
    configuration and `sasl.kerberos.service.name` to indicate the name of the service
    they are connecting to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The service name for the Kafka service should be specified for clients.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Clients may use principals without hostname.
  prefs: []
  type: TYPE_NORMAL
- en: The short name of the principal is used as the client identity by default. For
    example, `User:Alice` is the client principal and `User:kafka` is the broker principal
    in the example. The broker configuration `sasl.kerberos.principal.to.local.rules`
    can be used to apply a list of rules to transform the fully qualified principal
    to a custom principal.
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use of SASL_SSL is recommended in production deployments using Kerberos to protect
    the authentication flow as well as data traffic on the connection after authentication.
    If TLS is not used to provide a secure transport layer, eavesdroppers on the network
    may gain enough information to mount a dictionary attack or brute-force attack
    to steal client credentials. It is safer to use randomly generated keys for brokers
    instead of keys generated from passwords that are easier to crack. Weak encryption
    algorithms like DES-MD5 should be avoided in favor of stronger algorithms. Access
    to keytab files must be restricted using filesystem permissions since any user
    in possession of the file may impersonate the user.
  prefs: []
  type: TYPE_NORMAL
- en: SASL/GSSAPI requires a secure DNS service for server authentication. Because
    denial-of-service attacks against the KDC or DNS service can result in authentication
    failures in clients, it is necessary to monitor the availability of these services.
    Kerberos also relies on loosely synchronized clocks with configurable variability
    to detect replay attacks. It is important to ensure that clock synchronization
    is secure.
  prefs: []
  type: TYPE_NORMAL
- en: SASL/PLAIN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RFC-4616](https://oreil.ly/wZrxB) defines a simple username/password authentication
    mechanism that can be used with TLS to provide secure authentication. During authentication,
    the client sends a username and password to the server, and the server verifies
    the password using its password store. Kafka has built-in SASL/PLAIN support that
    can be integrated with a secure external password database using a custom callback
    handler.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/PLAIN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The default implementation of SASL/PLAIN uses the broker’s JAAS configuration
    as the password store. All client usernames and passwords are included as login
    options, and the broker verifies that the password provided by a client during
    authentication matches one of these entries. A broker username and password are
    required only if SASL/PLAIN is used for inter-broker communication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The username and password used for inter-broker connections initiated by the
    broker.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: When Alice’s client connects to the broker, the password provided by Alice is
    validated against this password in the broker’s config.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients must be configured with username and password for authentication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The built-in implementation that stores all passwords in every broker’s JAAS
    configuration is insecure and not very flexible since all brokers will need to
    be restarted to add or remove a user. When using SASL/PLAIN in production, a custom
    server callback handler can be used to integrate brokers with a secure third-party
    password server. Custom callback handlers can also be used to support password
    rotation. On the server side, a server callback handler should support both old
    and new passwords for an overlapping period until all clients switch to the new
    password. The following example shows a callback handler that verifies encrypted
    passwords from files generated using the Apache tool `htpasswd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use multiple password files so that we can support password rotation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We pass pathnames of password files as a JAAS option in the broker configuration.
    Custom broker configuration options may also be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We check if the password matches in any of the files, allowing both old and
    new passwords to be used for a period of time.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We use `htpasswd` for simplicity. A secure database can be used for production
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Brokers are configured with the password validation callback handler and its
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'On the client side, a client callback handler that implements `org.apache.kafka.​com⁠mon.security.auth.AuthenticateCallbackHandler`
    can be used to load passwords dynamically at runtime when a connection is established
    instead of loading statically from the JAAS configuration during startup. Passwords
    may be loaded from encrypted files or using an external secure server to improve
    security. The following example loads passwords dynamically from a file using
    configuration classes in Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We load the config file within the callback to ensure we use the latest password
    to support password rotation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying configuration library returns the actual password value even
    if the password is externalized.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We define password configs with the `PASSWORD` type to ensure that passwords
    are not included in log entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients as well as brokers that use SASL/PLAIN for inter-broker communication
    can be configured with the client-side callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Security considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since SASL/PLAIN transmits clear-text passwords over the wire, the PLAIN mechanism
    should be enabled only with encryption using SASL_SSL to provide a secure transport
    layer. Passwords stored in clear text in the JAAS configuration of brokers and
    clients are not secure, so consider encrypting or externalizing these passwords
    in a secure password store. Instead of using the built-in password store that
    stores all client passwords in the broker JAAS configuration, use a secure external
    password server that stores passwords securely and enforces strong password policies.
  prefs: []
  type: TYPE_NORMAL
- en: Clear-Text Passwords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Avoid clear-text passwords in configuration files even if the files can be protected
    using filesystem permissions. Consider externalizing or encrypting passwords to
    ensure that passwords are not inadvertently exposed. Kafka’s password protection
    feature is described later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: SASL/SCRAM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RFC-5802](https://oreil.ly/dXe3y) introduces a secure username/password authentication
    mechanism that addresses the security concerns with password authentication mechanisms
    like SASL/PLAIN, which send passwords over the wire. The Salted Challenge Response
    Authentication Mechanism (SCRAM) avoids transmitting clear-text passwords and
    stores passwords in a format that makes it impractical to impersonate clients.
    Salting combines passwords with some random data before applying a one-way cryptographic
    hash function to store passwords securely. Kafka has a built-in SCRAM provider
    that can be used in deployments with secure ZooKeeper without the need for additional
    password servers. The SCRAM mechanisms `SCRAM-SHA-256` and `SCRAM-SHA-512` are
    supported by the Kafka provider.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/SCRAM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An initial set of users can be created after starting ZooKeeper prior to starting
    brokers. Brokers load SCRAM user metadata into an in-memory cache during startup,
    ensuring that all users, including the broker user for inter-broker communication,
    can authenticate successfully. Users can be added or deleted at any time. Brokers
    keep the cache up-to-date using notifications based on a ZooKeeper watcher. In
    this example, we create a user with the principal `User:Alice` and password `Alice-password`
    for SASL mechanism `SCRAM-SHA-512`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'One or more SCRAM mechanisms can be enabled on a listener by configuring the
    mechanisms on the broker. A username and password are required for brokers only
    if the listener is used for inter-broker communication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Username and password for inter-broker connections initiated by the broker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients must be configured to use one of the SASL mechanisms enabled in the
    broker, and the client JAAS configuration must include a username and password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can add new SCRAM users using `--add-config` and delete users using the
    `--delete-config` option of the configs tool. When an existing user is deleted,
    new connections cannot be established for that user, but existing connections
    of the user will continue to work. A reauthentication interval can be configured
    for the broker to limit the amount of time existing connections may continue to
    operate after a user is deleted. The following example deletes the `SCRAM-SHA-512`
    config for `Alice` to remove Alice’s credentials for that mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Security considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SCRAM applies a one-way cryptographic hash function on the password combined
    with a random salt to avoid the actual password being transmitted over the wire
    or stored in a database. However, any password-based system is only as secure
    as the passwords. Strong password policies must be enforced to protect the system
    from brute-force or dictionary attacks. Kafka provides safeguards by supporting
    only the strong hashing algorithms SHA-256 and SHA-512 and avoiding weaker algorithms
    like SHA-1\. This is combined with a high default iteration count of 4,096 and
    unique random salts for every stored key to limit the impact if ZooKeeper security
    is compromised.
  prefs: []
  type: TYPE_NORMAL
- en: You should take additional precautions to protect the keys transmitted during
    handshake and the keys stored in ZooKeeper to protect against brute-force attacks.
    SCRAM must be used with `SASL_SSL` as the security protocol to avoid eavesdroppers
    from gaining access to hashed keys during authentication. ZooKeeper must also
    be SSL-enabled, and ZooKeeper data must be protected using disk encryption to
    ensure that stored keys cannot be retrieved even if the store is compromised.
    In deployments without a secure ZooKeeper, SCRAM callbacks can be used to integrate
    with a secure external credential store.
  prefs: []
  type: TYPE_NORMAL
- en: SASL/OAUTHBEARER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OAuth is an authorization framework that enables applications to obtain limited
    access to HTTP services. [RFC-7628](https://oreil.ly/sPBfv) defines the OAUTHBEARER
    SASL mechanism that enables credentials obtained using OAuth 2.0 to access protected
    resources in non-HTTP protocols. OAUTHBEARER avoids security vulnerabilities in
    mechanisms that use long-term passwords by using OAuth 2.0 bearer tokens with
    a shorter lifetime and limited resource access. Kafka supports SASL/OAUTHBEARER
    for client authentication, enabling integration with third-party OAuth servers.
    The built-in implementation of OAUTHBEARER uses unsecured JSON Web Tokens (JWTs)
    and is not suitable for production use. Custom callbacks can be added to integrate
    with standard OAuth servers to provide secure authentication using the OAUTHBEARER
    mechanism in production deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SASL/OAUTHBEARER
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The built-in implementation of SASL/OAUTHBEARER in Kafka does not validate
    tokens and hence only requires the login module to be specified in the JAAS configuration.
    If the listener is used for inter-broker communication, details of the token used
    for client connections initiated by brokers must also be provided. The option
    `unsecuredLoginStringClaim_sub` is the subject claim that determines the `KafkaPrincipal`
    for the connection by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Subject claim for the token used for inter-broker connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clients must be configured with the subject claim option `unsecuredLoginStringClaim_sub`.
    Other claims and token lifetime may also be configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`User:Alice` is the default `KafkaPrincipal` for connections using this configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate Kafka with third-party OAuth servers for using bearer tokens in
    production, Kafka clients must be configured with `sasl.login.callback.handler.class`
    to acquire tokens from the OAuth server using the long-term password or a refresh
    token. If OAUTHBEARER is used for inter-broker communication, brokers must also
    be configured with a login callback handler to acquire tokens for client connections
    created by the broker for inter-broker communication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Clients must acquire a token from the OAuth server and set a valid token on
    the callback.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO12-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The client may also include optional extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Brokers must also be configured with a server callback handler using `listener.name.<listener-name>.oauthbearer.sasl.server.callback.handler.​class`
    for validating tokens provided by the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO13-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`OAuthBearerValidatorCallback` contains the token from the client. Brokers
    validate this token.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO13-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Brokers validate any optional extensions from the client.
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since SASL/OAUTHBEARER clients send OAuth 2.0 bearer tokens over the network
    and these tokens may be used to impersonate clients, TLS must be enabled to encrypt
    authentication traffic. Short-lived tokens can be used to limit exposure if tokens
    are compromised. Reauthentication may be enabled for brokers to prevent connections
    outliving the tokens used for authentication. A reauthentication interval configured
    on brokers, combined with token revocation support, limit the amount of time an
    existing connection may continue to use a token after revocation.
  prefs: []
  type: TYPE_NORMAL
- en: Delegation tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delegation tokens are shared secrets between Kafka brokers and clients that
    provide a lightweight configuration mechanism without the requirement to distribute
    SSL key stores or Kerberos keytabs to client applications. Delegation tokens can
    be used to reduce the load on authentication servers, like the Kerberos Key Distribution
    Center (KDC). Frameworks like Kafka Connect can use delegation tokens to simplify
    security configuration for workers. A client that has authenticated with Kafka
    brokers can create delegation tokens for the same user principal and distribute
    these tokens to workers, which can then authenticate directly with Kafka brokers.
    Each delegation token consists of a token identifier and a hash-based message
    authentication code (HMAC) used as a shared secret. Client authentication with
    delegation tokens is performed using SASL/SCRAM with the token identifier as username
    and HMAC as the password.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delegation tokens can be created or renewed using the Kafka Admin API or the
    `delegation-tokens` command. To create delegation tokens for the principal `User:Alice`,
    the client must be authenticated using Alice’s credentials for any authentication
    protocol other than delegation tokens. Clients authenticated using delegation
    tokens cannot create other delegation tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO14-1)'
  prefs: []
  type: TYPE_NORMAL
- en: If Alice runs this command, the generated token can be used to impersonate Alice.
    The owner of this token is `User:Alice`. We also configure `User:Bob` as a token
    renewer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO14-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The renewal command can be run by the token owner (Alice) or the token renewer
    (Bob).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring delegation tokens
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To create and validate delegation tokens, all brokers must be configured with
    the same master key using the configuration option `delegation.token.master.key`.
    This key can only be rotated by restarting all brokers. All existing tokens should
    be deleted before updating the master key since they can no longer be used, and
    new tokens should be created after the key is updated on all brokers.
  prefs: []
  type: TYPE_NORMAL
- en: 'At least one of the SASL/SCRAM mechanisms must be enabled on brokers to support
    authentication using delegation tokens. Clients should be configured to use SCRAM
    with a token identifier as username and token HMAC as the password. The `Kafka​P⁠rincipal`
    for the connections using this configuration will be the original principal associated
    with the token, e.g., `User:Alice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO15-1)'
  prefs: []
  type: TYPE_NORMAL
- en: SCRAM configuration with `tokenauth` is used to configure delegation tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like the built-in SCRAM implementation, delegation tokens are suitable for production
    use only in deployments where ZooKeeper is secure. All the security considerations
    described under SCRAM also apply to delegation tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The master key used by brokers for generating tokens must be protected using
    encryption or by externalizing the key in a secure password store. Short-lived
    delegation tokens can be used to limit exposure if a token is compromised. Reauthentication
    can be enabled in brokers to prevent connections operating with expired tokens
    and to limit the amount of time existing connections may continue to operate after
    token deletion.
  prefs: []
  type: TYPE_NORMAL
- en: Reauthentication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw earlier, Kafka brokers perform client authentication when a connection
    is established by the client. Client credentials are verified by the brokers,
    and the connection authenticates successfully if the credentials are valid at
    that time. Some security mechanisms like Kerberos and OAuth use credentials with
    a limited lifetime. Kafka uses a background login thread to acquire new credentials
    before the old ones expire, but the new credentials are used only to authenticate
    new connections by default. Existing connections that were authenticated with
    old credentials continue to process requests until disconnection occurs due to
    a request timeout, an idle timeout, or network errors. Long-lived connections
    may continue to process requests long after the credentials used to authenticate
    the connections expire. Kafka brokers support reauthentication for connections
    authenticated using SASL using the configuration option `connections.max.reauth.ms`.
    When this option is set to a positive integer, Kafka brokers determine the session
    lifetime for SASL connections and inform clients of this lifetime during the SASL
    handshake. Session lifetime is the lower of the remaining lifetime of the credential
    or `connections.max.reauth.ms`. Any connection that doesn’t reauthenticate within
    this interval is terminated by the broker. Clients perform reauthentication using
    the latest credentials acquired by the background login thread or injected using
    custom callbacks. Reauthentication can be used to tighten security in several
    scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: For SASL mechanisms like GSSAPI and OAUTHBEARER that use credentials with a
    limited lifetime, reauthentication guarantees that all active connections are
    associated with valid credentials. Short-lived credentials limit exposure in case
    credentials that are compromised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Password-based SASL mechanisms like PLAIN and SCRAM can support password rotation
    by adding periodic login. Reauthentication limits the amount of time requests
    are processed on connections authenticated with the old password. Custom server
    callback that allows both old and new passwords for a period of time can be used
    to avoid outages until all clients migrate to the new password.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connections.max.reauth.ms` forces reauthentication in all SASL mechanisms,
    including those with nonexpiring credentials. This limits the amount of time a
    credential may be associated with an active connection after it has been revoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connections from clients without SASL reauthentication support are terminated
    on session expiry, forcing the clients to reconnect and authenticate again, thus
    providing the same security guarantees for expired or revoked credentials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compromised Users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a user is compromised, action must be taken to remove the user from the system
    as soon as possible. All new connections will fail to authenticate with Kafka
    brokers once the user is removed from the authentication server. Existing connections
    will continue to process requests until the next reauthentication timeout. If
    `connections.max.reauth.ms` is not configured, no timeout is applied and existing
    connections may continue to use the compromised user’s identity for a long time.
    Kafka does not support SSL renegotiation due to known vulnerabilities during renegotiation
    in older SSL protocols. Newer protocols like TLSv1.3 do not support renegotiation.
    So, existing SSL connections may continue to use revoked or expired certificates.
    `Deny` ACLs for the user principal can be used to prevent these connections from
    performing any operation. Since ACL changes are applied with very small latencies
    across all brokers, this is the quickest way to disable access for compromised
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Security Updates Without Downtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka deployments need regular maintenance to rotate secrets, apply security
    fixes, and update to the latest security protocols. Many of these maintenance
    tasks are performed using rolling updates where one by one, brokers are shut down
    and restarted with an updated configuration. Some tasks like updating SSL key
    stores and trust stores can be performed using dynamic config updates without
    restarting brokers.
  prefs: []
  type: TYPE_NORMAL
- en: 'When adding a new security protocol to an existing deployment, a new listener
    can be added to brokers with the new protocol while retaining the old listener
    with the old protocol to ensure that client applications can continue to function
    using the old listener during the update. For example, the following sequence
    can be used to switch from PLAINTEXT to SASL_SSL in an existing deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a new listener on a new port to each broker using the Kafka configs tool.
    Use a single config update command to update `listeners` and `advertised.​lis⁠teners`
    to include the old listener as well as the new listener, and provide all the configuration
    options for the new SASL_SSL listener with the listener prefix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify all client applications to use the new SASL_SSL listener.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If inter-broker communication is being updated to use the new SASL_SSL listener,
    perform a rolling update of brokers with the new `inter.broker.​lis⁠tener.name`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the configs tool to remove the old listener from `listeners` and `advertised.listeners`
    and to remove any unused configuration options of the old listener.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SASL mechanisms can be added or removed from existing SASL listeners without
    downtime using rolling updates on the same listener port. The following sequence
    switches the mechanism from PLAIN to SCRAM-SHA-256:'
  prefs: []
  type: TYPE_NORMAL
- en: Add all existing users to the SCRAM store using the Kafka configs tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `sasl.enabled.mechanisms=PLAIN,SCRAM-SHA-256`, configure `list⁠ener.​name.<_listener-name_>.scram-sha-256.sasl.jaas.config`
    for the listener, and perform a rolling update of brokers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify all client applications to use `sasl.mechanism=SCRAM-SHA-256` and update
    `sasl.jaas.config` to use SCRAM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the listener is used for inter-broker communication, use a rolling update
    of brokers to set `sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform another rolling update of brokers to remove the PLAIN mechanism. Set
    `sasl.enabled.mechanisms=SCRAM-SHA-256` and remove `listener.name.​<listener-name>.plain.sasl.jaas.config`
    and any other configuration options for PLAIN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encryption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Encryption is used to preserve data privacy and data integrity. As we discussed
    earlier, Kafka listeners using SSL and SASL_SSL security protocols use TLS as
    the transport layer, providing secure encrypted channels that protect data transmitted
    over an insecure network. TLS cipher suites can be restricted to strengthen security
    and adhere to security requirements like the Federal Information Processing Standard
    (FIPS).
  prefs: []
  type: TYPE_NORMAL
- en: Additional measures must be taken to protect data at rest to ensure that sensitive
    data cannot be retrieved even by users with physical access to the disk that stores
    Kafka logs. To avoid security breaches even if the disk is stolen, physical storage
    can be encrypted using whole disk encryption or volume encryption.
  prefs: []
  type: TYPE_NORMAL
- en: While encryption of transport layer and data storage may provide adequate protection
    in many deployments, additional protection may be required to avoid granting automatic
    data access to platform administrators. Unencrypted data present in broker memory
    may appear in heap dumps, and administrators with direct access to the disk will
    be able to access these, as well as Kafka logs containing potentially sensitive
    data. In deployments with highly sensitive data or Personally Identifiable Information
    (PII), extra measures are required to preserve data privacy. To comply with regulatory
    requirements, especially in cloud deployments, it is necessary to guarantee that
    confidential data cannot be accessed by platform administrators or cloud providers
    by any means. Custom encryption providers can be plugged into Kafka clients to
    implement end-to-end encryption that guarantees that the entire data flow is encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#writing_messages_to_kafka) on Kafka producers, we saw
    that *serializers* are used to convert messages into the byte array stored in
    Kafka logs, and in [Chapter 4](ch04.html#reading_data_from_kafka) on Kafka consumers,
    we saw that *deserializers* converted the byte array back to the message. Serializers
    and deserializers can be integrated with an encryption library to perform encryption
    of the message during serialization, and decryption during deserialization. Message
    encryption is typically performed using symmetric encryption algorithms like AES.
    A shared encryption key stored in a key management system (KMS) enables producers
    to encrypt the message and consumers to decrypt the message. Brokers do not require
    access to the encryption key and never see the unencrypted contents of the message,
    making this approach safe to use in cloud environments. Encryption parameters
    that are required to decrypt the message may be stored in message headers or in
    the message payload if older consumers without header support need access to the
    message. A digital signature may also be included in message headers to verify
    message integrity.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-2](#fig-2-end-to-end-encryption) shows a Kafka data flow with end-to-end
    encryption.'
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1102](assets/kdg2_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. End-to-end encryption
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We send a message using a Kafka producer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The producer uses an encryption key from KMS to encrypt the message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encrypted message is sent to the broker. The broker stores the encrypted
    message in the partition logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The broker sends the encrypted message to consumers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The consumer uses the encryption key from KMS to decrypt the message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Producers and consumers must be configured with credentials to obtain shared
    keys from KMS. Periodic key rotation is recommended to harden security, since
    frequent rotation limits the number of compromised messages in case of a breach
    and also protects against brute-force attacks. Consumption must be supported with
    both old and new keys during the retention period of messages encrypted with the
    old key. Many KMS systems support graceful key rotation out of the box for symmetric
    encryption without requiring any special handling in Kafka clients. For compacted
    topics, messages encrypted with old keys may be retained for a long time, and
    it may be necessary to re-encrypt old messages. To avoid interference with newer
    messages, producers and consumers must be offline during this process.
  prefs: []
  type: TYPE_NORMAL
- en: Compression of Encrypted Messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compressing messages after encryption is unlikely to provide any benefit in
    terms of space reduction compared to compressing prior to encryption. Serializers
    may be configured to perform compression before encrypting the message, or applications
    may be configured to perform compression prior to producing messages. In either
    case, it is better to disable compression in Kafka since it adds overhead without
    providing any additional benefit. For messages transmitted over an insecure transport
    layer, known security exploits of compressed encrypted messages must also be taken
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: In many environments, especially when TLS is used as the transport layer, message
    keys do not require encryption since they typically do not contain sensitive data
    like message payloads. But in some cases, clear-text keys may not comply with
    regulatory requirements. Since message keys are used for partitioning and compaction,
    transformation of keys must preserve the required hash equivalence to ensure that
    a key retains the same hash value even if encryption parameters are altered. One
    approach would be to store a secure hash of the original key as the message key
    and store the encrypted message key in the message payload or in a header. Since
    Kafka serializes message key and value independently, a producer interceptor can
    be used to perform this transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorization is the process that determines what operations you are allowed
    to perform on which resources. Kafka brokers manage access control using a customizable
    authorizer. We saw earlier that whenever connections are established from a client
    to a broker, the broker authenticates the client and associates a `KafkaPrincipal`
    that represents the client identity with the connection. When a request is processed,
    the broker verifies that the principal associated with the connection is authorized
    to perform that request. For example, when Alice’s producer attempts to write
    a new customer order record to the topic `customerOrders`, the broker verifies
    that `User:Alice` is authorized to write to that topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka has a built-in authorizer, `AclAuthorizer`, that can be enabled by configuring
    the authorizer class name as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: SimpleAclAuthorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`AclAuthorizer` was introduced in Apache Kafka 2.3\. Older versions from 0.9.0.0
    onward had a built-in authorizer, `kafka.security.auth.SimpleAclAuthorizer`, which
    has been deprecated but is still supported.'
  prefs: []
  type: TYPE_NORMAL
- en: AclAuthorizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`AclAuthorizer` supports fine-grained access control for Kafka resources using
    access control lists (ACLs). ACLs are stored in ZooKeeper and cached in memory
    by every broker to enable high-performance lookup for authorizing requests. ACLs
    are loaded into the cache when the broker starts up, and the cache is kept up-to-date
    using notifications based on a ZooKeeper watcher. Every Kafka request is authorized
    by verifying that the `KafkaPrincipal` associated with the connection has permissions
    to perform the requested operation on the requested resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each ACL binding consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Resource type: `Cluster|Topic|Group|TransactionalId|DelegationToken`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattern type: `Literal|Prefixed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resource name: Name of the resource or prefix, or the wildcard `*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operation: `Describe|Create|Delete|Alter|Read|Write|DescribeConfigs|AlterConfigs`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Permission type: `Allow|Deny`; `Deny` has higher precedence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principal: Kafka principal represented as <principalType>:<principalName>,
    e.g., `User:Bob` or `Group:Sales`. ACLs may use `User:*` to grant access to all
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Host: Source IP address of the client connection or `*` if all hosts are authorized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, an ACL may specify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`AclAuthorizer` authorizes an action if there are no `Deny` ACLs that match
    the action and there is at least one `Allow` ACL that matches the action. `Describe`
    permission is implicitly granted if `Read`, `Write`, `Alter`, or `Delete` permission
    is granted. `DescribeConfigs` permission is implicitly granted if `AlterConfigs`
    permission is granted.'
  prefs: []
  type: TYPE_NORMAL
- en: Wildcard ACLs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ACLs with pattern type `Literal` and resource name `*` are used as wildcard
    ACLs that match all resource names of a resource type.
  prefs: []
  type: TYPE_NORMAL
- en: Brokers must be granted `Cluster:ClusterAction` access in order to authorize
    controller requests and replica fetch requests. Producers require `Topic:Write`
    for producing to a topic. For idempotent produce without transactions, producers
    must also be granted `Cluster:IdempotentWrite`. Transactional producers require
    `TransactionalId:Write` access to the transaction IS and `Group:Read` for consumer
    groups to commit offsets. Consumers require `Topic:Read` to consume from a topic
    and `Group:Read` for the consumer group if using group management or offset management.
    Administrative operations require appropriate `Create`, `Delete`, `Describe`,
    `Alter`, `DescribeConfigs`, or `AlterConfigs` access. [Table 11-1](#table-1-acls)
    shows the Kafka requests to which each ACL is applied.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-1\. Access granted for each Kafka ACL
  prefs: []
  type: TYPE_NORMAL
- en: '| ACL | Kafka requests | Notes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:ClusterAction` | Inter-broker requests, including controller requests
    and follower fetch requests for replication | Should only be granted to brokers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:Create` | `CreateTopics` and auto-topic creation | Use `Topic:Create`
    for fine-grained access control to create specific topics. |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:Alter` | `CreateAcls`, `DeleteAcls`, `AlterReplicaLogDirs`, `ElectReplicaLeader`,
    `Alter​PartitionReassignments` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:AlterConfigs` | `AlterConfigs` and `IncrementalAlterConfigs` for
    broker and broker logger, `AlterClientQuotas` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:Describe` | `DescribeAcls`, `DescribeLogDirs`, `ListGroups`, `ListPartitionReassignments`,
    describing authorized operations for cluster in Metadata request | Use `Group:Describe`
    for fine-grained access control for ListGroups. |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:DescribeConfigs` | `DescribeConfigs` for broker and broker logger,
    `DescribeClientQuotas` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Cluster:IdempotentWrite` | Idempotent `InitProducerId` and `Produce` requests
    | Only required for nontransactional idempotent producers. |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Create` | `CreateTopics` and auto-topic creation |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Delete` | `DeleteTopics`, `DeleteRecords` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Alter` | `CreatePartitions` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:AlterConfigs` | `AlterConfigs` and `IncrementalAlterConfigs` for topics
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Describe` | Metadata request for topic, `OffsetForLeaderEpoch`, `ListOffset`,
    `OffsetFetch` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:DescribeConfigs` | `DescribeConfigs` for topics, for returning configs
    in `CreateTopics` response |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Read` | `Consumer Fetch`, `OffsetCommit`, `TxnOffsetCommit`, `OffsetDelete`
    | Should be granted to consumers. |'
  prefs: []
  type: TYPE_TB
- en: '| `Topic:Write` | `Produce`, `AddPartitionToTxn` | Should be granted to producers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Group:Read` | `JoinGroup`, `SyncGroup`, `LeaveGroup`, `Heartbeat`, `OffsetCommit`,
    `AddOffsetsToTxn`, `TxnOffsetCommit` | Required for consumers using consumer group
    management or Kafka-based offset management. Also required for transactional producers
    to commit offsets within a transaction. |'
  prefs: []
  type: TYPE_TB
- en: '| `Group:Describe` | `FindCoordinator`, `DescribeGroup`, `ListGroups`, `OffsetFetch`
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| `Group:Delete` | `DeleteGroups`, `OffsetDelete` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `TransactionalId:Write` | `Produce` and `InitProducerId` with transactions,
    `AddPartitionToTxn`, `AddOffsetsToTxn`, `TxnOffsetCommit`, `EndTxn` | Required
    for transactional producers. |'
  prefs: []
  type: TYPE_TB
- en: '| `TransactionalId:​Describe` | `FindCoordinator` for transaction coordinator
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| `DelegationToken:​Describe` | `DescribeTokens` |  |'
  prefs: []
  type: TYPE_TB
- en: 'Kafka provides a tool for managing ACLs using the authorizer configured in
    brokers. ACLs can be created directly in ZooKeeper as well. This is useful to
    create broker ACLs prior to starting brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO16-1)'
  prefs: []
  type: TYPE_NORMAL
- en: ACLs for broker user are created directly in ZooKeeper.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO16-2)'
  prefs: []
  type: TYPE_NORMAL
- en: By default, the ACLs command grants literal ACLs. `User:Alice` is granted access
    to write to the topic `customerOrders`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO16-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The prefixed ACL grants permission for Bob to read all topics starting with
    `customer`.
  prefs: []
  type: TYPE_NORMAL
- en: '`AclAuthorizer` has two configuration options to grant broad access to resources
    or principals in order to simplify management of ACLs, especially when adding
    authorization to existing clusters for the first time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Super users are granted access for all operations on all resources without any
    restrictions and cannot be denied access using `Deny` ACLs. If Carol’s credentials
    are compromised, Carol must be removed from `super.users`, and brokers must be
    restarted to apply the changes. It is safer to grant specific access using ACLs
    to users in production systems to ensure access can be revoked easily, if required.
  prefs: []
  type: TYPE_NORMAL
- en: Super User Separator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike other list configurations in Kafka that are comma-separated, `super.users`
    are separated by a semicolon since user principals such as distinguished names
    from SSL certificates often contain commas.
  prefs: []
  type: TYPE_NORMAL
- en: If `allow.everyone.if.no.acl.found` is enabled, all users are granted access
    to resources without any ACLs. This option may be useful when enabling authorization
    for the first time in a cluster or during development, but is not suitable for
    production use since access may be granted unintentionally to new resources. Access
    may also be unexpectedly removed when ACLs for a matching prefix or wildcard are
    added if the condition for `no.acl.found` no longer applies.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Authorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authorization can be customized in Kafka to implement additional restrictions
    or add new types of access control, like role-based access control.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following custom authorizer restricts usage of some requests to the internal
    listener alone. For simplicity, the requests and listener name are hard-coded
    here, but they can be configured using custom authorizer properties instead for
    flexibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO17-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Authorizers are given the request context with metadata that includes listener
    names, security protocol, request types, etc., enabling custom authorizers to
    add or remove restrictions based on the context.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO17-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We reuse functionality from the built-in Kafka authorizer using the public API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka authorizer can also be integrated with external systems to support group-based
    access control or role-based access control. Different principal types can be
    used to create ACLs for group principals or role principals. For instance, roles
    and groups from an LDAP server can be used to periodically populate `groups` and
    `roles` in the Scala class below to support `Allow` ACLs at different levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO18-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Groups to which each user belongs, populated from an external source like LDAP.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO18-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Roles associated with each user, populated from an external source like LDAP.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO18-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We perform authorization for the user as well as for all the groups and roles
    of the user.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO18-4)'
  prefs: []
  type: TYPE_NORMAL
- en: If any of the contexts are authorized, we return `ALLOWED`. Note that this example
    doesn’t support `Deny` ACLs for groups or roles.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_securing_kafka_CO18-5)'
  prefs: []
  type: TYPE_NORMAL
- en: We create an authorization context for each principal with the same metadata
    as the original context.
  prefs: []
  type: TYPE_NORMAL
- en: 'ACLs can be assigned for the group `Sales` or the role `Operator` using the
    standard Kafka ACL tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO19-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the principal `Group:Sales` with the custom principal type `Group` to
    create an ACL that applies to users belonging to the group `Sales`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO19-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the principal `Role:Operator` with the custom principal type `Role` to
    create an ACL that applies to users with the role `Operator`.
  prefs: []
  type: TYPE_NORMAL
- en: Security Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since `AclAuthorizer` stores ACLs in ZooKeeper, access to ZooKeeper should be
    restricted. Deployments without a secure ZooKeeper can implement custom authorizers
    to store ACLs in a secure external database.
  prefs: []
  type: TYPE_NORMAL
- en: In large organizations with a large number of users, managing ACLs for individual
    resources may become very cumbersome. Reserving different resource prefixes for
    different departments enables the use of prefixed ACLs that minimize the number
    of ACLs required. This can be combined with group- or role-based ACLs, as shown
    in the example earlier, to further simplify access control in large deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Restricting user access using the principle of least privilege can limit exposure
    if a user is compromised. This means granting access only to the resources necessary
    for each user principal to perform their operations, and removing ACLs when they
    are no longer required. ACLs should be removed immediately when a user principal
    is no longer in use, for instance, when a person leaves the organization. Long-running
    applications can be configured with service credentials rather than credentials
    associated with a specific user to avoid any disruption when employees leave the
    organization. Since long-lived connections with a user principal may continue
    to process requests even after the user has been removed from the system, `Deny`
    ACLs can be used to ensure that the principal is not unintentionally granted access
    through ACLs with wildcard principals. Reuse of principals must be avoided if
    possible to prevent access from being granted to connections using the older version
    of a principal.
  prefs: []
  type: TYPE_NORMAL
- en: Auditing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka brokers can be configured to generate comprehensive *log4j* logs for auditing
    and debugging. The logging level as well as the appenders used for logging and
    their configuration options can be specified in *log4j.properties*. The logger
    instances `kafka.authorizer.logger` used for authorization logging and kafka.request.​log⁠ger
    used for request logging can be configured independently to customize the log
    level and retention for audit logging. Production systems can use frameworks like
    the Elastic Stack to analyze and visualize these logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Authorizers generate `INFO`-level log entries for every attempted operation
    for which access was denied, and log entries at the `DEBUG` level for every operation
    for which access was granted. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Request logging generated at the `DEBUG` level also includes details of the
    user principal and client host. Full details of the request are included if the
    request logger is configured to log at the `TRACE` level. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Authorizer and request logs can be analyzed to detect suspicious activities.
    Metrics that track authentication failures, as well as authorization failure logs,
    can be extremely useful for auditing and provide valuable information in the event
    of an attack or unauthorized access. For end-to-end auditability and traceability
    of messages, audit metadata can be included in message headers when messages are
    produced. End-to-end encryption can be used to protect the integrity of this metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Securing ZooKeeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZooKeeper stores Kafka metadata that is critical for maintaining the availability
    of Kafka clusters, and hence it is vital to secure ZooKeeper in addition to securing
    Kafka. ZooKeeper supports authentication using SASL/GSSAPI for Kerberos authentication
    and SASL/DIGEST-MD5 for username/password authentication. ZooKeeper also added
    TLS support in 3.5.0, enabling mutual authentication as well as encryption of
    data in transit. Note that SASL/DIGEST-MD5 should only be used with TLS encryption
    and is not suitable for production use due to known security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: SASL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SASL configuration for ZooKeeper is provided using the Java system property
    `java.security.auth.login.config`. The property must be set to a JAAS configuration
    file that contains a login section with the appropriate login module and its options
    for the ZooKeeper server. Kafka brokers must be configured with the client-side
    login section for ZooKeeper clients to talk to SASL-enabled ZooKeeper servers.
    The `Server` section that follows provides the JAAS configuration for the ZooKeeper
    server to enable Kerberos authentication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable SASL authentication on ZooKeeper servers, configure authentication
    providers in the ZooKeeper configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Broker Principal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, ZooKeeper uses the full Kerberos principal, e.g., `kafka/broker1.example.com@EXAMPLE.COM`,
    as the client identity. When ACLs are enabled for ZooKeeper authorization, ZooKeeper
    servers should be configured with `kerberos.removeHostFromPrincipal=​true` and
    `kerberos.removeRealmFromPrincipal=true` to ensure that all brokers have the same
    principal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka brokers must be configured to authenticate to ZooKeeper using SASL with
    a JAAS configuration file that provides client credentials for the broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: SSL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SSL may be enabled on any ZooKeeper endpoint, including those that use SASL
    authentication. Like Kafka, SSL may be configured to enable client authentication,
    but unlike Kafka, connections with both SASL and SSL client authentication authenticate
    using both protocols and associate multiple principals with the connection. ZooKeeper
    authorizer grants access to a resource if any of the principals associated with
    the connection have access.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure SSL on a ZooKeeper server, a key store with the hostname of the
    server or a wildcarded host should be configured. If client authentication is
    enabled, a trust store to validate client certificates is also required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To configure SSL for Kafka connections to ZooKeeper, brokers should be configured
    with a trust store to validate ZooKeeper certificates. If client authentication
    is enabled, a key store is also required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Authorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authorization can be enabled for ZooKeeper nodes by setting ACLs for the path.
    When brokers are configured with `zookeeper.set.acl=true`, the broker sets ACLs
    for ZooKeeper nodes when creating the node. By default, metadata nodes are readable
    by everyone but modifiable only by brokers. Additional ACLs may be added if required
    for internal admin users who may need to update metadata directly in ZooKeeper.
    Sensitive paths, like nodes containing SCRAM credentials, are not world-readable
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: Securing the Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we discussed the options for locking down access to
    Kafka and ZooKeeper in order to safeguard Kafka deployments. Security design for
    a production system should use a threat model that addresses security threats
    not just for individual components but also for the system as a whole. Threat
    models build an abstraction of the system and identify potential threats and the
    associated risks. Once the threats are evaluated, documented, and prioritized
    based on risks, mitigation strategies must be implemented for each potential threat
    to ensure that the whole system is protected. When assessing potential threats,
    it is important to consider external threats as well as insider threats. For systems
    that store Personally Identifiable Information (PII) or other sensitive data,
    additional measures to comply with regulatory policies must also be implemented.
    An in-depth discussion of standard threat modeling techniques is outside the scope
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to protecting data in Kafka and metadata in ZooKeeper using secure
    authentication, authorization, and encryption, extra steps must be taken to ensure
    that the platform is secure. Defenses may include network firewall solutions to
    protect the network and encryption to protect physical storage. Key stores, trust
    stores, and Kerberos keytab files that contain credentials used for authentication
    must be protected using filesystem permissions. Access to configuration files
    containing security-critical information like credentials must be restricted.
    Since passwords stored in clear-text in configuration files are insecure even
    if access is restricted, Kafka supports externalizing passwords in a secure store.
  prefs: []
  type: TYPE_NORMAL
- en: Password Protection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customizable configuration providers can be configured for Kafka brokers and
    clients to retrieve passwords from a secure third-party password store. Passwords
    may also be stored in encrypted form in configuration files with custom configuration
    providers that perform decryption.
  prefs: []
  type: TYPE_NORMAL
- en: 'The custom configuration provider that follows uses the tool `gpg` to decrypt
    broker or client properties stored in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_securing_kafka_CO20-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We provide the passphrase for decoding passwords to the process in the environment
    variable `PASSPHRASE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_securing_kafka_CO20-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We decrypt the configs using `gpg`. The return value contains the full set of
    decrypted configs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_securing_kafka_CO20-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We parse the configs in `data` as Java properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_securing_kafka_CO20-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We fail fast with a `RuntimeException` if an error is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_securing_kafka_CO20-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Caller may request a subset of keys from the path; here we get all values and
    return the requested subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall that in the section on SASL/PLAIN, we used standard Kafka configuration
    classes to load credentials from an external file. We can now encrypt that file
    using `gpg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We now add indirect configs and config provider options to the original properties
    file so that Kafka clients load their credentials from the encrypted file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Sensitive broker configuration options can also be stored encrypted in ZooKeeper
    using the Kafka configs tool without using custom providers. The following command
    can be executed before starting brokers to store encrypted SSL key store passwords
    for brokers in ZooKeeper. The password encoder secret must be configured in each
    broker’s configuration file to decrypt the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The frequency and scale of data breaches have been increasing over the last
    decade as cyberattacks have become increasingly sophisticated. In addition to
    the significant cost of isolating and resolving breaches and the cost of outages
    until security fixes have been applied, data breaches may also result in regulatory
    penalties and long-term damage to brand reputation. In this chapter, we explored
    the vast array of options available to guarantee the confidentiality, integrity,
    and availability of data stored in Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the example data flow at the start of this chapter, we reviewed
    the options available for different aspects of security throughout the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: Client authenticity
  prefs: []
  type: TYPE_NORMAL
- en: When Alice’s client establishes connection to a Kafka broker, a listener using
    SASL or SSL with client authentication can verify that the connection is really
    from Alice and not an imposter. Reauthentication can configured to limit exposure
    in case a user is compromised.
  prefs: []
  type: TYPE_NORMAL
- en: Server authenticity
  prefs: []
  type: TYPE_NORMAL
- en: Alice’s client can verify that its connection is to the genuine broker using
    SSL with hostname validation or using SASL mechanisms with mutual authentication,
    like Kerberos or SCRAM.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  prefs: []
  type: TYPE_NORMAL
- en: Use of SSL to encrypt data in transit protects data from eavesdroppers. Disk
    or volume encryption protects data at rest even if the disk is stolen. For highly
    sensitive data, end-to-end encryption provides fine-grained data access control
    and ensures that cloud providers and platform administrators with physical access
    to network and disks cannot access the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  prefs: []
  type: TYPE_NORMAL
- en: SSL can be used to detect tampering of data over an insecure network. Digital
    signatures can be included in messages to verify integrity when using end-to-end
    encryption.
  prefs: []
  type: TYPE_NORMAL
- en: Access control
  prefs: []
  type: TYPE_NORMAL
- en: Every operation performed by Alice, Bob, and even brokers is authorized using
    a customizable authorizer. Kafka has a built-in authorizer that enables fine-grained
    access control using ACLs.
  prefs: []
  type: TYPE_NORMAL
- en: Auditability
  prefs: []
  type: TYPE_NORMAL
- en: Authorizer logs and request logs can be used to track operations and attempted
    operations for auditing and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs: []
  type: TYPE_NORMAL
- en: A combination of quotas and configuration options to manage connections can
    be used to protect brokers from denial-of-service attacks. ZooKeeper can be secured
    using SSL, SASL, and ACLs to ensure that the metadata needed to ensure the availability
    of Kafka brokers is secure.
  prefs: []
  type: TYPE_NORMAL
- en: With the wide choice of options available for security, choosing the appropriate
    options for each use case can be a daunting task. We reviewed the security concerns
    to consider for each security mechanism, and the controls and policies that can
    be adopted to limit the potential attack surface. We also reviewed the additional
    measures required to lock down ZooKeeper and the rest of the platform. The standard
    security technologies supported by Kafka and the various extension points to integrate
    with the existing security infrastructure in your organization enable you to build
    consistent security solutions to protect the whole platform.
  prefs: []
  type: TYPE_NORMAL

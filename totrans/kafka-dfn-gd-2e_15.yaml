- en: Chapter 13\. Monitoring Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。监控Kafka
- en: The Apache Kafka applications have numerous measurements for their operation—so
    many, in fact, that it can easily become confusing as to what is important to
    watch and what can be set aside. These range from simple metrics about the overall
    rate of traffic, to detailed timing metrics for every request type, to per-topic
    and per-partition metrics. They provide a detailed view into every operation in
    the broker, but they can also make you the bane of whoever is responsible for
    managing your monitoring system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka应用程序有许多用于其操作的测量值，事实上有很多，以至于很容易变得令人困惑，不知道要观察什么是重要的，什么可以搁置。这些范围从关于流量总体速率的简单指标，到每种请求类型的详细定时指标，再到每个主题和每个分区的指标。它们提供了对代理中每个操作的详细视图，但也可能使您成为负责管理监控系统的人的梦魇。
- en: This chapter will detail the most critical metrics to monitor all the time and
    how to respond to them. We’ll also describe some of the more important metrics
    to have on hand when debugging problems. This is not an exhaustive list of the
    metrics that are available, however, because the list changes frequently, and
    many will only be informative to a hard-core Kafka developer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍始终监控的最关键指标以及如何对其做出响应。我们还将描述在调试问题时手头上最重要的一些指标。然而，这不是一个详尽的可用指标列表，因为列表经常变化，许多指标只对硬核Kafka开发人员有用。
- en: Metric Basics
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标基础知识
- en: Before getting into the specific metrics provided by the Kafka broker and clients,
    let’s discuss the basics of how to monitor Java applications and some best practices
    around monitoring and alerting. This will provide a basis for understanding how
    to monitor the applications and why the specific metrics described later in this
    chapter have been chosen as the most important.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入由Kafka代理和客户端提供的具体指标之前，让我们讨论一下如何监控Java应用程序的基础知识以及一些关于监控和警报的最佳实践。这将为理解如何监控应用程序以及为什么后面描述的特定指标被选择为最重要的提供基础。
- en: Where Are the Metrics?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标在哪里？
- en: All of the metrics exposed by Kafka can be accessed via the Java Management
    Extensions (JMX) interface. The easiest way to use them in an external monitoring
    system is to use a collection agent provided by your monitoring system and attach
    it to the Kafka process. This may be a separate process that runs on the system
    and connects to the JMX interface, such as with the Nagios XI `check_jmx` plug-in
    or `jmxtrans`. You can also utilize a JMX agent that runs directly in the Kafka
    process to access metrics via an HTTP connection, such as Jolokia or MX4J.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka公开的所有指标都可以通过Java管理扩展（JMX）接口访问。在外部监控系统中使用它们的最简单方法是使用监控系统提供的收集代理，并将其附加到Kafka进程上。这可能是在系统上运行并连接到JMX接口的单独进程，例如Nagios
    XI的`check_jmx`插件或`jmxtrans`。您还可以利用直接在Kafka进程中运行的JMX代理通过HTTP连接访问指标，例如Jolokia或MX4J。
- en: An in-depth discussion of how to set up monitoring agents is outside the scope
    of this chapter, and there are far too many choices to do justice to all of them.
    If your organization does not currently have experience with monitoring Java applications,
    it may be worthwhile to instead consider monitoring as a service. There are many
    companies that offer monitoring agents, metrics collection points, storage, graphing,
    and alerting in a services package. They can assist you further with setting up
    the monitoring agents required.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如何设置监控代理的深入讨论超出了本章的范围，而且有太多选择，无法公平地对所有选择进行公正。如果您的组织目前没有监控Java应用程序的经验，可能值得考虑监控作为一项服务。有许多公司提供监控代理、指标收集点、存储、绘图和警报的服务包。他们可以帮助您进一步设置所需的监控代理。
- en: Finding the JMX Port
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找JMX端口
- en: To aid with configuring applications that connect to JMX on the Kafka broker
    directly, such as monitoring systems, the broker sets the configured JMX port
    in the broker information that is stored in ZooKeeper. The `/brokers/ids/<ID>`
    znode contains JSON-formatted data for the broker, including `hostname` and `jmx_port`
    keys. However, it should be noted that remote JMX is disabled by default in Kafka
    for security reasons. If you are going to enable it, you must properly configure
    security for the port. This is because JMX not only allows a view into the state
    of the application, it also allows code execution. It is highly recommended that
    you use a JMX metrics agent that is loaded into the application.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助配置直接连接到Kafka代理的应用程序（如监控系统），代理在存储在ZooKeeper中的代理信息中设置了配置的JMX端口。`/brokers/ids/<ID>`
    znode包含代理的JSON格式数据，包括`hostname`和`jmx_port`键。但是，应该注意的是，出于安全原因，Kafka默认情况下禁用了远程JMX。如果您要启用它，必须正确配置端口的安全性。这是因为JMX不仅允许查看应用程序的状态，还允许执行代码。强烈建议您使用加载到应用程序中的JMX指标代理。
- en: Nonapplication metrics
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非应用程序指标
- en: Not all metrics will come from Kafka itself. There are five general groupings
    of where you can get your metrics from. [Table 13-1](#table1001) describes the
    categories when we are monitoring the Kafka brokers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有指标都来自Kafka本身。您可以从五个一般分组中获取指标。[表13-1](#table1001)描述了我们在监控Kafka代理时的类别。
- en: Table 13-1\. Metric sources
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-1。指标来源
- en: '| Category | Description |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|类别|描述|'
- en: '| --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Application metrics | These are the metrics you get from Kafka itself, from
    the JMX interface. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|应用程序指标|这些是您从Kafka本身获取的指标，来自JMX接口。|'
- en: '| Logs | Another type of monitoring data that comes from Kafka itself. Because
    it is some form of text or structured data, and not just a number, it requires
    a little more processing. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|日志|来自Kafka本身的另一种监控数据类型。因为它是某种形式的文本或结构化数据，而不仅仅是一个数字，所以需要更多的处理。|'
- en: '| Infrastructure metrics | These metrics come from systems that you have in
    front of Kafka but are still within the request path and under your control. An
    example is a load balancer. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|基础设施指标|这些指标来自于您在Kafka前面的系统，但仍然在请求路径内并在您的控制范围内。一个例子是负载均衡器。|'
- en: '| Synthetic clients | This is data from tools that are external to your Kafka
    deployment, just like a client, but are under your direct control and are typically
    not performing the same work as your clients. An external monitor like Kafka Monitor
    falls in this category. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 合成客户端 | 这是来自与您的Kafka部署外部工具的数据，就像客户端一样，但在您的直接控制下，通常不执行与您的客户端相同的工作。像Kafka Monitor这样的外部监视器属于这一类别。
    |'
- en: '| Client metrics | These are metrics that are exposed by the Kafka clients
    that connect to your cluster. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 客户端指标 | 这些是由连接到您的集群的Kafka客户端公开的指标。 |'
- en: Logs generated by Kafka are discussed later in this chapter, as are client metrics.
    We will also touch very briefly on synthetic metrics. Infrastructure metrics,
    however, are dependent on your specific environment and are outside the scope
    of the discussion here. The further along in your Kafka journey you are, the more
    important these metric sources will be to fully understanding how your applications
    are running, as the lower in the list, the more objective a view of Kafka they
    provide. For example, relying on metrics from your brokers will suffice at the
    start, but later on you will want a more objective view of how they are performing.
    A familiar example for the value of objective measurements is monitoring the health
    of a website. The web server is running properly, and all of the metrics it is
    reporting say that it is working. However, there is a problem with the network
    between your web server and your external users, which means that none of your
    users can reach the web server. A synthetic client that is running outside your
    network and checks the accessibility of the website would detect this and alert
    you to the situation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka生成的日志将在本章后面讨论，客户端指标也是如此。我们还将简要涉及合成指标。然而，基础设施指标取决于您的特定环境，并且超出了这里讨论的范围。在您的Kafka旅程中越深入，这些指标来源对于充分了解应用程序的运行方式就越重要，因为在列表中越靠后，它们提供的对Kafka的客观视图就越多。例如，在开始阶段依赖经纪人的指标就足够了，但以后您会希望更客观地了解它们的表现。客观测量价值的一个熟悉例子是监控网站的健康状况。Web服务器正常运行，并且它报告的所有指标都表明它正在工作。然而，您的Web服务器和外部用户之间的网络存在问题，这意味着您的用户无法访问Web服务器。在您的网络之外运行的合成客户端将检测到这一情况并向您发出警报。
- en: What Metrics Do I Need?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我需要哪些指标？
- en: The specific metrics that are important to you is a question that is nearly
    as loaded as what the best editor to use is. It will depend significantly on what
    you intend to do with them, what tools you have available for collecting data,
    how far along in using Kafka you are, and how much time you have available to
    spend on building infrastructure around Kafka. A broker internals developer will
    have far different needs than a site reliability engineer who is running a Kafka
    deployment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对您重要的具体指标几乎与您要使用的最佳编辑器一样重要。这将大大取决于您打算如何使用它们，您有哪些可用于收集数据的工具，您在使用Kafka方面的进展如何，以及您有多少时间可用于围绕Kafka构建基础设施。一个经纪人内部开发人员的需求将远远不同于运行Kafka部署的站点可靠性工程师的需求。
- en: Alerting or debugging?
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警报还是调试？
- en: The first question you should ask yourself is whether or not your primary goal
    is to alert you when there is a problem with Kafka, or to debug problems that
    happen. The answer will usually involve a little of both, but knowing whether
    a metric is for one or the other will allow you to treat it differently once it
    is collected.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该问自己的第一个问题是，您的主要目标是在Kafka出现问题时警报您，还是调试出现的问题。答案通常会涉及两者，但知道一个指标是用于哪个目的将使您在收集后对其进行不同处理。
- en: A metric that is destined for alerting is useful for a very short period of
    time—typically, not much longer than the amount of time it takes to respond to
    a problem. You can measure this on the order of hours, or maybe days. These metrics
    will be consumed by automation that responds to known problems for you, as well
    as the human operators in cases where automation does not exist yet. It is usually
    important for these metrics to be more objective, as a problem that does not impact
    clients is far less critical than one that does.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于警报的指标在很短的时间内非常有用——通常不会超过解决问题所需的时间。您可以测量几个小时，或者可能几天。这些指标将被自动化消耗，自动化将为您响应已知问题，以及在自动化尚不存在的情况下由人工操作员消耗。这些指标通常更为客观，因为不影响客户端的问题远不及影响客户端的问题严重。
- en: Data that is primarily for debugging has a longer time horizon because you are
    frequently diagnosing problems that have existed for some time, or taking a deeper
    look at a more complex problem. This data will need to remain available for days
    or weeks past when it is collected. It is also usually going to be more subjective
    measurements, or data from the Kafka application itself. Keep in mind that it
    is not always necessary to collect this data into a monitoring system. If the
    metrics are used for debugging problems in place, it is sufficient that the metrics
    are available when needed. You do not need to overwhelm the monitoring system
    by collecting tens of thousands of values on an ongoing basis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 主要用于调试的数据具有更长的时间范围，因为您经常诊断已经存在一段时间的问题，或者深入研究更复杂的问题。这些数据将需要在收集后的几天或几周内保持可用。通常还会是更主观的测量，或者来自Kafka应用程序本身的数据。请记住，不一定需要将这些数据收集到监控系统中。如果指标用于现场调试问题，则在需要时可用即可。您无需通过持续收集成千上万个值来压倒监控系统。
- en: Historical Metrics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史指标
- en: There is a third type of data that you will need eventually, and that is historical
    data on your application. The most common use for historical data is for capacity
    management purposes, and so it includes information about resources used, including
    compute resources, storage, and network. These metrics will need to be stored
    for a very long period of time, measured in years. You also may need to collect
    additional metadata to put the metrics into context, such as when brokers were
    added to or removed from the cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，您还将需要应用程序的历史数据。历史数据最常见的用途是用于容量管理，因此包括有关使用的资源的信息，包括计算资源、存储和网络。这些指标需要长时间存储，以年为单位。您还可能需要收集额外的元数据来将指标放入上下文中，例如代理何时添加到集群或从集群中删除。
- en: Automation or humans?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化还是人类？
- en: 'Another question to consider is who the consumer of the metrics will be. If
    the metrics are consumed by automation, they should be very specific. It’s OK
    to have a large number of metrics, each describing small details, because this
    is why computers exist: to process a lot of data. The more specific the data is,
    the easier it is to create automation that acts on it, because the data does not
    leave as much room for interpretation as to its meaning. On the other hand, if
    the metrics will be consumed by humans, presenting a large number of metrics will
    be overwhelming. This becomes even more important when defining alerts based on
    those measurements. It is far too easy to succumb to “alert fatigue,” where there
    are so many alerts going off that it is difficult to know how severe the problem
    is. It is also hard to properly define thresholds for every metric and keep them
    up-to-date. When the alerts are overwhelming or often incorrect, we begin to not
    trust that the alerts are correctly describing the state of our applications.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要考虑的一个问题是指标的使用者是谁。如果指标由自动化程序使用，它们应该非常具体。拥有大量描述细节的指标是可以接受的，因为这正是计算机存在的原因：处理大量数据。数据越具体，就越容易创建基于其操作的自动化程序，因为数据不会留下太多关于其含义的解释空间。另一方面，如果指标将由人类使用，呈现大量指标将会令人不知所措。在基于这些测量值定义警报时，这变得更加重要。很容易陷入“警报疲劳”，因为有太多警报响起，很难知道问题有多严重。正确定义每个指标的阈值并使其保持最新也很困难。当警报过多或经常不正确时，我们开始不相信警报是否正确描述了我们应用程序的状态。
- en: Think about the operations of a car. To properly adjust the ratio of air to
    fuel while the car is running, the computer needs a number of measurements of
    air density, fuel, exhaust, and other minutiae about the operation of the engine.
    These measurements would be overwhelming to the human operator of the vehicle,
    however. Instead, we have a “Check Engine” light. A single indicator tells you
    that there is a problem, and there is a way to find out more detailed information
    to tell you exactly what the problem is. Throughout this chapter, we will identify
    the metrics that will provide the highest amount of coverage to keep your alerting
    simple.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 想想汽车的运行。为了在汽车运行时正确调整空气与燃料的比例，计算机需要对空气密度、燃料、排气和发动机运行等细微之处进行多次测量。然而，这些测量对车辆的人类操作者来说将是不堪重负的。相反，我们有一个“发动机故障”指示灯。一个指示器告诉您有问题，并且有一种方法可以获取更详细的信息，告诉您问题的确切所在。在本章中，我们将确定提供最高覆盖率的指标，以保持您的警报简单。
- en: Application Health Checks
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序健康检查
- en: 'No matter how you collect metrics from Kafka, you should make sure that you
    have a way to also monitor the overall health of the application process via a
    simple health check. This can be done in two ways:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您如何从Kafka收集指标，都应确保有一种方法来通过简单的健康检查监控应用程序进程的整体健康状况。这可以通过两种方式实现：
- en: An external process that reports whether the broker is up or down (health check)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告代理是否正常运行的外部过程（健康检查）
- en: Alerting on the lack of metrics being reported by the Kafka broker (sometimes
    called *stale metrics*)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对Kafka代理报告的指标缺失进行警报（有时称为*陈旧指标*）
- en: Though the second method works, it can make it difficult to differentiate between
    a failure of the Kafka broker and a failure of the monitoring system itself.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第二种方法有效，但它可能会使难以区分Kafka代理的故障和监控系统本身的故障。
- en: For the Kafka broker, this can simply be connecting to the external port (the
    same port that clients use to connect to the broker) to check that it responds.
    For client applications, it can be more complex, ranging from a simple check of
    whether the process is running, to an internal method that determines application
    health.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kafka代理，这可以简单地连接到外部端口（客户端用于连接代理的相同端口）以检查其响应。对于客户端应用程序，可能会更复杂，从简单检查进程是否正在运行，到确定应用程序健康状况的内部方法。
- en: Service-Level Objectives
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务级目标
- en: 'One area of monitoring that is especially critical for infrastructure services,
    such as Kafka, is that of service-level objectives, or SLOs. This is how we communicate
    to our clients what level of service they can expect from the infrastructure service.
    The clients want to be able to treat services like Kafka as an opaque system:
    they do not want or need to understand the internals of how it works—only the
    interface that they are using and knowing it will do what they need it to do.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的一个特别关键的领域是基础设施服务，比如Kafka，其中的服务级目标或SLO。这是我们向客户传达基础设施服务可以提供的服务水平。客户希望能够将Kafka等服务视为不透明系统：他们不希望也不需要了解其内部工作原理，只需要了解他们正在使用的接口，并知道它将按照他们的需求进行操作。
- en: Service-Level Definitions
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务级定义
- en: Before discussing SLOs in Kafka, there must be agreement on the terminology
    that is used. Frequently, you will hear engineers, managers, executives, and everyone
    else use terms in the “service-level” space incorrectly, which leads to confusion
    about what is actually being talked about.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论Kafka中的SLO之前，必须就所使用的术语达成一致。经常会听到工程师、经理、高管和其他人在“服务级”领域错误地使用术语，这导致对实际讨论的内容产生困惑。
- en: A *service-level indicator* (SLI) is a metric that describes one aspect of a
    service’s reliability. It should be closely aligned with your client’s experience,
    so it is usually true that the more objective these measurements are, the better
    they are. In a request processing system, such as Kafka, it is usually best to
    express these measurements as a ratio between the number of good events and the
    total number of events—for example, the proportion of requests to a web server
    that return a 2xx, 3xx, or 4xx response.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: A *service-level objective* (SLO), which can also be called a *service-level
    threshold* (SLT), combines an SLI with a target value. A common way to express
    the target is by the number of nines (99.9% is “three nines”), though it is by
    no means required. The SLO should also include a time frame that it is measured
    over, frequently on the scale of days. For example, 99% of requests to the web
    server must return a 2xx, 3xx, or 4xx response over 7 days.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: A *service-level agreement* (SLA) is a contract between a service provider and
    a client. It usually includes several SLOs, as well as details about how they
    are measured and reported, how the client seeks support from the service provider,
    and penalties that the service provider will be subject to if they are not performing
    within the SLA. For example, an SLA for the preceding SLO might state that if
    the service provider is not operating within the SLO, they will refund all fees
    paid by the client for the time period that the service was not within the SLO.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Operational-Level Agreement
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *operational-level agreement* (OLA) is less frequently used. It describes
    agreements between multiple internal services or support providers in the overall
    delivery of an SLA. The goal is to assure that the multiple activities that are
    necessary to fulfill the SLA are properly described and accounted for in the day-to-day
    operations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: It is very common to hear people talk about SLAs when they really mean SLOs.
    While those who are providing a service to paying clients may have SLAs with those
    clients, it is rare that the engineers running the applications are responsible
    for anything more than the performance of that service within the SLOs. In addition,
    those who only have internal clients (i.e., are running Kafka as internal data
    infrastructure for a much larger service) generally do not have SLAs with those
    internal customers. This should not prevent you from setting and communicating
    SLOs, however, as doing that will lead to fewer assumptions by customers as to
    how they think Kafka should be performing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: What Metrics Make Good SLIs?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, the metrics for your SLIs should be gathered using something external
    to the Kafka brokers. The reason for this is that SLOs should describe whether
    or not the typical user of your service is happy, and you can’t measure that subjectively.
    Your clients do not care if you think your service is running correctly; it is
    their experience (in aggregate) that matters. This means that infrastructure metrics
    are OK, synthetic clients are good, and client-side metrics are probably the best
    for most of your SLIs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: While by no means an exhaustive list, the most common SLIs that are used in
    request/response and data storage systems are in [Table 13-2](#table1002).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Customers Always Want More
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some SLOs that your customers may be interested in that are important
    to them but not within your control. For example, they may be concerned about
    the correctness or freshness of the data produced to Kafka. Do not agree to support
    SLOs that you are not responsible for, as that will only lead to taking on work
    that dilutes the core job of keeping Kafka running properly. Make sure to connect
    them with the proper group to set up understanding, and agreements, around these
    additional requirements.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-2\. Types of SLIs
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '| Availability | Is the client able to make a request and get a response? |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| Latency | How quickly is the response returned? |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Quality | Does the response include a proper response? |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Security | Are the request and response appropriately protected, whether
    that is authorization or encryption? |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| Throughput | Can the client get enough data, fast enough? |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: Keep in mind that it is usually better for your SLIs to be based on a counter
    of events that fall inside the thresholds of the SLO. This means that ideally,
    each event would be individually checked to see if it meets the threshold of the
    SLO. This rules out quantile metrics as good SLIs, as those will only tell you
    that 90% of your events were below a given value without allowing you to control
    what that value is. However, aggregating values into buckets (e.g., “less than
    10 ms,” “10–50 ms,” “50–100 ms,” etc.) can be useful when working with SLOs, especially
    when you are not yet sure what a good threshold is. This will give you a view
    into the distribution of the events within the range of the SLO, and you can configure
    the buckets so that the boundaries are reasonable values for the SLO threshold.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Using SLOs in Alerting
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In short, SLOs should inform your primary alerts. The reason for this is that
    the SLOs describe problems from your customers’ point of view, and those are the
    ones that you should be concerned about first. Generally speaking, if a problem
    does not impact your clients, it does not need to wake you up at night. SLOs will
    also tell you about the problems that you don’t know how to detect because you’ve
    never seen them before. They won’t tell you what those problems are, but they
    will tell you that they exist.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is that it’s very difficult to use an SLO directly as an alert.
    SLOs are best for long timescales, such as a week, as we want to report them to
    management and customers in a way that can be consumed. In addition, by the time
    the SLO alert fires, it’s too late—you’re already operating outside of the SLO.
    Some will use a derivative value to provide an early warning, but the best way
    to approach using SLOs for alerting is to observe the rate at which you are burning
    through your SLO over its timeframe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s assume that your Kafka cluster receives one million requests
    per week, and you have an SLO defined that states that 99.9% of requests must
    send out the first byte of response within 10 ms. This means that over the week,
    you can have up to one thousand requests that respond slower than this and everything
    will still be OK. Normally, you see one request like this every hour, which is
    about 168 bad requests a week, measured from Sunday to Saturday. You have a metric
    that shows this as the SLO burn rate, and one request an hour at one million requests
    a week is a burn rate of 0.1% per hour.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: On Tuesday at 10 a.m., your metric changes and now shows that the burn rate
    is 0.4% per hour. This isn’t great, but it’s still not a problem because you’ll
    be well within the SLO by the end of the week. You open a ticket to take a look
    at the problem but go back to some higher-priority work. On Wednesday at 2 p.m.,
    the burn rate jumps to 2% per hour and your alerts go off. You know that at this
    rate, you’ll breach the SLO by lunchtime on Friday. Dropping everything, you diagnose
    the problem, and after about 4 hours you have the burn rate back down to 0.4%
    per hour, and it stays there for the rest of the week. By using the burn rate,
    you were able to avoid breaching the SLO for the week.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: For more information on utilizing SLOs and the burn rate for alerting, you will
    find that [*Site Reliability Engineering*](https://oreil.ly/bPBxC) and [*The Site
    Reliability Workbook*](https://oreil.ly/qSmOc), both edited by Betsy Beyer et
    al. (O’Reilly), are excellent resources.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Broker Metrics
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many Kafka broker metrics. Many of them are low-level measurements,
    added by developers when investigating a specific issue or in anticipation of
    needing information for debugging purposes later. There are metrics providing
    information about nearly every function within the broker, but the most common
    ones provide the information needed to run Kafka on a daily basis.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多Kafka经纪人指标。其中许多是低级测量，由开发人员在调查特定问题或预期以后需要调试信息时添加的。这些指标提供有关经纪人内几乎每个功能的信息，但最常见的指标提供了日常运行Kafka所需的信息。
- en: Who Watches the Watchers?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁来监视监视者？
- en: Many organizations use Kafka for collecting application metrics, system metrics,
    and logs for consumption by a central monitoring system. This is an excellent
    way to decouple the applications from the monitoring system, but it presents a
    specific concern for Kafka itself. If you use this same system for monitoring
    Kafka itself, it is very likely that you will never know when Kafka is broken
    because the data flow for your monitoring system will be broken as well.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织使用Kafka收集应用程序指标、系统指标和日志，供中央监控系统使用。这是将应用程序与监控系统解耦的绝佳方式，但对于Kafka本身来说，也提出了特定的问题。如果您使用相同的系统来监视Kafka本身，很可能您永远不会知道Kafka何时出现故障，因为监控系统的数据流也会中断。
- en: There are many ways that this can be addressed. One way is to use a separate
    monitoring system for Kafka that does not have a dependency on Kafka. Another
    way, if you have multiple datacenters, is to make sure that the metrics for the
    Kafka cluster in datacenter A are produced to datacenter B, and vice versa. However
    you decide to handle it, make sure that the monitoring and alerting for Kafka
    does not depend on Kafka working.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以解决这个问题。一种方法是为Kafka使用一个不依赖于Kafka的单独监控系统。另一种方法是，如果您有多个数据中心，要确保数据中心A的Kafka集群的指标被生成到数据中心B，反之亦然。无论您决定如何处理，都要确保Kafka的监控和警报不依赖于Kafka的正常工作。
- en: In this section, we’ll start by discussing the high-level workflow for diagnosing
    problems with your Kafka cluster, referencing the metrics that are useful. Those,
    and other metrics, are described in more detail later in the chapter. This is
    by no means an exhaustive list of broker metrics, but rather several “must have”
    metrics for checking on the health of the broker and the cluster. We’ll wrap up
    with a discussion on logging before moving on to client metrics.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论诊断Kafka集群问题的高级工作流程，参考有用的指标。这些指标和其他指标将在本章后面更详细地描述。这绝不是经纪人指标的详尽清单，而是检查经纪人和集群健康状况的几个“必备”指标。最后，我们将讨论日志记录，然后转向客户端指标。
- en: Diagnosing Cluster Problems
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 诊断集群问题
- en: 'When it comes to problems with a Kafka cluster, there are three major categories:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及Kafka集群的问题时，有三个主要类别：
- en: Single-broker problems
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单经纪人问题
- en: Overloaded clusters
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超载的集群
- en: Controller problems
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器问题
- en: Issues with individual brokers are, by far, the easiest to diagnose and respond
    to. These will show up as outliers in the metrics for the cluster and are frequently
    related to slow or failing storage devices or compute restraints from other applications
    on the system. To detect them, make sure you are monitoring the availability of
    the individual servers, as well as the status of the storage devices, utilizing
    the operating system (OS) metrics.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个经纪人的问题相比，诊断和响应集群问题要容易得多。这些问题将显示为集群指标中的异常值，并且通常与存储设备的缓慢或故障，或系统中其他应用程序的计算限制有关。要检测它们，确保您正在监视单个服务器的可用性，以及存储设备的状态，利用操作系统（OS）指标。
- en: Absent a problem identified at the OS or hardware level, however, the cause
    is almost always an imbalance in the load of the Kafka cluster. While Kafka attempts
    to keep the data within the cluster evenly spread across all brokers, this does
    not mean that client access to that data is evenly distributed. It also does not
    detect issues such as hot partitions. It is highly recommended that you utilize
    an external tool for keeping the cluster balanced at all times. One such tool
    is [Cruise Control](https://oreil.ly/rLybu), an application that continually monitors
    the cluster and rebalances partitions within it. It also provides a number of
    other administrative functions, such as adding and removing brokers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在操作系统或硬件级别没有发现问题的情况下，问题几乎总是Kafka集群负载不平衡。虽然Kafka试图使集群中的数据均匀分布在所有经纪人之间，但这并不意味着客户端对该数据的访问是均匀分布的。它也无法检测到热分区等问题。强烈建议您始终使用外部工具来保持集群的平衡。其中一个工具是[Cruise
    Control](https://oreil.ly/rLybu)，这是一个不断监视集群并在其中重新平衡分区的应用程序。它还提供许多其他管理功能，例如添加和删除经纪人。
- en: Preferred Replica Elections
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首选副本选举
- en: The first step before trying to diagnose a problem further is to ensure that
    you have run a preferred replica election (see [Chapter 12](ch12.html#administering_kafka))
    recently. Kafka brokers do not automatically take partition leadership back (unless
    auto leader rebalance is enabled) after they have released leadership (e.g., when
    the broker has failed or been shut down). This means that it’s very easy for leader
    replicas to become unbalanced in a cluster. The preferred replica election is
    safe and easy to run, so it’s a good idea to do that first and see if the problem
    goes away.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步诊断问题之前的第一步是确保您最近已运行了首选副本选举（参见[第12章](ch12.html#administering_kafka)）。Kafka经纪人在释放领导权后（例如，当经纪人失败或关闭时），不会自动重新接管分区领导权（除非启用了自动领导者重新平衡）。这意味着领导副本在集群中很容易变得不平衡。首选副本选举是安全且易于运行的，因此最好先这样做，看看问题是否消失。
- en: Overloaded clusters are another problem that is easy to detect. If the cluster
    is balanced, and many of the brokers are showing elevated latency for requests
    or a low request handler pool idle ratio, you are reaching the limits of your
    brokers to serve traffic for this cluster. You may find upon deeper inspection
    that you have a client that has changed its request pattern and is now causing
    problems. Even when this happens, however, there may be little you can do about
    changing the client. The solutions available to you are either to reduce the load
    to the cluster or increase the number of brokers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 过载的集群是另一个容易检测到的问题。如果集群是平衡的，并且许多代理显示出增加的请求延迟或低请求处理程序池空闲比率，那么您的代理已经达到了为该集群提供流量的极限。在深入检查后，您可能会发现有一个客户端改变了其请求模式，现在导致了问题。然而，即使发生这种情况，您可能无法改变客户端。您可以采取的解决方案要么是减少对集群的负载，要么是增加代理的数量。
- en: Problems with the controller in the Kafka cluster are much more difficult to
    diagnose and often fall into the category of bugs in Kafka itself. These issues
    manifest as broker metadata being out of sync, offline replicas when the brokers
    appear to be fine, and topic control actions like creation not happening properly.
    If you’re scratching your head over a problem in the cluster and saying “That’s
    really weird,” there is a very good chance that it is because the controller did
    something unpredictable and bad. There are not a lot of ways to monitor the controller,
    but monitoring the active controller count as well as the controller queue size
    will give you a high-level indicator if there is a problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群中控制器的问题要难以诊断得多，通常属于Kafka本身的错误类别。这些问题表现为代理元数据不同步、代理离线时代理似乎正常，以及主题控制操作（如创建）未能正确进行。如果您在集群中遇到问题并说“这真的很奇怪”，那么很有可能是因为控制器做了一些不可预测且不好的事情。监控控制器的方法并不多，但监控活动控制器计数以及控制器队列大小将为您提供一个高级别的指标，以判断是否存在问题。
- en: The Art of Under-Replicated Partitions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未复制的分区的艺术
- en: One of the most popular metrics to use when monitoring Kafka is under-replicated
    partitions. This measurement, provided on each broker in a cluster, gives a count
    of the number of partitions for which the broker is the leader replica, where
    the follower replicas are not caught up. This single measurement provides insight
    into a number of problems with the Kafka cluster, from a broker being down to
    resource exhaustion. With the wide variety of problems that this metric can indicate,
    it is worthy of an in-depth look at how to respond to a value other than zero.
    Many of the metrics used in diagnosing these types of problems will be described
    later in this chapter. See [Table 13-3](#table1003) for more details on under-replicated
    partitions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监控Kafka时使用的最流行的指标之一是未复制的分区。在集群中的每个代理上提供的这个度量值，给出了代理是领导副本的分区数量，其中跟随者副本没有赶上。这个单一的度量值可以揭示Kafka集群的许多问题，从代理宕机到资源耗尽。由于这个度量值可以指示的问题种类繁多，因此值得深入研究如何应对非零值。本章后面将描述用于诊断这些问题的许多度量值。有关未复制的分区的更多详细信息，请参见[表13-3](#table1003)。
- en: Table 13-3\. Metrics and their corresponding under-replicated partitions
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-3。度量值及其对应的未复制的分区
- en: '| Metric name | Under-replicated partitions |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 未复制的分区 |'
- en: '| --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions`
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions`
    |'
- en: '| Value range | Integer, zero or greater |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 整数，零或更大 |'
- en: The URP Alerting Trap
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未复制的分区警报陷阱
- en: In the previous edition of this book, as well as in many conference talks, the
    authors have spoken at length about the fact that the under-replicated partitions
    (URP) metric should be your primary alerting metric because of how many problems
    it describes. This approach has a significant number of problems, not the least
    of which is that the URP metric can frequently be nonzero for benign reasons.
    This means that as someone operating a Kafka cluster, you will receive false alerts,
    which lead to the alert being ignored. It also requires a significant amount of
    knowledge to be able to understand what the metric is telling you. For this reason,
    we no longer recommend the use of URP for alerting. Instead, you should depend
    on SLO-based alerting to detect unknown problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的上一版以及许多会议演讲中，作者们长时间地谈到了未复制的分区（URP）度量应该是您的主要警报度量，因为它描述了多少问题。这种方法存在大量问题，其中最主要的问题之一是，未复制的分区度量通常由于良性原因而频繁出现非零值。这意味着作为Kafka集群的运维人员，您将收到错误警报，从而忽略警报。这也需要相当多的知识才能理解度量值告诉您的信息。因此，我们不再建议使用URP进行警报。相反，您应该依赖基于SLO的警报来检测未知问题。
- en: A steady (unchanging) number of under-replicated partitions reported by many
    of the brokers in a cluster normally indicates that one of the brokers in the
    cluster is offline. The count of under-replicated partitions across the entire
    cluster will equal the number of partitions that are assigned to that broker,
    and the broker that is down will not report a metric. In this case, you will need
    to investigate what has happened to that broker and resolve that situation. This
    is often a hardware failure, but it could also be an OS or Java issue that has
    caused the problem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中许多代理报告的未复制的分区数量保持稳定（不变），通常表明集群中的一个代理已经离线。整个集群中的未复制的分区数量将等于分配给该代理的分区数量，而宕机的代理将不会报告度量值。在这种情况下，您需要调查发生了什么，并解决这种情况。这通常是硬件故障，但也可能是导致问题的操作系统或Java问题。
- en: If the number of under-replicated partitions is fluctuating, or if the number
    is steady but there are no brokers offline, this typically indicates a performance
    issue in the cluster. These types of problems are much harder to diagnose due
    to their variety, but there are several steps you can work through to narrow it
    down to the most likely causes. The first step is to try and determine if the
    problem relates to a single broker or to the entire cluster. This can sometimes
    be a difficult question to answer. If the under-replicated partitions are on a
    single broker, as in the following example, then that broker is typically the
    problem. The error shows that other brokers are having a problem replicating messages
    from that one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: If several brokers have under-replicated partitions, it could be a cluster problem,
    but it might still be a single broker. In that case, it would be because a single
    broker is having problems replicating messages from everywhere, and you’ll have
    to figure out which broker it is. One way to do this is to get a list of under-replicated
    partitions for the cluster and see if there is a specific broker that is common
    to all of the partitions that are under-replicated. Using the `kafka-topics.sh`
    tool (discussed in detail in [Chapter 12](ch12.html#administering_kafka)), you
    can get a list of under-replicated partitions to look for a common thread.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, list under-replicated partitions in a cluster:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the common broker is number 2\. This indicates that this broker
    is having a problem with message replication and will lead us to focus our investigation
    on that one broker. If there is no common broker, there is likely a cluster-wide
    problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Cluster-level problems
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cluster problems usually fall into one of two categories:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Unbalanced load
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource exhaustion
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first problem, unbalanced partitions or leadership, is the easiest to find
    even though fixing it can be an involved process. In order to diagnose this problem,
    you will need several metrics from the brokers in the cluster:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Partition count
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leader partition count
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All topics messages in rate
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All topics bytes in rate
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All topics bytes out rate
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine these metrics. In a perfectly balanced cluster, the numbers will be
    even across all brokers in the cluster, as in [Table 13-4](#table1004).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-4\. Utilization metrics
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '| Broker | Partitions | Leaders | Messages in | Bytes in | Bytes out |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| 1 | 100 | 50 | 13130 msg/s | 3.56 MBps | 9.45 MBps |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| 2 | 101 | 49 | 12842 msg/s | 3.66 MBps | 9.25 MBps |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| 3 | 100 | 50 | 13086 msg/s | 3.23 MBps | 9.82 MBps |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: This indicates that all the brokers are taking approximately the same amount
    of traffic. Assuming you have already run a preferred replica election, a large
    deviation indicates that the traffic is not balanced within the cluster. To resolve
    this, you will need to move partitions from the heavily loaded brokers to the
    less heavily loaded brokers. This is done using the `kafka-reassign-partitions.sh`
    tool described in [Chapter 12](ch12.html#administering_kafka).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Helpers for Balancing Clusters
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kafka broker itself does not provide for automatic reassignment of partitions
    in a cluster. This means that balancing traffic within a Kafka cluster can be
    a mind-numbing process of manually reviewing long lists of metrics and trying
    to come up with a replica assignment that works. To help with this, some organizations
    have developed automated tools for performing this task. One example is the `kafka-assigner`
    tool that LinkedIn has released in the open source [kafka-tools repository on
    GitHub](https://oreil.ly/8ilPw). Some enterprise offerings for Kafka support also
    provide this feature.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common cluster performance issue is exceeding the capacity of the brokers
    to serve requests. There are many possible bottlenecks that could slow things
    down: CPU, disk IO, and network throughput are a few of the most common. Disk
    utilization is not one of them, as the brokers will operate properly right up
    until the disk is filled, and then this disk will fail abruptly. In order to diagnose
    a capacity problem, there are many metrics you can track at the OS level, including:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: CPU utilization
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inbound network throughput
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outbound network throughput
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk average wait time
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk percent utilization
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exhausting any of these resources will typically show up as the same problem:
    under-replicated partitions. It’s critical to remember that the broker replication
    process operates in exactly the same way that other Kafka clients do. If your
    cluster is having problems with replication, then your customers are having problems
    with producing and consuming messages as well. It makes sense to develop a baseline
    for these metrics when your cluster is operating correctly and then set thresholds
    that indicate a developing problem long before you run out of capacity. You will
    also want to review the trend for these metrics as the traffic to your cluster
    increases over time. As far as Kafka broker metrics are concerned, the `All Topics
    Bytes In Rate` is a good guideline to show cluster usage.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Host-level problems
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If the performance problem with Kafka is not present in the entire cluster
    and can be isolated to one or two brokers, it’s time to examine that server and
    see what makes it different from the rest of the cluster. These types of problems
    fall into several general categories:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Hardware failures
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conflicts with another process
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local configuration differences
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical Servers and Problems
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A server and its OS is a complex machine with thousands of components, any of
    which could have problems and cause either a complete failure or just a performance
    degradation. It’s impossible for us to cover everything that can fail in this
    book—numerous volumes have been written, and will continue to be, on this subject.
    But we can discuss some of the most common problems that are seen. This section
    will focus on issues with a typical server running a Linux OS.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Hardware failures are sometimes obvious, like when the server just stops working,
    but it’s the less obvious problems that cause performance issues. These are usually
    soft failures that allow the system to keep running but degrade operation. This
    could be a bad bit of memory, where the system has detected the problem and bypassed
    that segment (reducing the overall available memory). The same can happen with
    a CPU failure. For problems such as these, you should be using the facilities
    that your hardware provides, such as an intelligent platform management interface
    (IPMI) to monitor hardware health. When there’s an active problem, looking at
    the kernel ring buffer using `dmesg` will help you to see log messages that are
    getting thrown to the system console.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The more common type of hardware failure that leads to a performance degradation
    in Kafka is a disk failure. Apache Kafka is dependent on the disk for persistence
    of messages, and producer performance is directly tied to how fast your disks
    commit those writes. Any deviation in this will show up as problems with the performance
    of the producers and the replica fetchers. The latter is what leads to under-replicated
    partitions. As such, it is important to monitor the health of the disks at all
    times and address any problems quickly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: One Bad Egg
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A single disk failure on a single broker can destroy the performance of an entire
    cluster. This is because the producer clients will connect to all brokers that
    lead partitions for a topic, and if you have followed best practices, those partitions
    will be evenly spread over the entire cluster. If one broker starts performing
    poorly and slowing down produce requests, this will cause back pressure in the
    producers, slowing down requests to all brokers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, make sure you are monitoring hardware status information for
    the disks from the IPMI, or the interface provided by your hardware. In addition,
    within the OS you should be running SMART (Self-Monitoring, Analysis and Reporting
    Technology) tools to both monitor and test the disks on a regular basis. This
    will alert you to a failure that is about to happen. It is also important to keep
    an eye on the disk controller, especially if it has RAID functionality, whether
    you are using hardware RAID or not. Many controllers have an onboard cache that
    is only used when the controller is healthy and the battery backup unit (BBU)
    is working. A failure of the BBU can result in the cache being disabled, degrading
    disk performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Networking is another area where partial failures will cause problems. Some
    of these problems are hardware issues, such as a bad network cable or connector.
    Some are configuration issues, which is usually a change in the speed or duplex
    settings for the connection, either on the server side or upstream on the networking
    hardware. Network configuration problems could also be OS issues, such as having
    the network buffers undersized or too many network connections taking up too much
    of the overall memory footprint. One of the key indicators of problems in this
    area will be the number of errors detected on the network interfaces. If the error
    count is increasing, there is probably an unaddressed issue.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: If there are no hardware problems, another common problem to look for is another
    application running on the system that is consuming resources and putting pressure
    on the Kafka broker. This could be something that was installed in error, or it
    could be a process that is supposed to be running, such as a monitoring agent,
    but is having problems. Use the tools on your system, such as `top`, to identify
    if there is a process that is using more CPU or memory than expected.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: If the other options have been exhausted and you have not yet found the source
    of the discrepancy on the host, a configuration difference has likely crept in,
    either with the broker or the system itself. Given the number of applications
    that are running on any single server and the number of configuration options
    for each of them, it can be a daunting task to find a discrepancy. This is why
    it is crucial that you utilize a configuration management system, such as [Chef](https://www.chef.io)
    or [Puppet](https://puppet.com), in order to maintain consistent configurations
    across your OSes and applications (including Kafka).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Broker Metrics
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to under-replicated partitions, there are other metrics that are
    present at the overall broker level that should be monitored. While you may not
    be inclined to set alert thresholds for all of them, they provide valuable information
    about your brokers and your cluster. They should be present in any monitoring
    dashboard you create.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Active controller count
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *active controller count* metric indicates whether the broker is currently
    the controller for the cluster. The metric will either be 0 or 1, with 1 showing
    that the broker is currently the controller. At all times, only one broker should
    be the controller, and one broker must always be the controller in the cluster.
    If two brokers say that they are currently the controller, this means that you
    have a problem where a controller thread that should have exited has become stuck.
    This can cause problems with not being able to execute administrative tasks, such
    as partition moves, properly. To remedy this, you will need to restart both brokers
    at the very least. However, when there is an extra controller in the cluster,
    there will often be problems performing a safe shutdown of a broker, and you will
    need to force stop the broker instead. See [Table 13-5](#table1005) for more details
    on active controller count.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-5\. Active controller count metric details
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Active controller count |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.controller:type=KafkaController,name=ActiveControllerCount`
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| Value range | Zero or one |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: If no broker claims to be the controller in the cluster, the cluster will fail
    to respond properly in the face of state changes, including topic or partition
    creation, or broker failures. In this situation, you must investigate further
    to find out why the controller threads are not working properly. For example,
    a network partition from the ZooKeeper cluster could result in a problem like
    this. Once that underlying problem is fixed, it is wise to restart all the brokers
    in the cluster in order to reset state for the controller threads.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Controller queue size
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *controller queue size* metric indicates how many requests the controller
    is currently waiting to process for the brokers. The metric will be 0 or more,
    with the value fluctuating frequently as new requests from brokers come in and
    administrative actions, such as creating partitions, moving partitions, and processing
    leader changes happen. Spikes in the metric are to be expected, but if this value
    continuously increases, or stays steady at a high value and does not drop, it
    indicates that the controller may be stuck. This can cause problems with not being
    able to execute administrative tasks properly. To remedy this, you will need to
    move the controller to a different broker, which requires shutting down the broker
    that is currently the controller. However, when the controller is stuck, there
    will often be problems performing a controlled shutdown of any broker. See [Table 13-6](#table1017)
    for more details on controller queue size.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-6\. Controller queue size metric details
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Controller queue size |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.controller:type=ControllerEventManager,name=EventQueueSize`
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Value range | Integer, zero or more |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: Request handler idle ratio
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kafka uses two thread pools for handling all client requests: *network threads*
    and *request handler threads* (also called *I/O threads*). The network threads
    are responsible for reading and writing data to the clients across the network.
    This does not require significant processing, which means that exhaustion of the
    network threads is less of a concern. The request handler threads, however, are
    responsible for servicing the client request itself, which includes reading or
    writing the messages to disk. As such, as the brokers get more heavily loaded,
    there is a significant impact on this thread pool. See [Table 13-7](#table1006)
    for more details on the request handler idle ratio.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-7\. Request handler idle ratio details
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Request handler average idle percentage |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent`
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Value range | Float, between zero and one inclusive |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: Intelligent Thread Usage
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it may seem like you will need hundreds of request handler threads, in
    reality you do not need to configure any more threads than you have CPUs in the
    broker. Apache Kafka is very smart about the way it uses the request handlers,
    making sure to offload to purgatory those requests that will take a long time
    to process. This is used, for example, when requests are being quoted or when
    more than one acknowledgment of produce requests is required.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The request handler idle ratio metric indicates the percentage of time the request
    handlers are not in use. The lower this number, the more loaded the broker is.
    Experience tells us that idle ratios lower than 20% indicate a potential problem,
    and lower than 10% is usually an active performance problem. Besides the cluster
    being undersized, there are two reasons for high thread utilization in this pool.
    The first is that there are not enough threads in the pool. In general, you should
    set the number of request handler threads equal to the number of processors in
    the system (including hyperthreaded processors).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The other common reason for high request handler thread utilization is that
    the threads are doing unnecessary work for each request. Prior to Kafka 0.10,
    the request handler thread was responsible for decompressing every incoming message
    batch, validating the messages and assigning offsets, and then recompressing the
    message batch with offsets before writing it to disk. To make matters worse, the
    compression methods were all behind a synchronous lock. As of version 0.10, there
    is a new message format that allows for relative offsets in a message batch. This
    means that newer producers will set relative offsets prior to sending the message
    batch, which allows the broker to skip recompression of the message batch. One
    of the single largest performance improvements you can make is to ensure that
    all producer and consumer clients support the 0.10 message format, and to change
    the message format version on the brokers to 0.10 as well. This will greatly reduce
    the utilization of the request handler threads.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: All topics bytes in
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *all topics bytes in* rate, expressed in bytes per second, is useful as
    a measurement of how much message traffic your brokers are receiving from producing
    clients. This is a good metric to trend over time to help you determine when you
    need to expand the cluster or do other growth-related work. It is also useful
    for evaluating if one broker in a cluster is receiving more traffic than the others,
    which would indicate that it is necessary to rebalance the partitions in the cluster.
    See [Table 13-8](#table1007) for more details.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-8\. All topics bytes in metric details
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Bytes in per second |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec` |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| Value range | Rates as doubles, count as integer |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: As this is the first rate metric discussed, it is worth a short discussion of
    the attributes that are provided by these types of metrics. All of the rate metrics
    have seven attributes, and choosing which ones to use depends on what type of
    measurement you want. The attributes provide a discrete count of events, as well
    as an average of the number of events over various periods of time. Make sure
    to use the metrics appropriately, or you will end up with a flawed view of the
    broker.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two attributes are not measurements, but they will help you understand
    the metric you are looking at:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '`EventType`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: This is the unit of measurement for all the attributes. In this case, it is
    “bytes.”
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '`RateUnit`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: For the rate attributes, this is the time period for the rate. In this case,
    it is “seconds.”
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'These two descriptive attributes tell us that the rates, regardless of the
    period of time they average over, are presented as a value of bytes per second.
    There are four rate attributes provided with different granularities:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '`OneMinuteRate`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: An average over the previous 1 minute
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '`FiveMinuteRate`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: An average over the previous 5 minutes
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '`FifteenMinuteRate`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: An average over the previous 15 minutes
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '`MeanRate`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: An average since the broker was started
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The `OneMinuteRate` will fluctuate quickly and provides more of a “point in
    time” view of the measurement. This is useful for seeing short spikes in traffic.
    The `MeanRate` will not vary much at all and provides an overall trend. Though
    `MeanRate` has its uses, it is probably not the metric you want to be alerted
    on. The `FiveMinuteRate` and `FifteenMinuteRate` provide a compromise between
    the two.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the rate attributes, there is a `Count` attribute as well. This
    is a constantly increasing value for the metric since the time the broker was
    started. For this metric, all topics bytes in, the `Count` represents the total
    number of bytes produced to the broker since the process was started. Utilized
    with a metrics system that supports countermetrics, this can give you an absolute
    view of the measurement instead of an averaged rate.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: All topics bytes out
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *all topics bytes out* rate, similar to the bytes in rate, is another overall
    growth metric. In this case, the bytes out rate shows the rate at which consumers
    are reading messages out. The outbound bytes rate may scale differently than the
    inbound bytes rate, thanks to Kafka’s capacity to handle multiple consumers with
    ease. There are many deployments of Kafka where the outbound rate can easily be
    six times the inbound rate! This is why it is important to observe and trend the
    outbound bytes rate separately. See [Table 13-9](#table1008) for more details.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-9\. All topics bytes out metric details
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Bytes out per second |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec` |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| Value range | Rates as doubles, count as integer |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: Replica Fetchers Included
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The outbound bytes rate *also* includes the replica traffic. This means that
    if all of the topics are configured with a replication factor of 2, you will see
    a bytes out rate equal to the bytes in rate when there are no consumer clients.
    If you have one consumer client reading all the messages in the cluster, then
    the bytes out rate will be twice the bytes in rate. This can be confusing when
    looking at the metrics if you’re not aware of what is counted.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: All topics messages in
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the byte rates described previously show the broker traffic in absolute
    terms of bytes, the *messages in* rate shows the number of individual messages,
    regardless of their size, produced per second. This is useful as a growth metric
    as a different measure of producer traffic. It can also be used in conjunction
    with the bytes in rate to determine an average message size. You may also see
    an imbalance in the brokers, just like with the bytes in rate, that will alert
    you to necessary maintenance work. See [Table 13-10](#table1009) for more details.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-10\. All topics messages in metric details
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Messages in per second |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec`
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Value range | Rates as doubles, count as integer |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: Why No Messages Out?
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: People often ask why there is no messages out metric for the Kafka broker. The
    reason is that when messages are consumed, the broker just sends the next batch
    to the consumer without expanding it to find out how many messages are inside.
    Therefore, the broker doesn’t really know how many messages were sent out. The
    only metric that can be provided is the number of fetches per second, which is
    a request rate, not a messages count.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Partition count
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *partition count* for a broker generally doesn’t change that much, as it
    is the total number of partitions assigned to that broker. This includes every
    replica the broker has, regardless of whether it is a leader or follower for that
    partition. Monitoring this is often more interesting in a cluster that has automatic
    topic creation enabled, as that can leave the creation of topics outside of the
    control of the person running the cluster. See [Table 13-11](#table1010) for more
    details.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-11\. Partition count metric details
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Partition count |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.server:type=ReplicaManager,name=PartitionCount` |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Value range | Integer, zero or greater |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: Leader count
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *leader count* metric shows the number of partitions that the broker is
    currently the leader for. As with most other measurements in the brokers, this
    one should be generally even across the brokers in the cluster. It is much more
    important to check the leader count on a regular basis, possibly alerting on it,
    as it will indicate when the cluster is imbalanced even if the number of replicas
    are perfectly balanced in count and size across the cluster. This is because a
    broker can drop leadership for a partition for many reasons, such as a ZooKeeper
    session expiration, and it will not automatically take leadership back once it
    recovers (except if you have enabled automatic leader rebalancing). In these cases,
    this metric will show fewer leaders, or often zero, which indicates that you need
    to run a preferred replica election to rebalance leadership in the cluster. See
    [Table 13-12](#table1011) for more details.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-12\. Leader count metric details
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Leader count |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.server:type=ReplicaManager,name=LeaderCount` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| Value range | Integer, zero or greater |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: A useful way to consume this metric is to use it along with the partition count
    to show a percentage of partitions that the broker is the leader for. In a well-balanced
    cluster that is using a replication factor of 2, all brokers should be leaders
    for approximately 50% of their partitions. If the replication factor in use is
    3, this percentage drops to 33%.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Offline partitions
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Along with the under-replicated partitions count, the *offline partitions*
    count is a critical metric for monitoring (see [Table 13-13](#table1012)). This
    measurement is only provided by the broker that is the controller for the cluster
    (all other brokers will report 0) and shows the number of partitions in the cluster
    that currently have no leader. Partitions without leaders can happen for two main
    reasons:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: All brokers hosting replicas for this partition are down
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No in-sync replica can take leadership due to message-count mismatches (with
    unclean leader election disabled)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 13-13\. Offline partitions count metric details
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Offline partitions count |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| JMX MBean | `kafka.controller:type=KafkaController,name=OfflinePartitionsCount`
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| Value range | Integer, zero or greater |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: In a production Kafka cluster, an offline partition may be impacting the producer
    clients, losing messages or causing back pressure in the application. This is
    most often a “site down” type of problem and will need to be addressed immediately.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Request metrics
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Kafka protocol, described in [Chapter 6](ch06.html#kafka_internals), has
    many different requests. Metrics are provided for how each of those requests performs.
    As of version 2.5.0, the following requests have metrics provided:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-14\. Request metrics names
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '| `AddOffsetsToTxn` | `AddPartitionsToTxn` | `AlterConfigs` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| `AlterPartitionReassignments` | `AlterReplicaLogDirs` | `ApiVersions` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| `ControlledShutdown` | `CreateAcls` | `CreateDelegationToken` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| `CreatePartitions` | `CreateTopics` | `DeleteAcls` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| `DeleteGroups` | `DeleteRecords` | `DeleteTopics` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| `DescribeAcls` | `DescribeConfigs` | `DescribeDelegationToken` |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| `DescribeGroups` | `DescribeLogDirs` | `ElectLeaders` |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| `EndTxn` | `ExpireDelegationToken` | `Fetch` |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| `FetchConsumer` | `FetchFollower` | `FindCoordinator` |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| `Heartbeat` | `IncrementalAlterConfigs` | `InitProducerId` |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| `JoinGroup` | `LeaderAndIsr` | `LeaveGroup` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| `ListGroups` | `ListOffsets` | `ListPartitionReassignments` |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| `Metadata` | `OffsetCommit` | `OffsetDelete` |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| `OffsetFetch` | `OffsetsForLeaderEpoch` | `Produce` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| `RenewDelegationToken` | `SaslAuthenticate` | `SaslHandshake` |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| `StopReplica` | `SyncGroup` | `TxnOffsetCommit` |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| `UpdateMetadata` | `WriteTxnMarkers` |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: For each of these requests, there are eight metrics provided, providing insight
    into each phase of the request processing. For example, for the `Fetch` request,
    the metrics shown in [Table 13-15](#table10_1) are available.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-15\. Fetch request metrics
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | JMX MBean |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| Total time | `kafka.network:``type=RequestMetrics,name=TotalTimeMs,request=Fetch`
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| Request queue time | `kafka.network:``type=RequestMetrics,name=RequestQueueTimeMs,request=Fetch`
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| Local time | `kafka.network:``type=RequestMetrics,name=LocalTimeMs,request=Fetch`
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| Remote time | `kafka.network:``type=RequestMetrics,name=RemoteTimeMs,request=Fetch`
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| Throttle time | `kafka.network:``type=RequestMetrics,name=ThrottleTimeMs,request=Fetch`
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| Response queue time | `kafka.network:``type=RequestMetrics,name=ResponseQueueTimeMs,request=Fetch`
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| Response send time | `kafka.network:``type=RequestMetrics,name=ResponseSendTimeMs,request=Fetch`
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| Requests per second | `kafka.network:``type=RequestMetrics,name=RequestsPerSec,request=Fetch`
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: The requests per second metric is a rate metric, as discussed earlier, and shows
    the total number of that type of request that has been received and processed
    over the time unit. This provides a view into the frequency of each request time,
    though it should be noted that many of the requests, such as `StopReplica` and
    `UpdateMetadata`, are infrequent.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'The seven *time* metrics each provide a set of percentiles for requests, as
    well as a discrete `Count` attribute, similar to rate metrics. The metrics are
    all calculated since the broker was started, so keep that in mind when looking
    at metrics that do not change for long periods of time; the longer your broker
    has been running, the more stable the numbers will be. The parts of request processing
    they represent are:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Total time
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The total amount of time the broker spends processing the request, from receiving
    it to sending the response back to the requester
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Request queue time
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time the request spends in queue after it has been received but
    before processing starts
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Local time
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time the partition leader spends processing a request, including
    sending it to disk (but not necessarily flushing it)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Remote time
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time spent waiting for the followers before request processing
    can complete
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Throttle time
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time the response must be held in order to slow the requestor
    down to satisfy client quota settings
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Response queue time
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time the response to the request spends in the queue before it
    can be sent to the requestor
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Response send time
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time spent actually sending the response
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'The attributes provided for each metric are:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '`Count`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Absolute count of number of requests since process start
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '`Min`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Minimum value for all requests
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '`Max`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Maximum value for all requests
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '`Mean`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Average value for all requests
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '`StdDev`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation of the request timing measurements as a whole
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '`Percentiles`'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '`50thPercentile`, `75thPercentile`, `95thPercentile`, `98thPercentile`, `99thPercentile`,
    `999thPercentile`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Percentile?
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Percentiles are a common way of looking at timing measurement. A 99th percentile
    measurement tells us that 99% of all values in the sample group (request timings,
    in this case) are less than the value of the metric. This means that 1% of the
    values are greater than the value specified. A common pattern is to view the average
    value and the 99% or 99.9% value. In this way, you can understand how the average
    request performs and what the outliers are.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Out of all of these metrics and attributes for requests, which are the important
    ones to monitor? At a minimum, you should collect at least the average and one
    of the higher percentiles (either 99% or 99.9%) for the total time metric, as
    well as the requests per second metric, for every request type. This gives a view
    into the overall performance of requests to the Kafka broker. If you can, you
    should also collect those measurements for the other six timing metrics for each
    request type, as this will allow you to narrow down any performance problems to
    a specific phase of request processing.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: For setting alert thresholds, the timing metrics can be difficult. The timing
    for a `Fetch` request, for example, can vary wildly depending on many factors,
    including settings on the client for how long it will wait for messages, how busy
    the particular topic being fetched is, and the speed of the network connection
    between the client and the broker. It can be very useful, however, to develop
    a baseline value for the 99.9th percentile measurement for at least the total
    time, especially for `Produce` requests, and alert on this. Much like the under-replicated
    partitions metric, a sharp increase in the 99.9th percentile for `Produce` requests
    can alert you to a wide range of performance problems.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Topic and Partition Metrics
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the many metrics available on the broker that describe the operation
    of the Kafka broker in general, there are topic- and partition-specific metrics.
    In larger clusters these can be numerous, and it may not be possible to collect
    all of them into a metrics system as a matter of normal operations. However, they
    are quite useful for debugging specific issues with a client. For example, the
    topic metrics can be used to identify a specific topic that is causing a large
    increase in traffic to the cluster. It also may be important to provide these
    metrics so that users of Kafka (the producer and consumer clients) are able to
    access them. Regardless of whether you are able to collect these metrics regularly,
    you should be aware of what is useful.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: For all the examples in [Table 13-16](#table10_2), we will be using the example
    topic name `*TOPICNAME*`, as well as partition 0\. When accessing the metrics
    described, make sure to substitute the topic name and partition number that are
    appropriate for your cluster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Per-topic metrics
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all the per-topic metrics, the measurements are very similar to the broker
    metrics described previously. In fact, the only difference is the provided topic
    name, and that the metrics will be specific to the named topic. Given the sheer
    number of metrics available, depending on the number of topics present in your
    cluster, these will almost certainly be metrics that you will not want to set
    up monitoring and alerts for. They are useful to provide to clients, however,
    so that they can evaluate and debug their own usage of Kafka.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-16\. Metrics for each topic
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | JMX MBean |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| Bytes in rate | `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Bytes out rate | `kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| Failed fetch rate | `kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| Failed produce rate | `kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Messages in rate | `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| Fetch request rate | `kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| Produce request rate | `kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: Per-partition metrics
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The per-partition metrics tend to be less useful on an ongoing basis than the
    per-topic metrics. Additionally, they are quite numerous as hundreds of topics
    can easily be thousands of partitions. Nevertheless, they can be useful in some
    limited situations. In particular, the partition-size metric indicates the amount
    of data (in bytes) that is currently being retained on disk for the partition
    ([Table 13-17](#table1013)). Combined, these will indicate the amount of data
    retained for a single topic, which can be useful in allocating costs for Kafka
    to individual clients. A discrepancy between the size of two partitions for the
    same topic can indicate a problem where the messages are not evenly distributed
    across the key that is being used when producing. The log-segment count metric
    shows the number of log-segment files on disk for the partition. This may be useful
    along with the partition size for resource tracking.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-17\. Metrics for each partition
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | JMX MBean |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| Partition size | `kafka.log:type=Log,name=Size,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| Log segment count | `kafka.log:type=Log,name=NumLogSegments,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Log end offset | `kafka.log:type=Log,name=LogEndOffset,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| Log start offset | `kafka.log:type=Log,name=LogStartOffset,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: The log end offset and log start offset metrics are the highest and lowest offsets
    for messages in that partition, respectively. It should be noted, however, that
    the difference between these two numbers does not necessarily indicate the number
    of messages in the partition, as log compaction can result in “missing” offsets
    that have been removed from the partition due to newer messages with the same
    key. In some environments, it could be useful to track these offsets for a partition.
    One such use case is to provide a more granular mapping of timestamp to offset,
    allowing for consumer clients to easily roll back offsets to a specific time (though
    this is less important with time-based index searching, introduced in Kafka 0.10.1).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Under-Replicated Partition Metrics
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a per-partition metric provided to indicate whether or not the partition
    is under-replicated. In general, this is not very useful in day-to-day operations,
    as there are too many metrics to gather and watch. It is much easier to monitor
    the broker-wide under-replicated partition count and then use the command-line
    tools (described in [Chapter 12](ch12.html#administering_kafka)) to determine
    the specific partitions that are under-replicated.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: JVM Monitoring
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the metrics provided by the Kafka broker, you should be monitoring
    a standard suite of measurements for all of your servers, as well as the Java
    Virtual Machine (JVM) itself. These will be useful to alert you to a situation,
    such as increasing garbage collection activity, that will degrade the performance
    of the broker. They will also provide insight into why you see changes in metrics
    downstream in the broker.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the JVM, the critical thing to monitor is the status of garbage collection
    (GC). The particular beans that you must monitor for this information will vary
    depending on the particular Java Runtime Environment (JRE) that you are using,
    as well as the specific GC settings in use. For an Oracle Java 1.8 JRE running
    with G1 garbage collection, the beans to use are shown in [Table 13-18](#table1014).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-18\. G1 garbage collection metrics
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | JMX MBean |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| Full GC cycles | `java.lang:type=GarbageCollector,name=G1 Old Generation`
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| Young GC cycles | `java.lang:type=GarbageCollector,name=G1 Young Generation`
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: Note that in the semantics of GC, “Old” and “Full” are the same thing. For each
    of these metrics, the two attributes to watch are `CollectionCount` and `CollectionTime`.
    The `CollectionCount` is the number of GC cycles of that type (Full or Young)
    since the JVM was started. The `CollectionTime` is the amount of time, in milliseconds,
    spent in that type of GC cycle since the JVM was started. As these measurements
    are counters, they can be used by a metrics system to tell you an absolute number
    of GC cycles and time spent in GC per unit of time. They can also be used to provide
    an average amount of time per GC cycle, though this is less useful in normal operations.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Each of these metrics also has a `LastGcInfo` attribute. This is a composite
    value, made up of five fields, that gives you information on the last GC cycle
    for the type of GC described by the bean. The important value to look at is the
    `duration` value, as this tells you how long, in milliseconds, the last GC cycle
    took. The other values in the composite (`GcThreadCount`, `id`, `startTime`, and
    `endTime`) are informational and not very useful. It’s important to note that
    you will not be able to see the timing of every GC cycle using this attribute,
    as young GC cycles in particular can happen frequently.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Java OS monitoring
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The JVM can provide you with some information on the OS through the `java.lang:type=OperatingSystem`
    bean. However, this information is limited and does not represent everything you
    need to know about the system running your broker. The two attributes that can
    be collected here that are of use, which are difficult to collect in the OS, are
    the `MaxFileDescriptorCount` and `OpenFileDescriptorCount` attributes. `MaxFileDescriptorCount`
    will tell you the maximum number of file descriptors (FDs) that the JVM is allowed
    to have open. The `OpenFileDescriptorCount` attribute tells you the number of
    FDs that are currently open. There will be FDs open for every log segment and
    network connection, and they can add up quickly. A problem closing network connections
    properly could cause the broker to rapidly exhaust the number allowed.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: OS Monitoring
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The JVM cannot provide us with all the information that we need to know about
    the system it is running on. For this reason, we must not only collect metrics
    from the broker but also from the OS itself. Most monitoring systems will provide
    agents that will collect more OS information than you could possibly be interested
    in. The main areas that are necessary to watch are CPU usage, memory usage, disk
    usage, disk I/O, and network usage.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'For CPU utilization, you will want to look at the system load average at the
    very least. This provides a single number that will indicate the relative utilization
    of the processors. In addition, it may also be useful to capture the percent usage
    of the CPU, broken down by type. Depending on the method of collection and your
    particular OS, you may have some or all of the following CPU percentage breakdowns
    (provided with the abbreviation used):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '`us`'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: The time spent in user space
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '`sy`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: The time spent in kernel space
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '`ni`'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: The time spent on low-priority processes
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '`id`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: The time spent idle
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '`wa`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: The time spent in wait (on disk)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '`hi`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The time spent handling hardware interrupts
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '`si`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: The time spent handling software interrupts
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '`st`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: The time waiting for the hypervisor
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: What Is System Load?
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While many know that system load is a measure of CPU usage on a system, most
    people misunderstand how it is measured. The load average is a count of the number
    of processes that are runnable and are waiting for a processor to execute on.
    Linux also includes threads that are in an uninterruptable sleep state, such as
    waiting for the disk. The load is presented as three numbers, which is the count
    averaged over the last minute, 5 minutes, and 15 minutes. In a single CPU system,
    a value of 1 would mean the system is 100% loaded, with a thread always waiting
    to execute. This means that on a multiple CPU system, the load average number
    that indicates 100% is equal to the number of CPUs in the system. For example,
    if there are 24 processors in the system, 100% would be a load average of 24.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka broker uses a significant amount of processing for handling requests.
    For this reason, keeping track of the CPU utilization is important when monitoring
    Kafka. Memory is less important to track for the broker itself, as Kafka will
    normally be run with a relatively small JVM heap size. It will use a small amount
    of memory outside of the heap for compression functions, but most of the system
    memory will be left to be used for cache. All the same, you should keep track
    of memory utilization to make sure other applications do not infringe on the broker.
    You will also want to make sure that swap memory is not being used by monitoring
    the amount of total and free swap memory.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Disk is by far the most important subsystem when it comes to Kafka. All messages
    are persisted to disk, so the performance of Kafka depends heavily on the performance
    of the disks. Monitoring usage of both disk space and inodes (*inodes* are the
    file and directory metadata objects for Unix filesystems) is important, as you
    need to assure that you are not running out of space. This is especially true
    for the partitions where Kafka data is being stored. It is also necessary to monitor
    the disk I/O statistics, as this will tell us that the disk is being used efficiently.
    For at least the disks where Kafka data is stored, monitor the reads and writes
    per second, the average read and write queue sizes, the average wait time, and
    the utilization percentage of the disk.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Finally, monitor the network utilization on the brokers. This is simply the
    amount of inbound and outbound network traffic, normally reported in bits per
    second. Keep in mind that every bit inbound to the Kafka broker will be a number
    of bits outbound equal to the replication factor of the topics, not including
    consumers. Depending on the number of consumers, outbound network traffic could
    easily be an order of magnitude larger than inbound traffic. Keep this in mind
    when setting thresholds for alerts.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No discussion of monitoring is complete without a word about logging. Like many
    applications, the Kafka broker will fill disks with log messages in minutes if
    you let it. In order to get useful information from logging, it is important to
    enable the right loggers at the right levels. By simply logging all messages at
    the `INFO` level, you will capture a significant amount of important information
    about the state of the broker. It is useful to separate a couple of loggers from
    this, however, in order to provide a cleaner set of log files.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: There are two loggers writing to separate files on disk. The first is `kafka.controller`,
    still at the `INFO` level. This logger is used to provide messages specifically
    regarding the cluster controller. At any time, only one broker will be the controller,
    and therefore only one broker will be writing to this logger. The information
    includes topic creation and modification, broker status changes, and cluster activities
    such as preferred replica elections and partition moves. The other logger to separate
    is `kafka.server.ClientQuotaManager`, also at the `INFO` level. This logger is
    used to show messages related to produce and consume quota activities. While this
    is useful information, it is better to not have it in the main broker log file.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: It is also helpful to log information regarding the status of the log compaction
    threads. There is no single metric to show the health of these threads, and it
    is possible for failure in compaction of a single partition to halt the log compaction
    threads entirely, and silently. Enabling the `kafka.log.LogCleaner`, `kafka.log.Cleaner`,
    and `kafka.log.LogCleanerManager` loggers at the `DEBUG` level will output information
    about the status of these threads. This will include information about each partition
    being compacted, including the size and number of messages in each. Under normal
    operations, this is not a lot of logging, which means that it can be enabled by
    default without overwhelming you.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: There is also some logging that may be useful to turn on when debugging issues
    with Kafka. One such logger is `kafka.request.logger`, turned on at either the
    `DEBUG` or `TRACE` levels. This logs information about every request sent to the
    broker. At the `DEBUG` level, the log includes connection end points, request
    timings, and summary information. At the `TRACE` level, it will also include topic
    and partition information—nearly all request information short of the message
    payload itself. At either level, this logger generates a significant amount of
    data, and it is not recommended to enable it unless necessary for debugging.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Client Monitoring
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All applications need monitoring. Those that instantiate a Kafka client, either
    a producer or consumer, have metrics specific to the client that should be captured.
    This section covers the official Java client libraries, though other implementations
    should have their own measurements available.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Producer Metrics
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kafka producer client has greatly compacted the metrics available by making
    them available as attributes on a small number of JMX MBeans. In contrast, the
    previous version of the producer client (which is no longer supported) used a
    larger number of MBeans but had more detail in many of the metrics (providing
    a greater number of percentile measurements and different moving averages). As
    a result, the overall number of metrics provided covers a wider surface area,
    but it can be more difficult to track outliers.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: All of the producer metrics have the client ID of the producer client in the
    bean names. In the examples provided, this has been replaced with `*CLIENTID*`.
    Where a bean name contains a broker ID, this has been replaced with `*BROKERID*`.
    Topic names have been replaced with `*TOPICNAME*`. See [Table 13-19](#table10_5)
    for an example.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-19\. Kafka producer metric MBeans
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | JMX MBean |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| Overall producer | `kafka.producer:type=producer-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| Per-broker | `kafka.producer:type=producer-node-metrics,client-id=*CLIENTID*,node-id=node-*BROKERID*`
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Per-topic | `kafka.producer:type=producer-topic-metrics,client-id=*CLIENTID*,topic=*TOPICNAME*`
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: Each of the metric beans in [Table 13-19](#table10_5) has multiple attributes
    available to describe the state of the producer. The particular attributes that
    are of the most use are described in the next section. Before proceeding, be sure
    you understand the semantics of how the producer works, as described in [Chapter 3](ch03.html#writing_messages_to_kafka).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Overall producer metrics
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The overall producer metrics bean provides attributes describing everything
    from the sizes of the message batches to the memory buffer utilization. While
    all of these measurements have their place in debugging, there are only a handful
    needed on a regular basis, and only a couple of those that should be monitored
    and have alerts. Note that while we will discuss several metrics that are averages
    (ending in `-avg`), there are also maximum values for each metric (ending in `-max`)
    that have limited usefulness.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: The `record-error-rate` is one attribute that you will definitely want to set
    an alert for. This metric should always be zero, and if it is anything greater
    than that, the producer is dropping messages it is trying to send to the Kafka
    brokers. The producer has a configured number of retries and a backoff between
    those, and once that has been exhausted, the messages (called *records* here)
    will be dropped. There is also a `record-retry-rate` attribute that can be tracked,
    but it is less critical than the error rate because retries are normal.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: The other metric to alert on is the `request-latency-avg`. This is the average
    amount of time a produce request sent to the brokers takes. You should be able
    to establish a baseline value for what this number should be in normal operations,
    and set an alert threshold above that. An increase in the request latency means
    that produce requests are getting slower. This could be due to networking issues,
    or it could indicate problems on the brokers. Either way, it’s a performance issue
    that will cause back pressure and other problems in your producing application.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these critical metrics, it is always good to know how much message
    traffic your producer is sending. Three attributes will provide three different
    views of this. The `outgoing-byte-rate` describes the messages in absolute size
    in bytes per second. The `record-send-rate` describes the traffic in terms of
    the number of messages produced per second. Finally, the `request-rate` provides
    the number of produce requests sent to the brokers per second. A single request
    contains one or more batches. A single batch contains one or more messages. And,
    of course, each message is made up of some number of bytes. These metrics are
    all useful to have on an application dashboard.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: There are also metrics that describe the size of records, requests, and batches.
    The `request-size-avg` metric provides the average size of the produce requests
    being sent to the brokers in bytes. The `batch-size-avg` provides the average
    size of a single message batch (which, by definition, is comprised of messages
    for a single topic partition) in bytes. The `record-size-avg` shows the average
    size of a single record in bytes. For a single-topic producer, this provides useful
    information about the messages being produced. For multiple-topic producers, such
    as MirrorMaker, it is less informative. Besides these three metrics, there is
    a `records-per-request-avg` metric that describes the average number of messages
    that are in a single produce request.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'The last overall producer metric attribute that is recommended is `record-queue-time-avg`.
    This measurement is the average amount of time, in milliseconds, that a single
    message waits in the producer, after the application sends it, before it is actually
    produced to Kafka. After an application calls the producer client to send a message
    (by calling the `send` method), the producer waits until one of two things happens:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: It has enough messages to fill a batch based on the `batch.size` configuration.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has been long enough since the last batch was sent based on the `linger.ms`
    configuration.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either of these two will cause the producer client to close the current batch
    it is building and send it to the brokers. The easiest way to understand it is
    that for busy topics, the first condition will apply, whereas for slow topics,
    the second will apply. The `record-queue-time-avg` measurement will indicate how
    long messages take to be produced, and therefore is helpful when tuning these
    two configurations to meet the latency requirements for your application.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Per-broker and per-topic metrics
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the overall producer metrics, there are metric beans that provide
    a limited set of attributes for the connection to each Kafka broker, as well as
    for each topic that is being produced. These measurements are useful for debugging
    problems in some cases, but they are not metrics that you are going to want to
    review on an ongoing basis. All of the attributes on these beans are the same
    as the attributes for the overall producer beans described previously and have
    the same meaning as described previously (except that they apply either to a specific
    broker or a specific topic).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: The most useful metric provided by the per-broker producer metrics is the `request-latency-avg
    measurement`. This is because this metric will be mostly stable (given stable
    batching of messages) and can still show a problem with connections to a specific
    broker. The other attributes, such as `outgoing-byte-rate` and `request-latency-avg`,
    tend to vary depending on what partitions each broker is leading. This means that
    what these measurements “should” be at any point in time can quickly change, depending
    on the state of the Kafka cluster.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: The topic metrics are a little more interesting than the per-broker metrics,
    but they will only be useful for producers that are working with more than one
    topic. They will also only be usable on a regular basis if the producer is not
    working with a lot of topics. For example, a MirrorMaker could be producing hundreds,
    or thousands, of topics. It is difficult to review all of those metrics, and nearly
    impossible to set reasonable alert thresholds on them. As with the per-broker
    metrics, the per-topic measurements are best used when investigating a specific
    problem. The `record-send-rate` and `record-error-rate` attributes, for example,
    can be used to isolate dropped messages to a specific topic (or validated to be
    across all topics). In addition, there is a `byte-rate` metric that provides the
    overall messages rate in bytes per second for the topic.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Consumer Metrics
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the producer client, the consumer in Kafka consolidates many of the
    metrics into attributes on just a few metric beans. These metrics have also eliminated
    the percentiles for latencies and the moving averages for rates, which were presenting
    in the deprecated Scala consumer, similar to the producer client. In the consumer,
    because the logic around consuming messages is a little more complex than just
    firing messages into the Kafka brokers, there are a few more metrics to deal with
    as well. See [Table 13-20](#table1015).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-20\. Kafka consumer metric MBeans
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | JMX MBean |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| Overall consumer | `kafka.consumer:type=consumer-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| Fetch manager | `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| Per-topic | `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*CLIENT​ID*,topic=*TOPICNAME*`
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| Per-broker | `kafka.consumer:type=consumer-node-metrics,client-id=*CLIENTID*,node-id=node-*BROKERID*`
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| Coordinator | `kafka.consumer:type=consumer-coordinator-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: Fetch manager metrics
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the consumer client, the overall consumer metric bean is less useful for
    us because the metrics of interest are located in the *fetch manager* beans instead.
    The overall consumer bean has metrics regarding the lower-level network operations,
    but the fetch manager bean has metrics regarding bytes, request, and record rates.
    Unlike the producer client, the metrics provided by the consumer are useful to
    look at but not useful for setting up alerts on.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: For the fetch manager, the one attribute you may want to set up monitoring and
    alerts for is `fetch-latency-avg`. As with the equivalent `request-latency-avg`
    in the producer client, this metric tells us how long fetch requests to the brokers
    take. The problem with alerting on this metric is that the latency is governed
    by the consumer configurations `fetch.min.bytes` and `fetch.max.wait.ms`. A slow
    topic will have erratic latencies, as sometimes the broker will respond quickly
    (when there are messages available), and sometimes it will not respond for `fetch.max.wait.ms`
    (when there are no messages available). When consuming topics that have more regular,
    and abundant, message traffic, this metric may be more useful to look at.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Wait! No Lag?
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best advice for all consumers is that you must monitor the consumer lag.
    So why do we not recommend monitoring the `records-lag-max` attribute on the fetch
    manager bean? This metric shows the current lag (the difference between the consumer’s
    offset and the broker’s log-end offset) for the partition that is the most behind.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with this is twofold: it only shows the lag for one partition,
    and it relies on proper functioning of the consumer. If you have no other option,
    use this attribute for lag and set up alerting for it. But the best practice is
    to use external lag monitoring, as will be described in [“Lag Monitoring”](#lag_monitor).'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: To know how much message traffic your consumer client is handling, you should
    capture the `bytes-consumed-rate` or the `records-consumed-rate`, or preferably
    both. These metrics describe the message traffic consumed by this client instance
    in bytes per second and messages per second, respectively. Some users set minimum
    thresholds on these metrics for alerting so that they are notified if the consumer
    is not doing enough work. You should be careful when doing this, however. Kafka
    is intended to decouple the consumer and producer clients, allowing them to operate
    independently. The rate at which the consumer is able to consume messages is often
    dependent on whether or not the producer is working correctly, so monitoring these
    metrics on the consumer makes assumptions about the state of the producer. This
    can lead to false alerts on the consumer clients.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: It is also good to understand the relationship among bytes, messages, and requests,
    and the fetch manager provides metrics to help with this. The `fetch-rate` measurement
    tells us the number of fetch requests per second that the consumer is performing.
    The `fetch-size-avg` metric gives the average size of those fetch requests in
    bytes. Finally, the `records-per-request-avg` metric gives us the average number
    of messages in each fetch request. Note that the consumer does not provide an
    equivalent to the producer `record-size-avg` metric to let us know what the average
    size of a message is. If this is important, you will need to infer it from the
    other metrics available or capture it in your application after receiving messages
    from the consumer client library.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Per-broker and per-topic metrics
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The metrics that are provided by the consumer client for each of the broker
    connections and each of the topics being consumed, as with the producer client,
    are useful for debugging issues with consumption, but will probably not be measurements
    that you review daily. As with the fetch manager, the `request-latency-avg` attribute
    provided by the per-broker metrics bean has limited usefulness, depending on the
    message traffic in the topics you are consuming. The `incoming-byte-rate` and
    `request-rate` metrics break down the consumed message metrics provided by the
    fetch manager into per-broker bytes per second and requests per second measurements,
    respectively. These can be used to help isolate problems that the consumer is
    having with the connection to a specific broker.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Per-topic metrics provided by the consumer client are useful if more than one
    topic is being consumed. Otherwise, these metrics will be the same as the fetch
    manager’s metrics and redundant to collect. On the other end of the spectrum,
    if the client is consuming many topics (Kafka MirrorMaker, for example) these
    metrics will be difficult to review. If you plan on collecting them, the most
    important metrics to gather are the `bytes-consumed-rate`, the `records-consumed-rate`,
    and the `fetch-size-avg`. The `bytes-consumed-rate` shows the absolute size in
    bytes consumed per second for the specific topic, while the `records-consumed-rate`
    shows the same information in terms of the number of messages. The `fetch-size-avg`
    provides the average size of each fetch request for the topic in bytes.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Consumer coordinator metrics
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described in [Chapter 4](ch04.html#reading_data_from_kafka), consumer clients
    generally work together as part of a consumer group. This group has coordination
    activities, such as group members joining, and heartbeat messages to the brokers
    to maintain group membership. The consumer coordinator is the part of the consumer
    client that is responsible for handling this work, and it maintains its own set
    of metrics. As with all metrics, there are many numbers provided but only a few
    key ones that you should monitor regularly.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The biggest problem that consumers can run into due to coordinator activities
    is a pause in consumption while the consumer group synchronizes. This is when
    the consumer instances in a group negotiate which partitions will be consumed
    by which individual client instances. Depending on the number of partitions that
    are being consumed, this can take some time. The coordinator provides the metric
    attribute `sync-time-avg`, which is the average amount of time, in milliseconds,
    that the sync activity takes. It is also useful to capture the `sync-rate` attribute,
    which is the number of group syncs that happen every second. For a stable consumer
    group, this number should be zero most of the time.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: The consumer needs to commit offsets to checkpoint its progress in consuming
    messages, either automatically on a regular interval or by manual checkpoints
    triggered in the application code. These commits are essentially just produce
    requests (though they have their own request type), in that the offset commit
    is a message produced to a special topic. The consumer coordinator provides the
    `commit-latency-avg` attribute, which measures the average amount of time that
    offset commits take. You should monitor this value just as you would the request
    latency in the producer. It should be possible to establish a baseline expected
    value for this metric, and set reasonable thresholds for alerting above that value.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: One final coordinator metric that can be useful to collect is `assigned-partitions`.
    This is a count of the number of partitions that the consumer client (as a single
    instance in the consumer group) has been assigned to consume. This is helpful
    because, when compared to this metric from other consumer clients in the group,
    it is possible to see the balance of load across the entire consumer group. We
    can use this to identify imbalances that might be caused by problems in the algorithm
    used by the consumer coordinator for distributing partitions to group members.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Quotas
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Kafka has the ability to throttle client requests in order to prevent
    one client from overwhelming the entire cluster. This is configurable for both
    producer and consumer clients and is expressed in terms of the permitted amount
    of traffic from an individual client ID to an individual broker in bytes per second.
    There is a broker configuration, which sets a default value for all clients, as
    well as per-client overrides that can be dynamically set. When the broker calculates
    that a client has exceeded its quota, it slows the client down by holding the
    response back to the client for enough time to keep the client under the quota.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka broker does not use error codes in the response to indicate that the
    client is being throttled. This means that it is not obvious to the application
    that throttling is happening without monitoring the metrics that are provided
    to show the amount of time that the client is being throttled. The metrics that
    must be monitored are shown in [Table 13-21](#table1016).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-21\. Metrics to monitor
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '| Client | Bean name |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| Consumer | bean `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=CLIENTID`,
    attribute `fetch-throttle-time-avg` |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| Producer | bean `kafka.producer:type=producer-metrics,client-id=CLIENTID`,
    attribute `produce-throttle-time-avg` |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: Quotas are not enabled by default on the Kafka brokers, but it is safe to monitor
    these metrics irrespective of whether or not you are currently using quotas. Monitoring
    them is a good practice as they may be enabled at some point in the future, and
    it’s easier to start with monitoring them as opposed to adding metrics later.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Lag Monitoring
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For Kafka consumers, the most important thing to monitor is the consumer lag.
    Measured in number of messages, this is the difference between the last message
    produced in a specific partition and the last message processed by the consumer.
    While this topic would normally be covered in the previous section on consumer
    client monitoring, it is one of the cases where external monitoring far surpasses
    what is available from the client itself. As mentioned previously, there is a
    lag metric in the consumer client, but using it is problematic. It only represents
    a single partition, the one that has the most lag, so it does not accurately show
    how far behind the consumer is. In addition, it requires proper operation of the
    consumer, because the metric is calculated by the consumer on each fetch request.
    If the consumer is broken or offline, the metric is either inaccurate or not available.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: The preferred method of consumer lag monitoring is to have an external process
    that can watch both the state of the partition on the broker, tracking the offset
    of the most recently produced message, and the state of the consumer, tracking
    the last offset the consumer group has committed for the partition. This provides
    an objective view that can be updated regardless of the status of the consumer
    itself. This checking must be performed for every partition that the consumer
    group consumes. For a large consumer, like MirrorMaker, this may mean tens of
    thousands of partitions.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 12](ch12.html#administering_kafka) provided information on using the
    command-line utilities to get consumer group information, including committed
    offsets and lag. Monitoring lag like this, however, presents its own problems.
    First, you must understand for each partition what is a reasonable amount of lag.
    A topic that receives 100 messages an hour will need a different threshold than
    a topic that receives 100,000 messages per second. Then, you must be able to consume
    all of the lag metrics into a monitoring system and set alerts on them. If you
    have a consumer group that consumes 100,000 partitions over 1,500 topics, you
    may find this to be a daunting task.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: One way to monitor consumer groups reduce this complexity is to use [Burrow](https://oreil.ly/supY1).
    This is an open source application, originally developed by LinkedIn, that provides
    consumer status monitoring by gathering lag information for all consumer groups
    in a cluster and calculating a single status for each group saying whether the
    consumer group is working properly, falling behind, or is stalled or stopped entirely.
    It does this without requiring thresholds by monitoring the progress that the
    consumer group is making on processing messages, though you can also get the message
    lag as an absolute number. There is an in-depth discussion of the reasoning and
    methodology behind how Burrow works on the [LinkedIn Engineering blog](http://bit.ly/2sanKZb).
    Deploying Burrow can be an easy way to provide monitoring for all consumers in
    a cluster, as well as in multiple clusters, and it can be easily integrated with
    your existing monitoring and alerting system.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: If there is no other option, the `records-lag-max` metric from the consumer
    client will provide at least a partial view of the consumer status. It is strongly
    suggested, however, that you utilize an external monitoring system like Burrow.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Monitoring
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another type of external monitoring that is recommended to determine if your
    Kafka clusters are working properly is an end-to-end monitoring system that provides
    a client point of view on the health of the Kafka cluster. Consumer and producer
    clients have metrics that can indicate that there might be a problem with the
    Kafka cluster, but this can be a guessing game as to whether increased latency
    is due to a problem with the client, the network, or Kafka itself. In addition,
    it means that if you are responsible for running the Kafka cluster, and not the
    clients, you would now have to monitor all of the clients as well. What you really
    need to know is:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Can I produce messages to the Kafka cluster?
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I consume messages from the Kafka cluster?
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an ideal world, you would be able to monitor this for every topic individually.
    However, in most situations it is not reasonable to inject synthetic traffic into
    every topic in order to do this. We can, however, at least provide those answers
    for every broker in the cluster, and that is what [Xinfra Monitor (formerly known
    as Kafka Monitor) does](https://oreil.ly/QqXD9). This tool, open sourced by the
    Kafka team at LinkedIn, continually produces and consumes data from a topic that
    is spread across all brokers in a cluster. It measures the availability of both
    produce and consume requests on each broker, as well as the total produce to consume
    latency. This type of monitoring is invaluable to be able to externally verify
    that the Kafka cluster is operating as intended, since just like consumer lag
    monitoring, the Kafka broker cannot report whether or not clients are able to
    use the cluster properly.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring is a key aspect of running Apache Kafka properly, which explains
    why so many teams spend a significant amount of their time perfecting that part
    of operations. Many organizations use Kafka to handle petabyte-scale data flows.
    Assuring that the data does not stop, and that messages are not lost, this is
    a critical business requirement. It is also our responsibility to assist users
    with monitoring how their applications use Kafka by providing the metrics that
    they need to do this.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we covered the basics of how to monitor Java applications, and
    specifically the Kafka applications. We reviewed a subset of the numerous metrics
    available in the Kafka broker, also touching on Java and OS monitoring, as well
    as logging. We then detailed the monitoring available in the Kafka client libraries,
    including quota monitoring. Finally, we discussed the use of external monitoring
    systems for consumer lag monitoring and end-to-end cluster availability. While
    certainly not an exhaustive list of the metrics that are available, this chapter
    reviewed the most critical ones to keep an eye on.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL

- en: Chapter 13\. Monitoring Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。监控Kafka
- en: The Apache Kafka applications have numerous measurements for their operation—so
    many, in fact, that it can easily become confusing as to what is important to
    watch and what can be set aside. These range from simple metrics about the overall
    rate of traffic, to detailed timing metrics for every request type, to per-topic
    and per-partition metrics. They provide a detailed view into every operation in
    the broker, but they can also make you the bane of whoever is responsible for
    managing your monitoring system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka应用程序有许多用于其操作的测量值，事实上有很多，以至于很容易变得令人困惑，不知道要观察什么是重要的，什么可以搁置。这些范围从关于流量总体速率的简单指标，到每种请求类型的详细定时指标，再到每个主题和每个分区的指标。它们提供了对代理中每个操作的详细视图，但也可能使您成为负责管理监控系统的人的梦魇。
- en: This chapter will detail the most critical metrics to monitor all the time and
    how to respond to them. We’ll also describe some of the more important metrics
    to have on hand when debugging problems. This is not an exhaustive list of the
    metrics that are available, however, because the list changes frequently, and
    many will only be informative to a hard-core Kafka developer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍始终监控的最关键指标以及如何对其做出响应。我们还将描述在调试问题时手头上最重要的一些指标。然而，这不是一个详尽的可用指标列表，因为列表经常变化，许多指标只对硬核Kafka开发人员有用。
- en: Metric Basics
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标基础知识
- en: Before getting into the specific metrics provided by the Kafka broker and clients,
    let’s discuss the basics of how to monitor Java applications and some best practices
    around monitoring and alerting. This will provide a basis for understanding how
    to monitor the applications and why the specific metrics described later in this
    chapter have been chosen as the most important.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入由Kafka代理和客户端提供的具体指标之前，让我们讨论一下如何监控Java应用程序的基础知识以及一些关于监控和警报的最佳实践。这将为理解如何监控应用程序以及为什么后面描述的特定指标被选择为最重要的提供基础。
- en: Where Are the Metrics?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标在哪里？
- en: All of the metrics exposed by Kafka can be accessed via the Java Management
    Extensions (JMX) interface. The easiest way to use them in an external monitoring
    system is to use a collection agent provided by your monitoring system and attach
    it to the Kafka process. This may be a separate process that runs on the system
    and connects to the JMX interface, such as with the Nagios XI `check_jmx` plug-in
    or `jmxtrans`. You can also utilize a JMX agent that runs directly in the Kafka
    process to access metrics via an HTTP connection, such as Jolokia or MX4J.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka公开的所有指标都可以通过Java管理扩展（JMX）接口访问。在外部监控系统中使用它们的最简单方法是使用监控系统提供的收集代理，并将其附加到Kafka进程上。这可能是在系统上运行并连接到JMX接口的单独进程，例如Nagios
    XI的`check_jmx`插件或`jmxtrans`。您还可以利用直接在Kafka进程中运行的JMX代理通过HTTP连接访问指标，例如Jolokia或MX4J。
- en: An in-depth discussion of how to set up monitoring agents is outside the scope
    of this chapter, and there are far too many choices to do justice to all of them.
    If your organization does not currently have experience with monitoring Java applications,
    it may be worthwhile to instead consider monitoring as a service. There are many
    companies that offer monitoring agents, metrics collection points, storage, graphing,
    and alerting in a services package. They can assist you further with setting up
    the monitoring agents required.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如何设置监控代理的深入讨论超出了本章的范围，而且有太多选择，无法公平地对所有选择进行公正。如果您的组织目前没有监控Java应用程序的经验，可能值得考虑监控作为一项服务。有许多公司提供监控代理、指标收集点、存储、绘图和警报的服务包。他们可以帮助您进一步设置所需的监控代理。
- en: Finding the JMX Port
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找JMX端口
- en: To aid with configuring applications that connect to JMX on the Kafka broker
    directly, such as monitoring systems, the broker sets the configured JMX port
    in the broker information that is stored in ZooKeeper. The `/brokers/ids/<ID>`
    znode contains JSON-formatted data for the broker, including `hostname` and `jmx_port`
    keys. However, it should be noted that remote JMX is disabled by default in Kafka
    for security reasons. If you are going to enable it, you must properly configure
    security for the port. This is because JMX not only allows a view into the state
    of the application, it also allows code execution. It is highly recommended that
    you use a JMX metrics agent that is loaded into the application.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助配置直接连接到Kafka代理的应用程序（如监控系统），代理在存储在ZooKeeper中的代理信息中设置了配置的JMX端口。`/brokers/ids/<ID>`
    znode包含代理的JSON格式数据，包括`hostname`和`jmx_port`键。但是，应该注意的是，出于安全原因，Kafka默认情况下禁用了远程JMX。如果您要启用它，必须正确配置端口的安全性。这是因为JMX不仅允许查看应用程序的状态，还允许执行代码。强烈建议您使用加载到应用程序中的JMX指标代理。
- en: Nonapplication metrics
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非应用程序指标
- en: Not all metrics will come from Kafka itself. There are five general groupings
    of where you can get your metrics from. [Table 13-1](#table1001) describes the
    categories when we are monitoring the Kafka brokers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有指标都来自Kafka本身。您可以从五个一般分组中获取指标。[表13-1](#table1001)描述了我们在监控Kafka代理时的类别。
- en: Table 13-1\. Metric sources
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-1。指标来源
- en: '| Category | Description |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|类别|描述|'
- en: '| --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Application metrics | These are the metrics you get from Kafka itself, from
    the JMX interface. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|应用程序指标|这些是您从Kafka本身获取的指标，来自JMX接口。|'
- en: '| Logs | Another type of monitoring data that comes from Kafka itself. Because
    it is some form of text or structured data, and not just a number, it requires
    a little more processing. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|日志|来自Kafka本身的另一种监控数据类型。因为它是某种形式的文本或结构化数据，而不仅仅是一个数字，所以需要更多的处理。|'
- en: '| Infrastructure metrics | These metrics come from systems that you have in
    front of Kafka but are still within the request path and under your control. An
    example is a load balancer. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|基础设施指标|这些指标来自于您在Kafka前面的系统，但仍然在请求路径内并在您的控制范围内。一个例子是负载均衡器。|'
- en: '| Synthetic clients | This is data from tools that are external to your Kafka
    deployment, just like a client, but are under your direct control and are typically
    not performing the same work as your clients. An external monitor like Kafka Monitor
    falls in this category. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 合成客户端 | 这是来自与您的Kafka部署外部工具的数据，就像客户端一样，但在您的直接控制下，通常不执行与您的客户端相同的工作。像Kafka Monitor这样的外部监视器属于这一类别。
    |'
- en: '| Client metrics | These are metrics that are exposed by the Kafka clients
    that connect to your cluster. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 客户端指标 | 这些是由连接到您的集群的Kafka客户端公开的指标。 |'
- en: Logs generated by Kafka are discussed later in this chapter, as are client metrics.
    We will also touch very briefly on synthetic metrics. Infrastructure metrics,
    however, are dependent on your specific environment and are outside the scope
    of the discussion here. The further along in your Kafka journey you are, the more
    important these metric sources will be to fully understanding how your applications
    are running, as the lower in the list, the more objective a view of Kafka they
    provide. For example, relying on metrics from your brokers will suffice at the
    start, but later on you will want a more objective view of how they are performing.
    A familiar example for the value of objective measurements is monitoring the health
    of a website. The web server is running properly, and all of the metrics it is
    reporting say that it is working. However, there is a problem with the network
    between your web server and your external users, which means that none of your
    users can reach the web server. A synthetic client that is running outside your
    network and checks the accessibility of the website would detect this and alert
    you to the situation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka生成的日志将在本章后面讨论，客户端指标也是如此。我们还将简要涉及合成指标。然而，基础设施指标取决于您的特定环境，并且超出了这里讨论的范围。在您的Kafka旅程中越深入，这些指标来源对于充分了解应用程序的运行方式就越重要，因为在列表中越靠后，它们提供的对Kafka的客观视图就越多。例如，在开始阶段依赖经纪人的指标就足够了，但以后您会希望更客观地了解它们的表现。客观测量价值的一个熟悉例子是监控网站的健康状况。Web服务器正常运行，并且它报告的所有指标都表明它正在工作。然而，您的Web服务器和外部用户之间的网络存在问题，这意味着您的用户无法访问Web服务器。在您的网络之外运行的合成客户端将检测到这一情况并向您发出警报。
- en: What Metrics Do I Need?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我需要哪些指标？
- en: The specific metrics that are important to you is a question that is nearly
    as loaded as what the best editor to use is. It will depend significantly on what
    you intend to do with them, what tools you have available for collecting data,
    how far along in using Kafka you are, and how much time you have available to
    spend on building infrastructure around Kafka. A broker internals developer will
    have far different needs than a site reliability engineer who is running a Kafka
    deployment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对您重要的具体指标几乎与您要使用的最佳编辑器一样重要。这将大大取决于您打算如何使用它们，您有哪些可用于收集数据的工具，您在使用Kafka方面的进展如何，以及您有多少时间可用于围绕Kafka构建基础设施。一个经纪人内部开发人员的需求将远远不同于运行Kafka部署的站点可靠性工程师的需求。
- en: Alerting or debugging?
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警报还是调试？
- en: The first question you should ask yourself is whether or not your primary goal
    is to alert you when there is a problem with Kafka, or to debug problems that
    happen. The answer will usually involve a little of both, but knowing whether
    a metric is for one or the other will allow you to treat it differently once it
    is collected.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该问自己的第一个问题是，您的主要目标是在Kafka出现问题时警报您，还是调试出现的问题。答案通常会涉及两者，但知道一个指标是用于哪个目的将使您在收集后对其进行不同处理。
- en: A metric that is destined for alerting is useful for a very short period of
    time—typically, not much longer than the amount of time it takes to respond to
    a problem. You can measure this on the order of hours, or maybe days. These metrics
    will be consumed by automation that responds to known problems for you, as well
    as the human operators in cases where automation does not exist yet. It is usually
    important for these metrics to be more objective, as a problem that does not impact
    clients is far less critical than one that does.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于警报的指标在很短的时间内非常有用——通常不会超过解决问题所需的时间。您可以测量几个小时，或者可能几天。这些指标将被自动化消耗，自动化将为您响应已知问题，以及在自动化尚不存在的情况下由人工操作员消耗。这些指标通常更为客观，因为不影响客户端的问题远不及影响客户端的问题严重。
- en: Data that is primarily for debugging has a longer time horizon because you are
    frequently diagnosing problems that have existed for some time, or taking a deeper
    look at a more complex problem. This data will need to remain available for days
    or weeks past when it is collected. It is also usually going to be more subjective
    measurements, or data from the Kafka application itself. Keep in mind that it
    is not always necessary to collect this data into a monitoring system. If the
    metrics are used for debugging problems in place, it is sufficient that the metrics
    are available when needed. You do not need to overwhelm the monitoring system
    by collecting tens of thousands of values on an ongoing basis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 主要用于调试的数据具有更长的时间范围，因为您经常诊断已经存在一段时间的问题，或者深入研究更复杂的问题。这些数据将需要在收集后的几天或几周内保持可用。通常还会是更主观的测量，或者来自Kafka应用程序本身的数据。请记住，不一定需要将这些数据收集到监控系统中。如果指标用于现场调试问题，则在需要时可用即可。您无需通过持续收集成千上万个值来压倒监控系统。
- en: Historical Metrics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史指标
- en: There is a third type of data that you will need eventually, and that is historical
    data on your application. The most common use for historical data is for capacity
    management purposes, and so it includes information about resources used, including
    compute resources, storage, and network. These metrics will need to be stored
    for a very long period of time, measured in years. You also may need to collect
    additional metadata to put the metrics into context, such as when brokers were
    added to or removed from the cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，您还将需要应用程序的历史数据。历史数据最常见的用途是用于容量管理，因此包括有关使用的资源的信息，包括计算资源、存储和网络。这些指标需要长时间存储，以年为单位。您还可能需要收集额外的元数据来将指标放入上下文中，例如代理何时添加到集群或从集群中删除。
- en: Automation or humans?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化还是人类？
- en: 'Another question to consider is who the consumer of the metrics will be. If
    the metrics are consumed by automation, they should be very specific. It’s OK
    to have a large number of metrics, each describing small details, because this
    is why computers exist: to process a lot of data. The more specific the data is,
    the easier it is to create automation that acts on it, because the data does not
    leave as much room for interpretation as to its meaning. On the other hand, if
    the metrics will be consumed by humans, presenting a large number of metrics will
    be overwhelming. This becomes even more important when defining alerts based on
    those measurements. It is far too easy to succumb to “alert fatigue,” where there
    are so many alerts going off that it is difficult to know how severe the problem
    is. It is also hard to properly define thresholds for every metric and keep them
    up-to-date. When the alerts are overwhelming or often incorrect, we begin to not
    trust that the alerts are correctly describing the state of our applications.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要考虑的一个问题是指标的使用者是谁。如果指标由自动化程序使用，它们应该非常具体。拥有大量描述细节的指标是可以接受的，因为这正是计算机存在的原因：处理大量数据。数据越具体，就越容易创建基于其操作的自动化程序，因为数据不会留下太多关于其含义的解释空间。另一方面，如果指标将由人类使用，呈现大量指标将会令人不知所措。在基于这些测量值定义警报时，这变得更加重要。很容易陷入“警报疲劳”，因为有太多警报响起，很难知道问题有多严重。正确定义每个指标的阈值并使其保持最新也很困难。当警报过多或经常不正确时，我们开始不相信警报是否正确描述了我们应用程序的状态。
- en: Think about the operations of a car. To properly adjust the ratio of air to
    fuel while the car is running, the computer needs a number of measurements of
    air density, fuel, exhaust, and other minutiae about the operation of the engine.
    These measurements would be overwhelming to the human operator of the vehicle,
    however. Instead, we have a “Check Engine” light. A single indicator tells you
    that there is a problem, and there is a way to find out more detailed information
    to tell you exactly what the problem is. Throughout this chapter, we will identify
    the metrics that will provide the highest amount of coverage to keep your alerting
    simple.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 想想汽车的运行。为了在汽车运行时正确调整空气与燃料的比例，计算机需要对空气密度、燃料、排气和发动机运行等细微之处进行多次测量。然而，这些测量对车辆的人类操作者来说将是不堪重负的。相反，我们有一个“发动机故障”指示灯。一个指示器告诉您有问题，并且有一种方法可以获取更详细的信息，告诉您问题的确切所在。在本章中，我们将确定提供最高覆盖率的指标，以保持您的警报简单。
- en: Application Health Checks
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序健康检查
- en: 'No matter how you collect metrics from Kafka, you should make sure that you
    have a way to also monitor the overall health of the application process via a
    simple health check. This can be done in two ways:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您如何从Kafka收集指标，都应确保有一种方法来通过简单的健康检查监控应用程序进程的整体健康状况。这可以通过两种方式实现：
- en: An external process that reports whether the broker is up or down (health check)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告代理是否正常运行的外部过程（健康检查）
- en: Alerting on the lack of metrics being reported by the Kafka broker (sometimes
    called *stale metrics*)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对Kafka代理报告的指标缺失进行警报（有时称为*陈旧指标*）
- en: Though the second method works, it can make it difficult to differentiate between
    a failure of the Kafka broker and a failure of the monitoring system itself.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第二种方法有效，但它可能会使难以区分Kafka代理的故障和监控系统本身的故障。
- en: For the Kafka broker, this can simply be connecting to the external port (the
    same port that clients use to connect to the broker) to check that it responds.
    For client applications, it can be more complex, ranging from a simple check of
    whether the process is running, to an internal method that determines application
    health.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kafka代理，这可以简单地连接到外部端口（客户端用于连接代理的相同端口）以检查其响应。对于客户端应用程序，可能会更复杂，从简单检查进程是否正在运行，到确定应用程序健康状况的内部方法。
- en: Service-Level Objectives
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务级目标
- en: 'One area of monitoring that is especially critical for infrastructure services,
    such as Kafka, is that of service-level objectives, or SLOs. This is how we communicate
    to our clients what level of service they can expect from the infrastructure service.
    The clients want to be able to treat services like Kafka as an opaque system:
    they do not want or need to understand the internals of how it works—only the
    interface that they are using and knowing it will do what they need it to do.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的一个特别关键的领域是基础设施服务，比如Kafka，其中的服务级目标或SLO。这是我们向客户传达基础设施服务可以提供的服务水平。客户希望能够将Kafka等服务视为不透明系统：他们不希望也不需要了解其内部工作原理，只需要了解他们正在使用的接口，并知道它将按照他们的需求进行操作。
- en: Service-Level Definitions
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务级定义
- en: Before discussing SLOs in Kafka, there must be agreement on the terminology
    that is used. Frequently, you will hear engineers, managers, executives, and everyone
    else use terms in the “service-level” space incorrectly, which leads to confusion
    about what is actually being talked about.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论Kafka中的SLO之前，必须就所使用的术语达成一致。经常会听到工程师、经理、高管和其他人在“服务级”领域错误地使用术语，这导致对实际讨论的内容产生困惑。
- en: A *service-level indicator* (SLI) is a metric that describes one aspect of a
    service’s reliability. It should be closely aligned with your client’s experience,
    so it is usually true that the more objective these measurements are, the better
    they are. In a request processing system, such as Kafka, it is usually best to
    express these measurements as a ratio between the number of good events and the
    total number of events—for example, the proportion of requests to a web server
    that return a 2xx, 3xx, or 4xx response.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务级指标*（SLI）是描述服务可靠性的指标。它应该与客户的体验密切相关，因此通常情况下，这些测量越客观，它们就越好。在请求处理系统（如Kafka）中，通常最好将这些测量表达为良好事件数量与总事件数量之间的比率，例如，返回2xx、3xx或4xx响应的网页服务器请求的比例。'
- en: A *service-level objective* (SLO), which can also be called a *service-level
    threshold* (SLT), combines an SLI with a target value. A common way to express
    the target is by the number of nines (99.9% is “three nines”), though it is by
    no means required. The SLO should also include a time frame that it is measured
    over, frequently on the scale of days. For example, 99% of requests to the web
    server must return a 2xx, 3xx, or 4xx response over 7 days.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务级目标*（SLO），也可以称为*服务级阈值*（SLT），将SLI与目标值结合起来。表达目标的常见方式是通过数量的nines（99.9%是“三个nines”），尽管这并不是必需的。SLO还应包括在其上进行测量的时间范围，通常在天的时间尺度上。例如，在7天内，网页服务器的请求中必须返回2xx、3xx或4xx响应的99%。'
- en: A *service-level agreement* (SLA) is a contract between a service provider and
    a client. It usually includes several SLOs, as well as details about how they
    are measured and reported, how the client seeks support from the service provider,
    and penalties that the service provider will be subject to if they are not performing
    within the SLA. For example, an SLA for the preceding SLO might state that if
    the service provider is not operating within the SLO, they will refund all fees
    paid by the client for the time period that the service was not within the SLO.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务级协议*（SLA）是服务提供商和客户之间的合同。它通常包括几个SLO，以及有关如何测量和报告它们、客户如何从服务提供商那里寻求支持以及服务提供商如果未能在SLA范围内执行将受到的处罚的详细信息。例如，前述SLO的SLA可能规定，如果服务提供商未能在SLO范围内运营，他们将退还客户支付的服务期间的所有费用。'
- en: Operational-Level Agreement
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运营级别协议
- en: The term *operational-level agreement* (OLA) is less frequently used. It describes
    agreements between multiple internal services or support providers in the overall
    delivery of an SLA. The goal is to assure that the multiple activities that are
    necessary to fulfill the SLA are properly described and accounted for in the day-to-day
    operations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*运营级别协议*（OLA）这个术语使用得较少。它描述了在SLA的整体交付中多个内部服务或支持提供者之间的协议。目标是确保履行SLA所必需的多个活动在日常运营中得到适当描述和核算。'
- en: It is very common to hear people talk about SLAs when they really mean SLOs.
    While those who are providing a service to paying clients may have SLAs with those
    clients, it is rare that the engineers running the applications are responsible
    for anything more than the performance of that service within the SLOs. In addition,
    those who only have internal clients (i.e., are running Kafka as internal data
    infrastructure for a much larger service) generally do not have SLAs with those
    internal customers. This should not prevent you from setting and communicating
    SLOs, however, as doing that will lead to fewer assumptions by customers as to
    how they think Kafka should be performing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常谈论SLA时实际上是指SLO。虽然向付费客户提供服务的人可能与这些客户有SLA，但负责运行应用程序的工程师很少负责超出SLO范围的任何事情。此外，只有内部客户（即为更大的服务运行Kafka作为内部数据基础设施的人）通常不与这些内部客户有SLA。然而，这不应阻止您设定和传达SLO，因为这样做将减少客户对他们认为Kafka应该表现如何的假设。
- en: What Metrics Make Good SLIs?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么指标可以成为良好的SLI？
- en: In general, the metrics for your SLIs should be gathered using something external
    to the Kafka brokers. The reason for this is that SLOs should describe whether
    or not the typical user of your service is happy, and you can’t measure that subjectively.
    Your clients do not care if you think your service is running correctly; it is
    their experience (in aggregate) that matters. This means that infrastructure metrics
    are OK, synthetic clients are good, and client-side metrics are probably the best
    for most of your SLIs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，SLI的指标应该使用Kafka经纪人之外的东西进行收集。原因是SLO应该描述您的服务的典型用户是否满意，而您无法主观地衡量这一点。您的客户不在乎您是否认为您的服务正在正确运行；重要的是他们的体验（总体而言）。这意味着基础设施指标是可以的，合成客户端是好的，而客户端指标对于大多数SLI来说可能是最好的。
- en: While by no means an exhaustive list, the most common SLIs that are used in
    request/response and data storage systems are in [Table 13-2](#table1002).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这并不是一个详尽的列表，但在请求/响应和数据存储系统中使用的最常见的SLI在[表13-2](#table1002)中。
- en: Customers Always Want More
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户总是想要更多
- en: There are some SLOs that your customers may be interested in that are important
    to them but not within your control. For example, they may be concerned about
    the correctness or freshness of the data produced to Kafka. Do not agree to support
    SLOs that you are not responsible for, as that will only lead to taking on work
    that dilutes the core job of keeping Kafka running properly. Make sure to connect
    them with the proper group to set up understanding, and agreements, around these
    additional requirements.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些SLO可能会引起客户的兴趣，但这些SLO对他们很重要，但不在你的控制范围内。例如，他们可能会关心Kafka生成的数据的正确性或新鲜度。不要同意支持你不负责的SLO，因为这只会导致承担削弱保持Kafka正常运行的核心工作的工作。确保将他们与适当的团队联系起来，以建立对这些额外要求的理解和协议。
- en: Table 13-2\. Types of SLIs
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-2\. SLI的类型
- en: '| Availability | Is the client able to make a request and get a response? |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 可用性 | 客户能否发出请求并获得响应？ |'
- en: '| --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Latency | How quickly is the response returned? |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 延迟 | 响应返回的速度有多快？ |'
- en: '| Quality | Does the response include a proper response? |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 质量 | 响应是否包含适当的响应？ |'
- en: '| Security | Are the request and response appropriately protected, whether
    that is authorization or encryption? |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 安全性 | 请求和响应是否得到适当的保护，无论是授权还是加密？ |'
- en: '| Throughput | Can the client get enough data, fast enough? |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 吞吐量 | 客户端是否能够快速获取足够的数据？ |'
- en: Keep in mind that it is usually better for your SLIs to be based on a counter
    of events that fall inside the thresholds of the SLO. This means that ideally,
    each event would be individually checked to see if it meets the threshold of the
    SLO. This rules out quantile metrics as good SLIs, as those will only tell you
    that 90% of your events were below a given value without allowing you to control
    what that value is. However, aggregating values into buckets (e.g., “less than
    10 ms,” “10–50 ms,” “50–100 ms,” etc.) can be useful when working with SLOs, especially
    when you are not yet sure what a good threshold is. This will give you a view
    into the distribution of the events within the range of the SLO, and you can configure
    the buckets so that the boundaries are reasonable values for the SLO threshold.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，通常最好让您的SLI基于落在SLO阈值内的事件计数。这意味着理想情况下，应逐个检查每个事件，以查看它是否满足SLO的阈值。这排除了分位数指标作为良好SLI的可能性，因为这些指标只会告诉您90%的事件低于给定值，而不允许您控制该值是多少。然而，将值聚合到桶中（例如，“小于10毫秒”，“10-50毫秒”，“50-100毫秒”等）在处理SLO时可能很有用，特别是当您还不确定良好阈值是什么时。这将使您了解SLO范围内事件的分布，并且您可以配置桶，使边界值成为SLO阈值的合理值。
- en: Using SLOs in Alerting
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用警报中的SLO
- en: In short, SLOs should inform your primary alerts. The reason for this is that
    the SLOs describe problems from your customers’ point of view, and those are the
    ones that you should be concerned about first. Generally speaking, if a problem
    does not impact your clients, it does not need to wake you up at night. SLOs will
    also tell you about the problems that you don’t know how to detect because you’ve
    never seen them before. They won’t tell you what those problems are, but they
    will tell you that they exist.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，SLO应该为您的主要警报提供信息。原因是SLO从客户的角度描述了问题，这些问题应该是您首先关心的问题。一般来说，如果问题不影响您的客户，那么它就不需要在夜间叫醒您。SLO还将告诉您有关您不知道如何检测的问题，因为您以前从未见过它们。它们不会告诉您这些问题是什么，但它们会告诉您这些问题存在。
- en: The challenge is that it’s very difficult to use an SLO directly as an alert.
    SLOs are best for long timescales, such as a week, as we want to report them to
    management and customers in a way that can be consumed. In addition, by the time
    the SLO alert fires, it’s too late—you’re already operating outside of the SLO.
    Some will use a derivative value to provide an early warning, but the best way
    to approach using SLOs for alerting is to observe the rate at which you are burning
    through your SLO over its timeframe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于直接将SLO用作警报非常困难。SLO最适合长时间尺度，例如一周，因为我们希望以可消化的方式向管理层和客户报告它们。此外，当SLO警报触发时，为时已晚
    - 您已经在SLO范围之外运行。有些人会使用导数值提供预警，但使用SLO进行警报的最佳方法是观察您在其时间范围内通过SLO的速率。
- en: As an example, let’s assume that your Kafka cluster receives one million requests
    per week, and you have an SLO defined that states that 99.9% of requests must
    send out the first byte of response within 10 ms. This means that over the week,
    you can have up to one thousand requests that respond slower than this and everything
    will still be OK. Normally, you see one request like this every hour, which is
    about 168 bad requests a week, measured from Sunday to Saturday. You have a metric
    that shows this as the SLO burn rate, and one request an hour at one million requests
    a week is a burn rate of 0.1% per hour.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您的Kafka集群每周接收一百万个请求，并且您定义了一个SLO，规定99.9％的请求必须在10毫秒内发送第一个响应字节。这意味着在一周内，您最多可以有一千个请求的响应速度慢于此，而一切仍将正常。通常，您每小时会看到一个这样的请求，这大约是每周168个不良请求，从周日到周六进行测量。您有一个指标显示这是SLO燃烧速率，每小时一个请求在一百万个请求每周中是0.1％的燃烧速率。
- en: On Tuesday at 10 a.m., your metric changes and now shows that the burn rate
    is 0.4% per hour. This isn’t great, but it’s still not a problem because you’ll
    be well within the SLO by the end of the week. You open a ticket to take a look
    at the problem but go back to some higher-priority work. On Wednesday at 2 p.m.,
    the burn rate jumps to 2% per hour and your alerts go off. You know that at this
    rate, you’ll breach the SLO by lunchtime on Friday. Dropping everything, you diagnose
    the problem, and after about 4 hours you have the burn rate back down to 0.4%
    per hour, and it stays there for the rest of the week. By using the burn rate,
    you were able to avoid breaching the SLO for the week.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在周二上午10点，您的指标发生变化，现在显示燃烧速率为每小时0.4％。这不是很好，但这还不是问题，因为到本周末时，您将远远在SLO范围内。您打开了一个工单来查看问题，但又回到了一些更高优先级的工作。在周三下午2点，燃烧速率跳升到每小时2％，您的警报响了。您知道以这个速度，您将在周五中午之前违反SLO。放下一切，您诊断了问题，大约4个小时后，您将燃烧速率降至每小时0.4％，并且一直保持在这个水平。通过使用燃烧速率，您成功避免了本周违反SLO。
- en: For more information on utilizing SLOs and the burn rate for alerting, you will
    find that [*Site Reliability Engineering*](https://oreil.ly/bPBxC) and [*The Site
    Reliability Workbook*](https://oreil.ly/qSmOc), both edited by Betsy Beyer et
    al. (O’Reilly), are excellent resources.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有关利用SLO和燃烧速率进行警报的更多信息，您会发现[*Site Reliability Engineering*](https://oreil.ly/bPBxC)和[*The
    Site Reliability Workbook*](https://oreil.ly/qSmOc)是优秀的资源，两者均由Betsy Beyer等人编辑（O'Reilly）。
- en: Kafka Broker Metrics
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Broker Metrics
- en: There are many Kafka broker metrics. Many of them are low-level measurements,
    added by developers when investigating a specific issue or in anticipation of
    needing information for debugging purposes later. There are metrics providing
    information about nearly every function within the broker, but the most common
    ones provide the information needed to run Kafka on a daily basis.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多Kafka经纪人指标。其中许多是低级测量，由开发人员在调查特定问题或预期以后需要调试信息时添加的。这些指标提供有关经纪人内几乎每个功能的信息，但最常见的指标提供了日常运行Kafka所需的信息。
- en: Who Watches the Watchers?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁来监视监视者？
- en: Many organizations use Kafka for collecting application metrics, system metrics,
    and logs for consumption by a central monitoring system. This is an excellent
    way to decouple the applications from the monitoring system, but it presents a
    specific concern for Kafka itself. If you use this same system for monitoring
    Kafka itself, it is very likely that you will never know when Kafka is broken
    because the data flow for your monitoring system will be broken as well.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织使用Kafka收集应用程序指标、系统指标和日志，供中央监控系统使用。这是将应用程序与监控系统解耦的绝佳方式，但对于Kafka本身来说，也提出了特定的问题。如果您使用相同的系统来监视Kafka本身，很可能您永远不会知道Kafka何时出现故障，因为监控系统的数据流也会中断。
- en: There are many ways that this can be addressed. One way is to use a separate
    monitoring system for Kafka that does not have a dependency on Kafka. Another
    way, if you have multiple datacenters, is to make sure that the metrics for the
    Kafka cluster in datacenter A are produced to datacenter B, and vice versa. However
    you decide to handle it, make sure that the monitoring and alerting for Kafka
    does not depend on Kafka working.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以解决这个问题。一种方法是为Kafka使用一个不依赖于Kafka的单独监控系统。另一种方法是，如果您有多个数据中心，要确保数据中心A的Kafka集群的指标被生成到数据中心B，反之亦然。无论您决定如何处理，都要确保Kafka的监控和警报不依赖于Kafka的正常工作。
- en: In this section, we’ll start by discussing the high-level workflow for diagnosing
    problems with your Kafka cluster, referencing the metrics that are useful. Those,
    and other metrics, are described in more detail later in the chapter. This is
    by no means an exhaustive list of broker metrics, but rather several “must have”
    metrics for checking on the health of the broker and the cluster. We’ll wrap up
    with a discussion on logging before moving on to client metrics.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论诊断Kafka集群问题的高级工作流程，参考有用的指标。这些指标和其他指标将在本章后面更详细地描述。这绝不是经纪人指标的详尽清单，而是检查经纪人和集群健康状况的几个“必备”指标。最后，我们将讨论日志记录，然后转向客户端指标。
- en: Diagnosing Cluster Problems
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 诊断集群问题
- en: 'When it comes to problems with a Kafka cluster, there are three major categories:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及Kafka集群的问题时，有三个主要类别：
- en: Single-broker problems
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单经纪人问题
- en: Overloaded clusters
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超载的集群
- en: Controller problems
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器问题
- en: Issues with individual brokers are, by far, the easiest to diagnose and respond
    to. These will show up as outliers in the metrics for the cluster and are frequently
    related to slow or failing storage devices or compute restraints from other applications
    on the system. To detect them, make sure you are monitoring the availability of
    the individual servers, as well as the status of the storage devices, utilizing
    the operating system (OS) metrics.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个经纪人的问题相比，诊断和响应集群问题要容易得多。这些问题将显示为集群指标中的异常值，并且通常与存储设备的缓慢或故障，或系统中其他应用程序的计算限制有关。要检测它们，确保您正在监视单个服务器的可用性，以及存储设备的状态，利用操作系统（OS）指标。
- en: Absent a problem identified at the OS or hardware level, however, the cause
    is almost always an imbalance in the load of the Kafka cluster. While Kafka attempts
    to keep the data within the cluster evenly spread across all brokers, this does
    not mean that client access to that data is evenly distributed. It also does not
    detect issues such as hot partitions. It is highly recommended that you utilize
    an external tool for keeping the cluster balanced at all times. One such tool
    is [Cruise Control](https://oreil.ly/rLybu), an application that continually monitors
    the cluster and rebalances partitions within it. It also provides a number of
    other administrative functions, such as adding and removing brokers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在操作系统或硬件级别没有发现问题的情况下，问题几乎总是Kafka集群负载不平衡。虽然Kafka试图使集群中的数据均匀分布在所有经纪人之间，但这并不意味着客户端对该数据的访问是均匀分布的。它也无法检测到热分区等问题。强烈建议您始终使用外部工具来保持集群的平衡。其中一个工具是[Cruise
    Control](https://oreil.ly/rLybu)，这是一个不断监视集群并在其中重新平衡分区的应用程序。它还提供许多其他管理功能，例如添加和删除经纪人。
- en: Preferred Replica Elections
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首选副本选举
- en: The first step before trying to diagnose a problem further is to ensure that
    you have run a preferred replica election (see [Chapter 12](ch12.html#administering_kafka))
    recently. Kafka brokers do not automatically take partition leadership back (unless
    auto leader rebalance is enabled) after they have released leadership (e.g., when
    the broker has failed or been shut down). This means that it’s very easy for leader
    replicas to become unbalanced in a cluster. The preferred replica election is
    safe and easy to run, so it’s a good idea to do that first and see if the problem
    goes away.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步诊断问题之前的第一步是确保您最近已运行了首选副本选举（参见[第12章](ch12.html#administering_kafka)）。Kafka经纪人在释放领导权后（例如，当经纪人失败或关闭时），不会自动重新接管分区领导权（除非启用了自动领导者重新平衡）。这意味着领导副本在集群中很容易变得不平衡。首选副本选举是安全且易于运行的，因此最好先这样做，看看问题是否消失。
- en: Overloaded clusters are another problem that is easy to detect. If the cluster
    is balanced, and many of the brokers are showing elevated latency for requests
    or a low request handler pool idle ratio, you are reaching the limits of your
    brokers to serve traffic for this cluster. You may find upon deeper inspection
    that you have a client that has changed its request pattern and is now causing
    problems. Even when this happens, however, there may be little you can do about
    changing the client. The solutions available to you are either to reduce the load
    to the cluster or increase the number of brokers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 过载的集群是另一个容易检测到的问题。如果集群是平衡的，并且许多代理显示出增加的请求延迟或低请求处理程序池空闲比率，那么您的代理已经达到了为该集群提供流量的极限。在深入检查后，您可能会发现有一个客户端改变了其请求模式，现在导致了问题。然而，即使发生这种情况，您可能无法改变客户端。您可以采取的解决方案要么是减少对集群的负载，要么是增加代理的数量。
- en: Problems with the controller in the Kafka cluster are much more difficult to
    diagnose and often fall into the category of bugs in Kafka itself. These issues
    manifest as broker metadata being out of sync, offline replicas when the brokers
    appear to be fine, and topic control actions like creation not happening properly.
    If you’re scratching your head over a problem in the cluster and saying “That’s
    really weird,” there is a very good chance that it is because the controller did
    something unpredictable and bad. There are not a lot of ways to monitor the controller,
    but monitoring the active controller count as well as the controller queue size
    will give you a high-level indicator if there is a problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群中控制器的问题要难以诊断得多，通常属于Kafka本身的错误类别。这些问题表现为代理元数据不同步、代理离线时代理似乎正常，以及主题控制操作（如创建）未能正确进行。如果您在集群中遇到问题并说“这真的很奇怪”，那么很有可能是因为控制器做了一些不可预测且不好的事情。监控控制器的方法并不多，但监控活动控制器计数以及控制器队列大小将为您提供一个高级别的指标，以判断是否存在问题。
- en: The Art of Under-Replicated Partitions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未复制的分区的艺术
- en: One of the most popular metrics to use when monitoring Kafka is under-replicated
    partitions. This measurement, provided on each broker in a cluster, gives a count
    of the number of partitions for which the broker is the leader replica, where
    the follower replicas are not caught up. This single measurement provides insight
    into a number of problems with the Kafka cluster, from a broker being down to
    resource exhaustion. With the wide variety of problems that this metric can indicate,
    it is worthy of an in-depth look at how to respond to a value other than zero.
    Many of the metrics used in diagnosing these types of problems will be described
    later in this chapter. See [Table 13-3](#table1003) for more details on under-replicated
    partitions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监控Kafka时使用的最流行的指标之一是未复制的分区。在集群中的每个代理上提供的这个度量值，给出了代理是领导副本的分区数量，其中跟随者副本没有赶上。这个单一的度量值可以揭示Kafka集群的许多问题，从代理宕机到资源耗尽。由于这个度量值可以指示的问题种类繁多，因此值得深入研究如何应对非零值。本章后面将描述用于诊断这些问题的许多度量值。有关未复制的分区的更多详细信息，请参见[表13-3](#table1003)。
- en: Table 13-3\. Metrics and their corresponding under-replicated partitions
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-3。度量值及其对应的未复制的分区
- en: '| Metric name | Under-replicated partitions |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 未复制的分区 |'
- en: '| --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions`
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions`
    |'
- en: '| Value range | Integer, zero or greater |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 整数，零或更大 |'
- en: The URP Alerting Trap
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未复制的分区警报陷阱
- en: In the previous edition of this book, as well as in many conference talks, the
    authors have spoken at length about the fact that the under-replicated partitions
    (URP) metric should be your primary alerting metric because of how many problems
    it describes. This approach has a significant number of problems, not the least
    of which is that the URP metric can frequently be nonzero for benign reasons.
    This means that as someone operating a Kafka cluster, you will receive false alerts,
    which lead to the alert being ignored. It also requires a significant amount of
    knowledge to be able to understand what the metric is telling you. For this reason,
    we no longer recommend the use of URP for alerting. Instead, you should depend
    on SLO-based alerting to detect unknown problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的上一版以及许多会议演讲中，作者们长时间地谈到了未复制的分区（URP）度量应该是您的主要警报度量，因为它描述了多少问题。这种方法存在大量问题，其中最主要的问题之一是，未复制的分区度量通常由于良性原因而频繁出现非零值。这意味着作为Kafka集群的运维人员，您将收到错误警报，从而忽略警报。这也需要相当多的知识才能理解度量值告诉您的信息。因此，我们不再建议使用URP进行警报。相反，您应该依赖基于SLO的警报来检测未知问题。
- en: A steady (unchanging) number of under-replicated partitions reported by many
    of the brokers in a cluster normally indicates that one of the brokers in the
    cluster is offline. The count of under-replicated partitions across the entire
    cluster will equal the number of partitions that are assigned to that broker,
    and the broker that is down will not report a metric. In this case, you will need
    to investigate what has happened to that broker and resolve that situation. This
    is often a hardware failure, but it could also be an OS or Java issue that has
    caused the problem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中许多代理报告的未复制的分区数量保持稳定（不变），通常表明集群中的一个代理已经离线。整个集群中的未复制的分区数量将等于分配给该代理的分区数量，而宕机的代理将不会报告度量值。在这种情况下，您需要调查发生了什么，并解决这种情况。这通常是硬件故障，但也可能是导致问题的操作系统或Java问题。
- en: If the number of under-replicated partitions is fluctuating, or if the number
    is steady but there are no brokers offline, this typically indicates a performance
    issue in the cluster. These types of problems are much harder to diagnose due
    to their variety, but there are several steps you can work through to narrow it
    down to the most likely causes. The first step is to try and determine if the
    problem relates to a single broker or to the entire cluster. This can sometimes
    be a difficult question to answer. If the under-replicated partitions are on a
    single broker, as in the following example, then that broker is typically the
    problem. The error shows that other brokers are having a problem replicating messages
    from that one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未复制的分区数量波动，或者数量稳定但没有经纪人离线，通常表明集群中存在性能问题。由于这些问题的多样性，这些类型的问题更难诊断，但有几个步骤可以帮助您缩小问题的可能原因。第一步是尝试确定问题是与单个经纪人相关还是与整个集群相关。有时这可能是一个难以回答的问题。如果未复制的分区在单个经纪人上，就像下面的例子一样，那么通常这个经纪人就是问题所在。错误显示其他经纪人在复制来自该经纪人的消息时出现问题。
- en: If several brokers have under-replicated partitions, it could be a cluster problem,
    but it might still be a single broker. In that case, it would be because a single
    broker is having problems replicating messages from everywhere, and you’ll have
    to figure out which broker it is. One way to do this is to get a list of under-replicated
    partitions for the cluster and see if there is a specific broker that is common
    to all of the partitions that are under-replicated. Using the `kafka-topics.sh`
    tool (discussed in detail in [Chapter 12](ch12.html#administering_kafka)), you
    can get a list of under-replicated partitions to look for a common thread.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有几个经纪人有未复制的分区，这可能是一个集群问题，但仍可能是一个单个经纪人的问题。在这种情况下，这可能是因为一个单个经纪人在复制来自任何地方的消息时出现问题，您将不得不找出是哪个经纪人。一种方法是获取集群的未复制分区列表，并查看是否有一个特定的经纪人是所有未复制分区的共同线索。使用`kafka-topics.sh`工具（在[第12章](ch12.html#administering_kafka)中有详细讨论），您可以获取未复制分区的列表，以寻找一个共同的线索。
- en: 'For example, list under-replicated partitions in a cluster:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在集群中列出未复制的分区：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the common broker is number 2\. This indicates that this broker
    is having a problem with message replication and will lead us to focus our investigation
    on that one broker. If there is no common broker, there is likely a cluster-wide
    problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，常见的经纪人是编号2。这表明这个经纪人在消息复制方面存在问题，将导致我们将调查重点放在这个经纪人身上。如果没有常见的经纪人，那么很可能是整个集群出现了问题。
- en: Cluster-level problems
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群级问题
- en: 'Cluster problems usually fall into one of two categories:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 集群问题通常分为两类：
- en: Unbalanced load
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平衡的负载
- en: Resource exhaustion
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源耗尽
- en: 'The first problem, unbalanced partitions or leadership, is the easiest to find
    even though fixing it can be an involved process. In order to diagnose this problem,
    you will need several metrics from the brokers in the cluster:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题，不平衡的分区或领导权，是最容易发现的，尽管修复它可能是一个复杂的过程。为了诊断这个问题，您需要从集群中的经纪人获取几个指标：
- en: Partition count
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区计数
- en: Leader partition count
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领导分区计数
- en: All topics messages in rate
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有主题的消息输入速率
- en: All topics bytes in rate
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有主题的字节输入速率
- en: All topics bytes out rate
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有主题的字节输出速率
- en: Examine these metrics. In a perfectly balanced cluster, the numbers will be
    even across all brokers in the cluster, as in [Table 13-4](#table1004).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这些指标。在一个完全平衡的集群中，所有经纪人的数字将是均匀的，如[表13-4](#table1004)中所示。
- en: Table 13-4\. Utilization metrics
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-4。利用率指标
- en: '| Broker | Partitions | Leaders | Messages in | Bytes in | Bytes out |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 经纪人 | 分区 | 领导者 | 消息数 | 字节输入 | 字节输出 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 100 | 50 | 13130 msg/s | 3.56 MBps | 9.45 MBps |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 100 | 50 | 13130 msg/s | 3.56 MBps | 9.45 MBps |'
- en: '| 2 | 101 | 49 | 12842 msg/s | 3.66 MBps | 9.25 MBps |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 101 | 49 | 12842 msg/s | 3.66 MBps | 9.25 MBps |'
- en: '| 3 | 100 | 50 | 13086 msg/s | 3.23 MBps | 9.82 MBps |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 50 | 13086 msg/s | 3.23 MBps | 9.82 MBps |'
- en: This indicates that all the brokers are taking approximately the same amount
    of traffic. Assuming you have already run a preferred replica election, a large
    deviation indicates that the traffic is not balanced within the cluster. To resolve
    this, you will need to move partitions from the heavily loaded brokers to the
    less heavily loaded brokers. This is done using the `kafka-reassign-partitions.sh`
    tool described in [Chapter 12](ch12.html#administering_kafka).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明所有经纪人大约承担着相同数量的流量。假设您已经运行了首选副本选举，大的偏差表明集群内的流量不平衡。为了解决这个问题，您需要将分区从负载较重的经纪人移动到负载较轻的经纪人。这是使用`kafka-reassign-partitions.sh`工具来完成的，该工具在[第12章](ch12.html#administering_kafka)中有描述。
- en: Helpers for Balancing Clusters
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于平衡集群的辅助工具
- en: The Kafka broker itself does not provide for automatic reassignment of partitions
    in a cluster. This means that balancing traffic within a Kafka cluster can be
    a mind-numbing process of manually reviewing long lists of metrics and trying
    to come up with a replica assignment that works. To help with this, some organizations
    have developed automated tools for performing this task. One example is the `kafka-assigner`
    tool that LinkedIn has released in the open source [kafka-tools repository on
    GitHub](https://oreil.ly/8ilPw). Some enterprise offerings for Kafka support also
    provide this feature.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka经纪人本身不提供集群中分区的自动重新分配。这意味着在Kafka集群中平衡流量可能是一个令人昏昏欲睡的过程，需要手动审查大量的指标列表，并尝试找出有效的副本分配。为了帮助解决这个问题，一些组织已经开发了自动化工具来执行这项任务。其中一个例子是LinkedIn在GitHub上发布的`kafka-assigner`工具（https://oreil.ly/8ilPw）。一些Kafka支持的企业产品也提供了这个功能。
- en: 'Another common cluster performance issue is exceeding the capacity of the brokers
    to serve requests. There are many possible bottlenecks that could slow things
    down: CPU, disk IO, and network throughput are a few of the most common. Disk
    utilization is not one of them, as the brokers will operate properly right up
    until the disk is filled, and then this disk will fail abruptly. In order to diagnose
    a capacity problem, there are many metrics you can track at the OS level, including:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的集群性能问题是超出代理提供请求的能力。可能会有许多可能的瓶颈会减慢速度：CPU、磁盘IO和网络吞吐量是最常见的几个。磁盘利用率不在其中，因为代理将正常运行，直到磁盘填满，然后磁盘将突然失败。为了诊断容量问题，您可以在操作系统级别跟踪许多指标，包括：
- en: CPU utilization
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU利用率
- en: Inbound network throughput
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入站网络吞吐量
- en: Outbound network throughput
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出站网络吞吐量
- en: Disk average wait time
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘平均等待时间
- en: Disk percent utilization
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘百分比利用率
- en: 'Exhausting any of these resources will typically show up as the same problem:
    under-replicated partitions. It’s critical to remember that the broker replication
    process operates in exactly the same way that other Kafka clients do. If your
    cluster is having problems with replication, then your customers are having problems
    with producing and consuming messages as well. It makes sense to develop a baseline
    for these metrics when your cluster is operating correctly and then set thresholds
    that indicate a developing problem long before you run out of capacity. You will
    also want to review the trend for these metrics as the traffic to your cluster
    increases over time. As far as Kafka broker metrics are concerned, the `All Topics
    Bytes In Rate` is a good guideline to show cluster usage.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 耗尽任何这些资源通常会表现为相同的问题：副本不足的分区。重要的是要记住，代理复制过程的操作方式与其他Kafka客户端完全相同。如果您的集群在复制方面出现问题，那么您的客户在生产和消费消息方面也会出现问题。在集群正常运行时，为这些指标制定基线，然后设置指示出现问题的阈值，远在容量耗尽之前就能指示出问题的发展。您还需要查看这些指标随着时间推移而增加到集群的流量。就Kafka代理指标而言，“所有主题字节输入速率”是显示集群使用情况的良好指南。
- en: Host-level problems
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主机级问题
- en: 'If the performance problem with Kafka is not present in the entire cluster
    and can be isolated to one or two brokers, it’s time to examine that server and
    see what makes it different from the rest of the cluster. These types of problems
    fall into several general categories:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Kafka的性能问题不是整个集群中存在，并且可以隔离到一个或两个代理，那么是时候检查该服务器，看看它与集群的其他部分有何不同了。这些问题属于几个一般类别：
- en: Hardware failures
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件故障
- en: Networking
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络
- en: Conflicts with another process
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与另一个进程冲突
- en: Local configuration differences
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地配置差异
- en: Typical Servers and Problems
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型服务器和问题
- en: A server and its OS is a complex machine with thousands of components, any of
    which could have problems and cause either a complete failure or just a performance
    degradation. It’s impossible for us to cover everything that can fail in this
    book—numerous volumes have been written, and will continue to be, on this subject.
    But we can discuss some of the most common problems that are seen. This section
    will focus on issues with a typical server running a Linux OS.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器及其操作系统是一个复杂的机器，有成千上万个组件，任何一个都可能出现问题，导致完全故障或性能下降。我们不可能在本书中涵盖所有可能出现故障的内容——已经有许多卷的书籍，而且将继续有关于这个主题的书籍。但我们可以讨论一些最常见的问题。本节将重点讨论运行Linux操作系统的典型服务器的问题。
- en: Hardware failures are sometimes obvious, like when the server just stops working,
    but it’s the less obvious problems that cause performance issues. These are usually
    soft failures that allow the system to keep running but degrade operation. This
    could be a bad bit of memory, where the system has detected the problem and bypassed
    that segment (reducing the overall available memory). The same can happen with
    a CPU failure. For problems such as these, you should be using the facilities
    that your hardware provides, such as an intelligent platform management interface
    (IPMI) to monitor hardware health. When there’s an active problem, looking at
    the kernel ring buffer using `dmesg` will help you to see log messages that are
    getting thrown to the system console.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件故障有时很明显，比如服务器突然停止工作，但是导致性能问题的是不太明显的问题。这些通常是允许系统继续运行但降低操作的软故障。这可能是一小部分内存出现问题，系统已检测到问题并绕过该段（减少了总可用内存）。CPU故障也可能发生相同的情况。对于这类问题，您应该使用硬件提供的设施，例如智能平台管理接口（IPMI）来监控硬件健康状况。当存在活动问题时，查看使用`dmesg`的内核环形缓冲区将帮助您查看被抛到系统控制台的日志消息。
- en: The more common type of hardware failure that leads to a performance degradation
    in Kafka is a disk failure. Apache Kafka is dependent on the disk for persistence
    of messages, and producer performance is directly tied to how fast your disks
    commit those writes. Any deviation in this will show up as problems with the performance
    of the producers and the replica fetchers. The latter is what leads to under-replicated
    partitions. As such, it is important to monitor the health of the disks at all
    times and address any problems quickly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 导致Kafka性能下降的更常见的硬件故障是磁盘故障。Apache Kafka依赖磁盘来持久化消息，生产者的性能直接取决于磁盘提交写入的速度。任何偏差都会表现为生产者和副本获取者性能的问题。后者是导致副本不足的分区。因此，随时监控磁盘的健康状况并及时解决任何问题非常重要。
- en: One Bad Egg
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个坏蛋
- en: A single disk failure on a single broker can destroy the performance of an entire
    cluster. This is because the producer clients will connect to all brokers that
    lead partitions for a topic, and if you have followed best practices, those partitions
    will be evenly spread over the entire cluster. If one broker starts performing
    poorly and slowing down produce requests, this will cause back pressure in the
    producers, slowing down requests to all brokers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 单个经纪人的单个磁盘故障可能破坏整个集群的性能。这是因为生产者客户端将连接到为主题引导分区的所有经纪人，如果您遵循最佳实践，这些分区将均匀分布在整个集群上。如果一个经纪人开始表现不佳并减慢生产请求，这将导致生产者中的背压，减慢对所有经纪人的请求。
- en: To begin with, make sure you are monitoring hardware status information for
    the disks from the IPMI, or the interface provided by your hardware. In addition,
    within the OS you should be running SMART (Self-Monitoring, Analysis and Reporting
    Technology) tools to both monitor and test the disks on a regular basis. This
    will alert you to a failure that is about to happen. It is also important to keep
    an eye on the disk controller, especially if it has RAID functionality, whether
    you are using hardware RAID or not. Many controllers have an onboard cache that
    is only used when the controller is healthy and the battery backup unit (BBU)
    is working. A failure of the BBU can result in the cache being disabled, degrading
    disk performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保您正在监视来自IPMI或硬件提供的接口的磁盘的硬件状态信息。此外，在操作系统中，您应该定期运行SMART（自我监控、分析和报告技术）工具来监视和测试磁盘。这将警示您即将发生的故障。另外，重要的是要密切关注磁盘控制器，特别是如果它具有RAID功能，无论您是使用硬件RAID还是其他方式。许多控制器具有内置缓存，仅在控制器健康且电池备份单元（BBU）正常工作时才会使用。BBU的故障可能导致缓存被禁用，降低磁盘性能。
- en: Networking is another area where partial failures will cause problems. Some
    of these problems are hardware issues, such as a bad network cable or connector.
    Some are configuration issues, which is usually a change in the speed or duplex
    settings for the connection, either on the server side or upstream on the networking
    hardware. Network configuration problems could also be OS issues, such as having
    the network buffers undersized or too many network connections taking up too much
    of the overall memory footprint. One of the key indicators of problems in this
    area will be the number of errors detected on the network interfaces. If the error
    count is increasing, there is probably an unaddressed issue.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是另一个部分故障会导致问题的领域。其中一些问题是硬件问题，例如糟糕的网络电缆或连接器。有些是配置问题，通常是连接的速度或双工设置的更改，无论是在服务器端还是在网络硬件上游。网络配置问题也可能是操作系统问题，例如网络缓冲区过小或太多的网络连接占用了整体内存占用量的太多。在这个领域问题的一个关键指标将是网络接口上检测到的错误数量。如果错误计数正在增加，那么可能存在未解决的问题。
- en: If there are no hardware problems, another common problem to look for is another
    application running on the system that is consuming resources and putting pressure
    on the Kafka broker. This could be something that was installed in error, or it
    could be a process that is supposed to be running, such as a monitoring agent,
    but is having problems. Use the tools on your system, such as `top`, to identify
    if there is a process that is using more CPU or memory than expected.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有硬件问题，要查找的另一个常见问题是系统上运行的另一个应用程序正在消耗资源并对Kafka经纪人施加压力。这可能是错误安装的东西，也可能是应该运行的进程，例如监控代理，但出现了问题。使用系统上的工具，如`top`，来识别是否有进程使用的CPU或内存超出预期。
- en: If the other options have been exhausted and you have not yet found the source
    of the discrepancy on the host, a configuration difference has likely crept in,
    either with the broker or the system itself. Given the number of applications
    that are running on any single server and the number of configuration options
    for each of them, it can be a daunting task to find a discrepancy. This is why
    it is crucial that you utilize a configuration management system, such as [Chef](https://www.chef.io)
    or [Puppet](https://puppet.com), in order to maintain consistent configurations
    across your OSes and applications (including Kafka).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其他选项已经耗尽，但您尚未找到主机上差异的来源，那么很可能是出现了配置差异，无论是经纪人还是系统本身。考虑到任何单个服务器上运行的应用程序数量以及每个应用程序的配置选项数量，要找到差异可能是一项艰巨的任务。这就是为什么您必须利用配置管理系统（如[Chef](https://www.chef.io)或[Puppet](https://puppet.com)）来维护OS和应用程序（包括Kafka）之间的一致配置的原因。
- en: Broker Metrics
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经纪人指标
- en: In addition to under-replicated partitions, there are other metrics that are
    present at the overall broker level that should be monitored. While you may not
    be inclined to set alert thresholds for all of them, they provide valuable information
    about your brokers and your cluster. They should be present in any monitoring
    dashboard you create.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了副本不足的分区之外，还有其他在整体经纪人级别存在的指标应该被监视。虽然您可能不倾向于为所有这些指标设置警报阈值，但它们提供了有关您的经纪人和集群的宝贵信息。它们应该出现在您创建的任何监控仪表板中。
- en: Active controller count
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动控制器计数
- en: The *active controller count* metric indicates whether the broker is currently
    the controller for the cluster. The metric will either be 0 or 1, with 1 showing
    that the broker is currently the controller. At all times, only one broker should
    be the controller, and one broker must always be the controller in the cluster.
    If two brokers say that they are currently the controller, this means that you
    have a problem where a controller thread that should have exited has become stuck.
    This can cause problems with not being able to execute administrative tasks, such
    as partition moves, properly. To remedy this, you will need to restart both brokers
    at the very least. However, when there is an extra controller in the cluster,
    there will often be problems performing a safe shutdown of a broker, and you will
    need to force stop the broker instead. See [Table 13-5](#table1005) for more details
    on active controller count.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*活动控制器计数*指标指示经纪人当前是否是集群的控制器。该指标将是0或1，其中1表示经纪人当前是控制器。始终只有一个经纪人应该是控制器，并且集群中必须始终有一个经纪人是控制器。如果两个经纪人表示它们当前是控制器，这意味着您遇到了一个控制器线程应该退出但却被卡住的问题。这可能导致无法正确执行管理任务，例如分区移动。为了解决这个问题，您至少需要重新启动两个经纪人。然而，当集群中有额外的控制器时，通常会出现无法安全关闭经纪人的问题，您将需要强制停止经纪人。有关活动控制器计数的更多详细信息，请参见[表13-5](#table1005)。'
- en: Table 13-5\. Active controller count metric details
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-5。活动控制器计数指标详细信息
- en: '| Metric name | Active controller count |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 活动控制器计数 |'
- en: '| --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.controller:type=KafkaController,name=ActiveControllerCount`
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.controller:type=KafkaController,name=ActiveControllerCount`
    |'
- en: '| Value range | Zero or one |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 零或一 |'
- en: If no broker claims to be the controller in the cluster, the cluster will fail
    to respond properly in the face of state changes, including topic or partition
    creation, or broker failures. In this situation, you must investigate further
    to find out why the controller threads are not working properly. For example,
    a network partition from the ZooKeeper cluster could result in a problem like
    this. Once that underlying problem is fixed, it is wise to restart all the brokers
    in the cluster in order to reset state for the controller threads.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有经纪人声称是集群中的控制器，集群将无法在状态更改时正确响应，包括主题或分区创建或经纪人故障。在这种情况下，您必须进一步调查为什么控制器线程无法正常工作。例如，来自ZooKeeper集群的网络分区可能导致这样的问题。一旦解决了潜在的问题，明智的做法是重新启动集群中的所有经纪人，以重置控制器线程的状态。
- en: Controller queue size
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制器队列大小
- en: The *controller queue size* metric indicates how many requests the controller
    is currently waiting to process for the brokers. The metric will be 0 or more,
    with the value fluctuating frequently as new requests from brokers come in and
    administrative actions, such as creating partitions, moving partitions, and processing
    leader changes happen. Spikes in the metric are to be expected, but if this value
    continuously increases, or stays steady at a high value and does not drop, it
    indicates that the controller may be stuck. This can cause problems with not being
    able to execute administrative tasks properly. To remedy this, you will need to
    move the controller to a different broker, which requires shutting down the broker
    that is currently the controller. However, when the controller is stuck, there
    will often be problems performing a controlled shutdown of any broker. See [Table 13-6](#table1017)
    for more details on controller queue size.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*控制器队列大小*指标指示控制器当前正在等待为经纪人处理多少请求。该指标将是0或更多，其值会随着来自经纪人的新请求和管理操作（例如创建分区、移动分区和处理领导者更改）的发生而经常波动。指标的波动是可以预期的，但如果该值持续增加，或者保持在一个高值并且不下降，这表明控制器可能被卡住。这可能导致无法正确执行管理任务的问题。为了解决这个问题，您需要将控制器移动到另一个经纪人，这需要关闭当前是控制器的经纪人。然而，当控制器被卡住时，通常会出现无法受控地关闭任何经纪人的问题。有关控制器队列大小的更多详细信息，请参见[表13-6](#table1017)。'
- en: Table 13-6\. Controller queue size metric details
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-6。控制器队列大小指标详细信息
- en: '| Metric name | Controller queue size |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 控制器队列大小 |'
- en: '| --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.controller:type=ControllerEventManager,name=EventQueueSize`
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.controller:type=ControllerEventManager,name=EventQueueSize`
    |'
- en: '| Value range | Integer, zero or more |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 整数，零或更多 |'
- en: Request handler idle ratio
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求处理程序空闲比率
- en: 'Kafka uses two thread pools for handling all client requests: *network threads*
    and *request handler threads* (also called *I/O threads*). The network threads
    are responsible for reading and writing data to the clients across the network.
    This does not require significant processing, which means that exhaustion of the
    network threads is less of a concern. The request handler threads, however, are
    responsible for servicing the client request itself, which includes reading or
    writing the messages to disk. As such, as the brokers get more heavily loaded,
    there is a significant impact on this thread pool. See [Table 13-7](#table1006)
    for more details on the request handler idle ratio.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用两个线程池来处理所有客户端请求：*网络线程*和*请求处理程序线程*（也称为*I/O线程*）。网络线程负责在网络上读取和写入客户端的数据。这不需要大量处理，这意味着网络线程的耗尽不太值得关注。然而，请求处理程序线程负责为客户端请求本身提供服务，包括将消息读取或写入磁盘。因此，随着经纪人负载更重，这个线程池会受到重大影响。有关请求处理程序空闲比率的更多详细信息，请参见[表13-7](#table1006)。
- en: Table 13-7\. Request handler idle ratio details
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-7。请求处理程序空闲比率详细信息
- en: '| Metric name | Request handler average idle percentage |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 请求处理程序平均空闲百分比 |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent`
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent`
    |'
- en: '| Value range | Float, between zero and one inclusive |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 浮点数，介于零和一之间 |'
- en: Intelligent Thread Usage
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能线程使用
- en: While it may seem like you will need hundreds of request handler threads, in
    reality you do not need to configure any more threads than you have CPUs in the
    broker. Apache Kafka is very smart about the way it uses the request handlers,
    making sure to offload to purgatory those requests that will take a long time
    to process. This is used, for example, when requests are being quoted or when
    more than one acknowledgment of produce requests is required.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来您可能需要数百个请求处理程序线程，但实际上您不需要配置比经纪人中的CPU更多的线程。Apache Kafka在使用请求处理程序的方式上非常聪明，确保将需要很长时间处理的请求转移到“炼狱”，例如在报价请求或需要多个生产请求确认时使用。
- en: The request handler idle ratio metric indicates the percentage of time the request
    handlers are not in use. The lower this number, the more loaded the broker is.
    Experience tells us that idle ratios lower than 20% indicate a potential problem,
    and lower than 10% is usually an active performance problem. Besides the cluster
    being undersized, there are two reasons for high thread utilization in this pool.
    The first is that there are not enough threads in the pool. In general, you should
    set the number of request handler threads equal to the number of processors in
    the system (including hyperthreaded processors).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请求处理程序空闲比指标表示请求处理程序未使用的时间百分比。这个数字越低，代表经纪人的负载越重。经验告诉我们，空闲比低于20%表示潜在问题，低于10%通常表示活跃的性能问题。除了集群容量不足外，这个池中高线程利用率的原因有两个。第一个是池中的线程不够。通常情况下，您应该将请求处理程序线程的数量设置为系统中处理器的数量（包括超线程处理器）。
- en: The other common reason for high request handler thread utilization is that
    the threads are doing unnecessary work for each request. Prior to Kafka 0.10,
    the request handler thread was responsible for decompressing every incoming message
    batch, validating the messages and assigning offsets, and then recompressing the
    message batch with offsets before writing it to disk. To make matters worse, the
    compression methods were all behind a synchronous lock. As of version 0.10, there
    is a new message format that allows for relative offsets in a message batch. This
    means that newer producers will set relative offsets prior to sending the message
    batch, which allows the broker to skip recompression of the message batch. One
    of the single largest performance improvements you can make is to ensure that
    all producer and consumer clients support the 0.10 message format, and to change
    the message format version on the brokers to 0.10 as well. This will greatly reduce
    the utilization of the request handler threads.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请求处理程序线程利用率较高的另一个常见原因是线程对每个请求都在做不必要的工作。在Kafka 0.10之前，请求处理程序线程负责解压每个传入的消息批次，验证消息并分配偏移量，然后在将消息批次写入磁盘之前重新压缩带有偏移量的消息批次。更糟糕的是，所有压缩方法都在同步锁后面。从0.10版本开始，有一种新的消息格式允许消息批次中的相对偏移量。这意味着更新的生产者将在发送消息批次之前设置相对偏移量，这样经纪人就可以跳过消息批次的重新压缩。您可以做的最大的性能改进之一是确保所有生产者和消费者客户端都支持0.10消息格式，并将经纪人上的消息格式版本也更改为0.10。这将大大减少请求处理程序线程的利用率。
- en: All topics bytes in
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有主题字节
- en: The *all topics bytes in* rate, expressed in bytes per second, is useful as
    a measurement of how much message traffic your brokers are receiving from producing
    clients. This is a good metric to trend over time to help you determine when you
    need to expand the cluster or do other growth-related work. It is also useful
    for evaluating if one broker in a cluster is receiving more traffic than the others,
    which would indicate that it is necessary to rebalance the partitions in the cluster.
    See [Table 13-8](#table1007) for more details.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以每秒字节为单位的*所有主题字节*速率对于衡量经纪人从生产客户端接收的消息流量非常有用。这是一个很好的指标，可以帮助您确定何时需要扩展集群或进行其他与增长相关的工作。它还有助于评估集群中的一个经纪人是否比其他经纪人接收更多的流量，这表明有必要重新平衡集群中的分区。有关更多详细信息，请参见[表13-8](#table1007)。
- en: Table 13-8\. All topics bytes in metric details
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-8。所有主题字节指标详情
- en: '| Metric name | Bytes in per second |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 每秒字节 |'
- en: '| --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec` |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec` |'
- en: '| Value range | Rates as doubles, count as integer |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 速率为双精度，计数为整数 |'
- en: As this is the first rate metric discussed, it is worth a short discussion of
    the attributes that are provided by these types of metrics. All of the rate metrics
    have seven attributes, and choosing which ones to use depends on what type of
    measurement you want. The attributes provide a discrete count of events, as well
    as an average of the number of events over various periods of time. Make sure
    to use the metrics appropriately, or you will end up with a flawed view of the
    broker.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是讨论的第一个速率指标，值得简要讨论一下这些类型指标提供的属性。所有速率指标都有七个属性，选择使用哪些取决于您想要的测量类型。这些属性提供了事件的离散计数，以及在不同时间段内事件数量的平均值。确保适当使用指标，否则您将得到有缺陷的经纪人视图。
- en: 'The first two attributes are not measurements, but they will help you understand
    the metric you are looking at:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个属性不是测量，但它们将帮助您理解您正在查看的指标：
- en: '`EventType`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`EventType`'
- en: This is the unit of measurement for all the attributes. In this case, it is
    “bytes.”
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有属性的测量单位。在这种情况下，它是“字节”。
- en: '`RateUnit`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`RateUnit`'
- en: For the rate attributes, this is the time period for the rate. In this case,
    it is “seconds.”
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于速率属性，这是速率的时间段。在这种情况下，它是“秒”。
- en: 'These two descriptive attributes tell us that the rates, regardless of the
    period of time they average over, are presented as a value of bytes per second.
    There are four rate attributes provided with different granularities:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个描述性属性告诉我们，无论它们平均的时间段是多长，速率都以每秒字节的值呈现。提供了四个不同粒度的速率属性：
- en: '`OneMinuteRate`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneMinuteRate`'
- en: An average over the previous 1 minute
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 过去1分钟的平均值
- en: '`FiveMinuteRate`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`FiveMinuteRate`'
- en: An average over the previous 5 minutes
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 过去5分钟的平均值
- en: '`FifteenMinuteRate`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`FifteenMinuteRate`'
- en: An average over the previous 15 minutes
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 过去15分钟的平均值
- en: '`MeanRate`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`MeanRate`'
- en: An average since the broker was started
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 自经纪人启动以来的平均值
- en: The `OneMinuteRate` will fluctuate quickly and provides more of a “point in
    time” view of the measurement. This is useful for seeing short spikes in traffic.
    The `MeanRate` will not vary much at all and provides an overall trend. Though
    `MeanRate` has its uses, it is probably not the metric you want to be alerted
    on. The `FiveMinuteRate` and `FifteenMinuteRate` provide a compromise between
    the two.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneMinuteRate`将快速波动，并提供更多“即时”视图的测量。这对于查看交通短暂激增很有用。`MeanRate`几乎不会变化，并提供总体趋势。虽然`MeanRate`有其用途，但它可能不是您想要收到警报的指标。`FiveMinuteRate`和`FifteenMinuteRate`在两者之间提供了一个折衷方案。'
- en: In addition to the rate attributes, there is a `Count` attribute as well. This
    is a constantly increasing value for the metric since the time the broker was
    started. For this metric, all topics bytes in, the `Count` represents the total
    number of bytes produced to the broker since the process was started. Utilized
    with a metrics system that supports countermetrics, this can give you an absolute
    view of the measurement instead of an averaged rate.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了速率属性之外，还有一个`Count`属性。这是自经纪人启动以来该指标的不断增加的值。对于此指标，所有主题的字节输入，`Count`表示自进程启动以来发送到经纪人的总字节数。与支持计数器指标的度量系统一起使用，这可以让您绝对查看测量结果，而不是平均速率。
- en: All topics bytes out
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有主题的字节输出
- en: The *all topics bytes out* rate, similar to the bytes in rate, is another overall
    growth metric. In this case, the bytes out rate shows the rate at which consumers
    are reading messages out. The outbound bytes rate may scale differently than the
    inbound bytes rate, thanks to Kafka’s capacity to handle multiple consumers with
    ease. There are many deployments of Kafka where the outbound rate can easily be
    six times the inbound rate! This is why it is important to observe and trend the
    outbound bytes rate separately. See [Table 13-9](#table1008) for more details.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有主题的字节输出*速率，类似于字节输入速率，是另一个总体增长指标。在这种情况下，字节输出速率显示消费者读取消息的速率。出站字节速率可能与入站字节速率不同，这要归功于Kafka轻松处理多个消费者的能力。有许多Kafka部署，其中出站速率很容易是入站速率的六倍！这就是为什么单独观察和趋势出站字节速率很重要。有关更多详细信息，请参见[表13-9](#table1008)。'
- en: Table 13-9\. All topics bytes out metric details
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-9。所有主题字节输出指标详细信息
- en: '| Metric name | Bytes out per second |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|指标名称|每秒输出的字节|'
- en: '| --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec` |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec`
- en: '| Value range | Rates as doubles, count as integer |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|值范围|速率为双倍，计数为整数|'
- en: Replica Fetchers Included
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包括副本获取器
- en: The outbound bytes rate *also* includes the replica traffic. This means that
    if all of the topics are configured with a replication factor of 2, you will see
    a bytes out rate equal to the bytes in rate when there are no consumer clients.
    If you have one consumer client reading all the messages in the cluster, then
    the bytes out rate will be twice the bytes in rate. This can be confusing when
    looking at the metrics if you’re not aware of what is counted.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 出站字节速率*还*包括副本流量。这意味着如果所有主题的配置副本因子为2，则当没有消费者客户端时，您将看到与字节输入速率相等的字节输出速率。如果有一个消费者客户端读取集群中的所有消息，则字节输出速率将是字节输入速率的两倍。如果您不知道计数的内容，这在查看指标时可能会让人困惑。
- en: All topics messages in
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有主题的消息输入
- en: While the byte rates described previously show the broker traffic in absolute
    terms of bytes, the *messages in* rate shows the number of individual messages,
    regardless of their size, produced per second. This is useful as a growth metric
    as a different measure of producer traffic. It can also be used in conjunction
    with the bytes in rate to determine an average message size. You may also see
    an imbalance in the brokers, just like with the bytes in rate, that will alert
    you to necessary maintenance work. See [Table 13-10](#table1009) for more details.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然先前描述的字节速率显示了字节的绝对值，但*消息输入*速率显示了每秒产生的个别消息的数量，而不考虑其大小。这作为生长指标很有用，作为生产者流量的不同度量。它也可以与字节输入速率一起使用，以确定平均消息大小。您还可能会看到经纪人的不平衡，就像字节输入速率一样，这将提醒您需要进行必要的维护工作。有关更多详细信息，请参见[表13-10](#table1009)。
- en: Table 13-10\. All topics messages in metric details
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-10。所有主题消息输入指标详细信息
- en: '| Metric name | Messages in per second |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|指标名称|每秒消息输入|'
- en: '| --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec`
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: JMX MBean | `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec`
- en: '| Value range | Rates as doubles, count as integer |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|值范围|速率为双倍，计数为整数|'
- en: Why No Messages Out?
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么没有消息输出？
- en: People often ask why there is no messages out metric for the Kafka broker. The
    reason is that when messages are consumed, the broker just sends the next batch
    to the consumer without expanding it to find out how many messages are inside.
    Therefore, the broker doesn’t really know how many messages were sent out. The
    only metric that can be provided is the number of fetches per second, which is
    a request rate, not a messages count.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常问为什么Kafka经纪人没有消息输出指标。原因是当消息被消耗时，经纪人只是将下一批消息发送给消费者，而不会扩展以找出其中有多少消息。因此，经纪人实际上不知道发送了多少消息。唯一可以提供的指标是每秒获取的次数，这是一个请求速率，而不是消息计数。
- en: Partition count
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区计数
- en: The *partition count* for a broker generally doesn’t change that much, as it
    is the total number of partitions assigned to that broker. This includes every
    replica the broker has, regardless of whether it is a leader or follower for that
    partition. Monitoring this is often more interesting in a cluster that has automatic
    topic creation enabled, as that can leave the creation of topics outside of the
    control of the person running the cluster. See [Table 13-11](#table1010) for more
    details.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*分区计数*对于经纪人通常不会有太大变化，因为它是分配给该经纪人的分区总数。这包括经纪人拥有的每个副本，无论它是否是该分区的领导者或追随者。在启用自动主题创建的集群中监视这一点通常更有趣，因为这可能会使主题的创建超出运行集群的人的控制范围。更多详情请参见[表13-11](#table1010)。'
- en: Table 13-11\. Partition count metric details
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-11. 分区计数指标详情
- en: '| Metric name | Partition count |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 分区计数 |'
- en: '| --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=ReplicaManager,name=PartitionCount` |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.server:type=ReplicaManager,name=PartitionCount` |'
- en: '| Value range | Integer, zero or greater |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 整数，零或更大 |'
- en: Leader count
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领导者计数
- en: The *leader count* metric shows the number of partitions that the broker is
    currently the leader for. As with most other measurements in the brokers, this
    one should be generally even across the brokers in the cluster. It is much more
    important to check the leader count on a regular basis, possibly alerting on it,
    as it will indicate when the cluster is imbalanced even if the number of replicas
    are perfectly balanced in count and size across the cluster. This is because a
    broker can drop leadership for a partition for many reasons, such as a ZooKeeper
    session expiration, and it will not automatically take leadership back once it
    recovers (except if you have enabled automatic leader rebalancing). In these cases,
    this metric will show fewer leaders, or often zero, which indicates that you need
    to run a preferred replica election to rebalance leadership in the cluster. See
    [Table 13-12](#table1011) for more details.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*领导者计数*指标显示了经纪人当前担任领导者的分区数量。与经纪人中的大多数其他测量一样，这个指标在集群中的经纪人之间应该是均匀的。定期检查领导者计数非常重要，可能会对其进行警报，因为即使副本的数量和大小在整个集群中完全平衡，它也会指示集群不平衡的情况。这是因为经纪人可以因为许多原因而放弃对分区的领导地位，比如ZooKeeper会话过期，并且一旦恢复（除非您启用了自动领导者重新平衡），它不会自动重新担任领导地位。在这些情况下，这个指标将显示较少的领导者，或者经常是零，这表明您需要运行首选副本选举来重新平衡集群中的领导地位。更多详情请参见[表13-12](#table1011)。'
- en: Table 13-12\. Leader count metric details
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-12. 领导者计数指标详情
- en: '| Metric name | Leader count |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: 指标名称 | 领导者计数
- en: '| --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.server:type=ReplicaManager,name=LeaderCount` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.server:type=ReplicaManager,name=LeaderCount` |'
- en: '| Value range | Integer, zero or greater |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 整数，零或更大 |'
- en: A useful way to consume this metric is to use it along with the partition count
    to show a percentage of partitions that the broker is the leader for. In a well-balanced
    cluster that is using a replication factor of 2, all brokers should be leaders
    for approximately 50% of their partitions. If the replication factor in use is
    3, this percentage drops to 33%.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '使用这个指标的一个有用的方法是将它与分区计数一起使用，以显示经纪人担任领导者的分区的百分比。在使用复制因子为2的平衡集群中，所有经纪人应该担任大约50%的分区的领导者。如果使用的复制因子是3，这个百分比会下降到33%。 '
- en: Offline partitions
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脱机分区
- en: 'Along with the under-replicated partitions count, the *offline partitions*
    count is a critical metric for monitoring (see [Table 13-13](#table1012)). This
    measurement is only provided by the broker that is the controller for the cluster
    (all other brokers will report 0) and shows the number of partitions in the cluster
    that currently have no leader. Partitions without leaders can happen for two main
    reasons:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除了未复制的分区计数之外，*脱机分区*计数是监控的关键指标（见[表13-13](#table1012)）。这个测量只由集群的控制器经纪人提供（所有其他经纪人将报告0），显示了当前在集群中没有领导者的分区数量。没有领导者的分区可能有两个主要原因：
- en: All brokers hosting replicas for this partition are down
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管此分区副本的所有经纪人都已宕机
- en: No in-sync replica can take leadership due to message-count mismatches (with
    unclean leader election disabled)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于消息计数不匹配（关闭不干净的领导者选举），没有同步的副本可以领导
- en: Table 13-13\. Offline partitions count metric details
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-13. 脱机分区计数指标详情
- en: '| Metric name | Offline partitions count |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 脱机分区计数 |'
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JMX MBean | `kafka.controller:type=KafkaController,name=OfflinePartitionsCount`
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| JMX MBean | `kafka.controller:type=KafkaController,name=OfflinePartitionsCount`
    |'
- en: '| Value range | Integer, zero or greater |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 值范围 | 整数，零或更大 |'
- en: In a production Kafka cluster, an offline partition may be impacting the producer
    clients, losing messages or causing back pressure in the application. This is
    most often a “site down” type of problem and will need to be addressed immediately.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产Kafka集群中，脱机分区可能会影响生产者客户端，导致消息丢失或在应用程序中造成背压。这往往是一种“站点宕机”类型的问题，需要立即解决。
- en: Request metrics
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求指标
- en: 'The Kafka protocol, described in [Chapter 6](ch06.html#kafka_internals), has
    many different requests. Metrics are provided for how each of those requests performs.
    As of version 2.5.0, the following requests have metrics provided:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka协议在[第6章](ch06.html#kafka_internals)中描述了许多不同的请求。针对每个请求的性能提供了指标。截至2.5.0版本，以下请求提供了指标：
- en: Table 13-14\. Request metrics names
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-14. 请求指标名称
- en: '| `AddOffsetsToTxn` | `AddPartitionsToTxn` | `AlterConfigs` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `AddOffsetsToTxn` | `AddPartitionsToTxn` | `AlterConfigs` |'
- en: '| `AlterPartitionReassignments` | `AlterReplicaLogDirs` | `ApiVersions` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `AlterPartitionReassignments` | `AlterReplicaLogDirs` | `ApiVersions` |'
- en: '| `ControlledShutdown` | `CreateAcls` | `CreateDelegationToken` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `ControlledShutdown` | `CreateAcls` | `CreateDelegationToken` |'
- en: '| `CreatePartitions` | `CreateTopics` | `DeleteAcls` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `CreatePartitions` | `CreateTopics` | `DeleteAcls` |'
- en: '| `DeleteGroups` | `DeleteRecords` | `DeleteTopics` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `DeleteGroups` | `DeleteRecords` | `DeleteTopics` |'
- en: '| `DescribeAcls` | `DescribeConfigs` | `DescribeDelegationToken` |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `DescribeAcls` | `DescribeConfigs` | `DescribeDelegationToken` |'
- en: '| `DescribeGroups` | `DescribeLogDirs` | `ElectLeaders` |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `DescribeGroups` | `DescribeLogDirs` | `ElectLeaders` |'
- en: '| `EndTxn` | `ExpireDelegationToken` | `Fetch` |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `EndTxn` | `ExpireDelegationToken` | `Fetch` |'
- en: '| `FetchConsumer` | `FetchFollower` | `FindCoordinator` |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `FetchConsumer` | `FetchFollower` | `FindCoordinator` |'
- en: '| `Heartbeat` | `IncrementalAlterConfigs` | `InitProducerId` |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `Heartbeat` | `IncrementalAlterConfigs` | `InitProducerId` |'
- en: '| `JoinGroup` | `LeaderAndIsr` | `LeaveGroup` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `JoinGroup` | `LeaderAndIsr` | `LeaveGroup` |'
- en: '| `ListGroups` | `ListOffsets` | `ListPartitionReassignments` |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `ListGroups` | `ListOffsets` | `ListPartitionReassignments` |'
- en: '| `Metadata` | `OffsetCommit` | `OffsetDelete` |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `Metadata` | `OffsetCommit` | `OffsetDelete` |'
- en: '| `OffsetFetch` | `OffsetsForLeaderEpoch` | `Produce` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `OffsetFetch` | `OffsetsForLeaderEpoch` | `Produce` |'
- en: '| `RenewDelegationToken` | `SaslAuthenticate` | `SaslHandshake` |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `RenewDelegationToken` | `SaslAuthenticate` | `SaslHandshake` |'
- en: '| `StopReplica` | `SyncGroup` | `TxnOffsetCommit` |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `StopReplica` | `SyncGroup` | `TxnOffsetCommit` |'
- en: '| `UpdateMetadata` | `WriteTxnMarkers` |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `UpdateMetadata` | `WriteTxnMarkers` |  |'
- en: For each of these requests, there are eight metrics provided, providing insight
    into each phase of the request processing. For example, for the `Fetch` request,
    the metrics shown in [Table 13-15](#table10_1) are available.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个请求，提供了八个度量标准，提供了对请求处理的每个阶段的洞察。例如，对于`Fetch`请求，在[Table 13-15](#table10_1)中显示的度量标准是可用的。
- en: Table 13-15\. Fetch request metrics
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-15。获取请求度量
- en: '| Name | JMX MBean |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | JMX MBean |'
- en: '| --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Total time | `kafka.network:``type=RequestMetrics,name=TotalTimeMs,request=Fetch`
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 总时间 | `kafka.network:``type=RequestMetrics,name=TotalTimeMs,request=Fetch`
    |'
- en: '| Request queue time | `kafka.network:``type=RequestMetrics,name=RequestQueueTimeMs,request=Fetch`
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 请求队列时间 | `kafka.network:``type=RequestMetrics,name=RequestQueueTimeMs,request=Fetch`
    |'
- en: '| Local time | `kafka.network:``type=RequestMetrics,name=LocalTimeMs,request=Fetch`
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 本地时间 | `kafka.network:``type=RequestMetrics,name=LocalTimeMs,request=Fetch`
    |'
- en: '| Remote time | `kafka.network:``type=RequestMetrics,name=RemoteTimeMs,request=Fetch`
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 远程时间 | `kafka.network:``type=RequestMetrics,name=RemoteTimeMs,request=Fetch`
    |'
- en: '| Throttle time | `kafka.network:``type=RequestMetrics,name=ThrottleTimeMs,request=Fetch`
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 限流时间 | `kafka.network:``type=RequestMetrics,name=ThrottleTimeMs,request=Fetch`
    |'
- en: '| Response queue time | `kafka.network:``type=RequestMetrics,name=ResponseQueueTimeMs,request=Fetch`
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 响应队列时间 | `kafka.network:``type=RequestMetrics,name=ResponseQueueTimeMs,request=Fetch`
    |'
- en: '| Response send time | `kafka.network:``type=RequestMetrics,name=ResponseSendTimeMs,request=Fetch`
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: 响应发送时间 | `kafka.network:``type=RequestMetrics,name=ResponseSendTimeMs,request=Fetch`
    |
- en: '| Requests per second | `kafka.network:``type=RequestMetrics,name=RequestsPerSec,request=Fetch`
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 每秒请求数 | `kafka.network:``type=RequestMetrics,name=RequestsPerSec,request=Fetch`
    |'
- en: The requests per second metric is a rate metric, as discussed earlier, and shows
    the total number of that type of request that has been received and processed
    over the time unit. This provides a view into the frequency of each request time,
    though it should be noted that many of the requests, such as `StopReplica` and
    `UpdateMetadata`, are infrequent.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒请求数度量是一个速率度量，如前所述，显示了在时间单位内接收和处理的该类型请求的总数。这提供了对每个请求时间频率的视图，尽管应该注意到，许多请求，如`StopReplica`和`UpdateMetadata`，是不频繁的。
- en: 'The seven *time* metrics each provide a set of percentiles for requests, as
    well as a discrete `Count` attribute, similar to rate metrics. The metrics are
    all calculated since the broker was started, so keep that in mind when looking
    at metrics that do not change for long periods of time; the longer your broker
    has been running, the more stable the numbers will be. The parts of request processing
    they represent are:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 七个*时间*度量标准为每个请求提供了一组百分位数，以及一个离散的`Count`属性，类似于速率度量标准。这些度量标准都是自代理启动以来计算的，因此在查看长时间不变的度量标准时要记住这一点；您的代理运行时间越长，数字就会越稳定。它们代表请求处理的部分是：
- en: Total time
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 总时间
- en: The total amount of time the broker spends processing the request, from receiving
    it to sending the response back to the requester
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 代理商处理请求所花费的总时间，从接收到发送响应给请求者
- en: Request queue time
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请求队列时间
- en: The amount of time the request spends in queue after it has been received but
    before processing starts
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请求在接收后但在处理开始之前在队列中花费的时间
- en: Local time
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本地时间
- en: The amount of time the partition leader spends processing a request, including
    sending it to disk (but not necessarily flushing it)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 分区领导者处理请求所花费的时间，包括将其发送到磁盘（但不一定刷新它）
- en: Remote time
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 远程时间
- en: The amount of time spent waiting for the followers before request processing
    can complete
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求处理完成之前等待追随者所花费的时间
- en: Throttle time
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 限流时间
- en: The amount of time the response must be held in order to slow the requestor
    down to satisfy client quota settings
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 响应必须保持的时间，以减慢请求者以满足客户端配额设置
- en: Response queue time
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 响应队列时间
- en: The amount of time the response to the request spends in the queue before it
    can be sent to the requestor
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 响应请求在发送给请求者之前在队列中花费的时间
- en: Response send time
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 响应发送时间
- en: The amount of time spent actually sending the response
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 实际发送响应所花费的时间
- en: 'The attributes provided for each metric are:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 每个度量标准提供的属性是：
- en: '`Count`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`计数`'
- en: Absolute count of number of requests since process start
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 自进程启动以来请求的绝对数量
- en: '`Min`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`最小值`'
- en: Minimum value for all requests
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 所有请求的最小值
- en: '`Max`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`最大值`'
- en: Maximum value for all requests
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 所有请求的最大值
- en: '`Mean`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`平均值`'
- en: Average value for all requests
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 所有请求的平均值
- en: '`StdDev`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`标准差`'
- en: The standard deviation of the request timing measurements as a whole
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 请求时间测量的标准差
- en: '`Percentiles`'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`百分位数`'
- en: '`50thPercentile`, `75thPercentile`, `95thPercentile`, `98thPercentile`, `99thPercentile`,
    `999thPercentile`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`50thPercentile`, `75thPercentile`, `95thPercentile`, `98thPercentile`, `99thPercentile`,
    `999thPercentile`'
- en: What Is a Percentile?
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是百分位数？
- en: Percentiles are a common way of looking at timing measurement. A 99th percentile
    measurement tells us that 99% of all values in the sample group (request timings,
    in this case) are less than the value of the metric. This means that 1% of the
    values are greater than the value specified. A common pattern is to view the average
    value and the 99% or 99.9% value. In this way, you can understand how the average
    request performs and what the outliers are.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 百分位数是查看时间测量的常见方法。第99百分位数测量告诉我们，样本组（在本例中为请求时间）中99%的所有值都小于指标的值。这意味着1%的值大于指定的值。常见的模式是查看平均值和99%或99.9%的值。通过这种方式，您可以了解平均请求的性能以及异常值是什么。
- en: Out of all of these metrics and attributes for requests, which are the important
    ones to monitor? At a minimum, you should collect at least the average and one
    of the higher percentiles (either 99% or 99.9%) for the total time metric, as
    well as the requests per second metric, for every request type. This gives a view
    into the overall performance of requests to the Kafka broker. If you can, you
    should also collect those measurements for the other six timing metrics for each
    request type, as this will allow you to narrow down any performance problems to
    a specific phase of request processing.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些请求的指标和属性中，哪些是重要的要监视的？至少，您应该至少收集每种请求类型的总时间指标的平均值和较高百分位数（99%或99.9%之一），以及每秒请求指标。这可以让您了解对Kafka经纪人的请求的整体性能。如果可以的话，您还应该为每种请求类型收集其他六个时间度量的测量，因为这将使您能够将任何性能问题缩小到请求处理的特定阶段。
- en: For setting alert thresholds, the timing metrics can be difficult. The timing
    for a `Fetch` request, for example, can vary wildly depending on many factors,
    including settings on the client for how long it will wait for messages, how busy
    the particular topic being fetched is, and the speed of the network connection
    between the client and the broker. It can be very useful, however, to develop
    a baseline value for the 99.9th percentile measurement for at least the total
    time, especially for `Produce` requests, and alert on this. Much like the under-replicated
    partitions metric, a sharp increase in the 99.9th percentile for `Produce` requests
    can alert you to a wide range of performance problems.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于设置警报阈值，时间度量可能会很困难。例如，`Fetch`请求的时间度量可能会因许多因素而大幅变化，包括客户端设置等待消息的时间、被获取的特定主题的繁忙程度以及客户端与经纪人之间的网络连接速度。然而，为至少`Produce`请求的总时间开发99.9th百分位数测量的基线值并对其进行警报可能非常有用。与未复制分区指标类似，`Produce`请求的99.9th百分位数急剧增加可能会提醒您存在各种性能问题。
- en: Topic and Partition Metrics
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题和分区指标
- en: In addition to the many metrics available on the broker that describe the operation
    of the Kafka broker in general, there are topic- and partition-specific metrics.
    In larger clusters these can be numerous, and it may not be possible to collect
    all of them into a metrics system as a matter of normal operations. However, they
    are quite useful for debugging specific issues with a client. For example, the
    topic metrics can be used to identify a specific topic that is causing a large
    increase in traffic to the cluster. It also may be important to provide these
    metrics so that users of Kafka (the producer and consumer clients) are able to
    access them. Regardless of whether you are able to collect these metrics regularly,
    you should be aware of what is useful.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 除了经纪人上可用的许多指标来描述Kafka经纪人的运行情况外，还有特定于主题和分区的指标。在较大的集群中，这些指标可能很多，可能无法将它们全部收集到指标系统中作为正常操作的一部分。然而，它们对于调试客户端的特定问题非常有用。例如，主题指标可用于识别导致集群流量大幅增加的特定主题。还可能重要的是提供这些指标，以便Kafka的用户（生产者和消费者客户端）能够访问它们。无论您是否能够定期收集这些指标，您都应该知道哪些是有用的。
- en: For all the examples in [Table 13-16](#table10_2), we will be using the example
    topic name `*TOPICNAME*`, as well as partition 0\. When accessing the metrics
    described, make sure to substitute the topic name and partition number that are
    appropriate for your cluster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[表13-16](#table10_2)中的所有示例，我们将使用示例主题名称`*TOPICNAME*`，以及分区0。在访问所描述的指标时，请确保替换适合您集群的主题名称和分区号。
- en: Per-topic metrics
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个主题的指标
- en: For all the per-topic metrics, the measurements are very similar to the broker
    metrics described previously. In fact, the only difference is the provided topic
    name, and that the metrics will be specific to the named topic. Given the sheer
    number of metrics available, depending on the number of topics present in your
    cluster, these will almost certainly be metrics that you will not want to set
    up monitoring and alerts for. They are useful to provide to clients, however,
    so that they can evaluate and debug their own usage of Kafka.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有每个主题的指标，测量非常类似于先前描述的经纪人指标。实际上，唯一的区别是提供的主题名称，以及指标将特定于命名的主题。鉴于可用的指标数量庞大，取决于集群中存在的主题数量，这些几乎肯定是您不希望为其设置监视和警报的指标。然而，它们对于提供给客户非常有用，以便他们可以评估和调试他们对Kafka的使用。
- en: Table 13-16\. Metrics for each topic
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-16。每个主题的指标
- en: '| Name | JMX MBean |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | JMX MBean |'
- en: '| --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Bytes in rate | `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 每秒输入字节数 | `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=*TOPICNAME*`
    |'
- en: '| Bytes out rate | `kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 每秒输出字节数 | `kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=*TOPICNAME*`
    |'
- en: '| Failed fetch rate | `kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 失败获取率 | `kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec,topic=*TOPICNAME*`
    |'
- en: '| Failed produce rate | `kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 失败生成率 | `kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec,topic=*TOPICNAME*`
    |'
- en: '| Messages in rate | `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 每秒消息数 | `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=*TOPICNAME*`
    |'
- en: '| Fetch request rate | `kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 获取请求速率 | `kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec,topic=*TOPICNAME*`
    |'
- en: '| Produce request rate | `kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec,topic=*TOPICNAME*`
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 生产请求速率 | `kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec,topic=*TOPICNAME*`
    |'
- en: Per-partition metrics
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个分区的指标
- en: The per-partition metrics tend to be less useful on an ongoing basis than the
    per-topic metrics. Additionally, they are quite numerous as hundreds of topics
    can easily be thousands of partitions. Nevertheless, they can be useful in some
    limited situations. In particular, the partition-size metric indicates the amount
    of data (in bytes) that is currently being retained on disk for the partition
    ([Table 13-17](#table1013)). Combined, these will indicate the amount of data
    retained for a single topic, which can be useful in allocating costs for Kafka
    to individual clients. A discrepancy between the size of two partitions for the
    same topic can indicate a problem where the messages are not evenly distributed
    across the key that is being used when producing. The log-segment count metric
    shows the number of log-segment files on disk for the partition. This may be useful
    along with the partition size for resource tracking.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区的指标在持续基础上往往不如每个主题的指标有用。此外，它们相当多，因为数百个主题很容易成千上万个分区。尽管如此，在某些有限的情况下它们可能是有用的。特别是，分区大小指标指示当前在分区上保留在磁盘上的数据量（以字节为单位）（[表13-17](#table1013)）。结合起来，这些将指示单个主题保留的数据量，这对于将Kafka的成本分配给个别客户端可能是有用的。同一主题的两个分区大小之间的差异可能表明存在问题，即在生成时消息未均匀分布在使用的键上。日志段计数指标显示分区上磁盘上的日志段文件数。这可能与分区大小一起用于资源跟踪。
- en: Table 13-17\. Metrics for each partition
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-17。每个分区的指标
- en: '| Name | JMX MBean |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | JMX MBean |'
- en: '| --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Partition size | `kafka.log:type=Log,name=Size,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 分区大小 | `kafka.log:type=Log,name=Size,topic=*TOPICNAME*,partition=0` |'
- en: '| Log segment count | `kafka.log:type=Log,name=NumLogSegments,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 日志段计数 | `kafka.log:type=Log,name=NumLogSegments,topic=*TOPICNAME*,partition=0`
    |'
- en: '| Log end offset | `kafka.log:type=Log,name=LogEndOffset,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 日志结束偏移量 | `kafka.log:type=Log,name=LogEndOffset,topic=*TOPICNAME*,partition=0`
    |'
- en: '| Log start offset | `kafka.log:type=Log,name=LogStartOffset,topic=*TOPICNAME*,partition=0`
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: 日志起始偏移量 | `kafka.log:type=Log,name=LogStartOffset,topic=*TOPICNAME*,partition=0`
    |
- en: The log end offset and log start offset metrics are the highest and lowest offsets
    for messages in that partition, respectively. It should be noted, however, that
    the difference between these two numbers does not necessarily indicate the number
    of messages in the partition, as log compaction can result in “missing” offsets
    that have been removed from the partition due to newer messages with the same
    key. In some environments, it could be useful to track these offsets for a partition.
    One such use case is to provide a more granular mapping of timestamp to offset,
    allowing for consumer clients to easily roll back offsets to a specific time (though
    this is less important with time-based index searching, introduced in Kafka 0.10.1).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 日志结束偏移量和日志起始偏移量指标分别是该分区中消息的最高和最低偏移量。然而，应该注意的是，这两个数字之间的差异并不一定表示分区中的消息数量，因为日志压缩可能导致已从分区中删除具有相同键的新消息而“丢失”的偏移量。在某些环境中，跟踪分区的这些偏移量可能是有用的。其中一个用例是提供时间戳到偏移量的更精细的映射，从而允许使用者客户端轻松地将偏移量回滚到特定时间（尽管在Kafka
    0.10.1中引入了基于时间的索引搜索，这就不那么重要了）。
- en: Under-Replicated Partition Metrics
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未复制分区指标
- en: There is a per-partition metric provided to indicate whether or not the partition
    is under-replicated. In general, this is not very useful in day-to-day operations,
    as there are too many metrics to gather and watch. It is much easier to monitor
    the broker-wide under-replicated partition count and then use the command-line
    tools (described in [Chapter 12](ch12.html#administering_kafka)) to determine
    the specific partitions that are under-replicated.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个每个分区的指标，用于指示分区是否未复制。一般来说，在日常操作中，这并不是非常有用，因为有太多的指标需要收集和监视。更容易的方法是监视整个代理范围内的未复制分区计数，然后使用命令行工具（在[第12章](ch12.html#administering_kafka)中描述）来确定未复制的特定分区。
- en: JVM Monitoring
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JVM监控
- en: In addition to the metrics provided by the Kafka broker, you should be monitoring
    a standard suite of measurements for all of your servers, as well as the Java
    Virtual Machine (JVM) itself. These will be useful to alert you to a situation,
    such as increasing garbage collection activity, that will degrade the performance
    of the broker. They will also provide insight into why you see changes in metrics
    downstream in the broker.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Kafka代理提供的指标之外，您还应该监视所有服务器以及Java虚拟机（JVM）本身的标准套件测量。这些将有助于警示您某种情况，例如增加的垃圾回收活动，这将降低代理的性能。它们还将提供有关为什么在代理中看到指标变化的见解。
- en: Garbage collection
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 垃圾回收
- en: For the JVM, the critical thing to monitor is the status of garbage collection
    (GC). The particular beans that you must monitor for this information will vary
    depending on the particular Java Runtime Environment (JRE) that you are using,
    as well as the specific GC settings in use. For an Oracle Java 1.8 JRE running
    with G1 garbage collection, the beans to use are shown in [Table 13-18](#table1014).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 对于JVM来说，需要监控的关键事项是垃圾回收（GC）的状态。您必须监视此信息的特定bean将取决于您使用的特定Java运行时环境（JRE），以及正在使用的特定GC设置。对于在Oracle
    Java 1.8 JRE上运行G1垃圾回收的情况，应使用的bean显示在[表13-18](#table1014)中。
- en: Table 13-18\. G1 garbage collection metrics
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-18。G1垃圾回收指标
- en: '| Name | JMX MBean |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | JMX MBean |'
- en: '| --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Full GC cycles | `java.lang:type=GarbageCollector,name=G1 Old Generation`
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 完整GC周期 | `java.lang:type=GarbageCollector,name=G1 Old Generation` |'
- en: '| Young GC cycles | `java.lang:type=GarbageCollector,name=G1 Young Generation`
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 年轻GC周期 | `java.lang:type=GarbageCollector,name=G1 Young Generation` |'
- en: Note that in the semantics of GC, “Old” and “Full” are the same thing. For each
    of these metrics, the two attributes to watch are `CollectionCount` and `CollectionTime`.
    The `CollectionCount` is the number of GC cycles of that type (Full or Young)
    since the JVM was started. The `CollectionTime` is the amount of time, in milliseconds,
    spent in that type of GC cycle since the JVM was started. As these measurements
    are counters, they can be used by a metrics system to tell you an absolute number
    of GC cycles and time spent in GC per unit of time. They can also be used to provide
    an average amount of time per GC cycle, though this is less useful in normal operations.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在GC的语义中，“Old”和“Full”是相同的。对于这些指标，要关注的两个属性是`CollectionCount`和`CollectionTime`。`CollectionCount`是自JVM启动以来该类型（完整或年轻）的GC周期数。`CollectionTime`是自JVM启动以来在该类型的GC周期中花费的时间（以毫秒为单位）。由于这些测量值是计数器，因此可以由度量系统用来告诉您每单位时间的GC周期数和GC花费的时间。它们还可以用来提供每个GC周期的平均时间，尽管在正常操作中这并不太有用。
- en: Each of these metrics also has a `LastGcInfo` attribute. This is a composite
    value, made up of five fields, that gives you information on the last GC cycle
    for the type of GC described by the bean. The important value to look at is the
    `duration` value, as this tells you how long, in milliseconds, the last GC cycle
    took. The other values in the composite (`GcThreadCount`, `id`, `startTime`, and
    `endTime`) are informational and not very useful. It’s important to note that
    you will not be able to see the timing of every GC cycle using this attribute,
    as young GC cycles in particular can happen frequently.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 每个指标还有一个`LastGcInfo`属性。这是一个复合值，由五个字段组成，为您提供有关由bean描述的GC类型的最后一个GC周期的信息。要查看的重要值是`duration`值，因为这告诉您上一个GC周期花费了多长时间（以毫秒为单位）。复合值中的其他值（`GcThreadCount`、`id`、`startTime`和`endTime`）是信息性的，并且没有太大用处。重要的是要注意，使用此属性，您将无法看到每个GC周期的时间，特别是年轻的GC周期可能经常发生。
- en: Java OS monitoring
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Java操作系统监控
- en: The JVM can provide you with some information on the OS through the `java.lang:type=OperatingSystem`
    bean. However, this information is limited and does not represent everything you
    need to know about the system running your broker. The two attributes that can
    be collected here that are of use, which are difficult to collect in the OS, are
    the `MaxFileDescriptorCount` and `OpenFileDescriptorCount` attributes. `MaxFileDescriptorCount`
    will tell you the maximum number of file descriptors (FDs) that the JVM is allowed
    to have open. The `OpenFileDescriptorCount` attribute tells you the number of
    FDs that are currently open. There will be FDs open for every log segment and
    network connection, and they can add up quickly. A problem closing network connections
    properly could cause the broker to rapidly exhaust the number allowed.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: JVM可以通过`java.lang:type=OperatingSystem` bean向您提供有关操作系统的一些信息。但是，这些信息有限，不能代表您需要了解的有关运行代理的系统的一切。可以在此处收集的两个有用属性，这些属性在操作系统中难以收集，分别是`MaxFileDescriptorCount`和`OpenFileDescriptorCount`属性。`MaxFileDescriptorCount`将告诉您JVM允许打开的文件描述符（FDs）的最大数量。`OpenFileDescriptorCount`属性告诉您当前打开的FDs的数量。每个日志段和网络连接都会打开FDs，并且它们可能会迅速累积。无法正确关闭网络连接可能会导致代理迅速耗尽允许的数量。
- en: OS Monitoring
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作系统监控
- en: The JVM cannot provide us with all the information that we need to know about
    the system it is running on. For this reason, we must not only collect metrics
    from the broker but also from the OS itself. Most monitoring systems will provide
    agents that will collect more OS information than you could possibly be interested
    in. The main areas that are necessary to watch are CPU usage, memory usage, disk
    usage, disk I/O, and network usage.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: JVM无法为我们提供关于其运行系统的所有信息。因此，我们不仅必须从代理收集经纪人的指标，还必须从操作系统本身收集指标。大多数监控系统将提供代理，这些代理将收集比您可能感兴趣的更多的操作系统信息。必须监视的主要领域是CPU使用率、内存使用率、磁盘使用率、磁盘I/O和网络使用率。
- en: 'For CPU utilization, you will want to look at the system load average at the
    very least. This provides a single number that will indicate the relative utilization
    of the processors. In addition, it may also be useful to capture the percent usage
    of the CPU, broken down by type. Depending on the method of collection and your
    particular OS, you may have some or all of the following CPU percentage breakdowns
    (provided with the abbreviation used):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CPU利用率，至少要查看系统负载平均值。这提供了一个单一的数字，指示处理器的相对利用率。此外，捕获按类型分解的CPU使用率的百分比也可能很有用。根据收集方法和特定的操作系统，您可能具有以下CPU百分比分解中的一些或全部（使用的缩写提供）：
- en: '`us`'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`us`'
- en: The time spent in user space
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户空间中所花费的时间
- en: '`sy`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`sy`'
- en: The time spent in kernel space
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核空间中所花费的时间
- en: '`ni`'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '`ni`'
- en: The time spent on low-priority processes
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 低优先级进程所花费的时间
- en: '`id`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`id`'
- en: The time spent idle
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 空闲时间
- en: '`wa`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`wa`'
- en: The time spent in wait (on disk)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在等待（在磁盘上）所花费的时间
- en: '`hi`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '`hi`'
- en: The time spent handling hardware interrupts
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 处理硬件中断所花费的时间
- en: '`si`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`si`'
- en: The time spent handling software interrupts
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 处理软件中断所花费的时间
- en: '`st`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`st`'
- en: The time waiting for the hypervisor
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 等待虚拟处理器的时间
- en: What Is System Load?
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是系统负载？
- en: While many know that system load is a measure of CPU usage on a system, most
    people misunderstand how it is measured. The load average is a count of the number
    of processes that are runnable and are waiting for a processor to execute on.
    Linux also includes threads that are in an uninterruptable sleep state, such as
    waiting for the disk. The load is presented as three numbers, which is the count
    averaged over the last minute, 5 minutes, and 15 minutes. In a single CPU system,
    a value of 1 would mean the system is 100% loaded, with a thread always waiting
    to execute. This means that on a multiple CPU system, the load average number
    that indicates 100% is equal to the number of CPUs in the system. For example,
    if there are 24 processors in the system, 100% would be a load average of 24.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多人知道系统负载是系统上CPU使用率的一个度量，但大多数人误解了它是如何被测量的。负载平均值是等待执行的进程数量的计数。Linux还包括处于不可中断睡眠状态的线程，比如等待磁盘的线程。负载以三个数字呈现，这是在过去一分钟、5分钟和15分钟内的平均计数。在单CPU系统中，值为1意味着系统负载100%，始终有一个线程在等待执行。这意味着在多CPU系统上，表示100%的负载平均数等于系统中的CPU数量。例如，如果系统中有24个处理器，100%将是24的负载平均值。
- en: The Kafka broker uses a significant amount of processing for handling requests.
    For this reason, keeping track of the CPU utilization is important when monitoring
    Kafka. Memory is less important to track for the broker itself, as Kafka will
    normally be run with a relatively small JVM heap size. It will use a small amount
    of memory outside of the heap for compression functions, but most of the system
    memory will be left to be used for cache. All the same, you should keep track
    of memory utilization to make sure other applications do not infringe on the broker.
    You will also want to make sure that swap memory is not being used by monitoring
    the amount of total and free swap memory.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka代理在处理请求时使用了大量的处理。因此，在监控Kafka时，跟踪CPU利用率是很重要的。对于代理本身来说，内存跟踪不那么重要，因为Kafka通常会以相对较小的JVM堆大小运行。它将在堆之外使用少量内存进行压缩功能，但大部分系统内存将被留用于缓存。尽管如此，您应该跟踪内存利用率，以确保其他应用程序不会侵占代理。您还需要确保没有使用交换内存，通过监控总交换内存和空闲交换内存的数量。
- en: Disk is by far the most important subsystem when it comes to Kafka. All messages
    are persisted to disk, so the performance of Kafka depends heavily on the performance
    of the disks. Monitoring usage of both disk space and inodes (*inodes* are the
    file and directory metadata objects for Unix filesystems) is important, as you
    need to assure that you are not running out of space. This is especially true
    for the partitions where Kafka data is being stored. It is also necessary to monitor
    the disk I/O statistics, as this will tell us that the disk is being used efficiently.
    For at least the disks where Kafka data is stored, monitor the reads and writes
    per second, the average read and write queue sizes, the average wait time, and
    the utilization percentage of the disk.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及Kafka时，磁盘是最重要的子系统。所有消息都持久保存在磁盘上，因此Kafka的性能严重依赖于磁盘的性能。监控磁盘空间和inode（*inode*是Unix文件系统的文件和目录元数据对象）的使用情况很重要，因为您需要确保没有空间。这对于存储Kafka数据的分区尤为重要。还需要监控磁盘I/O统计信息，因为这将告诉我们磁盘是否被有效地使用。对于存储Kafka数据的磁盘，至少要监控每秒的读写次数，平均读写队列大小，平均等待时间和磁盘的利用率。
- en: Finally, monitor the network utilization on the brokers. This is simply the
    amount of inbound and outbound network traffic, normally reported in bits per
    second. Keep in mind that every bit inbound to the Kafka broker will be a number
    of bits outbound equal to the replication factor of the topics, not including
    consumers. Depending on the number of consumers, outbound network traffic could
    easily be an order of magnitude larger than inbound traffic. Keep this in mind
    when setting thresholds for alerts.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，监控代理的网络利用率。这只是入站和出站网络流量的数量，通常以每秒位数报告。请记住，发送到Kafka代理的每个位都将是等于主题的复制因子的出站位数，不包括消费者。根据消费者的数量，出站网络流量可能比入站流量容易大一个数量级。在设置警报阈值时要记住这一点。
- en: Logging
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志
- en: No discussion of monitoring is complete without a word about logging. Like many
    applications, the Kafka broker will fill disks with log messages in minutes if
    you let it. In order to get useful information from logging, it is important to
    enable the right loggers at the right levels. By simply logging all messages at
    the `INFO` level, you will capture a significant amount of important information
    about the state of the broker. It is useful to separate a couple of loggers from
    this, however, in order to provide a cleaner set of log files.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的讨论没有日志是不完整的。像许多应用程序一样，如果你允许的话，Kafka代理将在几分钟内用日志消息填满磁盘。为了从日志中获得有用的信息，重要的是在正确的级别启用正确的记录器。通过简单地在“INFO”级别记录所有消息，您将捕获关于代理状态的大量重要信息。然而，为了提供一组更清洁的日志文件，有必要从中分离出一些记录器。
- en: There are two loggers writing to separate files on disk. The first is `kafka.controller`,
    still at the `INFO` level. This logger is used to provide messages specifically
    regarding the cluster controller. At any time, only one broker will be the controller,
    and therefore only one broker will be writing to this logger. The information
    includes topic creation and modification, broker status changes, and cluster activities
    such as preferred replica elections and partition moves. The other logger to separate
    is `kafka.server.ClientQuotaManager`, also at the `INFO` level. This logger is
    used to show messages related to produce and consume quota activities. While this
    is useful information, it is better to not have it in the main broker log file.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个记录器分别写入磁盘上的不同文件。第一个是 `kafka.controller`，仍然处于 `INFO` 级别。此记录器用于提供关于集群控制器的特定消息。任何时候，只有一个经纪人将成为控制器，因此只有一个经纪人将写入此记录器。信息包括主题创建和修改、经纪人状态更改以及集群活动，如首选副本选举和分区移动。另一个分开的记录器是
    `kafka.server.ClientQuotaManager`，也处于 `INFO` 级别。此记录器用于显示与生产和消费配额活动相关的消息。虽然这是有用的信息，但最好不要将其放在主经纪人日志文件中。
- en: It is also helpful to log information regarding the status of the log compaction
    threads. There is no single metric to show the health of these threads, and it
    is possible for failure in compaction of a single partition to halt the log compaction
    threads entirely, and silently. Enabling the `kafka.log.LogCleaner`, `kafka.log.Cleaner`,
    and `kafka.log.LogCleanerManager` loggers at the `DEBUG` level will output information
    about the status of these threads. This will include information about each partition
    being compacted, including the size and number of messages in each. Under normal
    operations, this is not a lot of logging, which means that it can be enabled by
    default without overwhelming you.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志以了解日志压缩线程的状态也是有帮助的。没有单个指标可以显示这些线程的健康状况，一个分区的压缩失败可能会完全停止日志压缩线程，并且悄无声息。在 `DEBUG`
    级别启用 `kafka.log.LogCleaner`、`kafka.log.Cleaner` 和 `kafka.log.LogCleanerManager`
    记录器将输出有关这些线程状态的信息。这将包括有关正在压缩的每个分区的信息，包括每个分区中的消息大小和数量。在正常操作下，这不是很多的日志记录，这意味着可以默认启用它而不会使您不堪重负。
- en: There is also some logging that may be useful to turn on when debugging issues
    with Kafka. One such logger is `kafka.request.logger`, turned on at either the
    `DEBUG` or `TRACE` levels. This logs information about every request sent to the
    broker. At the `DEBUG` level, the log includes connection end points, request
    timings, and summary information. At the `TRACE` level, it will also include topic
    and partition information—nearly all request information short of the message
    payload itself. At either level, this logger generates a significant amount of
    data, and it is not recommended to enable it unless necessary for debugging.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些日志记录可能在调试Kafka问题时有用。其中一个记录器是 `kafka.request.logger`，在 `DEBUG` 或 `TRACE`
    级别打开。这将记录发送到经纪人的每个请求的信息。在 `DEBUG` 级别，日志包括连接端点、请求时间和摘要信息。在 `TRACE` 级别，它还将包括主题和分区信息——几乎所有请求信息，除了消息有效载荷本身。在任何级别，此记录器会生成大量数据，除非必要进行调试，否则不建议启用它。
- en: Client Monitoring
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端监控
- en: All applications need monitoring. Those that instantiate a Kafka client, either
    a producer or consumer, have metrics specific to the client that should be captured.
    This section covers the official Java client libraries, though other implementations
    should have their own measurements available.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 所有应用程序都需要监控。实例化Kafka客户端（生产者或消费者）的应用程序具有应该捕获的特定于客户端的指标。本节涵盖了官方的Java客户端库，尽管其他实现应该有它们自己的可用测量。
- en: Producer Metrics
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产者指标
- en: The Kafka producer client has greatly compacted the metrics available by making
    them available as attributes on a small number of JMX MBeans. In contrast, the
    previous version of the producer client (which is no longer supported) used a
    larger number of MBeans but had more detail in many of the metrics (providing
    a greater number of percentile measurements and different moving averages). As
    a result, the overall number of metrics provided covers a wider surface area,
    but it can be more difficult to track outliers.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka生产者客户端通过将可用的指标作为少量JMX MBean的属性而大大压缩了这些指标。相比之下，之前的生产者客户端（不再受支持）使用了更多的MBean，但在许多指标中有更多的细节（提供了更多的百分位数测量和不同的移动平均值）。因此，提供的指标总数涵盖了更广泛的范围，但更难以跟踪异常值。
- en: All of the producer metrics have the client ID of the producer client in the
    bean names. In the examples provided, this has been replaced with `*CLIENTID*`.
    Where a bean name contains a broker ID, this has been replaced with `*BROKERID*`.
    Topic names have been replaced with `*TOPICNAME*`. See [Table 13-19](#table10_5)
    for an example.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 所有生产者指标在bean名称中都有生产者客户端的客户端ID。在提供的示例中，这已被替换为 `*CLIENTID*`。其中bean名称包含经纪人ID，这已被替换为
    `*BROKERID*`。主题名称已被替换为 `*TOPICNAME*`。有关示例，请参见[表13-19](#table10_5)。
- en: Table 13-19\. Kafka producer metric MBeans
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-19. Kafka生产者指标MBeans
- en: '| Name | JMX MBean |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | JMX MBean |'
- en: '| --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Overall producer | `kafka.producer:type=producer-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 总体生产者 | `kafka.producer:type=producer-metrics,client-id=*CLIENTID*` |'
- en: '| Per-broker | `kafka.producer:type=producer-node-metrics,client-id=*CLIENTID*,node-id=node-*BROKERID*`
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 按经纪人 | `kafka.producer:type=producer-node-metrics,client-id=*CLIENTID*,node-id=node-*BROKERID*`
    |'
- en: '| Per-topic | `kafka.producer:type=producer-topic-metrics,client-id=*CLIENTID*,topic=*TOPICNAME*`
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 按主题 | `kafka.producer:type=producer-topic-metrics,client-id=*CLIENTID*,topic=*TOPICNAME*`
    |'
- en: Each of the metric beans in [Table 13-19](#table10_5) has multiple attributes
    available to describe the state of the producer. The particular attributes that
    are of the most use are described in the next section. Before proceeding, be sure
    you understand the semantics of how the producer works, as described in [Chapter 3](ch03.html#writing_messages_to_kafka).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[表13-19](#table10_5)中的每个指标bean都有多个属性可用于描述生产者的状态。下一节将描述最有用的特定属性。在继续之前，请确保您了解生产者的工作语义，如[第3章](ch03.html#writing_messages_to_kafka)中所述。'
- en: Overall producer metrics
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体生产者指标
- en: The overall producer metrics bean provides attributes describing everything
    from the sizes of the message batches to the memory buffer utilization. While
    all of these measurements have their place in debugging, there are only a handful
    needed on a regular basis, and only a couple of those that should be monitored
    and have alerts. Note that while we will discuss several metrics that are averages
    (ending in `-avg`), there are also maximum values for each metric (ending in `-max`)
    that have limited usefulness.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 总体生产者指标bean提供了描述消息批次大小到内存缓冲区利用率的属性。虽然所有这些测量都在调试中有其用处，但通常只有少数需要定期使用，其中只有少数需要监视并设置警报。请注意，虽然我们将讨论几个平均值的指标（以“-avg”结尾），但每个指标也有最大值（以“-max”结尾），其有限的用处。
- en: The `record-error-rate` is one attribute that you will definitely want to set
    an alert for. This metric should always be zero, and if it is anything greater
    than that, the producer is dropping messages it is trying to send to the Kafka
    brokers. The producer has a configured number of retries and a backoff between
    those, and once that has been exhausted, the messages (called *records* here)
    will be dropped. There is also a `record-retry-rate` attribute that can be tracked,
    but it is less critical than the error rate because retries are normal.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '`record-error-rate`是一个绝对需要设置警报的属性。这个指标应该始终为零，如果大于零，表示生产者正在丢弃它试图发送到Kafka代理的消息。生产者有一个配置的重试次数和重试之间的间隔，一旦耗尽，消息（这里称为*记录*）将被丢弃。还有一个
    `record-retry-rate`属性可以进行跟踪，但它不像错误率那么关键，因为重试是正常的。'
- en: The other metric to alert on is the `request-latency-avg`. This is the average
    amount of time a produce request sent to the brokers takes. You should be able
    to establish a baseline value for what this number should be in normal operations,
    and set an alert threshold above that. An increase in the request latency means
    that produce requests are getting slower. This could be due to networking issues,
    or it could indicate problems on the brokers. Either way, it’s a performance issue
    that will cause back pressure and other problems in your producing application.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要警报的指标是 `request-latency-avg`。这是发送到代理的生成请求所花费的平均时间。您应该能够建立正常操作中此数字应该是多少的基线值，并设置高于该值的警报阈值。请求延迟的增加意味着生成请求变慢。这可能是由于网络问题，也可能表明代理存在问题。无论哪种情况，这都是一个性能问题，会在生成应用程序中引起背压和其他问题。
- en: In addition to these critical metrics, it is always good to know how much message
    traffic your producer is sending. Three attributes will provide three different
    views of this. The `outgoing-byte-rate` describes the messages in absolute size
    in bytes per second. The `record-send-rate` describes the traffic in terms of
    the number of messages produced per second. Finally, the `request-rate` provides
    the number of produce requests sent to the brokers per second. A single request
    contains one or more batches. A single batch contains one or more messages. And,
    of course, each message is made up of some number of bytes. These metrics are
    all useful to have on an application dashboard.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些关键指标之外，了解生产者发送了多少消息流量总是很有用的。三个属性将提供这方面的三种不同视图。 `outgoing-byte-rate`描述了每秒以字节为单位的绝对大小的消息。
    `record-send-rate`描述了每秒产生的消息数量。最后，`request-rate`提供了每秒发送到代理的生成请求的数量。单个请求包含一个或多个批次。单个批次包含一个或多个消息。当然，每个消息由一些字节组成。这些指标都对应用程序仪表板上有用。
- en: There are also metrics that describe the size of records, requests, and batches.
    The `request-size-avg` metric provides the average size of the produce requests
    being sent to the brokers in bytes. The `batch-size-avg` provides the average
    size of a single message batch (which, by definition, is comprised of messages
    for a single topic partition) in bytes. The `record-size-avg` shows the average
    size of a single record in bytes. For a single-topic producer, this provides useful
    information about the messages being produced. For multiple-topic producers, such
    as MirrorMaker, it is less informative. Besides these three metrics, there is
    a `records-per-request-avg` metric that describes the average number of messages
    that are in a single produce request.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些描述记录、请求和批处理大小的指标。 `request-size-avg`指标提供了发送到代理的生产请求的平均大小（以字节为单位）。 `batch-size-avg`提供了单个消息批次的平均大小（根据定义，批次由单个主题分区的消息组成），以字节为单位。
    `record-size-avg`显示单个记录的平均大小（以字节为单位）。对于单个主题的生产者，这提供了有关生成的消息的有用信息。对于多主题的生产者，例如MirrorMaker，这种信息就不那么有用了。除了这三个指标之外，还有一个
    `records-per-request-avg`指标，描述了单个生成请求中的消息平均数量。
- en: 'The last overall producer metric attribute that is recommended is `record-queue-time-avg`.
    This measurement is the average amount of time, in milliseconds, that a single
    message waits in the producer, after the application sends it, before it is actually
    produced to Kafka. After an application calls the producer client to send a message
    (by calling the `send` method), the producer waits until one of two things happens:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 建议的最后一个总体生产者指标属性是 `record-queue-time-avg`。这个度量是在应用程序发送消息后，消息在生产者中等待的平均时间（以毫秒为单位），直到实际产生到Kafka。应用程序调用生产者客户端发送消息（通过调用
    `send` 方法）后，生产者会等待直到发生以下两种情况之一：
- en: It has enough messages to fill a batch based on the `batch.size` configuration.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有足够的消息来填充基于 `batch.size` 配置的批处理。
- en: It has been long enough since the last batch was sent based on the `linger.ms`
    configuration.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据`linger.ms`配置，自上次发送批处理以来已经过了足够长的时间。
- en: Either of these two will cause the producer client to close the current batch
    it is building and send it to the brokers. The easiest way to understand it is
    that for busy topics, the first condition will apply, whereas for slow topics,
    the second will apply. The `record-queue-time-avg` measurement will indicate how
    long messages take to be produced, and therefore is helpful when tuning these
    two configurations to meet the latency requirements for your application.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者中的任何一个都将导致生产者客户端关闭它正在构建的当前批次并将其发送到代理。最容易理解的方法是，对于繁忙的主题，将应用第一个条件，而对于慢速主题，将应用第二个条件。`record-queue-time-avg`测量将指示消息需要多长时间才能被生成，因此在调整这两个配置以满足应用程序的延迟要求时是有帮助的。
- en: Per-broker and per-topic metrics
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个代理和每个主题的度量
- en: In addition to the overall producer metrics, there are metric beans that provide
    a limited set of attributes for the connection to each Kafka broker, as well as
    for each topic that is being produced. These measurements are useful for debugging
    problems in some cases, but they are not metrics that you are going to want to
    review on an ongoing basis. All of the attributes on these beans are the same
    as the attributes for the overall producer beans described previously and have
    the same meaning as described previously (except that they apply either to a specific
    broker or a specific topic).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 除了整体生产者度量之外，还有度量bean为与每个Kafka代理的连接以及正在生成的每个主题提供了一组有限的属性。在某些情况下，这些测量对于调试问题是有用的，但这些不是您希望经常审查的度量。这些bean上的所有属性与先前描述的整体生产者bean的属性相同，并且具有与先前描述相同的含义（除了它们适用于特定代理或特定主题）。
- en: The most useful metric provided by the per-broker producer metrics is the `request-latency-avg
    measurement`. This is because this metric will be mostly stable (given stable
    batching of messages) and can still show a problem with connections to a specific
    broker. The other attributes, such as `outgoing-byte-rate` and `request-latency-avg`,
    tend to vary depending on what partitions each broker is leading. This means that
    what these measurements “should” be at any point in time can quickly change, depending
    on the state of the Kafka cluster.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理生产者度量提供的最有用的度量是`request-latency-avg`测量。这是因为这个度量大部分时间会保持稳定（假设消息的批处理稳定），并且仍然可以显示与特定代理的连接问题。其他属性，例如`outgoing-byte-rate`和`request-latency-avg`，往往会根据每个代理领导的分区而变化。这意味着这些测量在任何时间点上“应该”是什么，可以很快地改变，具体取决于Kafka集群的状态。
- en: The topic metrics are a little more interesting than the per-broker metrics,
    but they will only be useful for producers that are working with more than one
    topic. They will also only be usable on a regular basis if the producer is not
    working with a lot of topics. For example, a MirrorMaker could be producing hundreds,
    or thousands, of topics. It is difficult to review all of those metrics, and nearly
    impossible to set reasonable alert thresholds on them. As with the per-broker
    metrics, the per-topic measurements are best used when investigating a specific
    problem. The `record-send-rate` and `record-error-rate` attributes, for example,
    can be used to isolate dropped messages to a specific topic (or validated to be
    across all topics). In addition, there is a `byte-rate` metric that provides the
    overall messages rate in bytes per second for the topic.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 主题度量比每个代理度量更有趣，但只对使用多个主题的生产者有用。如果生产者不使用很多主题，这些度量也只能在常规情况下使用。例如，MirrorMaker可能会生成数百个或数千个主题。很难审查所有这些度量，并且几乎不可能对其设置合理的警报阈值。与每个代理度量一样，当调查特定问题时，每个主题的测量最好用于。例如，例如，`record-send-rate`和`record-error-rate`属性可以用于将丢弃的消息隔离到特定主题（或验证是否跨所有主题）。此外，还有一个`byte-rate`度量，它提供了主题每秒字节的整体消息速率。
- en: Consumer Metrics
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消费者度量
- en: Similar to the producer client, the consumer in Kafka consolidates many of the
    metrics into attributes on just a few metric beans. These metrics have also eliminated
    the percentiles for latencies and the moving averages for rates, which were presenting
    in the deprecated Scala consumer, similar to the producer client. In the consumer,
    because the logic around consuming messages is a little more complex than just
    firing messages into the Kafka brokers, there are a few more metrics to deal with
    as well. See [Table 13-20](#table1015).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 与生产者客户端类似，Kafka中的消费者将许多指标合并为几个度量bean上的属性。这些度量也消除了延迟的百分位数和速率的移动平均值，这些在已弃用的Scala消费者中呈现，类似于生产者客户端。在消费者中，由于处理消息消费的逻辑比仅仅将消息发送到Kafka代理更复杂，因此还有一些更多的度量要处理。有关更多信息，请参见[表13-20](#table1015)。
- en: Table 13-20\. Kafka consumer metric MBeans
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 13-20表。Kafka消费者度量MBeans
- en: '| Name | JMX MBean |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | JMX MBean |'
- en: '| --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Overall consumer | `kafka.consumer:type=consumer-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 整体消费者 | `kafka.consumer:type=consumer-metrics,client-id=*CLIENTID*` |'
- en: '| Fetch manager | `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 获取管理器 | `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*CLIENTID*`
    |'
- en: '| Per-topic | `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*CLIENT​ID*,topic=*TOPICNAME*`
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 每个主题 | `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*CLIENT​ID*,topic=*TOPICNAME*`
    |'
- en: '| Per-broker | `kafka.consumer:type=consumer-node-metrics,client-id=*CLIENTID*,node-id=node-*BROKERID*`
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 每个代理 | `kafka.consumer:type=consumer-node-metrics,client-id=*CLIENTID*,node-id=node-*BROKERID*`
    |'
- en: '| Coordinator | `kafka.consumer:type=consumer-coordinator-metrics,client-id=*CLIENTID*`
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 协调员 | `kafka.consumer:type=consumer-coordinator-metrics,client-id=*CLIENTID*`
    |'
- en: Fetch manager metrics
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取管理器度量
- en: In the consumer client, the overall consumer metric bean is less useful for
    us because the metrics of interest are located in the *fetch manager* beans instead.
    The overall consumer bean has metrics regarding the lower-level network operations,
    but the fetch manager bean has metrics regarding bytes, request, and record rates.
    Unlike the producer client, the metrics provided by the consumer are useful to
    look at but not useful for setting up alerts on.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者客户端中，总体消费者度量bean对我们来说不太有用，因为我们感兴趣的度量位于*fetch manager* bean中。总体消费者bean具有有关较低级别网络操作的度量，但fetch
    manager bean具有有关字节、请求和记录速率的度量。与生产者客户端不同，消费者提供的度量是有用的，但不适合设置警报。
- en: For the fetch manager, the one attribute you may want to set up monitoring and
    alerts for is `fetch-latency-avg`. As with the equivalent `request-latency-avg`
    in the producer client, this metric tells us how long fetch requests to the brokers
    take. The problem with alerting on this metric is that the latency is governed
    by the consumer configurations `fetch.min.bytes` and `fetch.max.wait.ms`. A slow
    topic will have erratic latencies, as sometimes the broker will respond quickly
    (when there are messages available), and sometimes it will not respond for `fetch.max.wait.ms`
    (when there are no messages available). When consuming topics that have more regular,
    and abundant, message traffic, this metric may be more useful to look at.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于fetch manager，您可能希望设置监视和警报的一个属性是`fetch-latency-avg`。与生产者客户端中的等效`request-latency-avg`一样，这个度量告诉我们向经纪人发出的获取请求需要多长时间。对此度量进行警报的问题在于延迟受消费者配置`fetch.min.bytes`和`fetch.max.wait.ms`的控制。一个慢的主题将具有不稳定的延迟，因为有时经纪人会快速响应（当有消息可用时），有时它将不会在`fetch.max.wait.ms`内响应（当没有消息可用时）。在消费具有更规律和丰富的消息流量的主题时，这个度量可能更有用。
- en: Wait! No Lag?
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等等！没有滞后？
- en: The best advice for all consumers is that you must monitor the consumer lag.
    So why do we not recommend monitoring the `records-lag-max` attribute on the fetch
    manager bean? This metric shows the current lag (the difference between the consumer’s
    offset and the broker’s log-end offset) for the partition that is the most behind.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有消费者的最佳建议是，您必须监视消费者滞后。那么为什么我们不建议监视fetch manager bean上的`records-lag-max`属性？这个度量显示了当前滞后（消费者偏移和经纪人日志结束偏移之间的差异），对于最滞后的分区。
- en: 'The problem with this is twofold: it only shows the lag for one partition,
    and it relies on proper functioning of the consumer. If you have no other option,
    use this attribute for lag and set up alerting for it. But the best practice is
    to use external lag monitoring, as will be described in [“Lag Monitoring”](#lag_monitor).'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题有两个方面：它只显示一个分区的滞后，并且依赖于消费者的正常运行。如果没有其他选择，可以使用此属性进行滞后并设置警报。但最佳做法是使用外部滞后监视，如[“滞后监视”](#lag_monitor)中所述。
- en: To know how much message traffic your consumer client is handling, you should
    capture the `bytes-consumed-rate` or the `records-consumed-rate`, or preferably
    both. These metrics describe the message traffic consumed by this client instance
    in bytes per second and messages per second, respectively. Some users set minimum
    thresholds on these metrics for alerting so that they are notified if the consumer
    is not doing enough work. You should be careful when doing this, however. Kafka
    is intended to decouple the consumer and producer clients, allowing them to operate
    independently. The rate at which the consumer is able to consume messages is often
    dependent on whether or not the producer is working correctly, so monitoring these
    metrics on the consumer makes assumptions about the state of the producer. This
    can lead to false alerts on the consumer clients.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解您的消费者客户端处理了多少消息流量，您应该捕获`bytes-consumed-rate`或`records-consumed-rate`，或者最好两者兼而有之。这些度量描述了此客户端实例每秒消耗的消息流量，分别以字节和每秒消息计算。一些用户对这些度量设置了最低阈值以进行警报，以便在消费者未能完成足够工作时收到通知。但是，在执行此操作时，您应该小心。Kafka旨在解耦消费者和生产者客户端，使它们能够独立运行。消费者能够消费消息的速率通常取决于生产者是否正常工作，因此在消费者上监视这些度量会对生产者的状态进行假设。这可能会导致对消费者客户端的错误警报。
- en: It is also good to understand the relationship among bytes, messages, and requests,
    and the fetch manager provides metrics to help with this. The `fetch-rate` measurement
    tells us the number of fetch requests per second that the consumer is performing.
    The `fetch-size-avg` metric gives the average size of those fetch requests in
    bytes. Finally, the `records-per-request-avg` metric gives us the average number
    of messages in each fetch request. Note that the consumer does not provide an
    equivalent to the producer `record-size-avg` metric to let us know what the average
    size of a message is. If this is important, you will need to infer it from the
    other metrics available or capture it in your application after receiving messages
    from the consumer client library.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 了解字节、消息和请求之间的关系也很重要，fetch manager提供了帮助的度量。`fetch-rate`度量告诉我们消费者每秒执行的获取请求数量。`fetch-size-avg`度量给出了这些获取请求的平均大小（以字节为单位）。最后，`records-per-request-avg`度量给出了每个获取请求中的平均消息数。请注意，消费者没有提供与生产者`record-size-avg`度量相当的度量，以告诉我们消息的平均大小。如果这很重要，您需要从其他可用的度量中推断出来，或者在从消费者客户端库接收消息后在应用程序中捕获它。
- en: Per-broker and per-topic metrics
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个经纪人和每个主题的度量
- en: The metrics that are provided by the consumer client for each of the broker
    connections and each of the topics being consumed, as with the producer client,
    are useful for debugging issues with consumption, but will probably not be measurements
    that you review daily. As with the fetch manager, the `request-latency-avg` attribute
    provided by the per-broker metrics bean has limited usefulness, depending on the
    message traffic in the topics you are consuming. The `incoming-byte-rate` and
    `request-rate` metrics break down the consumed message metrics provided by the
    fetch manager into per-broker bytes per second and requests per second measurements,
    respectively. These can be used to help isolate problems that the consumer is
    having with the connection to a specific broker.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 与生产者客户端一样，消费者客户端为每个经纪人连接和每个被消费的主题提供的指标对于调试消费问题非常有用，但可能不会是您每天审查的测量值。与获取管理器一样，由每个经纪人指标bean提供的`request-latency-avg`属性具有有限的用处，具体取决于您正在消费的主题中的消息流量。`incoming-byte-rate`和`request-rate`指标将获取管理器提供的消耗消息指标分解为每个经纪人每秒字节和每秒请求的测量值。这些可以用于帮助隔离消费者与特定经纪人连接存在的问题。
- en: Per-topic metrics provided by the consumer client are useful if more than one
    topic is being consumed. Otherwise, these metrics will be the same as the fetch
    manager’s metrics and redundant to collect. On the other end of the spectrum,
    if the client is consuming many topics (Kafka MirrorMaker, for example) these
    metrics will be difficult to review. If you plan on collecting them, the most
    important metrics to gather are the `bytes-consumed-rate`, the `records-consumed-rate`,
    and the `fetch-size-avg`. The `bytes-consumed-rate` shows the absolute size in
    bytes consumed per second for the specific topic, while the `records-consumed-rate`
    shows the same information in terms of the number of messages. The `fetch-size-avg`
    provides the average size of each fetch request for the topic in bytes.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者客户端提供的按主题划分的指标在消费多个主题时非常有用。否则，这些指标将与获取管理器的指标相同，并且收集起来是多余的。另一方面，如果客户端正在消费多个主题（例如Kafka
    MirrorMaker），这些指标将很难进行审查。如果您计划收集它们，那么收集最重要的指标是`bytes-consumed-rate`、`records-consumed-rate`和`fetch-size-avg`。`bytes-consumed-rate`显示每秒针对特定主题消耗的绝对字节大小，而`records-consumed-rate`显示相同信息，但是以消息数量为单位。`fetch-size-avg`提供了每个主题的平均获取请求大小（以字节为单位）。
- en: Consumer coordinator metrics
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消费者协调器指标
- en: As described in [Chapter 4](ch04.html#reading_data_from_kafka), consumer clients
    generally work together as part of a consumer group. This group has coordination
    activities, such as group members joining, and heartbeat messages to the brokers
    to maintain group membership. The consumer coordinator is the part of the consumer
    client that is responsible for handling this work, and it maintains its own set
    of metrics. As with all metrics, there are many numbers provided but only a few
    key ones that you should monitor regularly.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第4章](ch04.html#reading_data_from_kafka)所述，消费者客户端通常作为消费者组的一部分共同工作。该组具有协调活动，例如组成员加入，以及向经纪人发送心跳消息以维护组成员资格。消费者协调器是负责处理这项工作的消费者客户端的一部分，并且它维护自己的一组指标。与所有指标一样，提供了许多数字，但只有少数几个关键数字需要定期监视。
- en: The biggest problem that consumers can run into due to coordinator activities
    is a pause in consumption while the consumer group synchronizes. This is when
    the consumer instances in a group negotiate which partitions will be consumed
    by which individual client instances. Depending on the number of partitions that
    are being consumed, this can take some time. The coordinator provides the metric
    attribute `sync-time-avg`, which is the average amount of time, in milliseconds,
    that the sync activity takes. It is also useful to capture the `sync-rate` attribute,
    which is the number of group syncs that happen every second. For a stable consumer
    group, this number should be zero most of the time.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 由于协调员活动而导致消费者可能遇到的最大问题是在消费暂停时，消费者组进行同步。这是消费者组实例协商由哪些个体客户端实例消费哪些分区的过程。根据正在消费的分区数量，这可能需要一些时间。协调员提供了`sync-time-avg`属性，它是同步活动所需的平均时间（以毫秒为单位）。捕获`sync-rate`属性也很有用，它是每秒发生的组同步次数。对于稳定的消费者组，这个数字大部分时间应该是零。
- en: The consumer needs to commit offsets to checkpoint its progress in consuming
    messages, either automatically on a regular interval or by manual checkpoints
    triggered in the application code. These commits are essentially just produce
    requests (though they have their own request type), in that the offset commit
    is a message produced to a special topic. The consumer coordinator provides the
    `commit-latency-avg` attribute, which measures the average amount of time that
    offset commits take. You should monitor this value just as you would the request
    latency in the producer. It should be possible to establish a baseline expected
    value for this metric, and set reasonable thresholds for alerting above that value.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者需要提交偏移量以检查其在消费消息时的进度，可以自动定期提交，也可以通过应用程序代码中触发的手动检查点来提交。这些提交本质上只是生产请求（尽管它们有自己的请求类型），因为偏移量提交是发送到特殊主题的消息。消费者协调器提供了`commit-latency-avg`属性，用于测量偏移量提交所需的平均时间。您应该像监视生产者的请求延迟一样监视这个值。应该能够建立此指标的基线预期值，并设置合理的阈值，以便在该值之上发出警报。
- en: One final coordinator metric that can be useful to collect is `assigned-partitions`.
    This is a count of the number of partitions that the consumer client (as a single
    instance in the consumer group) has been assigned to consume. This is helpful
    because, when compared to this metric from other consumer clients in the group,
    it is possible to see the balance of load across the entire consumer group. We
    can use this to identify imbalances that might be caused by problems in the algorithm
    used by the consumer coordinator for distributing partitions to group members.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个有用的协调器指标是`assigned-partitions`。这是消费者客户端（作为消费者组中的单个实例）被分配消费的分区数量。这很有帮助，因为与消费者组中其他消费者客户端的此指标相比，可以看到整个消费者组的负载平衡。我们可以使用这个来识别可能由消费者协调器用于将分区分配给组成员的算法中的问题引起的不平衡。
- en: Quotas
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配额
- en: Apache Kafka has the ability to throttle client requests in order to prevent
    one client from overwhelming the entire cluster. This is configurable for both
    producer and consumer clients and is expressed in terms of the permitted amount
    of traffic from an individual client ID to an individual broker in bytes per second.
    There is a broker configuration, which sets a default value for all clients, as
    well as per-client overrides that can be dynamically set. When the broker calculates
    that a client has exceeded its quota, it slows the client down by holding the
    response back to the client for enough time to keep the client under the quota.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka具有限制客户端请求的能力，以防止一个客户端压倒整个集群。这对生产者和消费者客户端都是可配置的，并且以每秒允许从单个客户端ID到单个代理的流量量来表示。有一个代理配置，为所有客户端设置默认值，以及可以动态设置的每个客户端覆盖。当代理计算出客户端已超出其配额时，它通过将响应保持在客户端足够长的时间来减慢客户端的速度，以使客户端保持在配额以下。
- en: The Kafka broker does not use error codes in the response to indicate that the
    client is being throttled. This means that it is not obvious to the application
    that throttling is happening without monitoring the metrics that are provided
    to show the amount of time that the client is being throttled. The metrics that
    must be monitored are shown in [Table 13-21](#table1016).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka代理在响应中不使用错误代码来指示客户端被限制。这意味着应用程序在没有监视提供的指标来显示客户端被限制的时间量时，不明显地发生了限制。必须监视的指标显示在[表13-21](#table1016)中。
- en: Table 13-21\. Metrics to monitor
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 表13-21\. 要监视的指标
- en: '| Client | Bean name |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 客户端 | Bean名称 |'
- en: '| --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Consumer | bean `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=CLIENTID`,
    attribute `fetch-throttle-time-avg` |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 消费者 | bean `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=CLIENTID`,
    属性 `fetch-throttle-time-avg` |'
- en: '| Producer | bean `kafka.producer:type=producer-metrics,client-id=CLIENTID`,
    attribute `produce-throttle-time-avg` |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 生产者 | bean `kafka.producer:type=producer-metrics,client-id=CLIENTID`, 属性
    `produce-throttle-time-avg` |'
- en: Quotas are not enabled by default on the Kafka brokers, but it is safe to monitor
    these metrics irrespective of whether or not you are currently using quotas. Monitoring
    them is a good practice as they may be enabled at some point in the future, and
    it’s easier to start with monitoring them as opposed to adding metrics later.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka代理默认情况下未启用配额，但无论您当前是否使用配额，监视这些指标都是安全的。监视它们是一个好习惯，因为它们可能在将来的某个时候被启用，而且从监视它们开始要比以后添加指标更容易。
- en: Lag Monitoring
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滞后监控
- en: For Kafka consumers, the most important thing to monitor is the consumer lag.
    Measured in number of messages, this is the difference between the last message
    produced in a specific partition and the last message processed by the consumer.
    While this topic would normally be covered in the previous section on consumer
    client monitoring, it is one of the cases where external monitoring far surpasses
    what is available from the client itself. As mentioned previously, there is a
    lag metric in the consumer client, but using it is problematic. It only represents
    a single partition, the one that has the most lag, so it does not accurately show
    how far behind the consumer is. In addition, it requires proper operation of the
    consumer, because the metric is calculated by the consumer on each fetch request.
    If the consumer is broken or offline, the metric is either inaccurate or not available.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kafka消费者，最重要的监控是消费者滞后。以消息数量衡量，这是特定分区中最后一条消息产生与消费者处理的最后一条消息之间的差异。虽然这个主题通常会在前一节关于消费者客户端监控中涵盖，但这是一个外部监控远远超过客户端本身提供的情况之一。如前所述，消费者客户端中有一个滞后指标，但使用它是有问题的。它只代表一个分区，即滞后最严重的分区，因此无法准确显示消费者滞后的程度。此外，它需要消费者的正常运行，因为该指标是由消费者在每次获取请求时计算的。如果消费者出现故障或离线，该指标要么不准确，要么不可用。
- en: The preferred method of consumer lag monitoring is to have an external process
    that can watch both the state of the partition on the broker, tracking the offset
    of the most recently produced message, and the state of the consumer, tracking
    the last offset the consumer group has committed for the partition. This provides
    an objective view that can be updated regardless of the status of the consumer
    itself. This checking must be performed for every partition that the consumer
    group consumes. For a large consumer, like MirrorMaker, this may mean tens of
    thousands of partitions.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者滞后监控的首选方法是有一个外部进程，可以监视代理上分区的状态，跟踪最近生成的消息的偏移量，以及消费者的状态，跟踪消费者组为分区提交的最后偏移量。这提供了一个客观的视图，可以更新，而不受消费者本身状态的影响。必须对消费者组消费的每个分区执行此检查。对于像MirrorMaker这样的大型消费者，这可能意味着数万个分区。
- en: '[Chapter 12](ch12.html#administering_kafka) provided information on using the
    command-line utilities to get consumer group information, including committed
    offsets and lag. Monitoring lag like this, however, presents its own problems.
    First, you must understand for each partition what is a reasonable amount of lag.
    A topic that receives 100 messages an hour will need a different threshold than
    a topic that receives 100,000 messages per second. Then, you must be able to consume
    all of the lag metrics into a monitoring system and set alerts on them. If you
    have a consumer group that consumes 100,000 partitions over 1,500 topics, you
    may find this to be a daunting task.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章](ch12.html#administering_kafka)提供了使用命令行实用程序获取消费者组信息，包括提交的偏移量和滞后的信息。然而，像这样监控滞后也存在自己的问题。首先，您必须了解对于每个分区，什么是合理的滞后量。每小时接收100条消息的主题将需要与每秒接收100,000条消息的主题不同的阈值。然后，您必须能够将所有滞后指标消耗到监控系统中，并对其设置警报。如果您有一个消费者组在1,500个主题上消费100,000个分区，您可能会发现这是一项艰巨的任务。'
- en: One way to monitor consumer groups reduce this complexity is to use [Burrow](https://oreil.ly/supY1).
    This is an open source application, originally developed by LinkedIn, that provides
    consumer status monitoring by gathering lag information for all consumer groups
    in a cluster and calculating a single status for each group saying whether the
    consumer group is working properly, falling behind, or is stalled or stopped entirely.
    It does this without requiring thresholds by monitoring the progress that the
    consumer group is making on processing messages, though you can also get the message
    lag as an absolute number. There is an in-depth discussion of the reasoning and
    methodology behind how Burrow works on the [LinkedIn Engineering blog](http://bit.ly/2sanKZb).
    Deploying Burrow can be an easy way to provide monitoring for all consumers in
    a cluster, as well as in multiple clusters, and it can be easily integrated with
    your existing monitoring and alerting system.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 监控消费者组减少这种复杂性的一种方法是使用[Burrow](https://oreil.ly/supY1)。这是一个开源应用程序，最初由LinkedIn开发，它通过收集集群中所有消费者组的滞后信息并计算每个组的单个状态来提供消费者状态监控，指出消费者组是否正常工作、落后或完全停滞或停止。它可以在不需要阈值的情况下监控消费者组在处理消息时的进度，尽管您也可以获得消息滞后的绝对数量。关于Burrow工作原理和方法背后的深入讨论可以在[LinkedIn工程博客](http://bit.ly/2sanKZb)上找到。部署Burrow可以轻松为集群中的所有消费者提供监控，以及在多个集群中，并且可以轻松集成到您现有的监控和警报系统中。
- en: If there is no other option, the `records-lag-max` metric from the consumer
    client will provide at least a partial view of the consumer status. It is strongly
    suggested, however, that you utilize an external monitoring system like Burrow.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他选择，消费者客户端的`records-lag-max`指标至少可以提供部分消费者状态的视图。然而，强烈建议您像Burrow这样利用外部监控系统。
- en: End-to-End Monitoring
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端监控
- en: 'Another type of external monitoring that is recommended to determine if your
    Kafka clusters are working properly is an end-to-end monitoring system that provides
    a client point of view on the health of the Kafka cluster. Consumer and producer
    clients have metrics that can indicate that there might be a problem with the
    Kafka cluster, but this can be a guessing game as to whether increased latency
    is due to a problem with the client, the network, or Kafka itself. In addition,
    it means that if you are responsible for running the Kafka cluster, and not the
    clients, you would now have to monitor all of the clients as well. What you really
    need to know is:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的另一种外部监控类型是端到端监控系统，它提供了客户端对Kafka集群健康状况的视角。消费者和生产者客户端具有可以指示Kafka集群可能存在问题的指标，但这可能是一个猜测游戏，增加的延迟是由于客户端、网络还是Kafka本身的问题。此外，这意味着如果您负责运行Kafka集群，而不是客户端，那么现在您还必须监控所有客户端。您真正需要知道的是：
- en: Can I produce messages to the Kafka cluster?
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以向Kafka集群生产消息吗？
- en: Can I consume messages from the Kafka cluster?
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以从Kafka集群中消费消息吗？
- en: In an ideal world, you would be able to monitor this for every topic individually.
    However, in most situations it is not reasonable to inject synthetic traffic into
    every topic in order to do this. We can, however, at least provide those answers
    for every broker in the cluster, and that is what [Xinfra Monitor (formerly known
    as Kafka Monitor) does](https://oreil.ly/QqXD9). This tool, open sourced by the
    Kafka team at LinkedIn, continually produces and consumes data from a topic that
    is spread across all brokers in a cluster. It measures the availability of both
    produce and consume requests on each broker, as well as the total produce to consume
    latency. This type of monitoring is invaluable to be able to externally verify
    that the Kafka cluster is operating as intended, since just like consumer lag
    monitoring, the Kafka broker cannot report whether or not clients are able to
    use the cluster properly.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，您将能够为每个主题单独监控这一点。然而，在大多数情况下，为了做到这一点，向每个主题注入合成流量是不合理的。然而，至少我们可以为集群中的每个代理提供这些答案，这就是[Xinfra
    Monitor（以前称为Kafka Monitor）](https://oreil.ly/QqXD9)所做的。这个工具是由LinkedIn的Kafka团队开源的，它不断地从跨越集群中所有代理的主题中生成和消费数据。它测量了每个代理上生产和消费请求的可用性，以及总的生产到消费延迟。这种类型的监控对于能够外部验证Kafka集群是否按预期运行是非常宝贵的，因为就像消费者滞后监控一样，Kafka代理无法报告客户端是否能够正确使用集群。
- en: Summary
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Monitoring is a key aspect of running Apache Kafka properly, which explains
    why so many teams spend a significant amount of their time perfecting that part
    of operations. Many organizations use Kafka to handle petabyte-scale data flows.
    Assuring that the data does not stop, and that messages are not lost, this is
    a critical business requirement. It is also our responsibility to assist users
    with monitoring how their applications use Kafka by providing the metrics that
    they need to do this.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 监控是正确运行Apache Kafka的关键方面，这解释了为什么许多团队花费大量时间完善运维的这一部分。许多组织使用Kafka来处理PB级别的数据流。确保数据不会停止，消息不会丢失，这是一个关键的业务需求。我们还有责任通过提供用户需要的指标来协助用户监控他们的应用程序如何使用Kafka。
- en: In this chapter we covered the basics of how to monitor Java applications, and
    specifically the Kafka applications. We reviewed a subset of the numerous metrics
    available in the Kafka broker, also touching on Java and OS monitoring, as well
    as logging. We then detailed the monitoring available in the Kafka client libraries,
    including quota monitoring. Finally, we discussed the use of external monitoring
    systems for consumer lag monitoring and end-to-end cluster availability. While
    certainly not an exhaustive list of the metrics that are available, this chapter
    reviewed the most critical ones to keep an eye on.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何监控Java应用程序的基础知识，特别是Kafka应用程序。我们回顾了Kafka broker中可用的众多指标的子集，还涉及了Java和操作系统的监控，以及日志记录。然后，我们详细介绍了Kafka客户端库中可用的监控，包括配额监控。最后，我们讨论了使用外部监控系统进行消费者滞后监控和端到端集群可用性。虽然这当然不是所有可用指标的详尽列表，但本章回顾了需要密切关注的最关键的指标。

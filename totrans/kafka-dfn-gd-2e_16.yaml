- en: Chapter 14\. Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。流处理
- en: Kafka was traditionally seen as a powerful message bus, capable of delivering
    streams of events but without processing or transformation capabilities. Kafka’s
    reliable stream delivery capabilities make it a perfect source of data for stream
    processing systems. Apache Storm, Apache Spark Streaming, Apache Flink, Apache
    Samza, and many more stream processing systems were built with Kafka often being
    their only reliable data source.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，Kafka被视为一个强大的消息总线，能够传递事件流，但没有处理或转换能力。Kafka可靠的流传递能力使其成为流处理系统的完美数据源。Apache
    Storm、Apache Spark Streaming、Apache Flink、Apache Samza等许多流处理系统都是以Kafka作为它们唯一可靠的数据源构建的。
- en: With the increased popularity of Apache Kafka, first as a simple message bus
    and later as a data integration system, many companies had a system containing
    many streams of interesting data, stored for long amounts of time and perfectly
    ordered, just waiting for some stream processing framework to show up and process
    them. In other words, in the same way that data processing was significantly more
    difficult before databases were invented, stream processing was held back by the
    lack of a stream processing platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Apache Kafka的日益流行，首先作为一个简单的消息总线，后来作为一个数据集成系统，许多公司拥有一个包含许多有趣数据流的系统，存储了很长时间并且完全有序，只等待一些流处理框架出现并处理它们。换句话说，就像在数据库发明之前数据处理要困难得多一样，流处理也因缺乏流处理平台而受到阻碍。
- en: Starting from version 0.10.0, Kafka does more than provide a reliable source
    of data streams to every popular stream processing framework. Now Kafka includes
    a powerful stream processing library as part of its collection of client libraries,
    called Kafka Streams (or sometimes Streams API). This allows developers to consume,
    process, and produce events in their own apps, without relying on an external
    processing framework.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本0.10.0开始，Kafka不仅为每个流行的流处理框架提供可靠的数据流源。现在，Kafka还包括一个强大的流处理库作为其客户端库集合的一部分，称为Kafka
    Streams（有时称为Streams API）。这使开发人员可以在其自己的应用程序中消费、处理和生成事件，而无需依赖外部处理框架。
- en: We’ll begin the chapter by explaining what we mean by stream processing (since
    this term is frequently misunderstood), then discuss some of the basic concepts
    of stream processing and the design patterns that are common to all stream processing
    systems. We’ll then dive into Apache Kafka’s stream processing library—its goals
    and architecture. We’ll give a small example of how to use Kafka Streams to calculate
    a moving average of stock prices. We’ll then discuss other examples of good stream
    processing use cases and finish off the chapter by providing a few criteria you
    can use when choosing which stream processing framework (if any) to use with Apache
    Kafka.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从解释我们所说的流处理是什么开始这一章（因为这个术语经常被误解），然后讨论流处理的一些基本概念和所有流处理系统共同的设计模式。然后我们将深入讨论Apache
    Kafka的流处理库——它的目标和架构。我们将举一个小例子，说明如何使用Kafka Streams来计算股票价格的移动平均值。然后我们将讨论其他一些良好的流处理用例示例，并在本章结束时提供一些标准，供您在选择与Apache
    Kafka一起使用的流处理框架（如果有的话）时使用。
- en: This chapter is intended as just a quick introduction to the large and fascinating
    world of stream processing and Kafka Streams. There are entire books written on
    these subjects.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章只是对流处理和Kafka Streams这个广阔而迷人的世界的一个简要介绍。有整本书专门讨论这些主题。
- en: 'Some books cover the basic concepts of stream processing from a data architecture
    perspective:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一些书籍从数据架构的角度涵盖了流处理的基本概念：
- en: '[*Making Sense of Stream Processing*](https://oreil.ly/omhmK) by Martin Kleppmann
    (O’Reilly) discusses the benefits of rethinking applications as stream processing
    applications and how to reorient data architectures around the idea of event streams.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Making Sense of Stream Processing*](https://oreil.ly/omhmK) 由Martin Kleppmann（O''Reilly）讨论了重新思考应用程序作为流处理应用程序的好处，以及如何围绕事件流的概念重新定位数据架构。'
- en: '[*Streaming Systems*](https://oreil.ly/vcBBF) by Tyler Akidau, Slava Chernyak,
    and Reuven Lax (O’Reilly) is a great general introduction to the topic of stream
    processing and some of the basic ideas in the space.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Streaming Systems*](https://oreil.ly/vcBBF) 由Tyler Akidau、Slava Chernyak和Reuven
    Lax（O''Reilly）是关于流处理主题的一个很好的概论，介绍了该领域的一些基本概念。'
- en: '[*Flow Architectures*](https://oreil.ly/ajOTG) by James Urquhart (O’Reilly)
    is targeted at CTOs and discusses the implications of stream processing to the
    business.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Flow Architectures*](https://oreil.ly/ajOTG) 由James Urquhart（O''Reilly）面向CTO，讨论了流处理对业务的影响。'
- en: 'Other books go into specific details of specific frameworks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其他书籍涉及特定框架的具体细节：
- en: '[*Mastering Kafka Streams and ksqlDB*](https://oreil.ly/5Ijpx) by Mitch Seymour
    (O’Reilly)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Mastering Kafka Streams and ksqlDB*](https://oreil.ly/5Ijpx) 由Mitch Seymour（O''Reilly）'
- en: '[*Kafka Streams in Action*](https://oreil.ly/TfUxs) by William P. Bejeck Jr.
    (Manning)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Kafka Streams in Action*](https://oreil.ly/TfUxs) 由William P. Bejeck Jr.（Manning）'
- en: '[*Event Streaming with Kafka Streams and ksqlDB*](https://oreil.ly/EK06e) by
    William P. Bejeck Jr. (Manning)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Event Streaming with Kafka Streams and ksqlDB*](https://oreil.ly/EK06e) 由William
    P. Bejeck Jr.（Manning）'
- en: '[*Stream Processing with Apache Flink*](https://oreil.ly/ransF) by Fabian Hueske
    and Vasiliki Kalavri (O’Reilly)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Stream Processing with Apache Flink*](https://oreil.ly/ransF) 由Fabian Hueske和Vasiliki
    Kalavri（O''Reilly）'
- en: '[*Stream Processing with Apache Spark*](https://oreil.ly/B0ODf) by Gerard Maas
    and Francois Garillot (O’Reilly)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Stream Processing with Apache Spark*](https://oreil.ly/B0ODf) 由Gerard Maas和Francois
    Garillot（O''Reilly）'
- en: Finally, Kafka Streams is still an evolving framework. Every major release deprecates
    APIs and modifies semantics. This chapter documents APIs and semantics as of Apache
    Kafka 2.8\. We avoided using any API that was planned for deprecation in release
    3.0, but our discussion of join semantics and timestamp handling does not include
    any of the changes planned for release 3.0.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Kafka Streams仍然是一个不断发展的框架。每个主要版本的发布都会弃用API并修改语义。本章记录了Apache Kafka 2.8的API和语义。我们避免使用计划在3.0版本中弃用的任何API，但我们对连接语义和时间戳处理的讨论不包括计划在3.0版本中的任何更改。
- en: What Is Stream Processing?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是流处理？
- en: There is a lot of confusion about what stream processing means. Many definitions
    mix up implementation details, performance requirements, data models, and many
    other aspects of software engineering. A similar thing has happened in the world
    of relational databases—the abstract definitions of the relational model are getting
    forever entangled in the implementation details and specific limitations of the
    popular database engines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流处理的含义存在很多混淆。许多定义混淆了实现细节、性能要求、数据模型和软件工程的许多其他方面。在关系数据库领域也发生了类似的事情——关系模型的抽象定义不断地与流行的数据库引擎的实现细节和特定限制纠缠在一起。
- en: The world of stream processing is still evolving, and just because a specific
    popular implementation does things in specific ways or has specific limitations
    doesn’t mean that those details are an inherent part of processing streams of
    data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理的世界仍在不断发展，仅仅因为特定的流行实现以特定的方式执行任务或具有特定的限制，并不意味着这些细节是数据流处理的固有部分。
- en: 'Let’s start at the beginning: What is a data stream (also called an *event
    stream* or *streaming data*)? First and foremost, a *data stream* is an abstraction
    representing an unbounded dataset. *Unbounded* means infinite and ever growing.
    The dataset is unbounded because over time, new records keep arriving. This definition
    is used by [Google](http://oreil.ly/1p1AKux), [Amazon](http://amzn.to/2sfc334),
    and pretty much everyone else.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始：什么是数据流（也称为*事件流*或*流数据*）？首先，*数据流*是表示无界数据集的抽象。*无界*意味着无限和不断增长。数据集是无界的，因为随着时间的推移，新的记录不断到达。这个定义被[Google](http://oreil.ly/1p1AKux)、[Amazon](http://amzn.to/2sfc334)和几乎所有其他人使用。
- en: Note that this simple model (a stream of events) can be used to represent just
    about every business activity we care to analyze. We can look at a stream of credit
    card transactions, stock trades, package deliveries, network events going through
    a switch, events reported by sensors in manufacturing equipment, emails sent,
    moves in a game, etc. The list of examples is endless because pretty much everything
    can be seen as a sequence of events.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个简单的模型（事件流）可以用来表示我们关心分析的几乎每一个业务活动。我们可以查看信用卡交易流、股票交易、包裹递送、通过交换机的网络事件、制造设备传感器报告的事件、发送的电子邮件、游戏中的动作等。例子的列表是无穷无尽的，因为几乎一切都可以被看作是一系列事件。
- en: 'There are a few other attributes of the event streams model, in addition to
    its unbounded nature:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其无界特性之外，事件流模型还有其他一些属性：
- en: Event streams are ordered
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流是有序的
- en: There is an inherent notion of which events occur before or after other events.
    This is clearest when looking at financial events. A sequence in which you first
    put money in your account and later spend the money is very different from a sequence
    at which you first spend the money and later cover your debt by depositing money
    back. The latter will incur overdraft charges, while the former will not. Note
    that this is one of the differences between an event stream and a database table—records
    in a table are always considered unordered, and the “order by” clause of SQL is
    not part of the relational model; it was added to assist in reporting.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 事件发生的先后顺序是固有的概念。在观察财务事件时，这一点最为明显。首先将钱存入账户，然后再花钱的顺序与首先花钱，然后再通过存钱来偿还债务的顺序是非常不同的。后者会产生透支费用，而前者则不会。请注意，这是事件流和数据库表之间的一个区别——表中的记录总是被视为无序的，“order
    by”子句不是关系模型的一部分；它是为了帮助报告而添加的。
- en: Immutable data records
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不可变的数据记录
- en: Events, once occurred, can never be modified. A financial transaction that is
    canceled does not disappear. Instead, an additional event is written to the stream,
    recording a cancelation of a previous transaction. When a customer returns merchandise
    to a shop, we don’t delete the fact that the merchandise was sold to them earlier,
    rather we record the return as an additional event. This is another difference
    between a data stream and a database table—we can delete or update records in
    a table, but those are all additional transactions that occur in the database,
    and as such can be recorded in a stream of events that records all transactions.
    If you are familiar with binlogs, WALs, or redo logs in databases, you can see
    that if we insert a record into a table and later delete it, the table will no
    longer contain the record, but the redo log will contain two transactions—the
    insert and the delete.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 事件一旦发生，就永远不能被修改。取消的财务交易不会消失。相反，会向流中写入一个额外的事件，记录了之前交易的取消。当顾客将商品退回商店时，我们不会删除之前卖给他们的商品的事实，而是将退货记录为一个额外的事件。这是数据流和数据库表之间的另一个区别——我们可以删除或更新表中的记录，但这些都是在数据库中发生的额外交易，并且可以记录在记录所有交易的事件流中。如果您熟悉数据库中的binlogs、WALs或重做日志，您会发现，如果我们向表中插入记录，然后将其删除，表将不再包含该记录，但重做日志将包含两个事务——插入和删除。
- en: Event streams are replayable
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 事件流是可重放的
- en: This is a desirable property. While it is easy to imagine nonreplayable streams
    (TCP packets streaming through a socket are generally nonreplayable), for most
    business applications, it is critical to be able to replay a raw stream of events
    that occurred months (and sometimes years) earlier. This is required to correct
    errors, try new methods of analysis, or perform audits. This is the reason we
    believe Kafka made stream processing so successful in modern businesses—it allows
    capturing and replaying a stream of events. Without this capability, stream processing
    would not be more than a lab toy for data scientists.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可取的特性。虽然很容易想象不可重放的流（通过套接字传输的TCP数据包通常是不可重放的），但对于大多数业务应用程序来说，能够重放发生几个月（有时甚至几年）前的原始事件流至关重要。这是为了纠正错误、尝试新的分析方法或进行审计。这就是我们认为Kafka在现代企业中如此成功的流处理的原因——它允许捕获和重放事件流。如果没有这种能力，流处理将不会超过数据科学家的实验室玩具。
- en: It is worth noting that neither the definition of event streams nor the attributes
    we later listed say anything about the data contained in the events or the number
    of events per second. The data differs from system to system—events can be tiny
    (sometimes only a few bytes) or very large (XML messages with many headers); they
    can also be completely unstructured key-value pairs, semi-structured JSON, or
    structured Avro or Protobuf messages. While it is often assumed that data streams
    are “big data” and involve millions of events per second, the same techniques
    we’ll discuss apply equally well (and often better) to smaller streams of events
    with only a few events per second or minute.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，无论事件流的定义还是我们后来列出的属性都没有提到事件中包含的数据或每秒事件的数量。数据因系统而异 - 事件可以很小（有时只有几个字节），也可以很大（具有许多标头的XML消息）；它们也可以是完全无结构的键值对、半结构化的JSON或结构化的Avro或Protobuf消息。虽然人们经常认为数据流是“大数据”，涉及每秒数百万事件，但我们将讨论的相同技术同样适用（而且通常更好）于每秒或每分钟只有几个事件的较小事件流。
- en: 'Now that we know what event streams are, it’s time to make sure we understand
    stream processing. Stream processing refers to the ongoing processing of one or
    more event streams. Stream processing is a programming paradigm—just like request-response
    and batch processing. Let’s look at how different programming paradigms compare
    to get a better understanding of how stream processing fits into software architectures:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了事件流是什么，是时候确保我们理解流处理了。流处理是指一个或多个事件流的持续处理。流处理是一种编程范式 - 就像请求-响应和批处理一样。让我们看看不同的编程范式如何比较，以更好地理解流处理如何融入软件架构中：
- en: Request-response
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请求-响应
- en: This is the lowest-latency paradigm, with response times ranging from submilliseconds
    to a few milliseconds, usually with the expectation that response times will be
    highly consistent. The mode of processing is usually blocking—an app sends a request
    and waits for the processing system to respond. In the database world, this paradigm
    is known as *online transaction processing* (OLTP). Point-of-sale systems, credit
    card processing, and time-tracking systems typically work in this paradigm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最低延迟的范式，响应时间从亚毫秒到几毫秒不等，通常期望响应时间高度一致。处理模式通常是阻塞的 - 应用程序发送请求并等待处理系统响应。在数据库世界中，这种范式被称为在线事务处理（OLTP）。销售点系统、信用卡处理和时间跟踪系统通常在这种范式下工作。
- en: Batch processing
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理
- en: This is the high-latency/high-throughput option. The processing system wakes
    up at set times—every day at 2:00 a.m., every hour on the hour, etc. It reads
    all required input (either all data available since the last execution, all data
    from the beginning of month, etc.), writes all required output, and goes away
    until the next time it is scheduled to run. Processing times range from minutes
    to hours, and users expect to read stale data when they are looking at results.
    In the database world, these are the data warehouse and business intelligence
    systems—data is loaded in huge batches once a day, reports are generated, and
    users look at the same reports until the next data load occurs. This paradigm
    often has great efficiency and economy of scale, but in recent years, businesses
    need the data available in shorter timeframes in order to make decision-making
    more timely and efficient. This puts huge pressure on systems that were written
    to exploit economy of scale—not to provide low-latency reporting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是高延迟/高吞吐量的选项。处理系统在设定的时间唤醒 - 每天凌晨2:00、每小时整点等。它读取所有必需的输入（自上次执行以来的所有数据，自月初以来的所有数据等），写入所有必需的输出，然后在下次计划运行之前离开。处理时间从几分钟到几小时不等，用户期望在查看结果时读取过时数据。在数据库世界中，这些是数据仓库和商业智能系统
    - 数据每天一次以大批量加载，生成报告，用户在下次数据加载之前查看相同的报告。这种范式通常具有很高的效率和规模经济，但近年来，企业需要更短时间内可用的数据，以使决策更及时和高效。这给了被编写为利用规模经济而不是提供低延迟报告的系统带来巨大压力。
- en: Stream processing
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理
- en: This is a continuous and nonblocking option. Stream processing fills the gap
    between the request-response world, where we wait for events that take two milliseconds
    to process, and the batch processing world, where data is processed once a day
    and takes eight hours to complete. Most business processes don’t require an immediate
    response within milliseconds but can’t wait for the next day either. Most business
    processes happen continuously, and as long as the business reports are updated
    continuously and the line of business apps can continuously respond, the processing
    can proceed without anyone waiting for a specific response within milliseconds.
    Business processes such as alerting on suspicious credit transactions or network
    activity, adjusting prices in real-time based on supply and demand, or tracking
    deliveries of packages are all a natural fit for continuous but nonblocking processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个持续且非阻塞的选项。流处理填补了请求-响应世界和批处理世界之间的差距，在请求-响应世界中，我们等待需要两毫秒处理的事件，在批处理世界中，数据一天处理一次，需要八小时才能完成。大多数业务流程不需要在毫秒内立即响应，但也不能等到第二天。大多数业务流程是持续发生的，只要业务报告持续更新，业务应用程序可以持续响应，处理就可以进行，而无需等待特定的毫秒级响应。例如，对可疑的信用交易或网络活动进行警报、根据供需实时调整价格，或跟踪包裹的交付等业务流程都非常适合持续但非阻塞的处理。
- en: It is important to note that the definition doesn’t mandate any specific framework,
    API, or feature. As long as we are continuously reading data from an unbounded
    dataset, doing something to it, and emitting output, we are doing stream processing.
    But the processing has to be continuous and ongoing. A process that starts every
    day at 2:00 a.m., reads 500 records from the stream, outputs a result, and goes
    away doesn’t quite cut it as far as stream processing goes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，该定义不强制使用任何特定的框架、API或功能。只要我们不断地从无界数据集中读取数据，对其进行处理并发出输出，我们就在进行流处理。但处理必须是持续的。每天凌晨2:00开始的过程，从流中读取500条记录，输出结果，然后消失，这在流处理方面并不够。
- en: Stream Processing Concepts
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理概念
- en: Stream processing is very similar to any type of data processing—we write code
    that receives data, does something with the data (a few transformations, aggregates,
    enrichments, etc.), and then place the result somewhere. However, there are some
    key concepts that are unique to stream processing and often cause confusion when
    someone who has data processing experience first attempts to write stream processing
    applications. Let’s take a look at a few of those concepts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理与任何类型的数据处理非常相似——我们编写代码接收数据，对数据进行处理（一些转换、聚合、丰富等），然后将结果放在某个地方。然而，有一些关键概念是流处理特有的，当有数据处理经验的人首次尝试编写流处理应用程序时，这些概念通常会引起混淆。让我们来看看其中的一些概念。
- en: Topology
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拓扑结构
- en: A stream processing application includes one or more processing topologies.
    A processing topology starts with one or more source streams that are passed through
    a graph of stream processors connected through event streams, until results are
    written to one or more sink streams. Each stream processor is a computational
    step applied to the stream of events in order to transform the events. Examples
    of some stream processors we’ll use in our examples are filter, count, group-by,
    and left-join. We often visualize stream processing applications by drawing the
    processing nodes and connecting them with arrows to show how events flow from
    one node to the next as the application is processing data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理应用包括一个或多个处理拓扑。处理拓扑从一个或多个源流开始，通过连接的事件流通过流处理器的图形，直到结果被写入一个或多个汇流流。每个流处理器都是应用于事件流的计算步骤，以转换事件。我们在示例中将使用的一些流处理器的示例包括过滤器、计数、分组和左连接。我们经常通过绘制处理节点并用箭头连接它们来可视化流处理应用，以显示事件如何从一个节点流向下一个节点，同时应用正在处理数据。
- en: Time
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间
- en: Time is probably the most important concept in stream processing and often the
    most confusing. For an idea of how complex time can get when discussing distributed
    systems, we recommend Justin Sheehy’s excellent paper, [“There Is No Now”](http://bit.ly/2rXXdLr).
    In the context of stream processing, having a common notion of time is critical
    because most stream applications perform operations on time windows. For example,
    our stream application might calculate a moving five-minute average of stock prices.
    In that case, we need to know what to do when one of our producers goes offline
    for two hours due to network issues and returns with two hours’ worth of data—most
    of the data will be relevant for five-minute time windows that have long passed
    and for which the result was already calculated and stored.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 时间可能是流处理中最重要的概念，也是最令人困惑的概念。在讨论分布式系统时，时间可能变得非常复杂，我们建议阅读Justin Sheehy的优秀论文[“There
    Is No Now”](http://bit.ly/2rXXdLr)。在流处理的上下文中，具有共同的时间概念至关重要，因为大多数流应用程序对时间窗口执行操作。例如，我们的流应用程序可能计算股票价格的移动五分钟平均值。在这种情况下，当我们的生产者由于网络问题下线两个小时并返回两小时的数据时，我们需要知道该如何处理——大部分数据将与已经过去并且结果已经计算和存储的五分钟时间窗口相关。
- en: 'Stream processing systems typically refer to the following notions of time:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理系统通常涉及以下时间概念：
- en: Event time
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间
- en: This is the time the events we are tracking occurred and the record was created—the
    time a measurement was taken, an item was sold at a shop, a user viewed a page
    on our website, etc. In version 0.10.0 and later, Kafka automatically adds the
    current time to producer records at the time they are created. If this does not
    match the application’s notion of *event time*, such as in cases where the Kafka
    record is created based on a database record sometime after the event occurred,
    then we recommend adding the event time as a field in the record itself so that
    both timestamps will be available for later processing. Event time is usually
    the time that matters most when processing stream data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们正在跟踪的事件发生的时间和记录创建的时间——测量时间、商店销售商品的时间、用户在我们网站上查看页面的时间等。在0.10.0版本及以后，Kafka会自动在生产者记录中添加当前时间。如果这与应用程序的*事件时间*不匹配，比如在某些情况下，Kafka记录是基于事件发生后的某个时间创建的数据库记录，那么我们建议在记录本身中添加事件时间字段，以便以后处理时两个时间戳都可用。事件时间通常是处理流数据时最重要的时间。
- en: Log append time
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 日志追加时间
- en: This is the time the event arrived at the Kafka broker and was stored there,
    also called *ingestion time*. In version 0.10.0 and later, Kafka brokers will
    automatically add this time to records they receive if Kafka is configured to
    do so or if the records arrive from older producers and contain no timestamps.
    This notion of time is typically less relevant for stream processing, since we
    are usually interested in the times the events occurred. For example, if we calculate
    the number of devices produced per day, we want to count devices that were actually
    produced on that day, even if there were network issues and the event only arrived
    to Kafka the following day. However, in cases where the real event time was not
    recorded, log append time can still be used consistently because it does not change
    after the record was created, and assuming no delays in the pipeline, it can be
    a reasonable approximation of event time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是事件到达Kafka代理并在那里存储的时间，也称为*摄取时间*。在0.10.0版本及以后，如果Kafka配置为这样做，或者记录来自旧的生产者并且不包含时间戳，Kafka代理将自动将此时间添加到它们接收的记录中。这种时间概念通常对流处理来说不太相关，因为我们通常对事件发生的时间感兴趣。例如，如果我们计算每天生产的设备数量，我们希望计算实际在当天生产的设备数量，即使存在网络问题，事件直到第二天才到达Kafka。然而，在真实事件时间未记录的情况下，日志追加时间仍然可以被一致使用，因为在记录创建后不会改变，并且假设管道没有延迟，它可以是事件时间的一个合理近似值。
- en: Processing time
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间
- en: This is the time at which a stream processing application received the event
    in order to perform some calculation. This time can be milliseconds, hours, or
    days after the event occurred. This notion of time assigns different timestamps
    to the same event depending on exactly when each stream processing application
    happened to read the event. It can even differ for two threads in the same application!
    Therefore, this notion of time is highly unreliable and best avoided.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是流处理应用程序接收事件以执行某些计算的时间。这个时间可以是事件发生后的毫秒、小时或天数。这种时间概念根据每个流处理应用程序读取事件的确切时间为同一事件分配不同的时间戳。甚至在同一应用程序的两个线程中也可能不同！因此，这种时间概念非常不可靠，最好避免使用。
- en: Kafka Streams assigns time to each event based on the `TimestampExtractor` interface.
    Developers of Kafka Streams applications can use different implementations of
    this interface, which can use either of the three time semantics explained previously
    or a completely different choice of timestamp, including extracting a timestamp
    from the contents of the event itself.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams根据`TimestampExtractor`接口为每个事件分配时间。Kafka Streams应用程序的开发人员可以使用此接口的不同实现，这些实现可以使用先前解释的三种时间语义之一，或者完全不同的时间戳选择，包括从事件内容中提取时间戳。
- en: 'When Kafka Streams writes output to a Kafka topic, it assigns a timestamp to
    each event based on the following rules:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kafka Streams将输出写入Kafka主题时，它根据以下规则为每个事件分配时间戳：
- en: When the output record maps directly to an input record, the output record will
    use the same timestamp as the input.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输出记录直接映射到输入记录时，输出记录将使用与输入相同的时间戳。
- en: When the output record is a result of an aggregation, the timestamp of the output
    record will be the maximum timestamp used in the aggregation.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输出记录是聚合的结果时，输出记录的时间戳将是聚合中使用的最大时间戳。
- en: When the output record is a result of joining two streams, the timestamp of
    the output record is the largest of the two records being joined. When a stream
    and a table are joined, the timestamp from the stream record is used.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输出记录是两个流的连接结果时，输出记录的时间戳是两个记录中较大的时间戳。当流和表进行连接时，使用流记录的时间戳。
- en: Finally, if the output record was generated by a Kafka Streams function that
    generates data in a specific schedule regardless of input, such as `punctuate()`,
    the output timestamp will depend on the current internal times of the stream processing
    app.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果输出记录是由Kafka Streams函数生成的，该函数会根据特定的时间表生成数据，而不考虑输入，例如`punctuate()`，输出时间戳将取决于流处理应用程序的当前内部时间。
- en: When using the Kafka Streams lower-level processing API rather than the DSL,
    Kafka Streams includes APIs for manipulating the timestamps of records directly,
    so developers can implement timestamp semantics that match the required business
    logic of the application.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Kafka Streams的低级处理API而不是DSL时，Kafka Streams包括用于直接操作记录时间戳的API，因此开发人员可以实现与应用程序所需业务逻辑相匹配的时间戳语义。
- en: Mind the Time Zone
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意时区
- en: When working with time, it is important to be mindful of time zones. The entire
    data pipeline should standardize on a single time zone; otherwise, results of
    stream operations will be confusing and often meaningless. If you must handle
    data streams with different time zones, you need to make sure you can convert
    events to a single time zone before performing operations on time windows. Often
    this means storing the time zone in the record itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理时间时，重要的是要注意时区。整个数据管道应该统一使用一个时区；否则，流操作的结果将会令人困惑并且通常毫无意义。如果必须处理具有不同时区的数据流，您需要确保在对时间窗口执行操作之前可以将事件转换为单一时区。通常这意味着将时区存储在记录本身中。
- en: State
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态
- en: As long as we only need to process each event individually, stream processing
    is a very simple activity. For example, if all we need to do is read a stream
    of online shopping transactions from Kafka, find the transactions over $10,000,
    and email the relevant salesperson, we can probably write this in just few lines
    of code using a Kafka consumer and SMTP library.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们只需要单独处理每个事件，流处理就是一项非常简单的活动。例如，如果我们只需要从Kafka中读取在线购物交易流，找到超过1万美元的交易，并向相关销售人员发送电子邮件，我们可能只需要使用Kafka消费者和SMTP库就可以写出几行代码。
- en: 'Stream processing becomes really interesting when we have operations that involve
    multiple events: counting the number of events by type, moving averages, joining
    two streams to create an enriched stream of information, etc. In those cases,
    it is not enough to look at each event by itself; we need to keep track of more
    information—how many events of each type did we see this hour, all events that
    require joining, sums, averages, etc. We call this information a *state*.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行涉及多个事件的操作时，流处理变得非常有趣：按类型计数事件的数量，移动平均值，将两个流连接起来创建丰富的信息流等。在这些情况下，仅仅查看每个事件是不够的；我们需要跟踪更多的信息——我们在这个小时内看到了多少个类型的事件，所有需要连接的事件，总和，平均值等。我们称这些信息为*状态*。
- en: It is often tempting to store the state in variables that are local to the stream
    processing app, such as a simple hash table to store moving counts. In fact, we
    did just that in many examples in this book. However, this is not a reliable approach
    for managing state in stream processing because when the stream processing application
    is stopped or crashes, the state is lost, which changes the results. This is usually
    not the desired outcome, so care should be taken to persist the most recent state
    and recover it when restarting the application.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理应用程序中，通常会诱人地将状态存储在本地变量中，例如使用简单的哈希表来存储移动计数。实际上，在本书的许多示例中，我们就是这样做的。然而，这并不是流处理中管理状态的可靠方法，因为当流处理应用程序停止或崩溃时，状态会丢失，从而改变结果。这通常不是期望的结果，因此应该小心地持久化最近的状态，并在重新启动应用程序时恢复它。
- en: 'Stream processing refers to several types of state:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理涉及几种类型的状态：
- en: Local or internal state
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本地或内部状态
- en: State that is accessible only by a specific instance of the stream processing
    application. This state is usually maintained and managed with an embedded, in-memory
    database running within the application. The advantage of local state is that
    it is extremely fast. The disadvantage is that we are limited to the amount of
    memory available. As a result, many of the design patterns in stream processing
    focus on ways to partition the data into substreams that can be processed using
    a limited amount of local state.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 只能由特定流处理应用程序实例访问的状态。这种状态通常是使用嵌入式内存数据库在应用程序内部维护和管理的。本地状态的优势在于它非常快速。缺点是我们受限于可用内存的数量。因此，流处理中的许多设计模式都专注于将数据分区为可以使用有限本地状态处理的子流。
- en: External state
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 外部状态
- en: State that is maintained in an external data store, often a NoSQL system like
    Cassandra. The advantages of an external state are its virtually unlimited size
    and the fact that it can be accessed from multiple instances of the application
    or even from different applications. The downside is the extra latency and complexity
    introduced with an additional system, as well as availability—the application
    needs to handle the possibility that the external system is not available. Most
    stream processing apps try to avoid having to deal with an external store, or
    at least limit the latency overhead by caching information in the local state
    and communicating with the external store as rarely as possible. This usually
    introduces challenges with maintaining consistency between the internal and external
    state.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部数据存储中维护的状态，通常是像Cassandra这样的NoSQL系统。外部状态的优势在于其几乎无限的大小，以及可以从应用程序的多个实例甚至不同的应用程序中访问。缺点是额外的延迟和复杂性，以及额外系统引入的可用性——应用程序需要处理外部系统不可用的可能性。大多数流处理应用程序都试图避免处理外部存储，或者至少通过在本地状态中缓存信息并尽可能少地与外部存储通信来限制延迟开销。这通常会引入在内部和外部状态之间保持一致性的挑战。
- en: Stream-Table Duality
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流-表二元性
- en: We are all familiar with database tables. A *table* is a collection of records,
    each identified by its primary key and containing a set of attributes as defined
    by a schema. Table records are mutable (i.e., tables allow update and delete operations).
    Querying a table allows checking the state of the data at a specific point in
    time. For example, by querying the `CUSTOMERS_CONTACTS` table in a database, we
    expect to find current contact details for all our customers. Unless the table
    was specifically designed to include history, we will not find their past contacts
    in the table.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都熟悉数据库表。*表*是记录的集合，每个记录由其主键标识，并包含由模式定义的一组属性。表记录是可变的（即，表允许更新和删除操作）。查询表允许检查特定时间点的数据状态。例如，通过查询数据库中的`CUSTOMERS_CONTACTS`表，我们期望找到所有客户的当前联系方式。除非表专门设计包含历史记录，否则我们在表中找不到他们过去的联系方式。
- en: Unlike tables, streams contain a history of changes. A *stream* is a string
    of events wherein each event caused a change. A table contains a current state
    of the world, which is the result of many changes. From this description, it is
    clear that streams and tables are two sides of the same coin—the world always
    changes, and sometimes we are interested in the events that caused those changes,
    whereas other times we are interested in the current state of the world. Systems
    that allow us to transition back and forth between the two ways of looking at
    data are more powerful than systems that support just one.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与表不同，流包含一系列变化的历史。*流*是一系列事件，其中每个事件都引起了变化。表包含了世界的当前状态，这是许多变化的结果。从这个描述中，很明显流和表是同一个硬币的两面——世界总是在变化，有时我们对引起这些变化的事件感兴趣，而其他时候我们对世界的当前状态感兴趣。允许我们在这两种数据观察方式之间进行转换的系统比只支持一种方式的系统更强大。
- en: To convert a table to a stream, we need to capture the changes that modify the
    table. Take all those `insert`, `update`, and `delete` events and store them in
    a stream. Most databases offer change data capture (CDC) solutions for capturing
    these changes, and there are many Kafka connectors that can pipe those changes
    into Kafka where they will be available for stream processing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要将表转换为流，我们需要捕获修改表的更改。获取所有这些`insert`、`update`和`delete`事件，并将它们存储在流中。大多数数据库都提供了用于捕获这些更改的变更数据捕获（CDC）解决方案，还有许多Kafka连接器可以将这些更改传输到Kafka中，以便进行流处理。
- en: To convert a stream to a table, we need to apply all the changes that the stream
    contains. This is also called *materializing* the stream. We create a table, either
    in memory, in an internal state store, or in an external database, and start going
    over all the events in the stream from beginning to end, changing the state as
    we go. When we finish, we have a table representing a state at a specific time
    that we can use.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要将流转换为表，我们需要应用流包含的所有更改。这也称为*实现*流。我们创建一个表，可以是在内存中、内部状态存储中或外部数据库中，并开始从头到尾遍历流中的所有事件，随着遍历的进行而改变状态。完成后，我们有一个表示特定时间状态的表，可以使用。
- en: 'Suppose we have a store selling shoes. A stream representation of our retail
    activity can be a stream of events:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一家出售鞋子的商店。我们零售活动的流表示可以是一系列事件的流：
- en: “Shipment arrived with red, blue, and green shoes.”
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “装运到达，带有红色、蓝色和绿色鞋子。”
- en: “Blue shoes sold.”
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “蓝鞋卖出。”
- en: “Red shoes sold.”
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “红鞋卖出。”
- en: “Blue shoes returned.”
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “蓝鞋退货。”
- en: “Green shoes sold.”
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “绿鞋卖出。”
- en: If we want to know what our inventory contains right now or how much money we
    made until now, we need to materialize the view. [Figure 14-1](#fig1101) shows
    that we currently have 299 red shoes. If we want to know how busy the store is,
    we can look at the entire stream and see that there were four customer events
    today. We may also want to investigate why the blue shoes were returned.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想知道我们的库存现在包含什么，或者到目前为止我们赚了多少钱，我们需要实现视图。[图14-1](#fig1101)显示我们目前有299双红鞋。如果我们想知道商店有多忙，我们可以查看整个流，并看到今天有四个客户事件。我们可能还想调查为什么蓝鞋被退回。
- en: '![kdg2 1401](assets/kdg2_1401.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1401](assets/kdg2_1401.png)'
- en: Figure 14-1\. Materializing inventory changes
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1。材料库存变化
- en: Time Windows
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间窗口
- en: 'Most operations on streams are windowed operations, operating on slices of
    time: moving averages, top products sold this week, 99th percentile load on the
    system, etc. Join operations on two streams are also windowed—we join events that
    occurred at the same slice of time. Very few people stop and think about the type
    of window they want for their operations. For example, when calculating moving
    averages, we want to know:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 流上的大多数操作都是窗口操作，操作时间片：移动平均线，本周销售的热门产品，系统上的99th百分位负载等。两个流的连接操作也是窗口化的——我们连接在同一时间片发生的事件。很少有人停下来思考他们的操作需要哪种类型的窗口。例如，当计算移动平均线时，我们想知道：
- en: Size of the window
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口的大小
- en: Do we want to calculate the average of all events in every five-minute window?
    Every 15-minute window? Or the entire day? Larger windows are smoother but they
    lag more—if the price increases, it will take longer to notice than with a smaller
    window. Kafka Streams also includes a *session window*, where the size of the
    window is defined by a period of inactivity. The developer defines a session gap,
    and all events that arrive continuously with gaps smaller than the defined session
    gap belong to the same session. A gap in arrivals will define a new session, and
    all events arriving after the gap, but before the next gap, will belong to the
    new session.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要计算每个五分钟窗口内所有事件的平均值吗？每个15分钟窗口？还是整天？较大的窗口更平滑，但滞后更多——如果价格上涨，要比较小的窗口需要更长的时间才能注意到。Kafka
    Streams还包括*会话窗口*，其中窗口的大小由不活动的时间段定义。开发人员定义了会话间隔，所有连续到达的事件，其间隔小于定义的会话间隔，都属于同一个会话。到达的间隔将定义一个新的会话，到达下一个间隔之前的所有事件将属于新的会话。
- en: How often the window moves (*advance interval*)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口移动的频率（*提前间隔*）
- en: Five-minute averages can update every minute, second, or every time there is
    a new event. Windows for which the size is a fixed time interval are called *hopping
    windows*. When the advance interval is equal to the window size, it is called
    a *tumbling window*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 五分钟平均数可以每分钟、每秒或每次有新事件时更新。窗口的大小是固定的时间间隔，称为*跳跃窗口*。当提前间隔等于窗口大小时，称为*滚动窗口*。
- en: How long the window remains updatable (*grace period*)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口保持可更新的时间长度（*宽限期*）
- en: Our five-minute moving average calculated the average for the 00:00–00:05 window.
    Now, an hour later, we are getting a few more input records with their *event
    time* showing 00:02\. Do we update the result for the 00:00–00:05 period? Or do
    we let bygones be bygones? Ideally, we’ll be able to define a certain time period
    during which events will get added to their respective time slice. For example,
    if the events were delayed up to four hours, we should recalculate the results
    and update. If events arrive later than that, we can ignore them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的五分钟移动平均线计算了00:00–00:05窗口的平均值。现在，一个小时后，我们又得到了一些输入记录，它们的*事件时间*显示为00:02。我们要更新00:00–00:05期间的结果吗？还是让过去的事情成为过去？理想情况下，我们将能够定义一个特定的时间段，在此期间事件将被添加到它们各自的时间片中。例如，如果事件延迟了四个小时，我们应该重新计算结果并更新。如果事件到达的时间晚于那个时间，我们可以忽略它们。
- en: Windows can be aligned to clock time—i.e., a five-minute window that moves every
    minute will have the first slice as 00:00–00:05 and the second as 00:01–00:06\.
    Or it can be unaligned and simply start whenever the app started, and then the
    first slice can be 03:17–03:22\. See [Figure 14-2](#fig1102) for the difference
    between these two types of windows.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口可以与时钟时间对齐——即，每分钟移动一次的五分钟窗口的第一个时间片为00:00–00:05，第二个时间片为00:01–00:06。或者它可以是不对齐的，只是在应用程序启动时开始，然后第一个时间片可以是03:17–03:22。参见[图14-2](#fig1102)了解这两种窗口类型之间的区别。
- en: '![kdg2 1402](assets/kdg2_1402.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1402](assets/kdg2_1402.png)'
- en: Figure 14-2\. Tumbling window versus hopping window
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2。滚动窗口与跳跃窗口
- en: Processing Guarantees
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理保证
- en: A key requirement for stream processing applications is the ability to process
    each record exactly once, regardless of failures. Without exactly-once guarantees,
    stream processing can’t be used in cases where accurate results are needed. As
    discussed in detail in [Chapter 8](ch08.html#exactly_once_semantics), Apache Kafka
    has support for exactly-once semantics with a transactional and idempotent producer.
    Kafka Streams uses Kafka’s transactions to implement exactly-once guarantees for
    stream processing applications. Every application that uses the Kafka Streams
    library can enable exactly-once guarantees by setting `processing.​guar⁠antee`
    to `exactly_once`. Kafka Streams version 2.6 or later includes a more efficient
    exactly-once implementation that requires Kafka brokers of version 2.5 or later.
    This efficient implementation can be enabled by setting `processing.​guar⁠antee`
    to `exactly_once_beta`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理应用程序的一个关键要求是能够确保每个记录只处理一次，而不受故障的影响。如果没有确切一次的保证，流处理无法用于需要准确结果的情况。正如在[第8章](ch08.html#exactly_once_semantics)中详细讨论的那样，Apache
    Kafka支持具有事务性和幂等性生产者的确切一次语义。Kafka Streams使用Kafka的事务来为流处理应用程序实现确切一次的保证。使用Kafka Streams库的每个应用程序都可以通过将`processing.​guar⁠antee`设置为`exactly_once`来启用确切一次的保证。Kafka
    Streams版本2.6或更高版本包括一个更高效的确切一次实现，需要Kafka经纪人的版本为2.5或更高版本。可以通过将`processing.​guar⁠antee`设置为`exactly_once_beta`来启用这种高效实现。
- en: Stream Processing Design Patterns
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理设计模式
- en: Every stream processing system is different—from the basic combination of a
    consumer, processing logic, and producer, to involved clusters like Spark Streaming
    with its machine learning libraries, and much in between. But there are some basic
    design patterns, which are known solutions to common requirements of stream processing
    architectures. We’ll review a few of those well-known patterns and show how they
    are used with a few examples.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每个流处理系统都是不同的——从基本的消费者、处理逻辑和生产者的组合，到像Spark Streaming这样的涉及机器学习库的集群，以及其中的许多其他系统。但是有一些基本的设计模式，这些模式是对流处理架构常见需求的已知解决方案。我们将回顾一些这些众所周知的模式，并展示它们如何在一些示例中使用。
- en: Single-Event Processing
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单事件处理
- en: The most basic pattern of stream processing is the processing of each event
    in isolation. This is also known as a *map/filter pattern* because it is commonly
    used to filter unnecessary events from the stream or transform each event. (The
    term *map* is based on the map/reduce pattern in which the map stage transforms
    events and the reduce stage aggregates them.)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理的最基本模式是独立处理每个事件。这也被称为*map/filter模式*，因为它通常用于从流中过滤不必要的事件或转换每个事件。（map一词基于map/reduce模式，其中map阶段转换事件，reduce阶段聚合事件。）
- en: In this pattern, the stream processing app consumes events from the stream,
    modifies each event, and then produces the events to another stream. An example
    is an app that reads log messages from a stream and writes `ERROR` events into
    a high-priority stream and the rest of the events into a low-priority stream.
    Another example is an application that reads events from a stream and modifies
    them from JSON to Avro. Such applications do not need to maintain state within
    the application because each event can be handled independently. This means that
    recovering from app failures or load-balancing is incredibly easy as there is
    no need to recover state; we can simply hand off the events to another instance
    of the app to process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式中，流处理应用程序从流中消费事件，修改每个事件，然后将事件生成到另一个流中。一个例子是一个应用程序从流中读取日志消息，并将“ERROR”事件写入高优先级流，其余事件写入低优先级流。另一个例子是一个应用程序从流中读取事件，并将其从JSON修改为Avro。这些应用程序不需要在应用程序内部维护状态，因为每个事件都可以独立处理。这意味着从应用程序故障或负载平衡中恢复非常容易，因为无需恢复状态；我们可以简单地将事件交给应用程序的另一个实例来处理。
- en: This pattern can be easily handled with a simple producer and consumer, as seen
    in [Figure 14-3](#fig1103).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式可以很容易地通过简单的生产者和消费者来处理，如[图14-3](#fig1103)所示。
- en: '![kdg2 1403](assets/kdg2_1403.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1403](assets/kdg2_1403.png)'
- en: Figure 14-3\. Single-event processing topology
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3\. 单事件处理拓扑
- en: Processing with Local State
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用本地状态进行处理
- en: Most stream processing applications are concerned with aggregating information,
    especially window aggregation. An example of this is finding the minimum and maximum
    stock prices for each day of trading and calculating a moving average.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数流处理应用都关注聚合信息，特别是窗口聚合。一个例子是找到每天交易的最低和最高股价，并计算移动平均值。
- en: These aggregations require maintaining a *state*. In our example, in order to
    calculate the minimum and average price each day, we need to store the minimum
    value, the sum, and the number of records we’ve seen up until the current time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些聚合需要维护一个*状态*。在我们的示例中，为了计算每天的最低和平均价格，我们需要存储最小值、总和和截至当前时间我们看到的记录数。
- en: All this can be done using *local* state (rather than a shared state) because
    each operation in our example is a *group by* aggregate. That is, we perform the
    aggregation per stock symbol, not on the entire stock market in general. We use
    a Kafka partitioner to make sure that all events with the same stock symbol are
    written to the same partition. Then, each instance of the application will get
    all the events from the partitions that are assigned to it (this is a Kafka consumer
    guarantee). This means that each instance of the application can maintain state
    for the subset of stock symbols that are written to the partitions that are assigned
    to it. See [Figure 14-4](#fig1104).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以使用*本地*状态（而不是共享状态）来完成，因为我们示例中的每个操作都是*group by*聚合。也就是说，我们按股票符号执行聚合，而不是在整个股票市场上执行聚合。我们使用Kafka分区器来确保所有具有相同股票符号的事件都写入同一个分区。然后，应用程序的每个实例将从分配给它的分区中获取所有事件（这是Kafka消费者的保证）。这意味着应用程序的每个实例可以为写入分配给它的分区的股票符号子集维护状态。参见[图14-4](#fig1104)。
- en: '![kdg2 1404](assets/kdg2_1404.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1404](assets/kdg2_1404.png)'
- en: Figure 14-4\. Topology for event processing with local state
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4\. 具有本地状态的事件处理拓扑
- en: 'Stream processing applications become significantly more complicated when the
    application has local state. There are several issues a stream processing application
    must address:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The local state ideally fits into the memory available to the application instance.
    Some local stores allow spilling to disk, but this has significant performance
    impact.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: We need to make sure the state is not lost when an application instance shuts
    down and that the state can be recovered when the instance starts again or is
    replaced by a different instance. This is something that Kafka Streams handles
    very well—local state is stored in-memory using embedded RocksDB, which also persists
    the data to disk for quick recovery after restarts. But all the changes to the
    local state are also sent to a Kafka topic. If a stream’s node goes down, the
    local state is not lost—it can be easily re-created by rereading the events from
    the Kafka topic. For example, if the local state contains “current minimum for
    IBM = 167.19,” we store this in Kafka so that later we can repopulate the local
    cache from this data. Kafka uses log compaction for these topics to make sure
    they don’t grow endlessly and that re-creating the state is always feasible.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Rebalancing
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Partitions sometimes get reassigned to a different consumer. When this happens,
    the instance that loses the partition must store the last good state, and the
    instance that receives the partition must know to recover the correct state.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Stream processing frameworks differ in how much they help the developer manage
    the local state they need. If our application requires maintaining local state,
    we make sure to check the framework and its guarantees. We’ll include a short
    comparison guide at the end of the chapter, but as we all know, software changes
    quickly and stream processing frameworks doubly so.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Multiphase Processing/Repartitioning
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local state is great if we need a *group by* type of aggregate. But what if
    we need a result that uses all available information? For example, suppose we
    want to publish the top 10 stocks each day—the 10 stocks that gained the most
    from opening to closing during each day of trading. Obviously, nothing we do locally
    on each application instance is enough because all the top 10 stocks could be
    in partitions assigned to other instances. What we need is a two-phase approach.
    First, we calculate the daily gain/loss for each stock symbol. We can do this
    on each instance with a local state. Then we write the results to a new topic
    with a single partition. This partition will be read by a single application instance
    that can then find the top 10 stocks for the day. The second topic, which contains
    just the daily summary for each stock symbol, is obviously much smaller with significantly
    less traffic than the topics that contain the trades themselves, and therefore
    it can be processed by a single instance of the application. Sometimes more steps
    are needed to produce the result. See [Figure 14-5](#fig1105).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: This type of multiphase processing is very familiar to those who write MapReduce
    code, where you often have to resort to multiple reduce phases. If you’ve ever
    written map-reduce code, you’ll remember that you needed a separate app for each
    reduce step. Unlike MapReduce, most stream processing frameworks allow including
    all steps in a single app, with the framework handling the details of which application
    instance (or worker) will run each step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![kdg2 1405](assets/kdg2_1405.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 14-5\. Topology that includes both local state and repartitioning steps
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Processing with External Lookup: Stream-Table Join'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes stream processing requires integration with data external to the stream—validating
    transactions against a set of rules stored in a database or enriching clickstream
    information with data about the users who clicked.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious idea on how to perform an external lookup for data enrichment is
    something like this: for every click event in the stream, look up the user in
    the profile database and write an event that includes the original click, plus
    the user age and gender, to another topic. See [Figure 14-6](#fig1106).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 进行数据丰富的外部查找的明显想法是这样的：对于流中的每个点击事件，查找个人资料数据库中的用户，并写入一个事件，其中包括原始点击事件以及用户的年龄和性别，写入另一个主题。见[图14-6](#fig1106)。
- en: '![kdg2 1406](assets/kdg2_1406.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1406](assets/kdg2_1406.png)'
- en: Figure 14-6\. Stream processing that includes an external data source
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-6\. 包括外部数据源的流处理
- en: The problem with this obvious idea is that an external lookup adds significant
    latency to the processing of every record—usually between 5 and 15 milliseconds.
    In many cases, this is not feasible. Often the additional load this places on
    the external data store is also not acceptable—stream processing systems can often
    handle 100K–500K events per second, but the database can only handle perhaps 10K
    events per second at reasonable performance. There is also added complexity around
    availability—our application will need to handle situations when the external
    DB is not available.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个明显的想法的问题在于，外部查找会给每条记录的处理增加显著的延迟，通常在5到15毫秒之间。在许多情况下，这是不可行的。通常，这会给外部数据存储增加额外的负载，这也是不可接受的。流处理系统通常可以处理10万至50万事件每秒，但数据库可能只能以合理的性能处理大约1万事件每秒。还有可用性方面的复杂性——我们的应用程序需要处理外部数据库不可用的情况。
- en: To get good performance and availability, we need to cache the information from
    the database in our stream processing application. Managing this cache can be
    challenging though—how do we prevent the information in the cache from getting
    stale? If we refresh events too often, we are still hammering the database, and
    the cache isn’t helping much. If we wait too long to get new events, we are doing
    stream processing with stale information.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得良好的性能和可用性，我们需要在流处理应用程序中缓存来自数据库的信息。然而，管理这个缓存可能会有挑战——我们如何防止缓存中的信息变得过时？如果我们太频繁地刷新事件，仍然会对数据库造成压力，而缓存并没有太大帮助。如果我们等待太久才获取新事件，我们就会使用过时的信息进行流处理。
- en: But if we can capture all the changes that happen to the database table in a
    stream of events, we can have our stream processing job listen to this stream
    and update the cache based on database change events. Capturing changes to the
    database as events in a stream is known as *change data capture* (CDC), and Kafka
    Connect has multiple connectors capable of performing CDC and converting database
    tables to a stream of change events. This allows us to keep our own private copy
    of the table and be notified whenever there is a database change event so we can
    update our own copy accordingly. See [Figure 14-7](#fig1107).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们可以捕获数据库表中发生的所有更改，并将其转换为事件流，我们可以让我们的流处理作业监听此流，并根据数据库更改事件更新缓存。将数据库更改捕获为事件流称为*变更数据捕获*（CDC），Kafka
    Connect具有多个连接器，能够执行CDC并将数据库表转换为更改事件流。这使我们能够保留表的私有副本，并在数据库发生更改事件时得到通知，以便我们相应地更新我们自己的副本。见[图14-7](#fig1107)。
- en: '![kdg2 1407](assets/kdg2_1407.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1407](assets/kdg2_1407.png)'
- en: Figure 14-7\. Topology joining a table and a stream of events, removing the
    need to involve an external data source in stream processing
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-7\. 连接表和事件流的拓扑结构，无需在流处理中涉及外部数据源。
- en: Then, when we get click events, we can look up the `user_id` at our local state
    and enrich the event. And because we are using a local state, this scales a lot
    better and will not affect the database and other apps using it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当我们收到点击事件时，我们可以在本地状态中查找`user_id`并丰富事件。由于我们使用的是本地状态，这样扩展得更好，不会影响数据库和其他使用它的应用程序。
- en: We refer to this as a *stream-table join* because one of the streams represents
    changes to a locally cached table.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其称为*流-表连接*，因为其中一个流代表对本地缓存表的更改。
- en: Table-Table Join
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表-表连接
- en: In the previous section we discussed how a table and a stream of update events
    are equivalent. We’ve discussed in detail how this works when joining a stream
    and a table. There is no reason why we can’t have those materialized tables in
    both sides of the join operation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了表和更新事件流的等效性。我们已经详细讨论了在连接流和表时的工作原理。我们没有理由不能在连接操作的两侧都有这些物化表。
- en: Joining two tables is always nonwindowed and joins the current state of both
    tables at the time the operation is performed. With Kafka Streams, we can perform
    an `equi-join`, in which both tables have the same key that is partitioned in
    the same way, and therefore the join operation can be efficiently distributed
    between a large number of application instances and machines.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 连接两个表始终是非窗口化的，并在执行操作时连接两个表的当前状态。使用Kafka Streams，我们可以执行`等值连接`，其中两个表具有相同的以相同方式分区的键，因此连接操作可以在大量应用程序实例和机器之间高效分布。
- en: Kafka Streams also supports `foreign-key join` of two tables—the key of one
    stream or table is joined with an arbitrary field from another stream or table.
    You can learn more about how it works in [“Crossing the Streams”](https://oreil.ly/f34U6),
    a talk from Kafka Summit 2020, or the more in-depth [blog post](https://oreil.ly/hlKNz).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams还支持两个表的`外键连接`——一个流或表的键与另一个流或表的任意字段进行连接。您可以在[Kafka Summit 2020](https://oreil.ly/f34U6)的演讲“Crossing
    the Streams”中了解更多信息，或者在更深入的[博客文章](https://oreil.ly/hlKNz)中了解它的工作原理。
- en: Streaming Join
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流连接
- en: Sometimes we want to join two real event streams rather than a stream with a
    table. What makes a stream “real”? If you recall the discussion at the beginning
    of the chapter, streams are unbounded. When we use a stream to represent a table,
    we can ignore most of the history in the stream because we only care about the
    current state in the table. But when we join two streams, we are joining the entire
    history, trying to match events in one stream with events in the other stream
    that have the same key and happened in the same time windows. This is why a streaming
    join is also called a *windowed join*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们想要连接两个真实的事件流，而不是一个带有表的流。什么使一个流“真实”？如果您回忆一下本章开头的讨论，流是无界的。当我们使用流来表示表时，我们可以忽略流中的大部分历史，因为我们只关心表中的当前状态。但是当我们连接两个流时，我们正在连接整个历史，试图将一个流中的事件与另一个流中在相同时间窗口内具有相同键和发生的事件进行匹配。这就是为什么流连接也被称为*窗口连接*。
- en: For example, let’s say that we have one stream with search queries that people
    entered into our website and another stream with clicks, which include clicks
    on search results. We want to match search queries with the results they clicked
    on so that we will know which result is most popular for which query. Obviously,
    we want to match results based on the search term but only match them within a
    certain time window. We assume the result is clicked seconds after the query was
    entered into our search engine. So we keep a small, few-seconds-long window on
    each stream and match the results from each window. See [Figure 14-8](#fig1108).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个包含人们输入到我们网站的搜索查询的流，以及另一个包含点击的流，其中包括对搜索结果的点击。我们希望匹配搜索查询与他们点击的结果，以便我们知道哪个结果对于哪个查询最受欢迎。显然，我们希望基于搜索词匹配结果，但只在特定时间窗口内匹配它们。我们假设结果是在输入到我们的搜索引擎后的几秒钟内被点击的。因此，我们在每个流上保留一个小的几秒钟的窗口，并匹配每个窗口中的结果。见[图14-8](#fig1108)。
- en: '![kdg2 1408](assets/kdg2_1408.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1408](assets/kdg2_1408.png)'
- en: Figure 14-8\. Joining two streams of events; these joins always involve a moving
    time window
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8。连接两个事件流；这些连接总是涉及移动时间窗口
- en: Kafka Streams supports `equi-joins`, where streams, queries, and clicks are
    partitioned on the same keys, which are also the join keys. This way, all the
    click events from `user_id:42` end up in partition 5 of the clicks topic, and
    all the search events for `user_id:42` end up in partition 5 of the search topic.
    Kafka Streams then makes sure that partition 5 of both topics is assigned to the
    same task. So this task sees all the relevant events for `user_id:42`. It maintains
    the join window for both topics in its embedded RocksDB state store, and this
    is how it can perform the join.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams支持`equi-joins`，其中流、查询和点击根据相同的键进行分区，这些键也是连接键。这样，所有来自`user_id:42`的点击事件最终都会进入点击主题的分区5，而所有`user_id:42`的搜索事件最终都会进入搜索主题的分区5。然后，Kafka
    Streams确保将两个主题的分区5分配给同一个任务。因此，该任务可以看到`user_id:42`的所有相关事件。它在其嵌入式RocksDB状态存储中维护了两个主题的连接窗口，这就是它执行连接的方式。
- en: Out-of-Sequence Events
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序错误的事件
- en: Handling events that arrive at the stream at the wrong time is a challenge not
    just in stream processing but also in traditional ETL systems. Out-of-sequence
    events happen quite frequently and expectedly in IoT scenarios ([Figure 14-9](#fig1109)).
    For example, a mobile device loses WiFi signal for a few hours and sends a few
    hours’ worth of events when it reconnects. This also happens when monitoring network
    equipment (a faulty switch doesn’t send diagnostics signals until it is repaired)
    or manufacturing (network connectivity in plants is notoriously unreliable, especially
    in developing countries).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 处理在错误时间到达流的事件不仅是流处理中的挑战，也是传统ETL系统中的挑战。在物联网场景中，顺序错误的事件经常而且预期地发生（[图14-9](#fig1109)）。例如，移动设备在几个小时内失去WiFi信号，并在重新连接时发送了几小时的事件。这也发生在监视网络设备（故障交换机在修复之前不会发送诊断信号）或制造业（工厂的网络连接在发展中国家尤其不可靠）。
- en: '![kdg2 1409](assets/kdg2_1409.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1409](assets/kdg2_1409.png)'
- en: Figure 14-9\. Out-of-sequence events
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-9。事件的顺序错误
- en: 'Our streams applications need to be able to handle those scenarios. This typically
    means the application has to do the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的流应用程序需要能够处理这些情况。这通常意味着应用程序必须执行以下操作：
- en: Recognize that an event is out of sequence—this requires that the application
    examines the event time and discovers that it is older than the current time.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到事件的顺序错误——这需要应用程序检查事件时间并发现它比当前时间更早。
- en: Define a time period during which it will attempt to reconcile out-of-sequence
    events. Perhaps a three-hour delay should be reconciled, and events over three
    weeks old can be thrown away.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个时间段，在此期间将尝试协调顺序错误的事件。也许应该协调三小时的延迟，并且超过三周的事件可以被丢弃。
- en: Have an in-band capability to reconcile this event. This is the main difference
    between streaming apps and batch jobs. If we have a daily batch job and a few
    events arrived after the job completed, we can usually just rerun yesterday’s
    job and update the events. With stream processing, there is no “rerun yesterday’s
    job”—the same continuous process needs to handle both old and new events at any
    given moment.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有内部能力来协调此事件。这是流应用程序和批处理作业之间的主要区别。如果我们有一个每日批处理作业，并且在作业完成后有一些事件到达，通常我们可以重新运行昨天的作业并更新事件。对于流处理，没有“重新运行昨天的作业”——同一连续过程需要在任何给定时刻处理旧事件和新事件。
- en: Be able to update results. If the results of the stream processing are written
    into a database, a *put* or *update* is enough to update the results. If the stream
    app sends results by email, updates may be trickier.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够更新结果。如果流处理的结果被写入数据库，*put*或*update*就足以更新结果。如果流应用程序通过电子邮件发送结果，更新可能会更加棘手。
- en: Several stream processing frameworks, including Google’s Dataflow and Kafka
    Streams, have built-in support for the notion of event time independent of the
    processing time, and the ability to handle events with event times that are older
    or newer than the current processing time. This is typically done by maintaining
    multiple aggregation windows available for update in the local state and giving
    developers the ability to configure how long to keep those window aggregates available
    for updates. Of course, the longer the aggregation windows are kept available
    for updates, the more memory is required to maintain the local state.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 包括Google的Dataflow和Kafka Streams在内的几个流处理框架都内置了对事件时间的支持，独立于处理时间，并且能够处理事件时间早于或晚于当前处理时间的事件。这通常是通过在本地状态中维护多个可用于更新的聚合窗口，并让开发人员能够配置保留这些窗口聚合可用于更新的时间长度来实现的。当然，聚合窗口保持可用于更新的时间越长，就需要更多的内存来维护本地状态。
- en: The Kafka Streams API always writes aggregation results to result topics. Those
    are usually `compacted topics`, which means that only the latest value for each
    key is preserved. In case the results of an aggregation window need to be updated
    as a result of a late event, Kafka Streams will simply write a new result for
    this aggregation window, which will effectively replace the previous result.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams API总是将聚合结果写入结果主题。这些通常是`压缩主题`，这意味着仅保留每个键的最新值。如果需要更新聚合窗口的结果以响应延迟事件，Kafka
    Streams将简单地为此聚合窗口写入新结果，这将有效地替换先前的结果。
- en: Reprocessing
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新处理
- en: 'The last important pattern is reprocessing events. There are two variants of
    this pattern:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个重要的模式是重新处理事件。这种模式有两种变体：
- en: We have an improved version of our stream processing application. We want to
    run the new version of the application on the same event stream as the old, produce
    a new stream of results that does not replace the first version, compare the results
    between the two versions, and at some point move clients to use the new results
    instead of the existing ones.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个改进版的流处理应用程序。我们希望在与旧版本相同的事件流上运行新版本的应用程序，生成一个不会替换第一个版本的新结果流，比较两个版本之间的结果，并在某个时候将客户端切换到使用新结果而不是现有结果。
- en: The existing stream processing app is buggy. We fix the bug, and we want to
    reprocess the event stream and recalculate our results
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的流处理应用程序存在错误。我们修复了错误，我们想重新处理事件流并重新计算结果
- en: 'The first use case is made simple by the fact that Apache Kafka stores the
    event streams in their entirety for long periods of time in a scalable data store.
    This means that having two versions of a stream processing application writing
    two result streams only requires the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个用例之所以变得简单，是因为Apache Kafka在可扩展的数据存储中长时间存储事件流的全部内容。这意味着运行两个版本的流处理应用程序并写入两个结果流只需要以下步骤：
- en: Spinning up the new version of the application as a new consumer group
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新版本的应用程序作为新的消费者组启动。
- en: Configuring the new version to start processing from the first offset of the
    input topics (so it will get its own copy of all events in the input streams)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置新版本从输入主题的第一个偏移量开始处理（这样它将获得输入流中所有事件的副本）
- en: Letting the new application continue processing, and switching the client applications
    to the new result stream when the new version of the processing job has caught
    up
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让新应用程序继续处理，并在新版本的处理作业赶上时切换客户端应用程序到新的结果流
- en: The second use case is more challenging—it requires “resetting” an existing
    app to start processing back at the beginning of the input streams, resetting
    the local state (so we won’t mix results from the two versions of the app), and
    possibly cleaning the previous output stream. While Kafka Streams has a tool for
    resetting the state for a stream processing app, our recommendation is to try
    to use the first method whenever sufficient capacity exists to run two copies
    of the app and generate two result streams. The first method is much safer—it
    allows switching back and forth between multiple versions and comparing results
    between versions, and doesn’t risk losing critical data or introducing errors
    during the cleanup process.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个用例更具挑战性——它需要“重置”现有应用程序，以便从输入流的开头重新开始处理，重置本地状态（这样我们就不会混合两个应用程序版本的结果），并可能清理先前的输出流。虽然Kafka
    Streams有一个用于重置流处理应用程序状态的工具，但我们建议在有足够的容量运行两个应用程序副本并生成两个结果流时尝试使用第一种方法。第一种方法更安全——它允许在多个版本之间来回切换并比较结果，而不会在清理过程中丢失关键数据或引入错误。
- en: Interactive Queries
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互式查询
- en: As discussed previously, stream processing applications have state, and this
    state can be distributed among many instances of the application. Most of the
    time the users of stream processing applications get the results of the processing
    by reading them from an output topic. In some cases, however, it is desirable
    to take a shortcut and read the results from the state store itself. This is common
    when the result is a table (e.g., the top 10 best-selling books) and the stream
    of results is really a stream of updates to this table—it is much faster and easier
    to just read the table directly from the stream processing application state.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，流处理应用程序具有状态，并且这种状态可以分布在应用程序的许多实例之间。大多数情况下，流处理应用程序的用户通过从输出主题中读取结果来获取处理结果。然而，在某些情况下，希望通过快捷方式从状态存储中读取结果。当结果是一个表时（例如，畅销书的前10名），并且结果流实际上是对该表的更新流时，直接从流处理应用程序状态中读取表会更快、更容易。
- en: Kafka Streams includes flexible APIs for [querying the state of a stream processing
    application](https://oreil.ly/pCGeC).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams包括灵活的API，用于[查询流处理应用程序的状态](https://oreil.ly/pCGeC)。
- en: Kafka Streams by Example
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams示例
- en: To demonstrate how these patterns are implemented in practice, we’ll show a
    few examples using the Apache Kafka Streams API. We are using this specific API
    because it is relatively simple to use and it ships with Apache Kafka, which we
    already have access to. It is important to remember that the patterns can be implemented
    in any stream processing framework and library—the patterns are universal, but
    the examples are specific.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka has two stream APIs—a low-level Processor API and a high-level
    Streams DSL. We will use the Kafka Streams DSL in our examples. The DSL allows
    us to define the stream processing application by defining a chain of transformations
    to events in the streams. Transformations can be as simple as a filter or as complex
    as a stream-to-stream join. The lower-level API allows us to create our own transformations.
    To learn more about the low-level Processor API, the [developer guide](https://oreil.ly/bQ5nE)
    has detailed information, and the presentation [“Beyond the DSL”](https://oreil.ly/4vson)
    is a great introduction.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: An application that uses the DSL API always starts with using the `StreamsBuilder`
    to create a processing *topology*—a directed acyclic graph (DAG) of transformations
    that are applied to the events in the streams. Then we create a `KafkaStreams`
    execution object from the topology. Starting the `KafkaStreams` object will start
    multiple threads, each applying the processing topology to events in the stream.
    The processing will conclude when we close the `KafkaStreams` object.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at a few examples that use Kafka Streams to implement some of the
    design patterns we just discussed. A simple word count example will be used to
    demonstrate the map/filter pattern and simple aggregates. Then we’ll move to an
    example where we calculate different statistics on stock market trades, which
    will allow us to demonstrate window aggregations. Finally, we’ll use ClickStream
    enrichment as an example to demonstrate streaming joins.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Word Count
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s walk through an abbreviated word count example for Kafka Streams. You
    can find the full example on [GitHub](http://bit.ly/2ri00gj).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you do when creating a stream processing app is configure Kafka
    Streams. Kafka Streams has a large number of possible configurations, which we
    won’t discuss here, but you can find them in the [documentation](http://bit.ly/2t7obPU).
    In addition, you can configure the producer and consumer embedded in Kafka Streams
    by adding any producer or consumer config to the `Properties` object:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_stream_processing_CO1-1)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Every Kafka Streams application must have an application ID. It is used to coordinate
    the instances of the application and also when naming the internal local stores
    and the topics related to them. This name must be unique for each Kafka Streams
    application working with the same Kafka cluster.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_stream_processing_CO1-2)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka Streams application always reads data from Kafka topics and writes
    its output to Kafka topics. As we’ll discuss later, Kafka Streams applications
    also use Kafka for coordination. So we had better tell our app where to find Kafka.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_stream_processing_CO1-3)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: When reading and writing data, our app will need to serialize and deserialize,
    so we provide default Serde classes. If needed, we can override these defaults
    later when building the streams topology.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the configuration, let’s build our streams topology:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_stream_processing_CO2-1)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We create a `StreamsBuilder` object and start defining a stream by pointing
    at the topic we’ll use as our input.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_stream_processing_CO2-2)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Each event we read from the source topic is a line of words; we split it up
    using a regular expression into a series of individual words. Then we take each
    word (currently a value of the event record) and put it in the event record key
    so it can be used in a group-by operation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_stream_processing_CO2-3)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_stream_processing_CO2-3)'
- en: We filter out the word *the*, just to show how easy filtering is.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们过滤掉了单词*the*，只是为了展示过滤有多容易。
- en: '[![4](assets/4.png)](#co_stream_processing_CO2-4)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_stream_processing_CO2-4)'
- en: And we group by key, so we now have a collection of events for each unique word.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们按键分组，所以现在我们对于每个唯一单词都有一个事件集合。
- en: '[![5](assets/5.png)](#co_stream_processing_CO2-5)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_stream_processing_CO2-5)'
- en: We count how many events we have in each collection. The result of counting
    is a `Long` data type. We convert it to a `String` so it will be easier for humans
    to read the results.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算每个集合中有多少事件。计数的结果是`Long`数据类型。我们将其转换为`String`，这样人类就可以更容易地阅读结果。
- en: '[![6](assets/6.png)](#co_stream_processing_CO2-6)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_stream_processing_CO2-6)'
- en: Only one thing left—write the results back to Kafka.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 只剩下一件事——将结果写回Kafka。
- en: 'Now that we have defined the flow of transformations that our application will
    run, we just need to…run it:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了应用程序将运行的转换流程，我们只需要…运行它：
- en: '[PRE2]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_stream_processing_CO3-1)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_stream_processing_CO3-1)'
- en: Define a `KafkaStreams` object based on our topology and the properties we defined.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们定义的拓扑和属性，定义一个`KafkaStreams`对象。
- en: '[![2](assets/2.png)](#co_stream_processing_CO3-2)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_stream_processing_CO3-2)'
- en: Start Kafka Streams.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Kafka Streams。
- en: '[![3](assets/3.png)](#co_stream_processing_CO3-3)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_stream_processing_CO3-3)'
- en: After a while, stop it.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，停止它。
- en: That’s it! In just a few short lines, we demonstrated how easy it is to implement
    a single event processing pattern (we applied a map and a filter on the events).
    We repartitioned the data by adding a group-by operator and then maintained simple
    local state when we counted the number of records that have each word as a key.
    Then we maintained simple local state when we counted the number of times each
    word appeared.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在几行简短的代码中，我们演示了实现单个事件处理模式的简易性（我们在事件上应用了map和filter）。我们通过添加group-by操作符重新分区数据，然后在计算每个单词作为键的记录数量时保持了简单的本地状态。然后我们在计算每个单词出现的次数时保持了简单的本地状态。
- en: At this point, we recommend running the full example. The [README in the GitHub
    repository](http://bit.ly/2sOXzUN) contains instructions on how to run the example.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们建议运行完整的示例。[GitHub存储库中的README](http://bit.ly/2sOXzUN)包含了如何运行示例的说明。
- en: Note that we can run the entire example on our machine without installing anything
    except Apache Kafka. If our input topic contains multiple partitions, we can run
    multiple instances of the `WordCount` application (just run the app in several
    different terminal tabs), and we have our first Kafka Streams processing cluster.
    The instances of the `WordCount` application talk to one another and coordinate
    the work. One of the biggest barriers to entry for some stream processing frameworks
    is that local mode is very easy to use, but then to run a production cluster,
    we need to install YARN or Mesos, then install the processing framework on all
    those machines, and then learn how to submit our app to the cluster. With the
    Kafka’s Streams API, we just start multiple instances of our app—and we have a
    cluster. The exact same app is running on our development machine and in production.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以在我们的机器上运行整个示例，而无需安装除了Apache Kafka之外的任何东西。如果我们的输入主题包含多个分区，我们可以运行多个`WordCount`应用程序的实例（只需在几个不同的终端标签中运行应用程序），然后我们就有了第一个Kafka
    Streams处理集群。`WordCount`应用程序的实例相互交流并协调工作。对于一些流处理框架来说，最大的入门障碍之一是本地模式非常容易使用，但是要运行生产集群，我们需要安装YARN或Mesos，然后在所有这些机器上安装处理框架，然后学习如何将我们的应用程序提交到集群。使用Kafka的Streams
    API，我们只需启动多个应用程序实例，就可以得到一个集群。完全相同的应用程序在我们的开发机器上和生产环境中运行。
- en: Stock Market Statistics
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 股票市场统计
- en: The next example is more involved—we will read a stream of stock market trading
    events that include the stock ticker, ask price, and ask size. In stock market
    trades, *ask price* is what a seller is asking for, whereas *bid price* is what
    the buyer is suggesting to pay. *Ask size* is the number of shares the seller
    is willing to sell at that price. For simplicity of the example, we’ll ignore
    bids completely. We also won’t include a timestamp in our data; instead, we’ll
    rely on event time populated by our Kafka producer.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例更加复杂——我们将读取一系列股票市场交易事件，其中包括股票代码、要价和要价大小。在股票市场交易中，*要价*是卖方要价，而*出价*是买方建议支付的价格。*要价大小*是卖方愿意以该价格出售的股票数量。为了简化示例，我们完全忽略了出价。我们也不会在我们的数据中包含时间戳；相反，我们将依赖由我们的Kafka生产者填充的事件时间。
- en: 'We will then create output streams that contain a few windowed statistics:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创建包含一些窗口统计信息的输出流：
- en: Best (i.e., minimum) ask price for every five-second window
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每五秒窗口的最佳（即最低）要价
- en: Number of trades for every five-second window
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每五秒窗口的交易数量
- en: Average ask price for every five-second window
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每五秒窗口的平均要价
- en: All statistics will be updated every second.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 所有统计数据将每秒更新一次。
- en: 'For simplicity, we’ll assume our exchange only has 10 stock tickers trading
    in it. The setup and configuration are very similar to those we used in [“Word
    Count”](#word_count_example):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们假设我们的交易所只有10个股票代码在交易中。设置和配置与我们在[“Word Count”](#word_count_example)中使用的非常相似。
- en: '[PRE3]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The main difference is the Serde classes used. In [“Word Count”](#word_count_example),
    we used strings for both key and value, and therefore used the `Serdes.String()`
    class as a serializer and deserializer for both. In this example, the key is still
    a string, but the value is a `Trade` object that contains the ticker symbol, ask
    price, and ask size. In order to serialize and deserialize this object (and a
    few other objects we used in this small app), we used the Gson library from Google
    to generate a JSON serializer and deserializer from our `Java` object. Then we
    created a small wrapper that created a Serde object from those. Here is how we
    created the Serde:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Nothing fancy, but remember to provide a Serde object for every object you want
    to store in Kafka—input, output, and, in some cases, intermediate results. To
    make this easier, we recommend generating these Serdes through a library like
    Gson, Avro, Protobuf, or something similar.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have everything configured, it’s time to build our topology:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_stream_processing_CO4-1)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We start by reading events from the input topic and performing a `groupByKey()`
    operation. Despite its name, this operation does not do any grouping. Rather,
    it ensures that the stream of events is partitioned based on the record key. Since
    we wrote the data into a topic with a key and didn’t modify the key before calling
    `groupByKey()`, the data is still partitioned by its key—so this method does nothing
    in this case.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_stream_processing_CO4-2)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We define the window—in this case, a window of five seconds, advancing every
    second.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_stream_processing_CO4-3)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: After we ensure correct partitioning and windowing, we start the aggregation.
    The `aggregate` method will split the stream into overlapping windows (a five-second
    window every second) and then apply an aggregate method on all the events in the
    window. The first parameter this method takes is a new object that will contain
    the results of the aggregation—`Tradestats`, in our case. This is an object we
    created to contain all the statistics we are interested in for each time window—minimum
    price, average price, and number of trades.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_stream_processing_CO4-4)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: We then supply a method for actually aggregating the records—in this case, an
    `add` method of the `Tradestats` object is used to update the minimum price, number
    of trades, and total prices in the window with the new record.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_stream_processing_CO4-5)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [“Stream Processing Design Patterns”](#stream_Processing_design),
    windowing aggregation requires maintaining a state and a local store in which
    the state will be maintained. The last parameter of the aggregate method is the
    configuration of the state store. `Materialized` is the store configuration object,
    and we configure the store name as `trade-aggregates`. This can be any unique
    name.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_stream_processing_CO4-6)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: As part of the state store configuration, we also provide a Serde object for
    serializing and deserializing the results of the aggregation (the `Tradestats`
    object).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_stream_processing_CO4-7)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The results of the aggregation is a *table* with the ticker and the time window
    as the primary key and the aggregation result as the value. We are turning the
    table back into a stream of events.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_stream_processing_CO4-8)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to update the average price—right now the aggregation results
    include the sum of prices and number of trades. We go over these records and use
    the existing statistics to calculate average price so we can include it in the
    output stream.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_stream_processing_CO4-9)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: And finally, we write the results back to the `stockstats-output` stream. Since
    the results are part of a windowing operation, we create a `WindowedSerde` that
    stores the result in a windowed data format that includes the window timestamp.
    The window size is passed as part of the Serde, even though it isn’t used in the
    serialization (deserialization requires the window size, because only the start
    time of the window is stored in the output topic).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: After we define the flow, we use it to generate a `KafkaStreams` object and
    run it, just like we did in [“Word Count”](#word_count_example).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'This example shows how to perform windowed aggregation on a stream—probably
    the most popular use case of stream processing. One thing to notice is how little
    work was needed to maintain the local state of the aggregation—just provide a
    Serde and name the state store. Yet this application will scale to multiple instances
    and automatically recover from a failure of each instance by shifting processing
    of some partitions to one of the surviving instances. We will see more on how
    it is done in [“Kafka Streams: Architecture Overview”](#kafkastreamsarchitecture).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: As usual, you can find the complete example, including instructions for running
    it, on [GitHub](http://bit.ly/2r6BLm1).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ClickStream Enrichment
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last example will demonstrate streaming joins by enriching a stream of clicks
    on a website. We will generate a stream of simulated clicks, a stream of updates
    to a fictional profile database table, and a stream of web searches. We will then
    join all three streams to get a 360-degree view into each user activity. What
    did the users search for? What did they click as a result? Did they change their
    “interests” in their user profile? These kinds of joins provide a rich data collection
    for analytics. Product recommendations are often based on this kind of information—the
    user searched for bikes, clicked on links for “Trek,” and is interested in travel,
    so we can advertise bikes from Trek, helmets, and bike tours to exotic locations
    like Nebraska.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Since configuring the app is similar to the previous examples, let’s skip this
    part and take a look at the topology for joining multiple streams:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_stream_processing_CO5-1)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a streams objects for the two streams we want to join—clicks
    and searches. When we create the stream object, we pass the input topic and the
    key and value Serde that will be used when consuming records out of the topic
    and deserializing them into input objects.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_stream_processing_CO5-2)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We also define a `KTable` for the user profiles. A `KTable` is a materialized
    store that is updated through a stream of changes.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_stream_processing_CO5-3)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Then we enrich the stream of clicks with user profile information by joining
    the stream of events with the profile table. In a stream-table join, each event
    in the stream receives information from the cached copy of the profile table.
    We are doing a left-join, so clicks without a known user will be preserved.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_stream_processing_CO5-4)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: This is the `join` method—it takes two values, one from the stream and one from
    the record, and returns a third value. Unlike in databases, we get to decide how
    to combine the two values into one result. In this case, we created one `activity`
    object that contains both the user details and the page viewed.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_stream_processing_CO5-5)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Next, we want to `join` the click information with searches performed by the
    same user. This is still a left `join`, but now we are joining two streams, not
    streaming to a table.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_stream_processing_CO5-6)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: This is the `join` method—we simply add the search terms to all the matching
    page views.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_stream_processing_CO5-7)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: This is the interesting part—a *stream-to-stream join* is a join with a time
    window. Joining all clicks and searches for each user doesn’t make much sense—we
    want to join each search with clicks that are related to it, that is, clicks that
    occurred a short period of time after the search. So we define a join window of
    one second. We invoke `of` to create a window of one second before and after each
    search, and then we call `before` with a zero-seconds interval to make sure we
    only join clicks that happen one second after each search and not before. The
    results will include relevant clicks, search terms, and the user profile. This
    will allow a full analysis of searches and their results.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有趣的部分 - *流到流连接* 是一个带有时间窗口的连接。连接每个用户的所有点击和搜索并没有太多意义 - 我们希望将每个搜索与与之相关的点击连接起来，也就是发生在搜索后的短时间内的点击。因此，我们定义了一个一秒钟的连接窗口。我们调用
    `of` 来创建一个搜索前后一秒钟的窗口，然后我们调用 `before` 以零秒的间隔来确保我们只连接每次搜索后一秒钟发生的点击而不是之前的点击。结果将包括相关的点击、搜索词和用户资料。这将允许对搜索及其结果进行全面分析。
- en: '[![8](assets/8.png)](#co_stream_processing_CO5-8)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#co_stream_processing_CO5-8)'
- en: We define the Serde of the join result here. This includes a Serde for the key
    that both sides of the join have in common and the Serde for both values that
    will be included in the result of the join. In this case, the key is the user
    ID, so we use a simple `Integer` Serde.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里定义连接结果的 Serde。这包括连接两侧共有的键的 Serde，以及将包含在连接结果中的两个值的 Serde。在这种情况下，键是用户 ID，所以我们使用一个简单的
    `Integer` Serde。
- en: After we define the flow, we use it to generate a `KafkaStreams` object and
    run it, just like we did in [“Word Count”](#word_count_example).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义流程之后，我们使用它来生成一个 `KafkaStreams` 对象并运行它，就像我们在 [“Word Count”](#word_count_example)
    中所做的那样。
- en: This example shows two different join patterns possible in stream processing.
    One joins a stream with a table to enrich all streaming events with information
    in the table. This is similar to joining a fact table with a dimension when running
    queries on a data warehouse. The second example joins two streams based on a time
    window. This operation is unique to stream processing.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了流处理中可能存在的两种不同的连接模式。一个是将流与表连接，以丰富表中的所有流事件信息。这类似于在数据仓库上运行查询时将事实表与维度表连接。第二个例子是基于时间窗口连接两个流。这个操作是流处理中独有的。
- en: As usual, you can find the complete example, including instructions for running
    it, on [GitHub](http://bit.ly/2sq096i).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您可以在 [GitHub](http://bit.ly/2sq096i) 上找到完整的示例，包括运行它的说明。
- en: 'Kafka Streams: Architecture Overview'
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams：架构概述
- en: The examples in the previous section demonstrated how to use the Kafka Streams
    API to implement a few well-known stream processing design patterns. But to understand
    better how Kafka’s Streams library actually works and scales, we need to peek
    under the covers and understand some of the design principles behind the API.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节的示例演示了如何使用 Kafka Streams API 实现一些众所周知的流处理设计模式。但要更好地理解 Kafka 的 Streams 库实际上是如何工作和扩展的，我们需要窥探一下其内部，并了解
    API 背后的一些设计原则。
- en: Building a Topology
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建拓扑
- en: Every streams application implements and executes one *topology*. Topology (also
    called DAG, or directed acyclic graph, in other stream processing frameworks)
    is a set of operations and transitions that every event moves through from input
    to output. [Figure 14-10](#fig1110) shows the topology in [“Word Count”](#word_count_example).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 每个流应用程序都实现并执行一个*拓扑*。拓扑（在其他流处理框架中也称为 DAG 或有向无环图）是一组操作和转换，每个事件都从输入到输出经过。[图 14-10](#fig1110)
    显示了 [“Word Count”](#word_count_example) 中的拓扑。
- en: '![kdg2 1410](assets/kdg2_1410.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1410](assets/kdg2_1410.png)'
- en: Figure 14-10\. Topology for the word-count stream processing example
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-10\. 单词计数流处理示例的拓扑
- en: Even a simple app has a nontrivial topology. The topology is made up of processors—those
    are the nodes in the topology graph (represented by circles in our diagram). Most
    processors implement an operation of the data—filter, map, aggregate, etc. There
    are also source processors, which consume data from a topic and pass it on, and
    sink processors, which take data from earlier processors and produce it to a topic.
    A topology always starts with one or more source processors and finishes with
    one or more sink processors.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一个简单的应用程序也有一个非平凡的拓扑结构。拓扑由处理器组成 - 这些是拓扑图中的节点（在我们的图表中用圆圈表示）。大多数处理器实现数据操作 -
    过滤、映射、聚合等。还有源处理器，它们从主题中消费数据并传递数据，以及汇处理器，它们从先前的处理器获取数据并将其生成到主题中。拓扑始终以一个或多个源处理器开始，并以一个或多个汇处理器结束。
- en: Optimizing a Topology
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化拓扑
- en: By default, Kafka Streams executes applications that were built with the DSL
    API by mapping each DSL method independently to a lower-level equivalent. By evaluating
    each DSL method independently, opportunities to optimize the overall resulting
    topology were missed.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kafka Streams 通过将每个 DSL 方法独立映射到较低级别的等效方法来执行使用 DSL API 构建的应用程序。通过独立评估每个
    DSL 方法，错过了优化整体拓扑的机会。
- en: 'However, note that the execution of a Kafka Streams application is a three-step
    process:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，Kafka Streams 应用程序的执行是一个三步过程：
- en: The logical topology is defined by creating `KStream` and `KTable` objects and
    performing DSL operations, such as filter and join, on them.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑拓扑是通过创建 `KStream` 和 `KTable` 对象并在它们上执行 DSL 操作（如过滤和连接）来定义的。
- en: '`StreamsBuilder.build()` generates a physical topology from the logical topology.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`StreamsBuilder.build()` 从逻辑拓扑生成物理拓扑。'
- en: '`KafkaStreams.start()` executes the topology—this is where data is consumed,
    processed, and produced.'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`KafkaStreams.start()` 执行拓扑 - 这是数据被消费、处理和生成的地方。'
- en: The second step, where the physical topology is generated from the logical definitions,
    is where overall optimizations to the plan can be applied.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，从逻辑定义生成物理拓扑，是可以应用整体优化计划的地方。
- en: Currently, Apache Kafka only contains a few optimizations, mostly around reusing
    topics where possible. These can be enabled by setting `StreamsConfig.​TOPOL⁠OGY_OPTIMIZATION`
    to `StreamsConfig.OPTIMIZE` and calling `build(props)`. If you only call `build()`
    without passing the config, optimization is still disabled. It is recommended
    to test applications with and without optimizations and to compare execution times
    and volumes of data written to Kafka, and of course, validate that the results
    are identical in various known scenarios.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Apache Kafka仅包含了一些优化，主要是围绕尽可能重用主题。这些可以通过将`StreamsConfig.​TOPOL⁠OGY_OPTIMIZATION`设置为`StreamsConfig.OPTIMIZE`并调用`build(props)`来启用。如果只调用`build()`而不传递配置，则仍然禁用优化。建议测试应用程序时启用和禁用优化，并比较执行时间和写入Kafka的数据量，当然，还要验证在各种已知场景中结果是否相同。
- en: Testing a Topology
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试拓扑结构
- en: Generally speaking, we want to test software before using it in scenarios where
    its successful execution is important. Automated testing is considered the gold
    standard. Repeatable tests that are evaluated every time a change is made to a
    software application or library enable fast iterations and easier troubleshooting.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们希望在重要的成功执行的场景中使用软件之前对其进行测试。自动化测试被认为是黄金标准。每次对软件应用程序或库进行更改时，都会进行可重复的测试，以实现快速迭代和更容易的故障排除。
- en: We want to apply the same kind of methodology to our Kafka Streams applications.
    In addition to automated end-to-end tests that run the stream processing application
    against a staging environment with generated data, we’ll want to also include
    faster, lighter-weight, and easier-to-debug unit and integration tests.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将相同的方法论应用到我们的Kafka Streams应用程序中。除了自动化的端到端测试，该测试会针对一个包含生成数据的暂存环境运行流处理应用程序，我们还希望包括更快、更轻量和更易于调试的单元测试和集成测试。
- en: The main testing tool for Kafka Streams applications is `TopologyTestDriver`.
    Since its introduction in version 1.1.0, its API has undergone significant improvements,
    and versions since 2.4 are convenient and easy to use. These tests look like normal
    unit tests. We define input data, produce it to mock input topics, run the topology
    with the test driver, read the results from mock output topics, and validate the
    result by comparing it to expected values.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams应用程序的主要测试工具是`TopologyTestDriver`。自从1.1.0版本引入以来，其API经历了重大改进，自2.4版本以来变得方便且易于使用。这些测试看起来像普通的单元测试。我们定义输入数据，将其生成到模拟输入主题，使用测试驱动程序运行拓扑，从模拟输出主题中读取结果，并通过将其与预期值进行比较来验证结果。
- en: We recommend using the `TopologyTestDriver` for testing stream processing applications,
    but since it does not simulate Kafka Streams caching behavior (an optimization
    not discussed in this book, entirely unrelated to the state store itself, which
    is simulated by this framework), there are entire classes of errors that it will
    not detect.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用`TopologyTestDriver`来测试流处理应用程序，但由于它不模拟Kafka Streams的缓存行为（这本书中未讨论的一种优化，与状态存储本身完全无关，这个框架模拟了它），因此它将无法检测到整个类别的错误。
- en: 'Unit tests are typically complemented by integration tests, and for Kafka Streams,
    there are two popular integration test frameworks: `EmbeddedKafkaCluster` and
    `Testcontainers`. The former runs Kafka brokers inside the JVM that runs the tests,
    while the latter runs Docker containers with Kafka brokers (and many other components,
    as needed for the tests). `Testcontainers` is recommended, since by using Docker
    it fully isolates Kafka, its dependencies, and its resource usage from the application
    we are trying to test.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试通常与集成测试相辅相成，对于Kafka Streams，有两种流行的集成测试框架：`EmbeddedKafkaCluster`和`Testcontainers`。前者在运行测试的JVM内部运行Kafka代理，而后者在Docker容器中运行Kafka代理（以及根据测试需要的许多其他组件）。推荐使用`Testcontainers`，因为它通过使用Docker完全隔离了Kafka及其依赖项和资源使用情况，使其与我们要测试的应用程序分离。
- en: This is just a short overview of Kafka Streams testing methodologies. We recommend
    reading the [“Testing Kafka Streams—A Deep Dive”](https://oreil.ly/RvTIA) blog
    post for deeper explanations and detailed code examples of topologies and tests.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是Kafka Streams测试方法论的简要概述。我们建议阅读[“测试Kafka Streams—深入探讨”](https://oreil.ly/RvTIA)博客文章，以获取更深入的解释和拓扑结构以及测试的详细代码示例。
- en: Scaling a Topology
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拓扑结构的扩展
- en: Kafka Streams scales by allowing multiple threads of executions within one instance
    of the application and by supporting load balancing between distributed instances
    of the application. We can run the Streams application on one machine with multiple
    threads or on multiple machines; in either case, all active threads in the application
    will balance the work involved in data processing.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams通过允许应用程序的一个实例内的多个执行线程以及支持在分布式应用程序的分布式实例之间进行负载平衡来实现扩展。我们可以在一个机器上使用多个线程或在多台机器上运行流应用程序；在任何情况下，应用程序中的所有活动线程都将平衡数据处理所涉及的工作。
- en: 'The Streams engine parallelizes execution of a topology by splitting it into
    tasks. The number of tasks is determined by the Streams engine and depends on
    the number of partitions in the topics that the application processes. Each task
    is responsible for a subset of the partitions: the task will subscribe to those
    partitions and consume events from them. For every event it consumes, the task
    will execute all the processing steps that apply to this partition in order before
    eventually writing the result to the sink. Those tasks are the basic unit of parallelism
    in Kafka Streams, because each task can execute independently of others. See [Figure 14-11](#fig1111).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 流引擎通过将拓扑结构分割成任务来并行执行。任务的数量由流引擎确定，并取决于应用程序处理的主题中的分区数量。每个任务负责一部分分区：任务将订阅这些分区并从中消费事件。对于它消费的每个事件，任务将按顺序执行适用于该分区的所有处理步骤，最终将结果写入到汇聚点。这些任务是Kafka
    Streams中的并行基本单元，因为每个任务可以独立执行。参见[图14-11](#fig1111)。
- en: '![kdg2 1411](assets/kdg2_1411.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1411](assets/kdg2_1411.png)'
- en: Figure 14-11\. Two tasks running the same topology—one for each partition in
    the input topic
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-11。运行相同拓扑的两个任务——一个用于输入主题中的每个分区
- en: 'The developer of the application can choose the number of threads each application
    instance will execute. If multiple threads are available, every thread will execute
    a subset of the tasks that the application creates. If multiple instances of the
    application are running on multiple servers, different tasks will execute for
    each thread on each server. This is the way streaming applications scale: we will
    have as many tasks as we have partitions in the topics we are processing. If we
    want to process faster, add more threads. If we run out of resources on the server,
    start another instance of the application on another server. Kafka will automatically
    coordinate work—it will assign each task its own subset of partitions, and each
    task will independently process events from those partitions and maintain its
    own local state with relevant aggregates if the topology requires this. See [Figure 14-12](#fig1112).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的开发人员可以选择每个应用程序实例将执行的线程数。如果有多个线程可用，每个线程将执行应用程序创建的任务的子集。如果应用程序的多个实例在多台服务器上运行，则每个服务器上的每个线程将执行不同的任务。这是流应用程序扩展的方式：我们将有与我们正在处理的主题中的分区数量相同的任务。如果我们想要更快地处理，就增加更多的线程。如果服务器资源不足，就在另一台服务器上启动应用程序的另一个实例。Kafka将自动协调工作-它将为每个任务分配其自己的分区子集，并且每个任务将独立地处理来自这些分区的事件，并在拓扑需要时维护其自己的本地状态与相关聚合。参见[图14-12](#fig1112)。
- en: Sometimes a processing step may require input from multiple partitions, which
    could create dependencies between tasks. For example, if we join two streams,
    as we did in the ClickStream example in [“ClickStream Enrichment”](#click_stream_enrich),
    we need data from a partition in each stream before we can emit a result. Kafka
    Streams handles this situation by assigning all the partitions needed for one
    join to the same task so that the task can consume from all the relevant partitions
    and perform the join independently. This is why Kafka Streams currently requires
    that all topics that participate in a join operation have the same number of partitions
    and be partitioned based on the join key.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，处理步骤可能需要来自多个分区的输入，这可能会在任务之间创建依赖关系。例如，如果我们像在[“ClickStream Enrichment”](#click_stream_enrich)中的ClickStream示例中那样加入两个流，我们需要在可以发出结果之前从每个流的一个分区中获取数据。Kafka
    Streams通过将一个连接所需的所有分区分配给同一个任务来处理这种情况，以便任务可以从所有相关分区中消费并独立执行连接。这就是为什么Kafka Streams目前要求参与连接操作的所有主题具有相同数量的分区，并且基于连接键进行分区。
- en: '![kdg2 1412](assets/kdg2_1412.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1412](assets/kdg2_1412.png)'
- en: Figure 14-12\. The stream processing tasks can run on multiple threads and multiple
    servers
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-12。流处理任务可以在多个线程和多个服务器上运行
- en: Another example of dependencies between tasks is when our application requires
    repartitioning. For instance, in the ClickStream example, all our events are keyed
    by the user ID. But what if we want to generate statistics per page? Or per zip
    code? Kafka Streams will repartition the data by zip code and run an aggregation
    of the data with the new partitions. If task 1 processes the data from partition
    1 and reaches a processor that repartitions the data (`groupBy` operation), it
    will need to *shuffle*, or send events to other tasks. Unlike other stream processor
    frameworks, Kafka Streams repartitions by writing the events to a new topic with
    new keys and partitions. Then another set of tasks reads events from the new topic
    and continues processing. The repartitioning steps break our topology into two
    subtopologies, each with its own tasks. The second set of tasks depends on the
    first, because it processes the results of the first subtopology. However, the
    first and second sets of tasks can still run independently and in parallel because
    the first set of tasks writes data into a topic at its own rate and the second
    set consumes from the topic and processes the events on its own. There is no communication
    and no shared resources between the tasks, and they don’t need to run on the same
    threads or servers. This is one of the more useful things Kafka does—reduce dependencies
    between different parts of a pipeline. See [Figure 14-13](#fig1113).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 任务之间的另一个依赖关系的例子是当我们的应用程序需要重新分配时。例如，在ClickStream示例中，我们所有的事件都是按用户ID进行分组的。但是，如果我们想要按页面或邮政编码生成统计信息怎么办？Kafka
    Streams将按邮政编码重新分配数据，并对新分区的数据进行聚合。如果任务1处理来自分区1的数据并到达一个重新分配数据的处理器（`groupBy`操作），它将需要*洗牌*，或者将事件发送到其他任务。与其他流处理框架不同，Kafka
    Streams通过将事件写入具有新键和分区的新主题来重新分配。然后，另一组任务从新主题中读取事件并继续处理。重新分配步骤将我们的拓扑分成两个子拓扑，每个子拓扑都有自己的任务。第二组任务依赖于第一组任务，因为它处理第一个子拓扑的结果。但是，第一组和第二组任务仍然可以独立并行运行，因为第一组任务以自己的速率将数据写入主题，而第二组从主题中消费并独立处理事件。任务之间没有通信，也没有共享资源，它们不需要在相同的线程或服务器上运行。这是Kafka做的更有用的事情之一-减少管道不同部分之间的依赖关系。参见[图14-13](#fig1113)。
- en: '![kdg2 1413](assets/kdg2_1413.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![kdg2 1413](assets/kdg2_1413.png)'
- en: Figure 14-13\. Two sets of tasks processing events with a topic for repartitioning
    events between them
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-13。两组任务处理具有用于在它们之间重新分配事件的主题的事件
- en: Surviving Failures
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生存故障
- en: The same model that allows us to scale our application also allows us to gracefully
    handle failures. First, Kafka is highly available, and therefore the data we persist
    to Kafka is also highly available. So if the application fails and needs to restart,
    it can look up its last position in the stream from Kafka and continue its processing
    from the last offset it committed before failing. Note that if the local state
    store is lost (e.g., because we needed to replace the server it was stored on),
    the streams application can always re-create it from the change log it stores
    in Kafka.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 允许我们扩展应用程序的相同模型也允许我们优雅地处理故障。首先，Kafka具有高可用性，因此我们持久化到Kafka的数据也具有高可用性。因此，如果应用程序失败并需要重新启动，它可以从Kafka中查找其在流中的最后位置，并从失败之前提交的最后偏移量继续处理。请注意，如果本地状态存储丢失（例如，因为我们需要替换存储在其上的服务器），流应用程序始终可以从其在Kafka中存储的更改日志中重新创建它。
- en: Kafka Streams also leverages Kafka’s consumer coordination to provide high availability
    for tasks. If a task failed but there are threads or other instances of the streams
    application that are active, the task will restart on one of the available threads.
    This is similar to how consumer groups handle the failure of one of the consumers
    in the group by assigning partitions to one of the remaining consumers. Kafka
    Streams benefited from improvements in Kafka’s consumer group coordination protocol,
    such as static group membership and cooperative rebalancing (described in [Chapter 4](ch04.html#reading_data_from_kafka)),
    as well as improvements to Kafka’s exactly-once semantics (described in [Chapter 8](ch08.html#exactly_once_semantics)).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: While the high-availability methods described here work well in theory, reality
    introduces some complexity. One important concern is the speed of recovery. When
    a thread has to start processing a task that used to run on a failed thread, it
    first needs to recover its saved state—the current aggregation windows, for instance.
    Often this is done by rereading internal topics from Kafka in order to warm up
    Kafka Streams state stores. During the time it takes to recover the state of a
    failed task, the stream processing job will not make progress on that subset of
    its data, leading to reduced availability and stale data.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, reducing recovery time often boils down to reducing the time it takes
    to recover the state. A key technique is to make sure all Kafka Streams topics
    are configured for aggressive compaction—by setting a low `min.compaction.lag.ms`
    and configuring the segment size to 100 MB instead of the default 1 GB (recall
    that the last segment in each partition, the active segment, is not compacted).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: For an even faster recovery, we recommend configuring `standby replica`—those
    are tasks that simply shadow active tasks in a stream processing application and
    keep the current state warm on a different server. When failover occurs, they
    already have the most current state and are ready to continue processing with
    almost no downtime.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: More information on both scalability and high availability in Kafka Streams
    is available in a [a blog post](https://oreil.ly/mj9Ca) and a [Kafka summit talk
    on the topic](https://oreil.ly/cUvKa).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Stream Processing Use Cases
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this chapter we’ve learned how to do stream processing—from general
    concepts and patterns to specific examples in Kafka Streams. At this point it
    may be worth looking at the common stream processing use cases. As explained in
    the beginning of the chapter, stream processing—or continuous processing—is useful
    in cases where we want our events to be processed in quick order rather than wait
    for hours until the next batch, but also where we are not expecting a response
    to arrive in milliseconds. This is all true but also very abstract. Let’s look
    at a few real scenarios that can be solved with stream processing:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Customer service
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we just reserved a room at a large hotel chain, and we expect an
    email confirmation and receipt. A few minutes after reserving, when the confirmation
    still hasn’t arrived, we call customer service to confirm our reservation. Suppose
    the customer service desk tells us, “I don’t see the order in our system, but
    the batch job that loads the data from the reservation system to the hotels and
    the customer service desk only runs once a day, so please call back tomorrow.
    You should see the email within 2–3 business days.” This doesn’t sound like very
    good service, yet we’ve had this conversation more than once with a large hotel
    chain. What we really want is for every system in the hotel chain to get an update
    about a new reservation seconds or minutes after the reservation is made, including
    the customer service center, the hotel, the system that sends email confirmations,
    the website, etc. We also want the customer service center to be able to immediately
    pull up all the details about any of our past visits to any of the hotels in the
    chain, and the reception desk at the hotel to know that we are a loyal customer
    so they can give us an upgrade. Building all those systems using stream processing
    applications allows them to receive and process updates in near real time, which
    makes for a better customer experience. With such a system, the customer would
    receive a confirmation email within minutes, their credit card would be charged
    on time, the receipt would be sent, and the service desk could immediately answer
    their questions regarding the reservation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Internet of Things
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'IoT can mean many things—from a home device for adjusting temperature and ordering
    refills of laundry detergent, to real-time quality control of pharmaceutical manufacturing.
    A very common use case when applying stream processing to sensors and devices
    is to try to predict when preventive maintenance is needed. This is similar to
    application monitoring but applied to hardware and is common in many industries,
    including manufacturing, telecommunications (identifying faulty cellphone towers),
    cable TV (identifying faulty box-top devices before users complain), and many
    more. Every case has its own pattern, but the goal is similar: process events
    arriving from devices at a large scale and identify patterns that signal that
    a device requires maintenance. These patterns can be dropped packets for a switch,
    more force required to tighten screws in manufacturing, or users restarting the
    box more frequently for cable TV.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Also known as *anomaly detection*, this is a very wide field that focuses on
    catching “cheaters” or bad actors in the system. Examples of fraud-detection applications
    include detecting credit card fraud, stock trading fraud, video-game cheaters,
    and cybersecurity risks. In all these fields, there are large benefits to catching
    fraud as early as possible, so a near real-time system that is capable of responding
    to events quickly—perhaps stopping a bad transaction before it is even approved—is
    much preferred to a batch job that detects fraud three days after the fact, when
    cleanup is much more complicated. This is, again, a problem of identifying patterns
    in a large-scale stream of events.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: In cybersecurity, there is a method known as *beaconing*. When the hacker plants
    malware inside the organization, it will occasionally reach outside to receive
    commands. It can be difficult to detect this activity since it can happen at any
    time and any frequency. Typically, networks are well defended against external
    attacks but more vulnerable to someone inside the organization reaching out. By
    processing the large stream of network connection events and recognizing a pattern
    of communication as abnormal (for example, detecting that this host typically
    doesn’t access those specific IPs), the security organization can be alerted early,
    before more harm is done.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: How to Choose a Stream Processing Framework
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When choosing a stream processing framework, it is important to consider the
    type of application you are planning on writing. Different types of applications
    call for different stream processing solutions:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Ingest
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Where the goal is to get data from one system to another, with some modification
    to the data to conform to the target system.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Low milliseconds actions
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Any application that requires almost immediate response. Some fraud-detection
    use cases fall within this bucket.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous microservices
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: These microservices perform a simple action on behalf of a larger business process,
    such as updating the inventory of a store. These applications may need to maintain
    local state caching events as a way to improve performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Near real-time data analytics
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: These streaming applications perform complex aggregations and joins in order
    to slice and dice the data and generate interesting, business-relevant insights.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'The stream processing system you will choose will depend a lot on the problem
    you are solving:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: If you are trying to solve an ingest problem, you should reconsider whether
    you want a stream processing system or a simpler ingest-focused system like Kafka
    Connect. If you are sure you want a stream processing system, you need to make
    sure it has both a good selection of connectors and high-quality connectors for
    the systems you are targeting.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are trying to solve a problem that requires low milliseconds actions,
    you should also reconsider your choice of streams. Request-response patterns are
    often better suited to this task. If you are sure you want a stream processing
    system, then you need to opt for one that supports an event-by-event low-latency
    model rather than one that focuses on microbatches.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are building asynchronous microservices, you need a stream processing
    system that integrates well with your message bus of choice (Kafka, hopefully),
    has change capture capabilities that easily deliver upstream changes to the microservice
    local state, and has the good support of a local store that can serve as a cache
    or materialized view of the microservice data.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are building a complex analytics engine, you also need a stream processing
    system with great support for a local store—this time, not for maintenance of
    local caches and materialized views but rather to support advanced aggregations,
    windows, and joins that are otherwise difficult to implement. The APIs should
    include support for custom aggregations, window operations, and multiple join
    types.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to use case–specific considerations, there are a few global considerations
    you should take into account:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Operability of the system
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Is it easy to deploy to production? Is it easy to monitor and troubleshoot?
    Is it easy to scale up and down when needed? Does it integrate well with your
    existing infrastructure? What if there is a mistake and you need to reprocess
    data?
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Usability of APIs and ease of debugging
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: I’ve seen orders-of-magnitude differences in the time it takes to write a high-quality
    application among different versions of the same framework. Development time and
    time-to-market are important, so you need to choose a system that makes you efficient.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Makes hard things easy
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Almost every system will claim they can do advanced windowed aggregations and
    maintain local stores, but the question is: do they make it easy for you? Do they
    handle gritty details around scale and recovery, or do they supply leaky abstractions
    and make you handle most of the mess? The more a system exposes clean APIs and
    abstractions and handles the gritty details on its own, the more productive developers
    will be.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Community
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Most stream processing applications you consider are going to be open source,
    and there’s no replacement for a vibrant and active community. Good community
    means you get new and exciting features on a regular basis, the quality is relatively
    good (no one wants to work on bad software), bugs get fixed quickly, and user
    questions get answers in a timely manner. It also means that if you get a strange
    error and Google it, you will find information about it because other people are
    using this system and seeing the same issues.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by explaining stream processing. We gave a formal definition
    and discussed the common attributes of the stream processing paradigm. We also
    compared it to other programming paradigms.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed important stream processing concepts. Those concepts were
    demonstrated with three example applications written with Kafka Streams.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: After going over all the details of these example applications, we gave an overview
    of the Kafka Streams architecture and explained how it works under the covers.
    We conclude the chapter, and the book, with several examples of stream processing
    use cases and advice on how to compare different stream processing frameworks.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL

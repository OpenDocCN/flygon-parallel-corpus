- en: Chapter 2. Machine Learning Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of this chapter is to provide a conceptual introduction to statistical
    **machine learning** (**ML**) techniques for those who might not normally be exposed
    to such approaches during their typical required statistical training. This chapter
    also aims to take a newcomer from minimal knowledge of machine learning all the
    way to a knowledgeable practitioner in a few steps. The second part of the chapter
    is focused on giving some recommendations for choosing the right machine learning
    algorithms depending on the application types and requirements. It will then lead
    through some best practices when applying large-scale machine learning pipelines. In
    a nutshell, the following topics will be discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical machine learning problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large scale machine learning APIs in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical machine learning best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right algorithm for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will try to define the term machine learning from the computer
    science, statistics and data analytical perspectives. Then we will show the steps
    of analytical machine learning applications. Finally, we will discuss some typical
    and emerging machine learning tasks and then name some practical machine learning
    problems that need to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in modern literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see how a renowned professor of machine learning, Tom Mitchell, Chair
    of the CMU Machine Learning Department and Professor at the Carnegie Mellon University
    defines the term machine learning in his literature (*Tom M. Mitchell, The Discipline
    of Machine Learning, CMU-ML-06-108, July 2006*, [http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf](http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine Learning is a natural outgrowth of the intersection of Computer Science
    and Statistics. We might say the defining question of Computer Science is ''How
    can we build machines that solve problems, and which problems are inherently tractable/intractable?''
    The question that largely defines Statistics is ''What can be inferred from data
    plus a set of modelling assumptions, with what reliability?'' The defining question
    for Machine Learning builds on both, but it is a distinct question. Whereas Computer
    Science has focused primarily on how to manually program computers, Machine Learning
    focuses on the question of how to get computers to program themselves (from experience
    plus some initial structure). Whereas Statistics has focused primarily on what
    conclusions can be inferred from data, Machine Learning incorporates additional
    questions about what computational architectures and algorithms can be used to
    most effectively capture, store, index, retrieve and merge these data, how multiple
    learning subtasks can be orchestrated in a larger system, and questions of computational
    tractability.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We believe that this definition from Prof. Tom is self-explanatory. However,
    we will provide some clearer understanding of machine learning in the next two
    sub-sections from the computer science, statistics, and data analytical perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interested readers should follow other resources to get more insights about
    machine learning and its theoretical perspective. Here we have provided some links
    as follows: *Machine learning*: [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine learning: what it is and why matters* - [http://www.sas.com/en_us/insights/analytics/machine-learning.html](http://www.sas.com/en_us/insights/analytics/machine-learning.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Gentle Introduction To Machine Learning*: [https://www.youtube.com/watch?v=NOm1zA_Cats](https://www.youtube.com/watch?v=NOm1zA_Cats).'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is machine learning, and how does it work*: [https://www.youtube.com/watch?v=elojMnjn4kk](https://www.youtube.com/watch?v=elojMnjn4kk).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to Data Analysis using Machine Learning*: [https://www.youtube.com/watch?v=U4IYsLgNgoY](https://www.youtube.com/watch?v=U4IYsLgNgoY).'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning and computer science
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning is a branch of computer science that studies the design of
    algorithms that can learn from its heuristics that typically evolved from the
    study of pattern recognition and computational learning theory in artificial intelligence.
    An interesting question came into the mind of Alan Turing about the machine, which
    is, *Can a machine think?* In fact, there are some good reasons to believe a sufficiently
    complex machine could one day pass the unrestricted Turing test; let's postpone
    this question until the Turing test, but gets passed. However, machines can learn
    at least. Subsequently, Arthur Samuel was the first man who defined the term **machine
    learning** as a f*ield of study that gives computers the ability to learn without
    being explicitly programmed* in 1959\. Typical machine learning tasks are concept
    learning, predictive modeling, classification, regression, clustering, dimensionality
    reduction, recommender system, deep learning and finding useful patterns from
    the large-scale dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate goal is to improve the learning in such a way that it becomes automatic,
    so that no human interactions are needed any more, or the level of human interaction
    is reduced as much as possible. Although machine learning is sometimes conflated
    with **Knowledge Discovery and Data Mining** (**KDDM**), the latter sub-field
    on the other hand focuses more on exploratory data analysis and is known as unsupervised
    learning - such as clustering analysis, anomaly detection, **Artificial Neural
    Networks** (**ANN**), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Other machine learning techniques include supervised learning, where a learning
    algorithm analyzes the training data and produces an inferred function that can
    be used for mapping new examples towards prediction. Classification and regression
    analysis are two typical examples of supervised learning. Reinforcement learning,
    on the other hand, is inspired by behaviorist psychology (see also [https://en.wikipedia.org/wiki/Behaviorism](https://en.wikipedia.org/wiki/Behaviorism)),
    which is is typically concerned with how a software agent performs an action in
    a new *environment* by maximizing the `reward` function. Dynamic programming and
    intelligent agent are two examples of reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Typical machine learning applications can be classified into scientific knowledge
    discovery and more commercial applications, ranging from Robotic or **Human Computer
    Interaction** (**HCI**) to anti-spam filtering and recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in statistics and data analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning reconnoitres the study and construction of algorithms (see
    also [https://en.wikipedia.org/wiki/Algorithm](https://en.wikipedia.org/wiki/Algorithm))
    that can learn (see also [https://en.wikipedia.org/wiki/Learning](https://en.wikipedia.org/wiki/Learning))
    from the heuristics and make meaningful predictions on data. However, in order
    to make data-driven predictions or decisions, such algorithms operate by building
    a model (see also [https://en.wikipedia.org/wiki/Mathematical_model](https://en.wikipedia.org/wiki/Mathematical_model))
    from training datasets, quicker than following a stringently static program or
    instructions. Machine learning is also closely related and often overlaps with
    the nature of computational statistics. Computational statistics is, on the other
    hand, an applied field of statistics that focuses on making predictions through
    a computerised approach. In addition, it has strong stalemates to mathematical
    optimisation, which delivers methods and computing tasks along with theory and
    application domains. The tasks that are not feasible in mathematics due to the
    demands for a strong background knowledge of mathematics, machine learning suits
    best and can be applied as the alternative to that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the field of data analytics, on the other hand, machine learning is
    a method used to devise complex models and algorithms that advance themselves
    towards prediction for a future outcome. These analytical models allow researchers,
    data scientists, engineers, and analysts to produce reliable, repeatable, and
    reproducible results and mine hidden insights through learning from past relationships
    (heuristics) and trends in the data. Again we will refer to a famous definition
    from Prof. Tom, where he explained what learning really means from the computer
    science perspective in the literature (*Tom M. Mitchell, The Discipline of Machine
    Learning, CMU-ML-06-108, July 2006*, [http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf](http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Therefore, we can conclude that a computer program or machines can:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn from data and histories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be improved with experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactively enhance a model that can be used to predict the outcomes of questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, the following diagram helps us to understand the whole process
    of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning in statistics and data analytics](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Machine learning at a glance.'
  prefs: []
  type: TYPE_NORMAL
- en: Typical machine learning workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical machine learning application involving several steps from input,
    processing to output that form a scientific workflow is shown in Figure 2\. The
    following steps are involved in typical machine learning applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the sample data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the data into the input format for the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-process the data and handle the missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into two sets, one for building the model (training dataset)
    and one for testing the model (test dataset or validation dataset).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the algorithm to build or train your ML model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions with the training data and observe the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test and evaluate the model with the test data or alternatively validate the
    model with some cross-validator technique using the third dataset, called the
    validation dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the model for better performance and accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale-up the model so that it can handle massive datasets in the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the ML model in commercialization:![Typical machine learning workflow](img/00069.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2: Machine learning workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: Often the machine learning algorithms have some ways to handle the skewness
    in the datasets; that skewness can sometimes be immensely skewed though. In step
    4, the experimental dataset is split often into a training set and test sets randomly,
    which is called sampling. The training dataset is used to train the model, whereas
    the test dataset is used to evaluate the performance of the best model at the
    very end. The better practice is to use the training dataset as much as you can
    to increase the generalization performance. On the other side, it is recommended
    to use the test dataset only once to avoid the overfitting and underfitting problem
    while computing the prediction error and the related metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overfitting is a statistical property by which random error and noise is described
    apart from the normal and underlying relationships. It mostly occurs when there
    are too many hyperparameters relative to the number of observations or features.
    Under fitting on the other hand refers to a model that can neither model the training
    data nor generalize to new data towards the model evaluation or adaptability.
  prefs: []
  type: TYPE_NORMAL
- en: However, these steps consist of several techniques and we will discuss those
    in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples* in detail. Step 9 and 10 are usually considered
    as advanced steps, and they consequently will be discussed in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning tasks or machine learning processes are typically classified
    into three broad categories, depending on the nature of the learning feedback
    available to a learning system. Supervised learning, unsupervised learning, and
    reinforcement learning; these three kinds of machine learning tasks are shown
    in *Figure 3*, and will be discussed in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning tasks](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Machine learning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **supervised learning** application makes predictions based on a set of examples,
    and the goal is to learn general rules that map inputs to outputs aligning with
    the real world. For example, a dataset for spam filtering usually contains spam
    messages as well as non-spam messages. Therefore, we could know which messages
    in a training set are spams or non-spams. Nevertheless, we might have the opportunity
    to use this information to train our model in order to classify new and unseen
    messages. Figure 4 shows the schematic diagram of the supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the dataset for training the ML model in this case is labeled
    with the value of interest and a supervised learning algorithm looks for patterns
    in those value labels. After the algorithm has found the required patterns, those
    patterns can be used to make predictions for unlabeled test data. This is the
    most popular and useful type of machine learning tasks, which is not an exception
    for Spark as well, where most of the algorithms are a supervised learning technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised learning](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Supervised learning in action.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In **unsupervised learning**, data points have no labels related or in other
    words, the correct classes of the training dataset in unsupervised learning are
    unknown, as shown in *Figure 5*. As a result, classes have to be inferred from
    the unstructured datasets, which implies that the goal of an unsupervised learning
    algorithm is to pre-process the data in some structured ways by describing its
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this obstacle in unsupervised learning, clustering techniques are
    used typically to group the unlabeled samples based on certain similarity measures,
    mining hidden patterns towards feature learning. More technically, we can write
    down a generative model, and then tell the data to find parameters that explain
    the data to us. Now what will happen next if we are not satisfied with the possibility
    of this elucidation? The answer is that we should tell the data to do it again
    until we are using some efficient algorithms or techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Now a new question may arise in your mind, why do we have to put labels on the
    data? Or cannot we just appreciate the data in its current order recognizing that
    each datum is unique and pre snowflake? In other words, with a little supervision,
    our data can grow up to be whatever it wants to be! So why should the unlabeled
    data be taken into consideration too?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, there are some deeper issues regarding this. For example, most of the
    variation in the data comes from phenomena that are irrelevant to our desired
    labeling scheme. A more realistic example would be how Gmail classifies e-mails
    as spam and ham using the supervised learning technique, where the data might
    use its parameters to explain its semantics, when all we care about is its syntactic
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is the technique where the model itself learns from
    a series of actions or behaviors. Complexity of datasets or sample complexity
    is very important in the reinforcement learning needed for the algorithms to learn
    a target function successfully. Moreover, in response to each data point for achieving
    the ultimate goal, maximization of the reward function should be ensured while
    interacting with an external environment, as demonstrated in *Figure 6*. To make
    the maximization easier, the reward function can either be exploited by penalizing
    the bad actions or rewarding for the good actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to achieve the highest reward, the algorithm should be modified with
    a strategy that also allows the machine or software agent to learn its behavior
    periodically. These behaviors can be learned once and for all, or the machine
    learning model can keep adapting as times passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement learning](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, reinforcement learning is common in robotics; the algorithm must
    choose the robot's next action based on a set of sensor readings. It is also a
    natural fit for **Internet of Things** (**IoT**) applications, where a computer
    program interacts with a dynamic environment in which it must perform a certain
    goal, without an explicit mentor. Another example is the game **Flappy Bird**,
    which has been trained to play itself.
  prefs: []
  type: TYPE_NORMAL
- en: Recommender system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A recommender system is an emerging application, which is a subclass of information
    filtering system use for making a prediction of the rating or preference from
    the users that they usually provide to an item. The concept of recommender systems
    has become very common in recent years and subsequently applied in different applications.
    The most popular ones are probably products (for examples, movies, music, books,
    research articles, news, search queries, social tags, and so on). Recommender
    systems can be typed into four categories typically:'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering system, where accumulation of a consumer's preferences
    and recommendations to other users is based on likeness in behavioral patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content-based systems, where the supervised machine learning is used to persuade
    a classifier to distinguish between interesting and uninteresting items for the
    users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid recommender systems is a recent research and hybrid approach (that is,
    combining collaborative filtering and content-based filtering). Netflix is a good
    example of such a recommendation system that uses **Restricted Boltzmann Machines**
    (**RBM**) and a form of the Matrix Factorization algorithm for large movie databases,
    such as IMDb. This recommendation, which simply recommends movies or dramas or
    streaming by comparing the watching and searching habits of similar users, is
    called rating prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge-based systems, where knowledge about users and products is used to
    reason what fulfills the user's requirements, using the perception tree, decision
    support systems, and case-based reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Between supervised and unsupervised learning, there is a small place for **semi-supervised
    learning**; where the ML model usually receives an incomplete training signal.
    More statistically, the ML model receives a training set with some of the target
    outputs missing. The semi-supervised learning is more or less assumption-based
    and often uses three kinds of assumption algorithms as the learning algorithm
    for the unlabeled datasets. The following assumptions are used: smoothness, cluster,
    and manifold assumption.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, semi-supervised learning can furthermore be denoted as a **weakly
    supervised** or **bootstrapping** technique for using the hidden wealth of unlabeled
    examples to enhance the learning from a small amount of labeled data. Emerging
    examples include *semi-supervised expectation minimization, and concept learning
    in human cognition and transitive SVMs*.
  prefs: []
  type: TYPE_NORMAL
- en: Practical machine learning problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What does machine learning really mean? We already saw some convincing definitions
    of this term as well as the meaning of the term *learning* at the very beginning
    of this chapter. However, the reality is machine learning itself is defined by
    the problems to be resolved. In this section, we will first emphasize the machine
    learning classes and then we will list some well-known and popularly-used examples
    of real world machine learning problems. The typical classes include classification,
    clustering, rule extraction, and regression, which will all be discussed.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will also discuss those problems based on the main taxonomy
    of standard machine learning problems. This is important, since knowing the type
    of problems we could face allows us to think about the data we need. Another important
    fact is that before knowing some practical machine learning problems, you might
    face difficulties in having an idea about developing your machine learning applications.
    In other words, to know the problem we need to know the data in the very first
    place. Therefore, the types of algorithm and their optimality to be addressed
    will be discussed throughout this chapter; data manipulation, however, will be
    discussed to dig-down the problems in [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem classes we mentioned above are standards for most of the problems
    we refer to in everyday life while doing and applying machine learning techniques.
    However, knowing only the ML classes is not enough we also need to know what type
    of problems machines are learning, since you will find many problems that are
    simply problem solving that does not help a ML model or agent to learn at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you think a problem is a machine learning problem, more technically, you
    are thinking of a decision problem that needs to be modeled from data that could
    be termed as a machine learning problem. In other words, as a data scientist or
    human expert, if you have enough time to answer a particular question by knowing
    the available dataset, you can more or less apply a suitable machine learning
    problem. Therefore, we can assume that a solvable problem using some ML algorithms
    would have mainly two parts - the data itself, which could be used to point to
    specific observations of the problem, and secondly the quantitative measurement
    of the quality of an available solution. Once you have succeeded in identifing
    a problem as an ML problem, you would probably be able to think about what types
    of problems you could formulate with it easily, or the type of aftermath your
    client will be asking for, or what sorts of requirements are to be satisfied.
    As already stated in the above section, the more frequently used machine learning
    classes are: classification, clustering, regression, and rule extraction. We will
    now provide a short overview of each class.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification and clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the experimental dataset is labeled, it means a class has been assigned to
    it already. For instance, spam/non-spam during spam e-mail detection or fraud/non-fraud
    during credit card fraud identification. However, if the dataset based on which
    the fundamental decision will be made or modeled is unlabeled, new labels need
    to be made manually or algorithmically. This might be difficult, and can be thought
    of a judgment problem. On the contrary, sculpting the differences or resemblances
    between several groups might be computationally harder.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering, on the other hand, handles the data that is not labeled or un-labeled.
    However, it still can be divided into groups based on similarity and other measures
    of natural structure in the data you have. Organizing pictures from a digital
    album by faces only without names could be an example, where the human users like
    us have to assign names to groups manually. Again, the same computational complexity
    might arise to label multiple image files manually; we will provide some examples
    in later chapters of how Spark provides several APIs to solve these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Rule extraction and regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the given dataset, propositional rules can be generated by means of antecedent
    and consequent in the *if...then* style that defines the behavior of a machine
    learning agent. This type of rule generation technique is commonly referred to
    as *rule extraction*. You might be wondering if such rules might exist, however,
    they are typically not directed. That means the methods used to discover statistically
    meaningful or statistically significant relationships between attributes in your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: An example of rule extraction is the mining association rules between items
    from business oriented transactional databases. Non-technically, a practical example
    could be the discovery of the relationship or association between the purchase
    of beer and diapers, which is illustrative of the desire and opportunity for the
    customers. However, some situation, might arise where some predictions out of
    the rules or data are not necessarily involved directly.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about the regression where the data is labeled with a real value.
    To be more exact, some floating point value rather than having labels in the data.
    The easiest way to understand an example would be time series data similar to
    the price of a stock or currency that changes over time. In these types of data,
    the regression task is to make a prediction for new and unpredicted data by some
    regression modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Most widely used machine learning problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will find an extensive amount of examples of the use of machine learning
    related problems in daily life, since they solve the difficult parts of the available
    problems that are widely used techniques or algorithms. We often use many desktop
    or web-based applications that solve your problems out of the data even without
    knowing that what underlying techniques have been used. You will be wondered to
    know that many of them actually use widely used machine learning algorithms to
    make your life easier. There are many machine learning problems around. Here we
    will mention some example problems that really represent what machine learning
    is all about:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spam detection or spam filtering**: Given some e-mails in an inbox, the task
    is to identify those e-mails that are spam and those that are non-spam (often
    called ham) e-mail messages. Now the challenging part is to develop an ML application
    that can be applied so that it can identify only the non-spam e-mails to stay
    in the inbox. and move the spam emails to the corresponding spam folder or delete
    them permanently from the email account. A typical example could be what you may
    do while using Gmail manually, but if you have an ML application, that application
    will do it automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection or outlier detection**: The anomaly detection deals with
    the identification of items, events, or observations that are unexpected or non-confirming
    to the expected patterns in a dataset; in other words, the identification of suspect
    patterns. The most common example is network anomaly detection using some machine
    learning applications. Now the challenging task is to develop an ML application
    that can be applied successfully to simply identify the unusual data points from
    the data propagating across the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Credit card fraud detection**: Credit card fraud is very common nowadays.
    Stealing credit card related information from online shopping and using it in
    an illegal way happens in many countries. Suppose you have a transactional database
    for a customer for a particular month. Now the challenging task is to develop
    an ML application to identify those transactions that were made by the customer
    themselves and those done by others illegally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice recognition**: Recognizing a voice and converting it into a corresponding
    text command and later performing some actions, as an intelligent agent does.
    The most widely used applications include Apple Siri, Samsung S-Voice, Amazon''s
    Echo (consumer space), and Microsoft Cortana (especially because Cortana has SDKs
    for extensibility and integration, and so on). Another example would be locking
    or unlocking your smartphone by using the recognised voice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Digit/character recognition**: Suppose you have a handwritten zip code or
    address or message on/inside an envelope, now the task of digit/character recognition
    is to identify and classify the digits or characters for each handwritten character
    that is made by different people. An efficient ML application could help in this
    regard to read and understand handwritten zip codes or characters and sort the
    contents of the envelope by the geographic region, or more technically, by the
    image segmentations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet of Things**: Large-scale sensor data analytics for prediction and
    classification from real-time streamed data. For example, smart living room monitoring
    including water level checking, room temperature checking, home appliances controlling,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaming analytics**: Analytics for sports, games, and console-based gaming
    profiles in order to predict upsell and target in-app purchases and modifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face detection**: Given a digital photo album of hundreds or thousands of
    photographs, the task is to identify those photos that resemble a given person.
    An efficient ML application, in this case, could help to organise photos by person.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product recommendation**: Provided a purchase history of a customer along
    with a large inventory of products, the target is to identify those products that
    the customer will likely be interested in purchasing with an ML system. Business
    and tech giants such as Amazon, Facebook, and Google Plus have this recommended
    feature for the users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stock trading**: Given the current and historical prices for a stock market,
    predict whether stock should be bought or sold in order to profit with the help
    of an ML system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some examples of machine learning that are emerging and the
    demands of current research:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy preserving data mining**: Mining customer''s purchase rules from
    the maximal frequent pattern and association rules from business oriented retail
    databases to increase purchases in the future'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Author name disambiguation**: Disambiguation performance is evaluated with
    manual verification of random samples of pairs from clustering results from a
    list of authors from a set of given publications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems**: Recommender system based on click stream data using
    association rule mining'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text mining**: Plagiarism checking from a given text corpus for example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: A lot of decisions these days are being made by business
    and tech companies based on the opinion of others, and it will be a good place
    to innovate machine learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech understanding**: Given an utterance from a user, the target is to
    identify the specific request made by the user. A model of this problem would
    allow a program to understand and make an attempt to fulfill that request. For
    example, iPhone with Siri and Samsung Voice Recorder in meeting mode have this
    feature implemented'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these problems are the hardest problems in artificial intelligence,
    natural language processing, and computer vision that can be addressed and solved
    using ML algorithms. Similarly, we will try to develop some ML applications emphasizing
    these problems in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Large scale machine learning APIs in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will describe two key concepts introduced by the Spark machine
    learning libraries (Spark MLlib and Spark ML) and the most widely used implemented
    algorithms that align with the supervised and unsupervised learning techniques
    we discussed in the above sections.
  prefs: []
  type: TYPE_NORMAL
- en: Spark machine learning libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As already stated, in the pre-Spark era, big data modelers typically used to
    build their ML models using statistical languages such as R, STATA, and SAS. Then
    the data engineers used to re-implement the same model in Java, for example, to
    deploy on Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: However, this kind of workflow lacks efficiency, scalability, throughput, and
    accuracy as well as extended execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Spark, the same ML model can be re-built, adopted, and deployed, making
    the whole workflow much more efficient, robust, and faster, which allows you to
    provide hands-on insight to increase the performance. The Spark machine learning
    libraries are divided into two packages: Spark MLlib (`spark.mllib`) and Spark
    ML (`spark.ml`).'
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MLlib is Spark''s scalable machine learning library, which is the extension
    of the Spark Core API that provides a library of easy to use machine learning
    algorithms. Algorithms are implemented and written in Java, Scala, and Python.
    Spark provides support for local vectors and matrix data types stored on a single
    machine, as well as distributed matrices backed by one or multiple RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark MLlib** |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| **ML tasks** | **Discrete** | **Continuous** |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Classification:Logistic regressionand regularized variantsLinear
    SVMNaïve BayesDecision treesRandom forestsGradient-boosted trees | Regression:Linear
    regressionand regularized variantsLinear least squaresLasso and ridge regressionIsotonic
    regression |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | Clustering:K-meansGaussian matrix**Power iteration clustering**
    (**PIC**)**Latent Dirichlet Allocation** (**LDA**)Bisecting K-meansStreaming K-means
    | Dimensionality reduction, matrix factorization:Principal components analysisSingular
    value decompositionAlternate least square |'
  prefs: []
  type: TYPE_TB
- en: '| Reinforcement | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Recommender systems | Collaborative filtering:Netflix recommendation | N/A
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Spark MLlib at a glance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Legend**: Continuous: making predictions about continuous variables, for
    example, prediction of the maximum temperature for the upcoming days'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discrete**: Assigning discrete class labels to particular observations as
    outcomes of a prediction, for example, in weather forecasting it could be the
    prediction of a sunny, rainy, or snowy day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The beauty of Spark MLlib is numerous. For example, the algorithms implemented
    using Scala, Java, and Python are highly-scalable and leverage Spark's ability
    to work with a massive amount of data. They are fast towards designed for parallel
    computing with in-memory based operation, which is 100 times faster compared to
    MapReduce data processing (they also support disk-based operation that is 10 times
    faster than what MapReduce has as normal data processing) using Dataset, DataFrame,
    or **Directed Acyclic Graph** (**DAG**)-based RDD APIs.
  prefs: []
  type: TYPE_NORMAL
- en: They are also diverse, since they cover common machine learning algorithms for
    regression analysis, classification, clustering, recommender systems, text analytics,
    frequent pattern mining, and they obviously cover all the steps required to build
    scalable machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark ML adds a new set of machine learning APIs to let users quickly assemble
    and configure practical machine learning pipelines on top of Datasets. Spark ML
    targets to offer a uniform set of high-level APIs built on top of DataFrames rather
    than RDDs that help users create and tune practical machine learning pipelines.
    Spark ML API standardizes machine learning algorithms to make the learning tasks
    easier to combine multiple algorithms into a single pipeline or data workflow
    for data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark ML uses the concept of DataFrame (although it''s obsolete in Java but
    still the main programming interface in Python and R), which is introduced in
    the Spark 1.3.0 release from Spark SQL as machine learning Datasets. The Datasets
    hold diverse data types such as columns storing text, feature vectors, and true
    labels for the data. In addition to this, Spark ML also uses the transformer to
    transform one DataFrame into another or vice-versa, where the concept of the estimator
    is used to fit on a DataFrame to produce a new transformer. The pipeline API,
    on the other hand, can restrain multiple transformers and estimators together
    to specify an ML data-workflow. The concept of the parameter was introduced to
    specify all the transformers and estimators to share a common API under an umbrella
    during the development of an ML application:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark ML** |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| **ML tasks** | **Discrete** | **Continuous** |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Classification:Logistic regressionDecision tree classifierRandom
    forest classifierGradient-boosted tree classifierMultilayer perception classifierOne-vs-Rest
    classifier | Regression:Linear regressionDecision tree regressionRandom forest
    regressionGradient-boosted tree regressionSurvival regression |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | Clustering:K-means**Latent Dirichlet allocation** (**LDA**)
    | Tree Ensembles:Random forestsGradient-boosted Trees |'
  prefs: []
  type: TYPE_TB
- en: '| Reinforcement | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Recommender systems | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Spark ML at a glance (legend same as Table 1).'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in table 2, Spark ML also provides several classifications, regression,
    decision trees, and tree ensembles as well as a clustering algorithm implemented
    for developing ML pipelines on top of DataFrames. The optimization algorithm under
    active implementation is called **Orthant-Wise Limited-memory QuasiNewton** (**OWL-QN**),
    which is also an advanced algorithm that is an extension of L-BFGS that can effectively
    handle L1 regularization and elastic net (see also at Spark ML Advanced topic,
    [https://spark.apache.org/docs/latest/ml-advanced.html](https://spark.apache.org/docs/latest/ml-advanced.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Important notes for practitioners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: However, currently only Pearson's and Spearman's correlation are supported and
    more are to be added in future Spark releases. Unlike the other statistical functions,
    stratified sampling is also supported by Spark and it can be performed on RDDs
    as key-value pairs; however, some functionalities are yet to be added to Python
    developers. Currently there are no reinforcement learning algorithm modules in
    Spark Machine Learning libraries (please refer to *Table 1* and *Table 2*). The
    current implementation of Spark MLlib provides a parallel implementation of FP-growth
    for mining frequent patterns and the association rules. However, you will have
    to customize the algorithm for mining maximal frequent patterns accordingly. We
    will provide a scalable ML application for mining privacy preserving maximal frequent
    pattern in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Another fact is that the current implementation of the collaborative based recommendation
    system in Spark does not support the use of real time stream data, however, in
    later chapters we will try to show a practical recommender system based on click
    stream data using association rule mining (see Mitchell, Tom M. *The Discipline
    of Machine Learning*, 2006, [http://www.cs.cmu.edu/](http://www.cs.cmu.edu/).
    CMU. Web. Dec. 2014). However, some algorithms are not available or are yet to
    be added to Spark ML, most notably dimensionality reduction is such an example.
  prefs: []
  type: TYPE_NORMAL
- en: However, developers can seamlessly combine the implementation of these techniques
    found in Spark MLlib with the rest of the algorithms found in Spark ML as hybrid
    or interoperable ML applications. Spark's neural networks and perception are brain-inspired
    learning algorithms covering multiclass, two-class, and regression problems that
    are not yet implemented in Spark ML APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Practical machine learning best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will describe some good machine learning practices that
    need to be followed before developing a machine learning application of particular
    interest, as described in *Figure 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Practical machine learning best practices](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Machine learning systematic process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A scalable and accurate ML application demand for following a systematic approach
    to its development from problem definition to presenting results can be summarized
    into four steps: problem definition and formulation, data preparation, finding
    suitable algorithms for machine learning, and finally, presenting the results
    after the machine learning model deployment. Well, these steps can be depicted
    as shown in *Figure 6*.'
  prefs: []
  type: TYPE_NORMAL
- en: Best practice before developing an ML application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The learning of a machine learning system can be formulated as the sum of representation,
    evaluation, and optimisation. In other words, according to Pedro D et al. (Pedro
    Domingos, *A Few Useful Things to Know about Machine Learning*, [https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning = Representation + Evaluation + Optimization*'
  prefs: []
  type: TYPE_NORMAL
- en: Taking this formulation into consideration, we will provide some recommendations
    for practitioners before getting into ML application development.
  prefs: []
  type: TYPE_NORMAL
- en: Good machine learning and data science worth huge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So what do we need for an effective machine learning applications development?
    We actually need four arsenals before we start developing an ML application; including:'
  prefs: []
  type: TYPE_NORMAL
- en: The data primitives (or the experimental data to be more frank).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pipeline synthesis tool (to understand the data and control flow during the
    machine learning steps).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An effective and robust error analysis tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A verification or validation tool (to verify or validate the prediction accuracy
    or performance of the ML model). However, most importantly, without some strong
    theoretical basement with good data science that is worth a huge amount, the whole
    process will be in vain. In fact, many data scientists and machine learning experts
    often quote something like this statement: *if you can pose your problem as a
    simple optimization problem then you is almost done* (see *Data Analytics & R*,
    [http://advanceddataanalytics.net/2015/01/31/condensed-news-7/](http://advanceddataanalytics.net/2015/01/31/condensed-news-7/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That means before you start your machine learning voyage, if you can identify
    if your problem is a machine learning problem, you will be able to find some suitable
    algorithms to develop your ML application altogether. Of course, in practice,
    most machine learning applications can't be changed into simple optimization problems.
    Therefore, it's the duty of a data scientist like you to manage and maintain complex
    datasets. After that, you will have to handle other issues such as the analytical
    problems that evolve when engineering the machine learning pipeline to tackle
    those issues we mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the best practice is to use Spark MLlib, Spark ML, GraphX, and Spark
    Core APIs along with the best practice data science heuristics for developing
    your machine learning applications together. Now you might think of getting benefits
    out of it; yes, the benefits are obvious, and they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Built-in distributed algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory and disk-based data computation and processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory capabilities for iterative workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithmic accuracy and performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster data cleaning, feature engineering and feature selection, training, and
    testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time visualization of the predictive results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning towards better performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptability for new datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability with the increasing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practice – feature engineering and algorithmic performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In best practice, feature engineering should be considered as one of the most
    important parts of machine learning. The thing is to find a better representation
    of features out of the experimental dataset non-technically. In parallel to this,
    which learning algorithms or techniques are to be used are also important. Parameter
    tuning, of course in addition, however, the final choice is more about  experimentation
    through the ML model you will be developing.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, however, it is trivial to grasp the naive performance baseline
    by means of an **out-of-the-box** method (also referred to as functionality or
    **OOTB** in short, which is a feature of a product of interest that works straight
    away after installing or configuring) and good data pre-processing. Therefore,
    you might be doing it continually in order to know where the baseline is and whether
    this performance is of a satisfactory level or good enough for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've trained all of your out-of-the-box methods, it's always recommended
    and is a good idea to try bagging them together. Moreover, in order to solve the
    ML problems, very often you might need to know the reality that computationally
    hard problems (shown in section 2, for example) need either domain-specific knowledge
    or lots of digging down in the data or both. Consequently, the combination of
    a widely accepted feature engineering technique and domain-specific knowledge
    would help your ML algorithm/application/system to solve prediction related problems.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, if you have the required dataset and a robust algorithm that
    can take the advantages of the dataset by learning the complex features, it's
    almost guaranteed that you will be successful. Furthermore, sometimes domain experts
    might be wrong in selecting the good features; therefore, incorporation of multiple
    domain experts (problem domain expert), more well-structured data, and ML expertise
    is always helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, sometimes it is recommended from our side to consider the
    error rate rather than only the accuracy. For example, suppose an ML system with
    99% accuracy and 50% errors is worse than the one with 90% accuracy but 25% errors,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of overfitting and underfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common mistake often made by novice data scientists is subject to the overfitting
    issue that might evolve while building your ML model by hearing without generalizing.
    More technically, if you evaluate your model on the training data instead of test
    or validated data, you probably won''t be able to articulate whether your model
    is overfitting or not. The common symptoms are:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive accuracy of the data used for training can be over accurate (that
    is, sometimes even 100%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And the model might show a little better compared to the random prediction for
    new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sometimes the ML model itself becomes under-fit for a particular tuning or
    data point, which means the model has become too simplistic. Our recommendation
    (like others as well we believe) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the dataset into two sets to detect overfitting situations, the first
    one being for training and model selection, called the training set; the second
    one is the test set for evaluating the model stated in place of the ML workflow
    section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you also could void the overfitting by consuming simpler models
    (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling
    the regularisation parameters of your ML model (if available)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the model with a correct data value of parameters to avoid both overfitting
    as well as underfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hastie et al. (Hastie Trevor, Tibshirani Robert, Friedman Jerome, *The Elements
    of Statistical Learning: Data Mining, Inference, and Prediction*, Second Edition,
    2009) on the other hand, have recommended splitting the large-scale dataset into
    three sets: Training set (50%), Validation set (25%), and Test set (25%) (roughly).
    They also suggested building the model using the training set and calculating
    the prediction errors using the validation set. The test set was recommended to
    be used to assess the generalization error of the final model.'
  prefs: []
  type: TYPE_NORMAL
- en: If the amount of labeled data available during the supervised learning is smaller,
    it is not recommended to split the datasets. In that case, use cross-validation
    or Train split techniques (this will be discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models,*
    with several examples). More specifically, divide the data set into 10 parts of
    (roughly) equal size, after that for each of these ten parts, train the classifier
    iteratively and use the 10th part to test the model.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned and combining Spark MLlib with Spark ML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step of the pipeline designing is to create the building blocks (as
    a directed or undirected graph consisting of nodes and edges) and make a link
    between those blocks. Nevertheless, as a data scientist, you should be focused
    on scaling and optimizing nodes (primitives) too, so that you are able to scale-up
    your application for handling large-scale datasets in the later stage to make
    your ML pipeline consistently perform. The pipeline process will also help you
    to make your model adaptive for new datasets. However, some of these primitives
    might be explicitly defined to particular domains and data types (for example,
    text, images, video, audio, and spatiotemporal).
  prefs: []
  type: TYPE_NORMAL
- en: 'And beyond these types of data, the primitives should also be working for the
    general purpose domain statistics or mathematics. The casting of your ML model
    in terms of these primitives will make your workflow more transparent, interpretable,
    accessible, and explainable. A recent example would be the ML-Matrix, which is
    a distributed matrix library that can be used on top of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stay tuned and combining Spark MLlib with Spark ML](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Stay tune and interoperate ML, MLlib, and GraphX.'
  prefs: []
  type: TYPE_NORMAL
- en: As we already stated in the previous section, as a developer you can seamlessly
    combine the implementation techniques in Spark MLlib along with the algorithms
    developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable
    ML applications on top of RDD, DataFrame, and Datasets, as shown in Figure 8\.
    For example, an IoT-based real-time application could be developed using a hybrid
    model. Therefore, the recommendation here is to stay tuned or synchronized with
    the latest technologies around you for the betterment of your ML application.
  prefs: []
  type: TYPE_NORMAL
- en: Making ML applications modular and simplifying pipeline synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another good and often used practice when building your ML pipeline is to make
    the ML system modular. Some supervised learning problems can be solved using very
    simple models commonly referred to as generalized linear models. However, it depends
    on the data you will be using and others simply don't.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to conglomerates a series of simple linear binary classifiers, try
    to employ a lightweight modular architecture. This might be at the workflow stems
    or at the algorithms level. The advantages are obvious, since the modular architecture
    of your application handles massive amounts of data flow in a parallel and distributed
    way. Consequently, we suggest you have the three key innovative mechanisms: weighted
    threshold sampling, logistic calibration, and intelligent data partitioning as
    mentioned in the literature (for example, Yu Jin; Nick Duffield; Jeffrey Erman;
    Patrick Haffner; Subhabrata Sen; Zhi Li Zhang, *A Modular Machine Learning System
    for Flow-Level Traffic Classification in Large Networks*, ACM Transactions on
    Knowledge Discovery from Data, V-6, Issue-1, March 2012). The target is to achieve
    scalability and high-throughput while attaining a high accuracy of the predicted
    results from your ML application/system. While primitives can serve as building
    blocks, you still need some other tools that enable users to build ML pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, workflow tools have become more common these days, and such tools
    exist for data engineers, data scientists, and even for business analysts such
    as Alteryx, RapidMiner, Alpine Data, and Dataiku. At this point, we are talking
    about and stressing the business analysts since at the very last phase your target
    customer will be a business company who will value your ML model, right? The latest
    release of Spark comes with Spark ML APIs for building machine learning pipelines
    and making a domain specific language (see [https://en.wikipedia.org/wiki/Domain-specific_language](https://en.wikipedia.org/wiki/Domain-specific_language))
    for pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking of an innovative ML system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: However, in order to develop the algorithms to learn the ML models continuously
    with the help of available data, the viewpoint behind the machine learning is
    to automate the creation of analytical models. Unremittingly evolving models produce
    increasingly positive results and reduce the need for human interaction. This
    enables the ML models to automatically produce reliable and repeatable predictions.
  prefs: []
  type: TYPE_NORMAL
- en: More technically, suppose you are planning to develop a recommender system using
    ML algorithms. So, what is the target of developing that recommender system? And
    what are some innovative ideas for product development in machine learning? These
    two are typical questions that should be considered before you start developing
    your ML application or system. Consistent innovation might be challenging, especially
    when stirring advancing with new ideas, it can also be tough to comprehend where
    the greatest benefit lies. Machine learning can provision innovation from end
    to end of a variety of paths, such as determining weaknesses with current products,
    predictive analysis, or identifying previously concealed patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, you will have to think of large-scale computing to train your
    ML model offline, and later on your recommender system has to be able to work
    as a conventional search engine analysis for online recommendations. Thus, your
    ML application will be valued by a business company if your system:'
  prefs: []
  type: TYPE_NORMAL
- en: Can forecast buying items using your machine learning application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can do product analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can work as an emerging trend in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking and becoming smarter about Big Data complexities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in Figure 9, new business models are the unavoidable extension of the
    available data utilisation, so consideration of big data and its business values
    can make the business analyst's job, life and thinking smarter, which results
    in your targeted company delivering value to customers. In addition to this, you
    will also have to investigate (analyze to be more exact) rival or better companies.
  prefs: []
  type: TYPE_NORMAL
- en: Now the question is, how do you collect and use enterprise data? Big data is
    not only about the size (volume), it is also related to its velocity, veracity,
    variety, and value. For these types of complexities, for example, velocity can
    be addressed using Spark Streaming since streaming-based data is also big data
    that needs a real-time analytical approach. Other parameters such as volume and
    variety can be handled using Spark Core and Spark MLlib/ML towards big data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, you will have to manage the data by hook or by crook. If you are able
    to manage the data, the insights from the data can really shake up the way businesses
    operate with the useful features of big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thinking and becoming smarter about Big Data complexities](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Machine learning in Big Data best practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, data alone is not enough (see Pedro Domingos, *A Few Useful
    Things to Know about Machine Learning,* [https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)),
    but extracting meaningful features from the data and putting semantics of data
    into the model is more important. This is like what most of the tech giants such
    as LinkedIn are developing through large-scale machine learning frameworks from
    feature targeting for their community, which is more or less a supervised learning
    technique. The workflow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the data, extract the feature, and set the target
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature and target join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a snapshot from the concatenated data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Partition the snapshot into two parts: training set and test set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the training set, prepare the sample data by sampling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model using the sampled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model from the previously developed persistent model, as well as
    the test data prepared in step 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the best model is found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the model for the target audience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So what's next? Your model also should be adaptable to large-scale dynamic data
    such as real-time streaming IoT data PLUS real-time feedback is also important
    so that your ML system can learn from the mistakes. The next sub-section discusses
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Applying machine learning to dynamic data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The reasons are obvious, since machine learning brings concrete and dynamic
    aspects to IoT projects. Recently, machine learning has experienced a pep talk
    in popularity amongst industrial companies and they profit out of the box. As
    a result, all but every IT vendor are precipitously announcing IoT platforms and
    consulting services. But achieving financial benefits through IoT data is not
    an easy job. Moreover, many businesses have failed to clearly determine what areas
    will change with the implementation of an IoT strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Considering these positive and negative issues together, your ML model should
    adapt to large dynamic data since the large-scale data means billions of records,
    large feature spaces, and low positive rates from the sparsity issue. Nevertheless,
    data is dynamic so consequently, the ML models have to be adaptive enough; otherwise
    you will have to face a bad experience or be lost in the black hole.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice after developing an ML application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The typical steps that are best practice after an ML model/system has been
    developed are: visualization for understanding the predictive values, model validation,
    error and accuracy analysis, model tuning, model adapting, and scaling up for
    handling large-scale datasets with ease.'
  prefs: []
  type: TYPE_NORMAL
- en: How to enable real-time ML visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Visualization provides an interactive interface to stay tune the ML model itself.
    Therefore, without visualizing the predictive results, it merely becomes difficult
    to further improve the performance of an ML application. The best practice could
    be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate some third-party tools along with GraphX for your visualization
    for large-scale graph related data (more to be discussed in *[Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data")*, *Advanced
    Machine Learning with Streaming and Graph Data*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For non-graph data, a call-back interface for the Spark ML algorithm to send
    and receive messages by incorporating other tools like Apache Kafka:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms decide when and what message to send
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms don't care how the message is delivered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A task channel to handle the message delivery service from the Spark Driver
    program to Spark Client or Spark cluster nodes. The task channel would be communicating
    using Spark Core at a lower level of abstraction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not care about the content of the message or recipient of the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The message is delivered from Spark Client to the browser or visualization
    client:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We recommend using HTML5 **Server-Sent Events** (**SSE**) and HTTP Chunked Response
    (PUSH) together. Incorporation of Spark with this type of technology will be discussed
    in [Chapter 10](part0079_split_000.html#2BASE2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 10.  Configuring and Working with External Libraries"), *Configuring
    and Working with External Libraries*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull is possible; however, it requires a message queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization using JavaScript frameworks such as `Plot.ly` (please refer to
    [https://plot.ly/](https://plot.ly/)) and `D3.js` (please refer to [https://d3js.org/](https://d3js.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do some error analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As algorithms become more prevalent, we need better tools for building complex
    hitherto, robust, and stable machine learning systems. A popular distributed framework
    like Apache Spark takes these ideas to extremely large datasets for the wider
    audience. Therefore, it would be better if we could bind approximation errors
    and convergence rates for the layered pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we can compute error bars for nodes, the next step would be to have
    a mechanism for extracting error bars for these pipelines. However, in practice,
    when the ML model is deployed for the production, we might need tools to confirm
    that the pipeline will work and will not do make malfunction or stop halfway through and
    that it can provide some expected measure of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping your ML application tuned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Devising one or two algorithms that perform solidly well on a simple problem
    can be considered as a good kick-off. However, sometimes you may be thirsty to
    get the best accuracy, by even sacrificing your valuable time and available computational
    resources. This would be a smarter way, and it will help you not only to squeeze
    out extra performance, but also to improve the results in terms of accuracy that
    you were receiving out of the machine learning algorithms you designed previously.
    In order to do that, when you tune the model and related algorithm, essentially,
    you must have a high confidence in the results.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, those results will be available after you specify the testing and
    validation. This means you should only be using those techniques that reduce the
    variance of the performance measure so that you can assess the algorithms that
    are running more smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In parallel, like most data practitioners, we also suggest you to use the cross-validation
    technique (also often called rotation estimation) with a reasonably high number
    of folds (that is, K-fold cross-validation, where a single subsample is used as
    the validation dataset for testing the model itself , and the remaining K-1 subsamples
    are used to train the data). Although the exact number of folds, or K, depends
    on your dataset, however, 10-fold cross-validation is commonly used, but most
    often the value of K remains unfixed. We will mention three strategies here that
    you will need to tune your machine learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm tuning**: Makes your machine learning algorithm parameterized.
    After that, adjust the value of those parameters (if they have multiple parameters)
    to influence the outcome of the overall learning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensembles**: Sometimes it is good to be naïve! Therefore, in order to get
    improved results, keep trying to combine the outcomes from multiple machine learning
    methods or algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extreme feature engineering**: If your data has complex and multi-dimensional
    structures embedded in it, ML algorithms know how to find and exploit it to make
    decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping your ML application adaptive and scale-up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Figure 10, the adaptive learning conglomerates the previous generations
    of rule-based, simple machine learning, and deep learning approaches to machine
    intelligence according to Rob Munro:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keeping your ML application adaptive and scale-up](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Four generation of machine intelligence (Figure courtesy of Rob
    Munro).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fourth generation of machine learning: adaptive learning, (`http://idibon.com/the-fourth-generation-of-machine-learning-adaptive-learning/#comment-175958`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Research also shows that adaptive learning is 95% accurate in predicting people''s
    intention to purchase a car, for example (please refer to Rob Munro, *The fourth
    generation of machine learning: Adaptive learning*, `http://idibon.com/the-fourth-generation-of-machine-learning-adaptive-learning/#comment-175958`).
    Moreover, if your ML application is adaptive with the new environment and new
    data, it is expected that if enough infrastructure is provided, your ML system
    can be scaled-up for the increasing data loads.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right algorithm for your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*What machine learning algorithm should I use?* is a very frequently asked
    question for the Naive machine learning practitioners, but the answer is always
    i*t depends on*. More elaborately:'
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the volume, quality, complexity, and the nature of the data that
    has to be tested/used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on external environments and parameters such as your computing system's
    configuration or underlying infrastructures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on what you want to do with the answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on how the mathematical and statistical formulation of the algorithm
    was translated into machine instructions for the computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And it depends on how much time you have
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 11* provides a complete work-flow for choosing the right algorithm
    for your ML problem. However, note that some tricks might not work-flow depending
    upon data and problem types:![Choosing the right algorithm for your application](img/00039.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 11: A work-flow for choosing the right algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality is, even the most experienced data scientists or data engineers
    can''t give a straight recommendation about which ML algorithm performs best before
    trying them all together. Most of the statements of agreement/disagreement begins
    with *It depends on...hmm...*Habitually, you might be contemplative if there are
    cheat sheets of machine learning algorithms and if so, how to use that cheat sheet.
    Several data scientists we talked to said that the only sure way to find the very
    best algorithm is to try all of them; therefore, there is no shortcut dude! Let''s
    make it clear, suppose you do have a set of data and you want to do some clustering.
    Thus, technically, this could be classification or regression if your data is
    labeled/unlabeled or values or training set data. Now, the first concern that
    evolves in your mind is:'
  prefs: []
  type: TYPE_NORMAL
- en: Which factors should I consider before choosing an appropriate algorithm? Or
    should I just choose an algorithm randomly?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I choose any data pre-processing algorithm or tools that can be applied
    to my data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What sort of feature engineering techniques should I be using to extract the
    useful features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What factors can improve the performance of my ML model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I adopt my ML application for new data types?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I scale-up my ML application for large-scale datasets? And so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will always expect the best answer that is much more justified and explains
    everything that someone should consider. In this section, we will try to answer
    these questions with our little machine learning knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations when choosing an algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recommendation or suggestions we are providing here are for the novice
    data scientist with learner machine learning to expert data scientists who are
    trying to choose an optimal algorithm to start with the Spark ML APIs. That means,
    it makes some overviews and oversimplifications, but it will point you in a safe
    direction, believe us! Suppose you are planning to develop an ML system to answer
    the following question based on the rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IF` feature X has property Z `THEN` do Y'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Affirmatively, there should be such rules:'
  prefs: []
  type: TYPE_NORMAL
- en: IF X `THEN` it is sensible to try Y using property Z and avoid W
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, what is sensible and what is not depends on:'
  prefs: []
  type: TYPE_NORMAL
- en: Your application and the expected complexity of the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of the data set (that is, how many rows/columns, how many independent cases).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is your dataset labeled or unlabeled?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type of data and the kind of measurement, since different nature of data suggests
    a different order or structure, right?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And obviously in practice your experience in applying different methods efficiently
    and intelligently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreover, if you want to have a general answer to a general problem, we recommend
    the Elements of Statistical Learning (Hastie Trevor, Tibshirani Robert, Friedman
    Jerome, *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*,
    Second Edition, 2009) for a fresh start. Nevertheless, we also recommend going
    with the following algorithmic properties that:'
  prefs: []
  type: TYPE_NORMAL
- en: Show excellent accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have fast training times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And the use of linearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Getting the most accurate results from your ML application isn't always indispensable.
    Depending on what you want to use it for, sometimes an approximation is adequate
    enough. If the situation is something like this, you may be able to reduce the
    processing time drastically by incorporating the better-estimated methods. When
    you are familiar with the workflow with the Spark machine learning APIs, you will
    enjoy the advantage of having more approximation methods, because those approximation
    methods will tend to avoid the overfitting problem out of your ML model automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Training time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The execution time requires finishing the data preprocessing or building the
    model and varies a great deal across different algorithms, the inherited complexities,
    and of course the robustness. The training time is often closely related to the
    accuracy. In addition, often you will discover that some of the algorithms you
    will be using are elusive to the number of data points compared to others. However,
    when your time is sufficient and especially when the dataset is larger, for doing
    all the formalities, it can get-up-and-go the choice of algorithm. Therefore,
    if you are concerned particularly with the time, try to sacrifice the accuracy
    or performance and use a simple algorithm that fulfils your minimum requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Linearity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many machine learning algorithms developed recently that make use
    of linearity (also available in the Spark MLlib and Spark ML). For example, the
    linear classification algorithms allow classes to be separated by plotting a differentiating
    straight line or otherwise by the higher-dimensional equivalents of the datasets.
    A linear regression algorithm, on the other hand, assumes that data trends follow
    a simple straight line. This assumption is not naive for some machine learning
    problems; however, there might be some other cases where the accuracy will be
    down. Despite their hazards, linear algorithms are very popular for the data engineers
    or data scientists as the first line of the outbreak. Moreover, these algorithms
    also tend to be algorithmically simple and fast to train your models during the
    whole process.
  prefs: []
  type: TYPE_NORMAL
- en: Talking to your data when choosing an algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will find many machine learning datasets available for free here at [http://machinelearningmastery.com/tour-of-real-world-machine-learning-problems/](http://machinelearningmastery.com/tour-of-real-world-machine-learning-problems/)
    or at the UC Irvine Machine Learning Repository (at [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)).
    The following data properties should also be placed first:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parameters or data properties are the handholds for a data scientist like you
    that gets to turn when setting up an algorithm. They are numbers that affect the
    algorithm's performance, such as error tolerance or the number of iterations,
    or options between variants of how the algorithm acts. The training time and accuracy
    of the algorithm can sometimes be quite sensitive to getting the right settings.
    Typically, algorithms with a large number of parameters require trial and error
    to find an optimal combination.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the fact that this is a great way to span the parameter space, the model
    building or training time increases exponentially with the increased number of
    parameters. This is a dilemma as well as a time-performance trade-off. The positive
    sides are having many parameters characteristically indicates greater flexibility
    of the ML algorithms. And secondly, your ML application achieves much better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: How large is your training set?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your training set is smaller, high bias with low variance classifiers such
    as Naive Bayes have an advantage over low bias with high variance classifiers
    such as kNN. Therefore, the latter will over fit. But low bias with high variance
    classifiers, on the other hand, start to win out as your training set grows linearly
    or exponentially since they have lower asymptotic errors. This is because high
    bias classifiers aren't powerful enough to provide accurate models. You can also
    think of this as a trade-off between generative models versus discriminative model
    distinction.
  prefs: []
  type: TYPE_NORMAL
- en: Number of features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For certain types of experimental datasets, the number of extracted features
    can be very large compared to the number of data points itself. This is often
    the case with genomics, biomedical, or textual data. A large number of features
    can swamp some learning algorithms, making training time ridiculously high. Support
    vector machines are particularly well suited in this case for its high accuracy,
    nice theoretical guarantees regarding overfitting, and an appropriate kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Special notes on widely used ML algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will provide some special notes for the most commonly used
    machine learning algorithm or techniques. The techniques we will emphasis are
    logistic regression, linear regression, recommender system, SVM, decision tree,
    random forest, Bayesian method and decision forests, decision jungles, and variants. Table
    3 shows the pros and cons of some widely used algorithms including where and when
    to chose these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Pros** | **Cons** | **Better at** |'
  prefs: []
  type: TYPE_TB
- en: '| **Linear regression (LR)** | Very fast and often runs in a constant timeEasy
    to understand the modellingLess prone to overfitting and underfitting Intrinsically
    simpleVery fast so less model building timeLess prone to overfitting and underfittingHas
    low variance | Often unable for complex data modellingOften unable to conceptualize
    the nonlinear relationships without transforming the input DatasetNot suitable
    for complex modellingWorks better with only single decision boundary Requires
    large sample size to achieve stable resultsHigh bias | Numerical dataset with
    large collection of featuresWidely used in biological, behavioral and social sciences
    to predict possible relationships among variablesWorks well for numerical as well
    as categorical variablesUsed in various fields, including the medical and social
    sciences |'
  prefs: []
  type: TYPE_TB
- en: '| **Decision trees (DT)** |  Less model building and prediction timeRobust
    against the noise and missing valuesHigh accuracy | Interpretation is hard with
    large and complex treesDuplication may occur within the same sub-treePossible
    issues with diagonal decision boundaries |  Targeting high accurate classificationMedical
    diagnosis and prognosisCredit risk analytics |'
  prefs: []
  type: TYPE_TB
- en: '| **Neural networks (NN)** |  Extremely powerful and robustCapable of modelling
    very complex relationshipsCan be working without knowing the underlying data |
    Highly overfitting and underfitting proneHigh training and prediction timeComputationally
    expensive requiring significant computing powerModel is not readable or reusable
    |  Image processingVideo processingHuman-intelligenceRoboticsDeep learning |'
  prefs: []
  type: TYPE_TB
- en: '| **Random forest (RF)** | Good for bagged treesLow varianceHigh accuracyCan
    handle the overfitting problem | Not as easy to visually and interpretHigh training
    and prediction time | When dealing with multiple features which may be correlatedBiomedical
    diagnosis and prognosisCan be applied both for classification and regression |'
  prefs: []
  type: TYPE_TB
- en: '| **Support vector machines (SVM)** | High accuracy | Susceptible to overfitting
    and underfittingNo numerical stabilityComputationally expensive requiring large
    computing power | Image classificationHandwriting recognition |'
  prefs: []
  type: TYPE_TB
- en: '| **K-nearest neighbors (K-NN)** | Simple and powerfulLazy training involvedCan
    be applied for both multiclass classification and regression | High training and
    prediction timeNeed to have accurate distance functionLow performance with high
    dimensional dataset | Low-dimensional datasetsAnomaly detection like outlier detectionFault
    detection in semiconductorGene expressionProtein-protein interaction |'
  prefs: []
  type: TYPE_TB
- en: '| **K-means** | Linear execution timePerform better than hierarchical clusteringExcellent
    with hyper-spherical  clusters | Repeatable and lack consistencyRequires prior
    knowledge of K | Is not a good choice if the natural clusters occurring in the
    dataset are non-sphericalGood for large dataset |'
  prefs: []
  type: TYPE_TB
- en: '| **Latent Dirichilet Allocation (LDA)** | Can be applied for large-scale text
    datasetsCan overcome the overfitting problem of pLSACan be applied for both document
    classification and clustering through topic modelling | Cannot be applied with
    high dimensional and complex texts databasesRequires the specification of the
    number of topicsCannot find the granularity at optimum levelHierarchical Dirichlet
    Process (HDP) is the better choice | Document classification and clustering through
    topic modelling from large-scale text datasetCan be applied in NLP and other text
    analytics |'
  prefs: []
  type: TYPE_TB
- en: '| **Naive Bayes (NB)** | Computationally fastSimple to implementWorks well
    with high dimensionsCan handle missing valuesIs adaptable since the model can
    be modified with new training data without rebuilding the model | Relies on independence
    assumption so performs badly if the assumption does not metRelatively low accuracy
    | When data has lots of missing valuesDependencies of features from each other
    are similar between featuresSpam filtering and classificationClassifying a news
    article about technology, politics, or sportsText mining |'
  prefs: []
  type: TYPE_TB
- en: '| **Singular Value decomposition (SVD) and Principal Component Analysis (PCA)**
    | Reflects the real intuitions about the dataAllows estimation probabilities in
    high-dimensional dataDramatic reduction in size of dataBoth are based on strong
    linear algebra | Too expensive for many applications like Twitter and web analyticsDisastrous
    for task with fine-grained classesNeed proper understanding of the linearityOften
    complexity is cubicComputationally slower | SVD is applied for low-rank matrix
    approximation, image processing, bioinformatics,  signal processing,  NLPPCA is
    used for interest rate derivatives portfolios, neuroscience and so onBoth are
    suitable for the dataset having high dimension and multivariate data |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Pros and cons of some widely used algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression and linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic regression is a powerful tool developed around the globe for its two-class
    and multiclass classification since it's fast as well as simple. The fact is that
    it uses an *S*-shaped curve instead of a straight line. making it a natural fit
    for partitioning data into groups. It provides linear class boundaries, so that
    when you use it, make sure a linear approximation is something you can survive
    with. Unlike the decision trees or SVMs, you also have a nice probabilistic interpretation,
    so you will be able to update your model to adapt for new datasets easily.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the recommendation is, use it if you want to have a flavor of probabilistic
    framework or if you expect to receive more training data in the future to be incorporated
    into your model. As mentioned previously, linear regression fits a line, plane,
    or hyperplane to the dataset. It's a workhorse, simple and fast, but it may be
    overly simplistic for some problems.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We already talked about the accuracy and performance issues of mostly used ML
    algorithms and tools. However, beyond the accuracy research on recommender systems
    is concern about finding another environmental factor or/and parameter diversity.
    Therefore, a recommendation system with good accuracy and higher intra-list diversity
    will be the winner. As a result, your product will be precious to your target
    customers. It would be, however, more effective to let the users re-rate the items,
    rather than showing new items only. If your clients have some extra requirements
    that need to be fulfilled, such as privacy or security, your system has to be
    able to deal with the privacy related issues.
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly important because customers have to provide some personal
    information as well, so it is recommended not to expose that sensitive information
    publicly.
  prefs: []
  type: TYPE_NORMAL
- en: Building user profiles using some robust techniques or algorithms such as collaborative
    filtering, on the other hand, could be problematic from the privacy perspective.
    Moreover, research in this area has found that user demographics information may
    influence how satisfied the other users are with recommendations (see also in
    Joeran Beel, Stefan Langer, Andreas Nürnberger, Marcel Genzmehr, *The Impact of
    Demographics (Age and Gender) and Other User Characteristics on Evaluating Recommender
    Systems*. In Trond Aalberg and Milena Dobreva and Christos Papatheodorou and Giannis
    Tsakonas and Charles Farrugia. *Proceedings of the 17th International Conference
    on Theory and Practice of Digital Libraries, Springer, pp. 400-404, Retrieved
    1 November 2013*).
  prefs: []
  type: TYPE_NORMAL
- en: Although the serendipity is a crucial measure of how surprising the recommendations
    are, ultimately trust needs to be built using the recommender system. This can
    be made possible by explaining how it generates the recommendations, and why it
    recommends an item even with little demographic information, from the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, if the user does not trust the system at all, they will not provide
    any demographic information or will not re-rate the items. A SVMs, according to
    *Cowley et al*. (G. C. Cawley and N. L. C. Talbot, *Over-fitting in model selection
    and subsequent selection bias in performance evaluation, Journal of Machine Learning
    Research, vol. 11, pp. 2079-2107, July 2010*), there are several advantages of
    Support Vector Machines:'
  prefs: []
  type: TYPE_NORMAL
- en: You can tackle the problem of the over-fitting problem since SVMs provide you
    with a regularization parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM use the kernel trick that helps to build the machine learning model via
    engineering the kernel with ease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An SVM algorithm is developed, designed, and defined based on a convex optimization
    problem, therefore, there is no concept of local minima
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a ballpark figure to a bound on the test error rate, where there is a
    significant and well-studied theory that works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These promising features of SVM really would help you, and it is suggested
    that it should be used frequently. On the other hand, the cons are:'
  prefs: []
  type: TYPE_NORMAL
- en: The theory only can really cover determination of the parameters for a given
    value of the regularization and kernel parameters. Therefore, you could only choose
    the kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There might be a worse scenario as well, where the kernel model itself can be
    quite sensitive to over-fitting during the model selection criterion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Decision trees are cool because of their usability they are easy to interpret
    and explain the machine learning problem around. In parallel, they can easily
    be handled for the feature related interactions. Most importantly, they are often
    non-parametric. Therefore, even if you are an ordinary data scientist with limited
    working proficiencies, you don''t need to be worried about the issues such as
    outliers, parameter setting, and tuning. Sometimes fundamentally, you can relay
    with the decision trees so that they will make your stress for handling issue
    of the data linearity, or more technically, whether your data is linearly separable
    or not, you need not be worried. On the contrary, there are some cons as well.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the decision tree will not be suitable, sometimes they don't
    support online learning for real-time datasets. In that case, you have to rebuild
    your tree when new examples or datasets come; more technically, gaining model
    adaptability would not be possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, if you are not aware, they will easily become over-fitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Random forests are quite popular and are a winner for the data scientist, since
    they are divine for a package with plenty of classification problems. They are
    usually slightly ahead of SVMs in terms of usability and have faster operation
    for most of the classification problems. In addition to this, they are also scalable
    when increasing the datasets you have available. In parallel, you don't need to
    be worried about tuning a cluster of parameters. On the contrary, you need to
    take care of many parameters and tuning when handling your data.
  prefs: []
  type: TYPE_NORMAL
- en: Decision forests, decision jungles, and variants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision forests, decision jungles, and boosted decision trees are all based
    on decision trees, a foundational machine learning concept that is less used.
    There are many variants of decision trees are there; nonetheless, they all do
    the same thing, which is subdividing the feature space into regions with the same
    label. In order to avoid the over-fitting problem, a large set of trees are constructed
    with mathematical and statistical formulations, where the trees are not correlated
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: The average of this is referred to as a decision forest; which is a tree that
    avoids the overfitting problem as stated earlier. However, the disadvantage is
    that decision forests can use a lot of memory. Decision jungles, on the other
    hand, are a variant that consume less memory by sacrificing a slightly longer
    training time. Fortunately, the boosted decision trees avoid overfitting by limiting
    the number of subdivision and the number of permitted data points in each region.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the experimental or sample dataset size is large, the Bayesian method often
    provides results for parametric models that are very similar to the results produced
    by other classical statistical methods. Some potential advantages of using the
    Bayesian method was summarized by Elam et al (W. T. Elam, B. Scruggs, F. Eggert,
    and J. A. Nicolosi, *Advantages and Disadvantages of Methods for Obtaining XRF
    NET Intensities*, Copyright ©JCPDS-International Centre for Diffraction Data 2011
    ISSN 1097-0002). For example, it provides a natural way of combining prior information
    with data. Therefore, as a data scientist, you can incorporate that past information
    regarding the parameters and form a prior distribution for future analysis for
    new datasets. It also provides inferences that are conditional on the data without
    the need of asymptotic approximation of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: It provides some suitable settings for a wide range of models, such as hierarchical
    models and missing data problems. There are also disadvantages of using Bayesian
    analysis. For example, it does not tell you how to select a prior over world models
    or even that there is no correct way to choose a prior. Therefore, if you do not
    proceed with caution, you might generate many false positive or false negative
    results that often come with a high computational cost, if the number of parameters
    in a model is large.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This ends our rather quick tour of machine learning and the best practice that
    needs to be followed. Although we have tried to cover some of the most basic things
    to remember, suitable data often beats better algorithms and better demand. Most
    importantly, to design good features out of your data might take a long time;
    however, it would very much aid you. However, if you have a large-scale dataset
    to be applied to your machine learning algorithms or model, whichever classification,
    clustering, or regression algorithm you use might not be a matter of fact concerning
    the machine learning classes and their respective classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it would be a wise decision to choose an appropriate machine learning
    algorithm that can fulfill requirements such as speed, memory usage, throughput,
    scalability, or usability. In addition to going over what we said in the sections
    above, if you are really concerned about achieving the accuracy, you should undoubtedly
    try a group of different classifiers to find the best one using the cross-validation
    technique or just use an ensemble method to choose them alltogether.
  prefs: []
  type: TYPE_NORMAL
- en: You can also be motivated and take a lesson from the Netflix Prize PLUS. We
    spoke at length about the Spark machine learning APIs, some best practice in ML
    application development, machine learning tasks and classes, some widely used
    best practices, and so on. However, we have not shown in depth analysis of the
    machine learning techniques. We intend to talk about this in more detail in [Chapter
    4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting
    Knowledge through Feature Engineering"), *Extracting Knowledge through Feature
    Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover in detail the DataFrame, Dataset, and **Resilient
    Distributed Dataset** (**RDD**) APIs for working with structured data targeting
    to provide a basic understanding of machine learning problems with the available
    data. Therefore, at the end, you will be able to apply from basic to complex data
    manipulation with ease.
  prefs: []
  type: TYPE_NORMAL

- en: Kernel Facilities and Helper Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The kernel is a standalone piece of software, as you'll see in this chapter,
    that does not make use of any C library. It implements any mechanism you may encounter
    in modern libraries, and even more, such as compression, string functions, and
    so on. We will walk step by step through the most important aspects of such capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topic:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the kernel container data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with the kernel sleeping mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using timers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delving into the kernel locking mechanism (mutex, spnlock)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deferring work using a kernel dedicated API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using IRQs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding container_of macro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to managing several data structures in the code, you''ll almost
    always need to embed one structure into another and retrieve them at any moment
    without being asked questions about memory offset or boundaries. Let''s say you
    have a `struct person` , as defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By only having a pointer on `age` or `name` , one can retrieve the whole structure
    wrapping (containing) that pointer. As the name says, `container_of` macro is
    used to find the container of the given field of a structure. The macro is defined
    in `include/linux/kernel.h` and looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t be afraid by the pointers; just see it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the elements of the preceding code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pointer` : This is the pointer to the field in the structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container_type` : This is the type of structure wrapping (containing) the
    pointer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container_field` : This is the name of the field to which `pointer` points
    inside the structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us consider the following container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us consider one of its instance, along with a pointer to the `name`
    member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Along with a pointer to the `name` member (`the_name_ptr` ), you can use the
    `container_of` macro in order to get a pointer to the whole structure (container)
    that wraps this member by using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`container_of` takes the offset of `name` at the beginning of the struct into
    account to get the correct pointer location. If you subtract the offset of the
    field `name` from the pointer `the_name_ptr` , you will get the correct location.
    It is what the macro''s last line does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying this to a real example, it gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s all you need to know about the `container_of` macro, and believe me,
    it is enough. In real drivers that we''ll develop further in the book, it looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`controller_of` macro is mainly used in generic containers in the kernel. In
    some examples in this book (starting from [Chapter 5](text00146.html) , *Platform
    Device Drivers* ), you will encounter the `container_of` macro.'
  prefs: []
  type: TYPE_NORMAL
- en: Linked lists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine you have a driver that manages more than one device, let''s say five
    devices. You may need to keep a track of each of them in your driver. What you
    need here is a linked list. Two types of linked list actually exist:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply linked list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doubly linked list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, kernel developers only implement circular doubly linked lists because
    this structure allows you to implement FIFO and LIFO, and kernel developers take
    care to maintain a minimal set of code. The header to be added in the code in
    order to support lists is `<linux/list.h>` . The data structure at the core of
    list implementation in the kernel is `struct list_head` structure, defined as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `struct list_head` is used in both the head of the list and each node.
    In the world of the kernel, before a data structure can be represented as a linked
    list, that structure must embed a `struct list_head` field. For example, let''s
    create a list of cars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can create a list for the car, we must change its structure in order
    to embed a `struct list_head` field. The structure becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to create a `struct list_head` variable that will always point
    to the head (first element) of our list. This instance of `list_head` is not associated
    to any car and is special:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create cars and add them to our list—`carlist` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It is as simple as that. Now, `carlist` contains two elements. Let us get deeper
    into the linked list API.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and initializing the list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to create and initialize the list:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dynamic method consists of a `struct list_head` and initializes it with
    the `INIT_LIST_HEAD` macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the expansion of `INIT_LIST_HEAD` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Static method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Static allocation is done through the `LIST_HEAD` macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`LIST_HEAD` s definition is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is its expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This assigns each pointer (`prev` and `next` ) inside the `name` field to point
    to `name` itself (just like `INIT_LIST_HEAD` does).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a list node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create new nodes, just create our data struct instance, and initialize their
    embedded `list_head` field. Using the car example, it will give the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As said earlier, use `INIT_LIST_HEAD,` which is a dynamically allocated list
    and usually part of another structure.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a list node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The kernel provides `list_add` to add a new entry to the list, which is a wrapper
    around the internal function `__list_add` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`__list_add` will take two known entries as a parameter, and inserts your elements
    between them. Its implementation in the kernel is quite easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of adding two cars in our list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This mode can be used to implement a stack. The other function to add an entry
    into the list is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This inserts the given new entry at the end of the list. Given our previous
    example, we can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This mode can be used to implement a queue.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a node from the list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'List handling is an easy task in kernel code. Deleting a node is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the preceding example, let us delete the red car:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`list_del` disconnects the `prev` and `next` pointers of the given entry, resulting
    in an entry removal. The memory allocated for the node is not freed yet; you need
    to do that manually with `kfree` .'
  prefs: []
  type: TYPE_NORMAL
- en: Linked list traversal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have the macro `list_for_each_entry(pos, head, member)` for list traversal.
  prefs: []
  type: TYPE_NORMAL
- en: '`head` is the list''s head node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`member` is the name of the list `struct list_head` within our data struct
    (in our case, it is `list` ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pos` is used for iteration. It is a loop cursor (just like `i` in `for(i=0;
    i<foo; i++)` ). `head` could be the head node of the linked list, or any entry,
    and we don''t care since we are dealing with a doubly linked list:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Why do we need the name of the `list_head` type field in our data structure?
    Look at the `list_for_each_entry` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Given this, we can understand that it is all about `container_of` 's power.
    Also bear in mind `list_for_each_entry_safe(pos, n, head, member)` .
  prefs: []
  type: TYPE_NORMAL
- en: Kernel sleeping mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sleeping is the mechanism by which a process relaxes a processor, with the possibility
    of handling another process. The reason why a processor can sleep could be for
    sensing data availability, or waiting for a resource to be free.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel scheduler manages a list of tasks to run, known as a run queue. Sleeping
    processes are not scheduled anymore, since they are removed from that run queue.
    Unless its state changes (that is, it wakes up), a sleeping process will never
    be executed. You may relax a processor as soon as one is waiting for something
    (resource or anything else), and make sure a condition or someone else will wake
    it up. That said, the Linux kernel eases the implementation of the sleeping mechanism
    by providing a set of functions and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Wait queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wait queues are essentially used to process blocked I/O, to wait for particular
    conditions to be true, and to sense data or resource availability. To understand
    how it works, let''s have a look at its structure in `include/linux/wait.h` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's pay attention to the `task_list` field. As you can see, it is a list.
    Every process you want to put to sleep is queued in that list (hence the name
    *wait queue* ) and put into a sleep state until a condition becomes true. The
    wait queue can be seen as nothing but a simple list of processes and a lock.
  prefs: []
  type: TYPE_NORMAL
- en: 'The functions you will always face when dealing with wait queues are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static declaration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Dynamic declaration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Blocking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Unblocking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`wait_event_interruptible` does not continuously poll, but simply evaluates
    the condition when it is called. If the condition is false, the process is put
    into a `TASK_INTERRUPTIBLE` state and removed from the run queue. The condition
    is then only rechecked each time you call `wake_up_interruptible` in the wait
    queue. If the condition is true when `wake_up_interruptible` runs, a process in
    the wait queue will be awakened, and its state set to `TASK_RUNNING` . Processes
    are awakened in the order they are put to sleep. To awaken all processes waiting
    in the queue, you should use `wake_up_interruptible_all` .'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the main functions are `wait_event` , `wake_up` , and `wake_up_all`
    . They are used with processes in the queue in an exclusive (uninterruptible)
    wait, since they can't be interrupted by the signal. They should be used only
    for critical tasks. Interruptible functions are just optional (but recommended).
    Since they can be interrupted by signals, you should check their return value.
    A nonzero value means your sleep has been interrupted by some sort of signal,
    and the driver should return `ERESTARTSYS` .
  prefs: []
  type: TYPE_NORMAL
- en: 'If someone has called `wake_up` or `wake_up_interruptible` and the condition
    is still `FALSE` , then nothing will happen. Without `wake_up` (or `wake_up_interuptible`
    ), process(es) will never be awakened. Here is an example of a wait queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, the current process (actually `insmod` ) will be
    put into sleep in the wait queue for 5 seconds and woken up by the work handler.
    The `dmesg` output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Delay and timer management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Time is one of the most used resources, right after memory. It is used to do
    almost everything: defer work, sleep, scheduling, timeout, and many other tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: There are the two categories of time. The kernel uses absolute time to know
    what time it is, that is, the date and time of the day, whereas relative time
    is used by, for example, the kernel scheduler. For absolute time, there is a hardware
    chip called **real-time clock** (**RTC** ). We will deal with such devices later
    in the book in [Chapter 18](text00398.html) , *RTC Drivers.* On the other side,
    to handle relative time, the kernel relies on a CPU feature (peripheral), called
    a timer, which, from the kernel's point of view, is called a *kernel timer* .
    Kernel timers are what we will talk about in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel timers are classified into two different parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard timers, or system timers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-resolution timers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard timers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard timers are kernel timers operating on the granularity of jiffies.
  prefs: []
  type: TYPE_NORMAL
- en: Jiffies and HZ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A jiffy is a kernel unit of time declared in `<linux/jiffies.h>` . To understand
    jiffies, we need to introduce a new constant HZ, which is the number of times
    `jiffies` is incremented in one second. Each increment is called a *tick* . In
    other words, HZ represents the size of a jiffy. HZ depends on the hardware and
    on the kernel version, and also determines how frequently the clock interrupt
    fires. This is configurable on some architecture, fixed on other ones.
  prefs: []
  type: TYPE_NORMAL
- en: What it means is that `jiffies` is incremented HZ times every second. If HZ
    = 1,000, then it is incremented 1,000 times (that is, one tick every 1/1,000 seconds).
    Once defined, the **programmable interrupt timer** (**PIT** ), which is a hardware
    component, is programmed with that value in order to increment jiffies when the
    PIT interrupt comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the platform, jiffies can lead to overflow. On a 32-bit system,
    HZ = 1,000 will result in about 50 days duration only, whereas the duration is
    about 600 million years on a 64-bit system. By storing jiffies in a 64-bit variable,
    the problem is solved. A second variable has then been introduced and defined
    in `<linux/jiffies.h>` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In this manner on 32-bit systems, `jiffies` will point to low-order 32-bits,
    and `jiffies_64` will point to high-order bits. On 64-bit platforms, `jiffies
    = jiffies_64` .
  prefs: []
  type: TYPE_NORMAL
- en: Timers API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A timer is represented in the kernel as an instance of `timer_list` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`expires` is an absolute value in jiffies. `entry` is a doubly linked list,
    and `data` is optional, and passed to the callback function.'
  prefs: []
  type: TYPE_NORMAL
- en: Timer setup initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are steps to initialize timers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up the timer:** Set up the timer, feeding the user-defined callback
    and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'One can also use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`setup_timer` is a wrapper around `init_timer` .'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting the expiration time:** When the timer is initialized, we need to
    set its expiration before the callback gets fired:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '**Releasing the timer:** When you are done with the timer, it needs to be released:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`del_timer` returns `void` whether it has deactivated a pending timer or not.
    Its return value is `0` on an inactive timer, or `1` on an active one. The last,
    `del_timer_sync` , waits for the handler to finish its execution, even those that
    may happen on another CPU. You should not hold a lock preventing the handler''s
    completion, otherwise it will result in a dead lock. You should release the timer
    in the module cleanup routine. You can independently check whether the timer is
    running or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This function checks whether there are any fired timer callbacks pending.
  prefs: []
  type: TYPE_NORMAL
- en: Standard timer example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: High resolution timers (HRTs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard timers are less accurate and do not suit real-time applications. High-resolution
    timers, introduced in kernel v2.6.16 (and enabled by the `CONFIG_HIGH_RES_TIMERS`
    option in the kernel configuration) have a resolution of microseconds (up to nanoseconds,
    depending on the platform), compared to milliseconds on standard timers. The standard
    timer depends on HZ (since they rely on jiffies), whereas HRT implementation is
    based on `ktime` .
  prefs: []
  type: TYPE_NORMAL
- en: Kernel and hardware must support an HRT before being used on your system. In
    other words, there must be an arch-dependent code implemented to access your hardware
    HRTs.
  prefs: []
  type: TYPE_NORMAL
- en: HRT API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The required headers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'An HRT is represented in the kernel as an instance of `hrtimer` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: HRT setup initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Initializing the hrtimer** : Before hrtimer initialization, you need to set
    up a `ktime` , which represents time duration. We will see how to achieve that
    in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '**Starting hrtimer** : hrtimer can be started as shown in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`mode` represents the expiry mode. It should be `HRTIMER_MODE_ABS` for an absolute
    time value, or `HRTIMER_MODE_REL` for a time value relative to now.'
  prefs: []
  type: TYPE_NORMAL
- en: '**hrtimer cancellation** : You can either cancel the timer or see whether it
    is possible to cancel it or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Both return `0` when the timer is not active and `1` when the timer is active.
    The difference between these two functions is that `hrtimer_try_to_cancel` fails
    if the timer is active or its callback is running, returning `-1` , whereas `hrtimer_cancel`
    will wait until the callback finishes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can independently check whether the hrtimer''s callback is still running
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Remember, `hrtimer_try_to_cancel` internally calls `hrtimer_callback_running`
    .
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent the timer from automatically restarting, the hrtimer callback
    function must return `HRTIMER_NORESTART` .
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check whether HRTs are available on your system by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking in the kernel config file, which should contain something like `CONFIG_HIGH_RES_TIMERS=y:
    zcat /proc/configs.gz | grep CONFIG_HIGH_RES_TIMERS` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the `cat /proc/timer_list` or `cat /proc/timer_list | grep resolution`
    result. The `.resolution` entry must show 1 nsecs and the event_handler must show
    `hrtimer_interrupts` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the `clock_getres` system call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From within the kernel code, by using `#ifdef CONFIG_HIGH_RES_TIMERS` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With HRTs enabled on your system, the accuracy of sleep and timer system calls
    do not depend on jiffies anymore, but they are still as accurate as HRTs are.
    It is the reason why some systems do not support `nanosleep()` , for example.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic tick/tickless kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With previous HZ options, the kernel is interrupted HZ times per second in order
    to reschedule tasks, even in an idle state. If HZ is set to 1,000, there will
    be 1,000 kernel interruptions per second, preventing the CPU from being idle for
    a long time, thus affecting CPU power consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at a kernel with no fixed or predefined ticks, where the ticks
    are disabled until some task needs to be performed. We call such a kernel a **tickless
    kernel** . In fact, tick activation is scheduled, based on the next action. The
    right name should be **dynamic tick kernel** . The kernel is responsible for task
    scheduling, and maintains a list of runnable tasks (the run queue) in the system.
    When there is no task to schedule, the scheduler switches to the idle thread,
    which enables dynamic tick by disabling the periodic tick until the next timer
    expires (a new task is queued for processing).
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, the kernel also maintains a list of the tasks timeouts (it then
    knows when and how long it has to sleep). In an idle state, if the next tick is
    further away than the lowest timeout in the tasks list timeout, the kernel programs
    the timer with that timeout value. When the timer expires, the kernel re-enables
    the periodic ticks back and invokes the scheduler, which then schedules the task
    associated with the timeout. This is how the tickless kernel removes the periodic
    tick and saves power when idle.
  prefs: []
  type: TYPE_NORMAL
- en: Delays and sleep in the kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Without going deep into the details, there are two types of delays, depending
    on the context your code runs in: atomic or nonatomic. The mandatory header to
    handle delays in the kernel is `#include <linux/delay>.`'
  prefs: []
  type: TYPE_NORMAL
- en: Atomic context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tasks in the atomic context (such as ISR) can''t sleep, and can''t be scheduled;
    it is the reason why busy-wait loops are used for delaying purposes in an atomic
    context. The kernel exposes the `Xdelay` family of functions that will spend time
    in a busy loop, long (based on jiffies) enough to achieve the desired delay:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ndelay(unsigned long nsecs)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`udelay(unsigned long usecs)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdelay(unsigned long msecs)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should always use `udelay()` since `ndelay()` precision depends on how accurate
    your hardware timer is (not always the case on an embedded SOC). Use of `mdelay()`
    is also discouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Timer handlers (callbacks) are executed in an atomic context, meaning that sleeping
    is not allowed at all. By *sleeping* , I mean any function that may result in
    sending the caller to sleep, such as allocating memory, locking a mutex, an explicit
    call to `sleep()` function, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Nonatomic context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a nonatomic context, the kernel provides the `sleep[_range]` family of functions
    and which function to use depends on how long you need to delay by:'
  prefs: []
  type: TYPE_NORMAL
- en: '`udelay(unsigned long usecs)` : Busy-wait loop based. You should use this function
    if you need to sleep for a few µsecs ( < ~10 us ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`usleep_range(unsigned long min, unsigned long max)` : Relies on hrtimers,
    and it is recommended to let this sleep for few ~µsecs or small msecs (10 us -
    20 ms), avoiding the busy-wait loop of `udelay()` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`msleep(unsigned long msecs)` : Backed by jiffies/legacy_timers. You should
    use this for larger, msecs sleep (10 ms+).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sleep and delay topics are well explained in *Documentation/timers/timers-howto.txt*
    in the kernel source.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel locking mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Locking is a mechanism that helps shares resources between different threads
    or processes. A shared resource is a data or a device that can be accessed by
    at least two user, simultaneously or no. Locking mechanisms prevent abusive access,
    for example, a process writing data when another one is reading in the same place,
    or two processes accessing the same device (the same GPIO for example). The kernel
    provides several locking mechanisms. The most important are:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semaphore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinlock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will only learn about mutexes and spinlock, since they are widely used in
    device drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mutual exclusion** (**mutex** ) is the de facto most used locking mechanism.
    To understand how it works, let''s see what its structure looks like in `include/linux/mutex.h`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have seen in the section *wait queue* , there is also a `list` type field
    in the structure: `wait_list` . The principle of sleeping is the same.'
  prefs: []
  type: TYPE_NORMAL
- en: Contenders are removed from the scheduler run queue and put onto the wait list
    (`wait_list` ) in a sleep state. The kernel then schedules and executes other
    tasks. When the lock is released, a waiter in the wait queue is woken, moved off
    the `wait_list` , and scheduled back.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using mutex requires only a few basic functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Statically:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Dynamically:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Acquire and release
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lock:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlock:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes, you may only need to check whether a mutex is locked or not. For
    that purpose, you can use the `int mutex_is_locked(struct mutex *lock)` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'What this function does is just check whether the mutex''s owner is empty (`NULL`
    ) or not. There is also `mutex_trylock` , that acquires the mutex if it is not
    already locked, and returns `1` ; otherwise, it returns `0` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As with the wait queue's interruptible family function, `mutex_lock_interruptible()`
    , which is recommended, will result in the driver being able to be interrupted
    by any signal, whereas with `mutex_lock_killable()` , only signals killing the
    process can interrupt the driver.
  prefs: []
  type: TYPE_NORMAL
- en: You should be very careful with `mutex_lock()` , and use it when you can guarantee
    that the mutex will be released, whatever happens. In the user context, it is
    recommended you always use `mutex_lock_interruptible()` to acquire the mutex,
    since `mutex_lock()` will not return if a signal is received (even a c*trl + c*
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a mutex implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Please have a look at `include/linux/mutex.h` in the kernel source to see the
    strict rules you must respect with mutexes. Here are some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Only one task can hold the mutex at a time; this is actually not a rule, but
    a fact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple unlocks are not permitted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They must be initialized through the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A task holding the mutex may not exit, since the mutex will remain locked, and
    possible contenders will wait (will sleep) forever
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory areas where held locks reside must not be freed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Held mutexes must not be reinitialized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since they involve rescheduling, mutexes may not be used in atomic contexts,
    such as tasklets and timers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with `wait_queue` , there is no polling mechanism with mutexes. Every time
    that `mutex_unlock` is called on a mutex, the kernel checks for waiters in `wait_list`
    . If any, one (and only one) of them is awakened and scheduled; they are woken
    in the same order in which they were put to sleep.
  prefs: []
  type: TYPE_NORMAL
- en: Spinlock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like mutex, spinlock is a mutual exclusion mechanism; it only has two states:'
  prefs: []
  type: TYPE_NORMAL
- en: locked (aquired)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: unlocked (released)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any thread that needs to acquire the spinlock will active loop until the lock
    is acquired, which breaks out of the loop. This is the point where mutex and spinlock
    differ. Since spinlock heavily consumes the CPU while looping, it should be used
    for very quick acquires, especially when time to hold the spinlock is less than
    time to reschedule. Spinlock should be released as soon as the critical task is
    done.
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid wasting CPU time by scheduling a thread that may probably
    spin, trying to acquire a lock held by another thread moved off the run queue,
    the kernel disables preemption whenever a code holding a spinlock is running.
    With preemption disabled, we prevent the spinlock holder from being moved off
    the run queue, which could lead waiting processes to spin for a long time and
    consume CPU.
  prefs: []
  type: TYPE_NORMAL
- en: As long as one holds a spinlock, other tasks may be spinning while waiting on
    it. By using spinlock, you asserts and guarantee that it will not be held for
    a long time. You can say it is better to spin in a loop, wasting CPU time, than
    the cost of sleeping your thread, context-shifting to another thread or process,
    and being woken up afterward. Spinning on a processor means no other task can
    run on that processor; it then makes no sense to use spinlock on a single core
    machine. In the best case, you will slow down the system; in the worst case, you
    will deadlock, as with mutexes. For this reason, the kernel just disables preemption
    in response to the `spin_lock(spinlock_t *lock)` function on single processor.
    On a single processor (core) system, you should use `spin_lock_irqsave()` and
    `spin_unlock_irqrestore()` , which will respectively disable the interrupts on
    the CPU, preventing interrupt concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you do not know in advance what system you will write the driver for,
    it is recommended you acquire a spinlock using `spin_lock_irqsave(spinlock_t *lock,
    unsigned long flags)` , which disables interrupts on the current processor (the
    processor where it is called) before taking the spinlock. `spin_lock_irqsave`
    internally calls `local_irq_save(flags);` , an architecture-dependent function
    to save the IRQ status, and `preempt_disable()` to disable preemption on the relevant
    CPU. You should then release the lock with `spin_unlock_irqrestore()` , which
    does the reverse operations that we previously enumerated. This is a code that
    does lock acquire and release. It is an IRQ handler, but let''s just focus on
    the lock aspect. We will discuss more about IRQ handlers in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Spinlock versus mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Used for concurrency in the kernel, spinlocks and mutexes each have their own
    objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutexes protect the process's critical resource, whereas spinlock protects the
    IRQ handler's critical sections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutexes put contenders to sleep until the lock is acquired, whereas spinlocks
    infinitely spin in a loop (consuming CPU) until the lock is acquired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the previous point, you can't hold spinlock for a long time, since
    waiters will waste CPU time waiting for the lock, whereas a mutex can be held
    as long as the resource needs to be protected, since contenders are put to sleep
    in a wait queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When dealing with spinlocks, please keep in mind that preemption is disabled
    only for threads holding spinlocks, not for spinning waiters.
  prefs: []
  type: TYPE_NORMAL
- en: Work deferring mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deferring is a method by which you schedule a piece of work to be executed
    in the future. It''s a way to report an action later. Obviously, the kernel provides
    facilities to implement such a mechanism; it allows you to defer functions, whatever
    their type, to be called and executed later. There are three of them in the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SoftIRQs** : Executed in an atomic context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tasklets** : Executed in an atomic context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workqueues** : Executed in a process context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softirqs and ksoftirqd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Software IRQ** ( **softirq** ) , or software interrupt is a deferring mechanism
    used only for very fast processing, since it runs with a disabled scheduler (in
    an interrupt context). You''ll rarely (almost never) want to deal with softirq
    directly. There are only networks and block device subsystems using softirq. Tasklets
    are an instantiation of softirqs, and will be sufficient in almost every case
    that you feel the need to use softirqs.'
  prefs: []
  type: TYPE_NORMAL
- en: ksoftirqd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most cases, softirqs are scheduled in hardware interrupts, which may arrive
    very quickly, faster than they can be serviced. They are then queued by the kernel
    in order to be processed later. **Ksoftirqds** are responsible for late execution
    (process context this time). A ksoftirqd is a per-CPU kernel thread raised to
    handle unserviced software interrupts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding `top` sample from my personal computer, you can see `ksoftirqd/n`
    entries, where `n` is the CPU number that the ksoftirqd runs on. CPU-consuming
    ksoftirqd may indicate an overloaded system or a system under **interrupts storm**
    , which is never good. You can have a look at `kernel/softirq.c` to see how ksoftirqds
    are designed.
  prefs: []
  type: TYPE_NORMAL
- en: Tasklets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tasklets are a bottom-half (we will see what this means later) mechanism built
    on top of softirqs. They are represented in the kernel as instances of struct
    `tasklet_struct` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Tasklets are not re-entrant by nature. A code is called reentrant if it can
    be interrupted anywhere in the middle of its execution, and then be safely called
    again. Tasklets are designed such that a tasklet can run on one and only one CPU
    simultaneously (even on an SMP system), which is the CPU it was scheduled on,
    but different tasklets may be run simultaneously on different CPUs. The tasklet
    API is quite basic and intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Declaring a tasklet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dynamically:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Statically:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one difference between the two functions; the former creates a tasklet
    already enabled and ready to be scheduled without any other function call, done
    by setting the `count` field to `0` , whereas the latter creates a tasklet disabled
    (done by setting `count` to `1` ), on which one has to call `tasklet_enable()`
    before the tasklet can be schedulable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Globally, setting the `count` field to `0` means that the tasklet is disabled
    and cannot be executed, whereas a nonzero value means the opposite.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling and disabling a tasklet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is one function to enable a tasklet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`tasklet_enable` simply enables the tasklet. In older kernel versions, you
    may find void `tasklet_hi_enable(struct tasklet_struct *)` is used, but those
    two functions do exactly the same thing. To disable a tasklet, call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`tasklet_disable` will disable the tasklet and return only when the tasklet
    has terminated its execution (if it was running), whereas `tasklet_disable_nosync`
    returns immediately, even if the termination has not occurred.'
  prefs: []
  type: TYPE_NORMAL
- en: Tasklet scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two scheduling functions for tasklet, depending on whether your tasklet
    has normal or higher priority:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel maintains normal priority and high priority tasklets in two different
    lists. `tasklet_schedule` adds the tasklet into the normal priority list, scheduling
    the associated softirq with a `TASKLET_SOFTIRQ` flag. With `tasklet_hi_schedule`
    , the tasklet is added into the high priority list, scheduling the associated
    softirq with a `HI_SOFTIRQ` flag. High priority tasklets are meant to be used
    for soft interrupt handlers with low latency requirements. There are some properties
    associated with tasklets you should know:'
  prefs: []
  type: TYPE_NORMAL
- en: Calling `tasklet_schedule` on a tasklet already scheduled, but whose execution
    has not started, will do nothing, resulting in the tasklet being executed only
    once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tasklet_schedule` can be called in a tasklet, meaning that a tasklet can reschedule
    itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High priority tasklets are always executed before normal ones. Abusive use of
    high priority tasks will increase the system latency. Only use them for really
    quick stuff.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can stop a tasklet using the `tasklet_kill` function that will prevent
    the tasklet from running again or wait for its completion before killing it if
    the tasklet is currently scheduled to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us check. Look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Work queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Added since Linux kernel 2.6, the most used and simple deferring mechanism is
    the work queue. It is the last one we will talk about in this chapter. As a deferring
    mechanism, it takes an opposite approach to the others we've seen, running only
    in a preemptible context. It is the only choice when you need to sleep in your
    bottom half (I will explain what a bottom half is later in the next section).
    By sleep, I mean process I/O data, hold mutexes, delay, and all the other tasks
    that may lead to sleep or move the task off the run queue.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that work queues are built on top of kernel threads, and this is
    the reason why I decided not to talk about the kernel thread as a deferring mechanism
    at all. However, there are two ways to deal with work queues in the kernel. First,
    there is a default shared work queue, handled by a set of kernel threads, each
    running on a CPU. Once you have work to schedule, you queue that work into the
    global work queue, which will be executed at the appropriate moment. The other
    method is to run the work queue in a dedicated kernel thread. It means whenever
    your work queue handler needs to be executed, your kernel thread is woken up to
    handle it, instead of one of the default predefined threads.
  prefs: []
  type: TYPE_NORMAL
- en: Structures and functions to call are different, depending on whether you chose
    a shared work queue or dedicated ones.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel-global workqueue – the shared queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unless you have no choice, or you need critical performance, or you need to
    control everything from the work queue initialization to the work scheduling,
    and if you only submit tasks occasionally, you should use the shared work queue
    provided by the kernel. With that queue being shared over the system, you should
    be nice, and should not monopolize the queue for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: Since the execution of the pending task on the queue is serialized on each CPU,
    you should not sleep for a long time because no other task on the queue will run
    until you wake up. You won't even know who you share the work queue with, so don't
    be surprised if your task takes longer to get the CPU. Work in the shared work
    queues is executed in a per-CPU thread called events/n, created by the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the work must also be initialized with the `INIT_WORK` macro.
    Since we are going to use the shared work queue, there is no need to create a
    work queue structure. We only need the `work_struct` structure that will be passed
    as an argument. There are three functions to schedule work on the shared work
    queue:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The version that ties the work on the current CPU:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The same but delayed function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The function that actually schedules the work on a given CPU:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The same as shown previously, but with a delay:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'All of these functions schedule the work given as an argument on to the system''s
    shared work queue `system_wq` , defined in `kernel/workqueue.c` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'A work already submitted to the shared queue can be cancelled with the `cancel_delayed_work`
    function. You can flush the shared workqueue with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the queue is shared over the system, one can''t really know how long
    `flush_scheduled_work()` may last before it returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: In order to pass data to my work queue handler, you may have noticed that in
    both examples, I've embedded my `work_struct` structure inside my custom data
    structure, and used `container_of` to retrieve it. It is the common way to pass
    data to the work queue handler.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated work queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, the work queue is represented as an instance of `struct workqueue_struct`
    . The work to be queued into the work queue is represented as an instance of `struct
    work_struct` . There are four steps involved prior to scheduling your work in
    your own kernel thread:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare/initialize a `struct workqueue_struct` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create your work function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `struct work_struct` so that your work function will be embedded into
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embed your work function in the `work_struct` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Programming syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following functions are defined in `include/linux/workqueue.h` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare work and work queue:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the worker function (the handler):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize our work queue and embed our work into:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We could have also created our work queues through a macro called `create_workqueue`
    . The difference between `create_workqueue` and `create_singlethread_workqueue`
    is that the former will create a work queue that in turn will create a separate
    kernel thread on each and every processor available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheduling work:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Queue after the given delay to the given worker thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: These functions return `false` if the work was already on a queue and `true`
    if otherwise. `delay` represents the number of jiffies to wait before queueing.
    You may use the helper function `msecs_to_jiffies` in order to convert the standard
    ms delay into jiffies. For example, to queue a work after 5 ms, you can use `queue_delayed_work(myqueue,
    &thework, msecs_to_jiffies(5));` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait on all pending work on the given work queue:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '`flush_workqueue` sleeps until all queued work has finished their execution.
    New incoming (enqueued) work does not affect the sleep. One may typically use
    this in driver shutdown handlers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cleanup:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use `cancel_work_sync()` or `cancel_delayed_work_sync` for synchronous cancellation,
    which will cancel the work if it is not already running, or block until the work
    has completed. The work will be cancelled even if it requeues itself. You must
    also ensure that the work queue on which the work was last queued can''t be destroyed
    before the handler returns. These functions are to be used respectively for nondelayed
    or delayed work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Since Linux kernel v4.8, it is possible to use `cancel_work` or `cancel_delayed_work`
    , which are asynchronous forms of cancellation. One must check whether the function
    returns true or no, and makes sure the work does not requeue itself. You must
    then explicitly flush the work queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The other is a different version of the same method and will create only a
    single thread for all the processors. In case you need a delay before the work
    is enqueued, feel free to use the following work initialization macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the preceding macros would imply that you should use the following functions
    to queue or schedule the work in the work queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '`queue_work` ties the work to the current CPU. You can specify the CPU on which
    the handler should run using the `queue_work_on` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'For delayed work, you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of using dedicated work queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Predefined (shared) workqueue and standard workqueue functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The predefined work queue is defined in `kernel/workqueue.c` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: It is nothing more than a standard work for which the kernel provides a custom
    API that simply wraps around the standard one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparisons between kernel predefined work queue functions and standard work
    queue functions are mentioned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Predefined work queue function** | **Equivalent standard work queue function**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `schedule_work(w)` | `queue_work(keventd_wq,w)` |'
  prefs: []
  type: TYPE_TB
- en: '| `schedule_delayed_work(w,d)` | `queue_delayed_work(keventd_wq,w,d)` (on any
    CPU) |'
  prefs: []
  type: TYPE_TB
- en: '| `schedule_delayed_work_on(cpu,w,d)` | `queue_delayed_work(keventd_wq,w,d)`
    (on a given CPU) |'
  prefs: []
  type: TYPE_TB
- en: '| `flush_scheduled_work()` | `flush_workqueue(keventd_wq)` |'
  prefs: []
  type: TYPE_TB
- en: Kernel threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Work queues run on top of kernel threads. You already use kernel threads when
    you use work queues. It is the reason why I have decided not to talk about the
    kernel thread API.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel interruption mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interrupt is the way a device halts the kernel, telling it that something
    interesting or important has happened. These are called IRQs on Linux systems.
    The main advantage interrupts offer is to avoid devices polling. It is up to the
    device to tell if there is a change in its state; it is not up to us to poll it.
  prefs: []
  type: TYPE_NORMAL
- en: In order to get notified when an interrupt occurs, you need to register to that
    IRQ, providing a function called interrupt handler that will be called every time
    that interrupt is raised.
  prefs: []
  type: TYPE_NORMAL
- en: Registering an interrupt handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can register a callback to be run when the interruption (or interrupt line)
    you are interested in gets fired. You can achieve that with the function `request_irq()`
    , declared in `<linux/interrupt.h>` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '`request_irq()` may fail, and return `0` on success. Other elements of the
    preceding code are outlined in detail as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`flags` : These should be a bitmask of the masks defined in `<linux/interrupt.h>`
    . The most used are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_TIMER:` Informs the kernel that this handler is originated by a system
    timer interrupt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_SHARED:` Used for interrupt lines that can be shared by two or more devices.
    Each device sharing the same line must have this flag set. If omitted, only one
    handler can be registered for the specified IRQ line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_ONESHOT:` Used essentially in the threaded IRQ. It instructs the kernel
    not to re-enable the interrupt when the hardirq handler has finished. It will
    remain disabled until the threaded handler has been run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In older kernel versions (until v2.6.35), there were `IRQF_DISABLED` flags,
    which asked the kernel to disable all interrupts when the handler is running.
    This flag is no longer used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` : This is used by the kernel to identify your driver in `/proc/interrupts`
    and `/proc/irq` *.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev` : Its primary goal is to pass as argument to the handler. This should
    be unique to each registered handler, since it is used to identify the device.
    It can be `NULL` for nonshared IRQs, but not for shared ones. The common way of
    using it is to provide a `device` structure, since it is both unique and may potentially
    be useful to the handler. That said, a pointer to any per-device data structure
    is sufficient:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '`handler` : This is the callback function that will run when the interrupt
    is fired. An interrupt handler''s structure looks like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'This contains the following code elements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`irq` : The numeric value of the IRQ (the same used in `request_irq` ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev` : The same as used in `request_irq` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both parameters are given to your handler by the kernel. There are only two
    values the handler can return, depending on whether your device originated the
    IRQ or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQ_NONE` : Your device is not the originator of that interrupt (it especially
    happens on shared IRQ lines)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQ_HANDLED` : Your device caused the interrupt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the processing, one may use the `IRQ_RETVAL(val)` macro, which
    will return `IRQ_HANDLED` if the value is nonzero, or `IRQ_NONE` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: When writing the interrupt handler, you don't have to worry about reentrancy,
    since the IRQ line serviced is disabled on all processors by the kernel in order
    to avoid recursive interrupt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The associated function to free the previously registered handler is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: If the specified IRQ is not shared, `free_irq` will not only remove the handler,
    but will also disable the line. If it is shared, only the handler identified through
    `dev` (which should be the same as that used in `request_irq` ) is removed, but
    the interrupt line still remains, and will be disabled only when the last handler
    is removed. `free_irq` will block until any executing interrupts for the specified
    IRQ have completed. You must then avoid both `request_irq` and `free_irq` in the
    interrupt context.
  prefs: []
  type: TYPE_NORMAL
- en: Interrupt handler and lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It goes without saying that you are in an atomic context and must only use
    spinlock for concurrency. Whenever there is global data accessible by both user
    code (the user task; that is, the system call) and interrupt code, this shared
    data should be protected by `spin_lock_irqsave()` in the user code. Let''s see
    why we can''t just use `spin_lock.` An interrupt handler will always have priority
    on the user task, even if that task is holding a spinlock. Simply disabling IRQ
    is not sufficient. An interrupt may happen on another CPU. It would be a disaster
    if a user task updating the data gets interrupted by an interrupt handler trying
    to access the same data. Using `spin_lock_irqsave()` will disable all interrupts
    on the local CPU, preventing the system call from being interrupted by any kind
    of interrupt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: When sharing data between different interrupt handlers (that is, the same driver
    managing two or more devices, each having its own IRQ line), one should also protect
    that data with `spin_lock_irqsave()` in those handlers, in order to prevent the
    other IRQs from being triggered and uselessly spinning.
  prefs: []
  type: TYPE_NORMAL
- en: Concept of bottom halves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bottom halves are mechanisms by which you split interrupt handlers into two
    part. This introduces another term, which is top half. Before discussing each
    of them, let us talk about their origin, and what problem they solve.
  prefs: []
  type: TYPE_NORMAL
- en: The problem – interrupt handler design limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether an interrupt handler holds a spinlock or not, preemption is disabled
    on the CPU running that handler. The more one wastes time in the handler, the
    less CPU is granted to the other task, which may considerably increase latency
    of other interrupts and so increase the latency of the whole system. The challenge
    is to acknowledge the device that raised the interrupt as quickly as possible
    in order to keep the system responsive.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux systems (actually on all OS, by hardware design), any interrupt handler
    runs with its current interrupt line disabled on all processors, and sometimes
    you may need to disable all interrupts on the CPU actually running the handler,
    but you definitely don't want to miss an interrupt. To meet this need, the concept
    of *halves* has been introduced.
  prefs: []
  type: TYPE_NORMAL
- en: The solution – bottom halves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This idea consists of splitting the handler into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first part, called the top half or hard-IRQ, which is the registered function
    using `request_irq()` that will eventually mask/hide interrupts (on the current
    CPU, except the one being serviced since it is already disabled by the kernel
    before running the handler) depending on the needs, performs quick and fast operations
    (essentially time-sensitive tasks, read/write hardware registers, and fast processing
    of this data), schedules the second and next part, and then acknowledges the line.
    All interrupts that are disabled must have been re-enabled just before exiting
    the bottom half.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second part, called the bottom half, will process time-consuming stuff,
    and run with interrupt re-enabled. This way, you have the chance not to miss an
    interrupt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bottom halves are designed using a work-deferring mechanism, which we have
    seen previously. Depending on which one you choose, it may run in a (software)
    interrupt context, or in a process context. Bottom halves'' mechanisms are:'
  prefs: []
  type: TYPE_NORMAL
- en: Softirqs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasklets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workqueues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threaded IRQs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softirqs and tasklets execute in a (software) interrupt context (meaning that
    preemption is disabled), Workqueues and threaded IRQs are executed in a process
    (or simply task) context, and can be preempted, but nothing prevents us from changing
    their real-time properties to fit your needs and change their preemption behavior
    (see `CONFIG_PREEMPT` or `CONFIG_PREEMPT_VOLUNTARY.` This also impacts the whole
    system). Bottom halves are not always possible. But when it is possible, it is
    certainly the best thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: Tasklets as bottom halves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The tasklet deferring mechanism is most used in DMA, network, and block device
    drivers. Just try the following command in the kernel source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see how to implement such a mechanism in our interrupt handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding sample, our tasklet will execute the function `my_tasklet_work()`
    .
  prefs: []
  type: TYPE_NORMAL
- en: Workqueue as bottom halves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s just start with a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding sample, we used either a wait queue or a work queue in order
    to wake up a possibly sleeping process waiting for us, or schedule a work depending
    on the value of a register. We have no shared data or resource, so there is no
    need to disable all other IRQs (`spin_lock_irq_disable` ).
  prefs: []
  type: TYPE_NORMAL
- en: Softirqs as bottom half
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As said in the beginning of this chapter, we will not discuss softirq. Tasklets
    will be enough everywhere you feel the need to use softirqs. Anyway, let's talk
    about their defaults.
  prefs: []
  type: TYPE_NORMAL
- en: Softirqs run in a software interrupt context, with preemption disabled, holding
    the CPU until they complete. Softirq should be fast; otherwise they may slow the
    system down. When, for any reason, a softirq prevents the kernel from scheduling
    other tasks, any new incoming softirq will be handled by **ksoftirqd** threads,
    running in a process context.
  prefs: []
  type: TYPE_NORMAL
- en: Threaded IRQs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main goal of threaded IRQs is reducing the time spent with interrupts disabled
    to a bare minimum. With threaded IRQs, the way you register an interrupt handler
    is a bit simplified. You does not even have to schedule the bottom half yourself.
    The core does that for us. The bottom half is then executed in a dedicated kernel
    thread. We do not use `request_irq()` anymore, but `request_threaded_irq()` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The `request_threaded_irq()` function accepts two functions in its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**@handler function** : This is the same function as the one registered with
    `request_irq()` . It represents the top-half function, which runs in an atomic
    context (or hard-IRQ). If it can process the interrupt faster so that you can
    get rid of the bottom half at all, it should return `IRQ_HANDLED` . But, if the
    interrupt processing needs more than 100 µs, as discussed previously, you should
    use the bottom half. In this case, it should return `IRQ_WAKE_THREAD` , which
    will result in scheduling the `thread_fn` function that must have been provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**@thread_fn function** : This represents the bottom half, as you would have
    scheduled in your top half. When the hard-IRQ handler (handler function) function
    returns `IRQ_WAKE_THREAD` , the kthread associated with this bottom half will
    be scheduled, invoking the `thread_fn` function when it comes to run the ktread.
    The `thread_fn` function must return `IRQ_HANDLED` when complete. After being
    executed, the kthread will not be rescheduled again until the IRQ is triggered
    again and the hard-IRQ returns `IRQ_WAKE_THREAD` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Everywhere that you would have used the work queue to schedule the bottom half,
    threaded IRQs can be used. `handler` and `thread_fn` must be defined in order
    to have a proper threaded IRQ. A default hard-IRQ handler will be installed by
    the kernel if `handler` is `NULL` and `thread_fn != NULL` (see the following),
    which will simply return `IRQ_WAKE_THREAD` to schedule the bottom half. `handler`
    is always called in an interrupt context, whether it has been provided by yourself
    or by the kernel by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: With threaded IRQs, the handler definition does not change, but the way it is
    registered changes a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Threaded bottom half
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simple following excerpt is a demonstration of how you can implement the
    threaded bottom half mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: When an interrupt handler is executed, the serviced IRQ is always disabled on
    all CPUs, and re-enabled when the hard-IRQ (top-half) finishes. But if for any
    reason you need the IRQ line not to be re-enabled after the top half, and to remain
    disabled until the threaded handler has been run, you should request the threaded
    IRQ with the flag `IRQF_ONESHOT` enabled (by just doing an OR operation as shown
    previously). The IRQ line will then be re-enabled after the bottom half has finished.
  prefs: []
  type: TYPE_NORMAL
- en: Invoking user-space applications from the kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'User-space applications are most of the time called from within the user space
    by other applications. Without going deep into the details, let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the API used (`call_usermodehelper` ) is a part of
    the Usermode-helper API, with all functions defined in `kernel/kmod.c` . Its use
    is quite simple; just a look inside `kmod.c` will give you an idea. You may be
    wondering what this API was defined for. It is used by the kernel, for example,
    for module (un)loading and cgroups management.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed about the fundamental elements to start driver
    development, presenting every mechanism frequently used in drivers. This chapter
    is very important, since it discusses topics other chapters in this book rely
    on. The next chapter for example, dealing with character devices, will use some
    of elements discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL

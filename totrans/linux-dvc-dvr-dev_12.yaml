- en: DMA – Direct Memory Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DMA is a feature of computer systems that allows devices to access the main
    system memory RAM without CPU intervention, which then allows them to devote themselves
    to other tasks. One typically uses it for accelerating network traffic, but it
    supports any kind of copy.
  prefs: []
  type: TYPE_NORMAL
- en: The DMA controller is the peripheral responsible for DMA management. One mostly
    finds it in modern processors and microcontrollers. DMA is a feature used to perform
    memory read and write operations without stealing CPU cycles. When one needs to
    transfer a block of data, the processor feeds the DMA controller with the source
    and destination addresses and the total number of bytes. The DMA controller then
    transfers the data from the source to the destination automatically, without stealing
    CPU cycles. When the number of bytes remaining reaches zero, the block transfer
    ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Coherent and non-coherent DMA mappings, as well as coherency issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMA engine API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMA and DT binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up DMA mappings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For any type of DMA transfer, one needs to provide source and destination addresses,
    as well as the number of words to transfer. In the case of a peripheral DMA, the
    peripheral's FIFO serves as either the source or the destination. When the peripheral
    serves as the source, a memory location (internal or external) serves as the destination
    address. When the peripheral serves as the destination, a memory location (internal
    or external) serves as the source address.
  prefs: []
  type: TYPE_NORMAL
- en: With a peripheral DMA, we specify either the source or the destination, depending
    on the direction of the transfer. In others words, a DMA transfer requires suitable
    memory mappings. This is what we will discuss in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Cache coherency and DMA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 11](http://kernel) , *Kernel Memory Management* , copies
    of recently accessed memory areas are stored in the cache. This applies to DMA
    memory too. The reality is that memory shared between two independent devices
    is generally the source of cache coherency problems. Cache incoherence is an issue
    coming from the fact that other devices may not be aware of an update from a writing
    device. On the other hand, cache coherency ensures that every write operation
    appears to occur instantaneously, so that all devices sharing the same memory
    region see exactly the same sequence of changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A well-explained situation of the coherency issue is illustrated in the following
    excerpt from LDD3:'
  prefs: []
  type: TYPE_NORMAL
- en: Let us imagine a CPU equipped with a cache and an external memory that can be
    accessed directly by devices using DMA. When the CPU accesses location X in the
    memory, the current value will be stored in the cache. Subsequent operations on
    X will update the cached copy of X, but not the external memory version of X,
    assuming a write-back cache. If the cache is not flushed to the memory before
    the next time a device tries to access X, the device will receive a stale value
    of X. Similarly, if the cached copy of X is not invalidated when a device writes
    a new value to the memory, then the CPU will operate on a stale value of X.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are actually two ways to address this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: A hardware-based solution. Such systems are **coherent systems** .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A software-based solution, where the OS is responsible for ensuring cache coherency.
    One calls such systems **non-coherent systems** .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMA mappings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any suitable DMA transfer requires suitable memory mapping. A DMA mapping consists
    of allocating a DMA buffer and generating a bus address for it. Devices actually
    use bus addresses. Bus addresses are each instance of the `dma_addr_t` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'One distinguishes two types of mapping: **coherent DMA mappings** and **streaming
    DMA mappings** . One can use the former over several transfers, which automatically
    addresses cache coherency issues. Therefore, it is too expensive. The streaming
    mapping has a lot of constraints and does not automatically address coherency
    issues, although, there is a solution for that, which consists of several function
    calls between each transfer. Coherent mapping usually exists for the life of the
    driver, whereas one streaming mapping is usually unmapped once the DMA transfer
    completes.'
  prefs: []
  type: TYPE_NORMAL
- en: One should use streaming mapping when one can and coherent mapping when one
    must.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to the code; the main header should include the following to handle DMA
    mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Coherent mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following function sets up a coherent mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function handles both the allocation and the mapping of the buffer, and
    returns a kernel virtual address for that buffer, which is `size` bytes wide and
    accessible by the CPU. `dev` is your device structure. The third argument is an
    output parameter that points to the associated bus address. Memory allocated for
    the mapping is guaranteed to be physically contiguous, and `flag` determines how
    memory should be allocated, which is usually `GFP_KERNEL` , or `GFP_ATOMIC` (if
    we are in an atomic context).
  prefs: []
  type: TYPE_NORMAL
- en: 'Do note that this mapping is said to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistent (coherent)** , since it allocates uncached unbuffered memory for
    a device for performing DMA'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronous** , because a write by either the device or the CPU can be immediately
    read by either without worrying about cache coherency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to free a mapping, one can use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here `cpu_addr` corresponds to the kernel virtual address returned by `dma_alloc_coherent()`
    . This mapping is expensive, and the minimum it can allocate is a page. In fact,
    it only allocates the number of pages that is the power of 2\. The order of pages
    is obtained with `int order = get_order(size)` . One should use this mapping for
    buffers that last the life of the device.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming DMA mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Streaming mapping has more constraints, and is different from coherent mapping
    for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Mappings need to work with a buffer that has already been allocated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mappings may accept several non-contiguous and scattered buffers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mapped buffer belongs to the device and not to the CPU anymore. Before the
    CPU can use the buffer, it should be unmapped first (after `dma_unmap_single()`
    or `dma_unmap_sg()` ). This is for caching purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For write transactions (CPU to device), the driver should place data in the
    buffer before the mapping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The direction the data should move into has to be specified, and the data should
    only be used based on this direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One may wonder why one should not access the buffer until it is unmapped. The
    reason is simple: CPU mapping is cacheable. The `dma_map_*()` family functions,
    which are used for streaming mapping, will first clean/invalidate the caches related
    to the buffer and rely on the CPU not to access it until the corresponding `dma_unmap_*()`
    . That will then invalidate (if necessary) the caches again, in case of any speculative
    fetches in the meantime, before the CPU may read any data written to memory by
    the device. Now the CPU can access the buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are actually two forms of streaming mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: Single buffer mapping, which allow only one-page mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scatter/gather mapping, which allows passing several buffers (scattered over
    memory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For either mapping, direction should be specified, by a symbol of type `enum
    dma_data_direction` , defined in `include/linux/dma-direction.h` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Single buffer mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is for occasional mapping. One can set up a single buffer with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The direction should be `DMA_TO_DEVICE` , `DMA_FROM_DEVICE` , or `DMA_BIDIRECTIONAL,`
    as described in the preceding code. `ptr` is the kernel virtual address of the
    buffer, and `dma_addr_t` is the returned bus address for the device. Make sure
    to use the direction that really fits your need, not just always `DMA_BIDIRECTIONAL`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'One should free the mapping with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Scatter/gather mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scatter/gather mappings are a special type of streaming DMA mapping where one
    can transfer several buffer regions in a single shot, instead of mapping each
    buffer individually and transferring them one by one. Suppose you have several
    buffers that might not be physically contiguous, all of which need to be transferred
    at the same time to or from the device. This situation may occur due to:'
  prefs: []
  type: TYPE_NORMAL
- en: A readv or writev system call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A disk I/O request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or simply a list of pages in a mapped kernel I/O buffer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The kernel represents the scatterlist as a coherent structure, `struct scatterlist`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to set up a scatterlist mapping, one should:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocate your scattered buffers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an array of the scatter list and fill it with allocated memory using
    `sg_set_buf().` Note that scatterlist entries must be of page size (except ends).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call `dma_map_sg()` on the scatterlist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once done with DMA, call `dma_unmap_sg()` to unmap the scatterlist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While one can send contents of several buffers over DMA one at a time by individually
    mapping each of them, scatter/gather can send them all at once by sending the
    pointer to the scatterlist to the device, along with a length, which is the number
    of entries in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The same rules described in the single-buffer mapping section apply to scatter/gather.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: DMA scatter/gather
  prefs: []
  type: TYPE_NORMAL
- en: '`dma_map_sg()` and `dma_unmap_sg()` take care of cache coherency. But if one
    needs to use the same mapping to access (read/write) the data between the DMA
    transfer, the buffers must be synced between each transfer in an appropriate manner,
    by either `dma_sync_sg_for_cpu()` if the CPU needs to access the buffers, or `dma_sync_sg_for_device()`
    if it is the device. Similar functions for single region mapping are `dma_sync_single_for_cpu()`
    and `dma_sync_single_for_device()` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There is no need to call the preceding functions again after the buffer(s) has
    been unmapped. You can just read the content.
  prefs: []
  type: TYPE_NORMAL
- en: Concept of completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will briefly describe completion and the necessary part of its
    API that the DMA transfer uses. For a complete description, please feel free to
    have a look at the kernel documentation at *Documentation/scheduler/completion.txt*
    . A common pattern in kernel programming involves initiating some activity outside
    of the current thread, then waiting for that activity to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Completion is a good alternative to `sleep()` when waiting for a buffer to be
    used. It is suitable for sensing data, which is exactly what the DMA callback
    does.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with completion requires this header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Like other kernel facility data structures, one can create instances of the
    `struct completion` structure either statically or dynamically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static declaration and initialization looks like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Dynamic allocation looks like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When the driver begins some work whose completion must be waited for (a DMA
    transaction in our case), it just has to pass the completion event to the `wait_for_completion()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When some other part of the code has decided that the completion has happened
    (transaction completes), it can wake up anybody (actually the code that needs
    to access DMA buffer) who is waiting with one of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As one can guess, `complete()` will wake up only one waiting process, while
    `complete_all()` will wake up every one waiting for that event. Completions are
    implemented in such a way that they will work properly even if `complete()` is
    called before `wait_for_completion()` .
  prefs: []
  type: TYPE_NORMAL
- en: Along with code samples used in the next sections, one will have a better understanding
    of how this works.
  prefs: []
  type: TYPE_NORMAL
- en: DMA engine API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DMA engine is a generic kernel framework for developing a DMA controller
    driver. The main goal of DMA is offloading the CPU when it comes to copy memory.
    One delegates a transaction (I/O data transfers) to the DMA engine by use of channels.
    A DMA engine, through its driver/API, exposes a set of channels, which can be
    used by other devices (slaves).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00033.gif)'
  prefs: []
  type: TYPE_IMG
- en: DMA Engine layout
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will simply walk through that (slave) API, which is applicable for
    slave DMA usage only. The mandatory header here is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The slave DMA usage is straightforward, and consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocate a DMA slave channel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set slave and controller specific parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get a descriptor for the transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Submit the transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Issue pending requests and wait for callback notification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One can see a DMA channel as a highway for I/O data transfer
  prefs: []
  type: TYPE_NORMAL
- en: Allocate a DMA slave channel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One requests a channel using `dma_request_channel()` . Its prototype is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`mask` is a bitmap mask that represents the capabilities the channel must satisfy.
    One uses it essentially to specify the transfer types the driver needs to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`The dma_cap_zero()` and `dma_cap_set()` functions are used to clear the mask
    and set the capability we need. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding excerpt, `dma_filter_fn` is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If `filter_fn` parameter (which is optional) is `NULL` , `dma_request_channel()`
    will simply return the first channel that satisfies the capability mask. Otherwise,
    when the mask parameter is insufficient for specifying the necessary channel,
    one can use the `filter_fn` routine as a filter for the available channels in
    the system. The kernel calls the `filter_fn` routine once for each free channel
    in the system. Upon seeing a suitable channel, `filter_fn` should return `DMA_ACK,`
    which will tag the given channel to be the return value from `dma_request_channel()`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'A channel allocated through this interface is exclusive to the caller, until
    `dma_release_channel()` is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Set slave and controller specific parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step introduces a new data structure, `struct dma_slave_config` , which
    represents the runtime configuration for the DMA slave channel. This allows clients
    to specify settings, such as the DMA direction, DMA addresses, bus width, DMA
    burst lengths, and so on, for the peripheral.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `struct dma_slave_config` structure looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the meaning of each element in the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`direction` : This indicates whether the data should go in or out on this slave
    channel, right now. The possible values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`src_addr` : This is the physical address (actually the bus address) of the
    buffer where the DMA slave data should be read (RX). This element is ignored if
    the source is memory. `dst_addr` is the physical address (actually the bus address)
    of the buffer where the DMA slave data should be written (TX), which is ignored
    if the source is memory. `src_addr_width` is the width in bytes of the source
    (RX) register where the DMA data should be read. If the source is memory, this
    may be ignored depending on the architecture. The legal values are 1, 2, 4, or
    8\. Therefore, `dst_addr_width` is the same as `src_addr_width` but for the destination
    target (TX).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any bus width must be one of the following enumerations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`src_maxburs` : This is the maximum number of words (here, consider words as
    units of the `src_addr_width` member, not in bytes) that can be sent in one burst
    to the device. Typically, something like half the FIFO depth on I/O peripherals
    so you do not overflow it. This may or may not be applicable on memory sources.
    `dst_maxburst` is the same as `src_maxburst` but for the destination target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding excerpt, one calls `dma_request_channel()` function in order
    to take the owner chip of the DMA channel, on which one calls `dmaengine_slave_config()`
    to apply its configuration. `dma_map_single()` is called in order to map rx and
    tx buffers, so that these can be used for purpose of DMA.
  prefs: []
  type: TYPE_NORMAL
- en: Get a descriptor for transaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you remember the first step of this section, when one requests a DMA channel,
    the return value is an instance of the `struct dma_chan` structure. If one looks
    at its definition in `include/linux/dmaengine.h` , one will notice that it contains
    a `struct dma_device *device` field, which represents the DMA device (the controller
    actually) that supplied the channel. The kernel driver of this controller is responsible
    (it is a rule imposed by the kernel API for DMA controller drivers) for exposing
    a set of functions to prepare DMA transactions, where each of them correspond
    to a DMA transaction type (enumerated in step 1). Depending on the transaction
    type, one has no choice but to choose the dedicated function. Some of these functions
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`device_prep_dma_memcpy()` : Prepares a memcpy operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_sg()` : Prepare a scatter/gather memcpy operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_xor()` : For a xor operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_xor_val()` : Prepares a xor validation operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_pq()` : Prepares a pq operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_pq_val()` : Prepares a pqzero_sum operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_memset()` : Prepares a memset operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_dma_memset_sg()` : For a memset operation over a scatterlist'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_slave_sg()` : Prepares a slave DMA operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_prep_interleaved_dma()` : Transfers an expression in a generic way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us have a look at `drivers/dma/imx-sdma.c` , which is the i.MX6 DMA controller
    (SDMA) driver. Each of these functions returns a pointer to a `struct dma_async_tx_descriptor`
    structure, which corresponds to the transaction descriptor. With memory-to-memory
    copy, one will use `device_prep_dma_memcpy` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, we should have used `dmaengine_prep_*` DMA engine API. Just note that
    these functions internally do what we just performed earlier. For example, for
    memory-to-memory, one could have used the `device_prep_dma_memcpy ()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Our sample becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Please have a look at `include/linux/dmaengine.h` , in the definition of a `struct
    dma_device` structure, to see how all of these hooks are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Submit the transaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To put the transaction in the driver pending queue, one uses `dmaengine_submit()`
    . Once the descriptor has been prepared and the callback information added, one
    should place it on the DMA engine drivers pending the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This function returns a cookie that one can use to check the progress of DMA
    activity through other DMA engines. `dmaengine_submit()` will not start the DMA
    operation, it merely adds it to the pending queue. How to start the transaction
    is discussed in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Issue pending DMA requests and wait for callback notification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting the transaction is the last step of the DMA transfer setup. One activates
    transactions in the pending queue of a channel by calling `dma_async_issue_pending()`
    on that channel. If the channel is idle then the first transaction in the queue
    is started and subsequent ones are queued up. On completion of a DMA operation,
    the next one in the queue is started and a tasklet triggered. This tasklet is
    in charge of calling the client driver completion callback routine for notification,
    if set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'An example would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `wait_for_completion()` function will block until our DMA callback gets
    called, which will update (complete) our completion variable in order to resume
    the previous blocked code. It is a suitable alternative to `while (!done) msleep(SOME_TIME);`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The DMA engine API function that actually issues pending transactions is `dmaengine_issue_pending(struct
    dma_chan *chan)` , which is a wrap around `dma_async_issue_pending()` .
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together – NXP SDMA (i.MX6)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SDMA engine is a programmable controller in the i.MX6 and each peripheral
    has its own copy function in this controller. One uses this `enum` to determine
    their addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Despite the generic DMA engine API, any constructor may provide its own custom
    data structure. This is the case for the `imx_dma_data` structure, which is a
    private data (used to describe the DMA device type one needs to use) that is to
    be passed to the `.private` field of the `struct dma_chan` in the filter callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'These structures and enum are all specific to i.MX and are defined in `include/linux/platform_data/dma-imx.h`
    . Now, let us write our kernel DMA module. It allocates two buffers (source and
    destination). Fill the source with predefined data, and perform a transaction
    in order to copy src into dst. One can improve this module by using data coming
    from user space (`copy_from_user()` ). This driver is inspired from the one provided
    in the imx-test package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the filter function. When one requests a DMA channel, the controller
    driver may perform a lookup in a list of channels (which it has). For fine-grained
    lookup, one can provide a callback method that will be called on each channel
    found. It is then up to the callback to choose the suitable channel to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`imx_dma_is_general_purpose` is a special function that checks the controller
    driver''s name. The `open` function will allocate the buffer and request the DMA
    channel, given our filter function as callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `release` function simply does the reverse of the `open` function; it frees
    the buffer and releases the DMA channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In the `read` function, we just compare the source and destination buffer and
    inform the user about the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We use completion in order to get notified (woken up) when the transaction
    has terminated. This callback is called after our transaction has finished and
    sets our completion variable to the complete state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `write` function, we fill our source buffer with the data, perform DMA
    mapping in order to get physical addresses that correspond to our source and destination
    buffer, and call `device_prep_dma_memcpy` to get a transaction descriptor. That
    transaction descriptor is then submitted to the DMA engine with `dmaengine_submit`
    , which does not perform our transaction yet. It is only after we have called
    `dma_async_issue_pending` on our DMA channel, that our pending transaction will
    be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is available in the repository of the book: `chapter-12/imx-sdma/imx-sdma-single.c`
    . There is also a module with which to perform the same task, but using scatter/gather
    mapping: `chapter-12/imx-sdma/imx-sdma-scatter-gather.c` .'
  prefs: []
  type: TYPE_NORMAL
- en: DMA DT binding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DT binding for the DMA channel depends on the DMA controller node, which is
    SoC dependent, and some parameters (such as DMA cells) may vary from one SoC to
    another. This example only focuses on the i.MX SDMA controller, which one can
    find in the kernel source, at *Documentation/devicetree/bindings/dma/fsl-imx-sdma.txt*
    .
  prefs: []
  type: TYPE_NORMAL
- en: Consumer binding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to the SDMA event-mapping table, the following code shows the DMA
    request signals for peripherals in i.MX 6Dual/ 6Quad:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The second cells (`25` and `26` ) in the DMA property correspond to the DMA
    request/event ID. Those values come from the SoC manuals (i.MX53 in our case).
    Please have a look at [https://community.nxp.com/servlet/JiveServlet/download/614186-1-373516/iMX6_Firmware_Guide.pdf](https://community.nxp.com/servlet/JiveServlet/download/614186-1-373516/iMX6_Firmware_Guide.pdf)
    and the Linux reference manual at [https://community.nxp.com/servlet/JiveServlet/download/614186-1-373515/i.MX_Linux_Reference_Manual.pdf](https://community.nxp.com/servlet/JiveServlet/download/614186-1-373515/i.MX_Linux_Reference_Manual.pdf)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The third cell indicates the priority to use. The driver code to request a
    specified parameter is defined next. One can find the complete code in `drivers/tty/serial/imx.c`
    in the kernel source tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The magic call here is the `dma_request_slave_channel()` function, which will
    parse the device node (in the DT) using `of_dma_request_slave_channel()` to gather
    channel settings, according to the DMA name (refer to the named resource in [Chapter
    6](text00162.html) , *The Concept of Device Tree* ).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DMA is a feature that one finds in many modern CPUs. This chapter gives you
    the necessary steps to get the most out of this device, using the kernel DMA mapping
    and DMA engine APIs. After this chapter, I have no doubt you will be able to set
    up at least a memory-to-memory DMA transfer. One can find further information
    at *Documentation/dmaengine/* , in the kernel source tree. Therefore, the next
    chapter deals with an entirely different subject—the Linux device model.
  prefs: []
  type: TYPE_NORMAL

- en: Distributed Application Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed advanced tips, tricks, and concepts that
    are useful when containerizing complex distributed applications, or when using
    Docker to automate sophisticated tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll introduce the concept of a distributed application architecture
    and discuss the various patterns and best practices that are required to run a
    distributed application successfully. Finally, we will discuss the additional
    requirements that need to be fulfilled to run such an application in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the distributed application architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After finishing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Name at least four characteristics of a distributed application architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List three to four patterns that need to be implemented for a production-ready
    distributed application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the distributed application architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to explain what we mean when we talk about a distributed
    application architecture. First, we need to make sure that all words or acronyms
    we use have a meaning and that we are all talking the same language.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this and subsequent chapters, we will talk about a lot about concepts that
    might not be familiar to everyone. To make sure we''re all talking the same language,
    let''s briefly introduce and describe the most important of these concepts or
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Terminology** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| VM | The acronym for virtual machine. This is a virtual computer. |'
  prefs: []
  type: TYPE_TB
- en: '| Node | An individual server used to run applications. This can be a physical server,
    often called bare-metal, or a VM. A can be a mainframe, supercomputer, standard business server,
    or even a Raspberry Pi. Nodes can be computers in a company''s own data center
    or in the cloud. Normally, a node is part of a cluster. |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster | A group of nodes connected by a network that are used to run distributed
    applications. |'
  prefs: []
  type: TYPE_TB
- en: '| Network | Physical and software-defined communication paths between individual
    nodes of a cluster and programs running on those nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| Port | A channel on which an application such as a web server listens for
    incoming requests. |'
  prefs: []
  type: TYPE_TB
- en: '| Service | This, unfortunately, is a very overloaded term and its real meaning
    depends on the context that it is used in. If we use the term *service *in the
    context of an application such as an application service, then it usually means
    that this is a piece of software that implements a limited set of functionality
    that is then used by other parts of the application. As we progress through this
    book, other types of services that have a slightly different definition will be
    discussed. |'
  prefs: []
  type: TYPE_TB
- en: 'Naively said, a distributed application architecture is the opposite of a monolithic
    application architecture, but it''s not unreasonable to look at this monolithic
    architecture first. Traditionally, most business applications have been written
    in such a way that the result can be seen as one single, tightly coupled program
    that runs on a named server somewhere in a data center. All its code is compiled
    into a single binary or a few very tightly coupled binaries that need to be co-located
    when running the application. The fact that the server, or more general host,
    that the application is running on has a well-defined name or static IP address
    is also important in this context. Let''s look at the following diagram to illustrate
    this type of application architecture a bit more clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/56242555-ab5b-4055-8cb5-e3e4b2e3f83f.png)Monolithic application
    architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see a **Server** named `blue-box-12a` with
    an **IP** address of `172.52.13.44` running an application called `pet-shop`,
    which is a monolith consisting of a main module and a few tightly coupled libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/425c8d7b-efb8-48e3-8791-cb2d99ee8862.png)Distributed application
    architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Here, all of a sudden, we don't have only a single named server anymore; instead,
    we have a lot of them, and they don't have human-friendly names, but rather some
    unique IDs that can be something like a **Universal Unique Identifier** (**UUID**).
    The pet shop application, all of a sudden, also does not consist of a single monolithic
    block anymore, but rather a plethora of interacting, yet loosely coupled, services
    such as **pet-api**, **pet-web**, and **pet-inventory**. Furthermore, each service
    runs in multiple instances in this cluster of servers or hosts.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we are discussing this in a book about Docker containers,
    and you are right to ask. While all the topics we're going to investigate apply
    equally to a world where containers do not (yet) exist, it is important to realize
    that containers and container orchestration engines help address all these problems
    in a much more efficient and straightforward way. Most of the problems that used
    to be very hard to solve in a distributed application architecture become quite
    simple in a containerized world.
  prefs: []
  type: TYPE_NORMAL
- en: Patterns and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distributed application architecture has many compelling benefits, but it
    also has one very significant drawback compared to a monolithic application architecture
    – the former is way more complex. To tame this complexity, the industry has come
    up with some important best practices and patterns. In the following sections,
    we are going to look into some of the most important ones in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Loosely coupled components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to address a complex subject has always been to divide it into
    smaller subproblems that are more manageable. As an example, it would be insanely
    complex to build a house in one single step. It is much easier to build the house
    from simple parts that are then combined into the final result.
  prefs: []
  type: TYPE_NORMAL
- en: The same also applies to software development. It is much easier to develop
    a very complex application if we divide this application into smaller components
    that interoperate and make up the overall application. Now, it is much easier
    to develop these components individually if they are only loosely coupled to each
    other. What this means is that component A makes no assumptions about the inner
    workings of, say, components B and C, and is only interested in how it can communicate
    with those two components across a well-defined interface.
  prefs: []
  type: TYPE_NORMAL
- en: If each component has a well-defined and simple public interface through which
    communication with the other components in the system and the outside world happens,
    then this enables us to develop each component individually, without implicit
    dependencies to other components. During the development process, other components
    in the system can easily be replaced by stubs or mocks to allow us to test our
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful versus stateless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every meaningful business application creates, modifies, or uses data. In IT,
    a synonym for data is *state*. An application service that creates or modifies
    persistent data is called a stateful component. Typical stateful components are
    database services or services that create files. On the other hand, application
    components that do not create or modify persistent data are called stateless components.
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed application architecture, stateless components are much simpler
    to handle than stateful components. Stateless components can be easily scaled
    up and down. Furthermore, they can be quickly and painlessly torn down and restarted
    on a completely different node of the cluster – all of this because they have
    no persistent data associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: Given that fact, it is helpful to design a system in a way that most of the
    application services are stateless. It is best to push all the stateful components
    to the boundary of the application and limit their number. Managing stateful components
    is hard.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we build applications that consist of many individual components or services
    that communicate with each other, we need a mechanism that allows the individual
    components to find each other in the cluster. Finding each other usually means
    that you need to know on which node the target component is running and on which
    port it is listening for communication. Most often, nodes are identified by an
    IP address and a port, which is just a number in a well-defined range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, we could tell **Service A**, which wants to communicate with a
    target, **Service B**, what the **IP** address and **port** of the target are.
    This could happen, for example, through an entry in a configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bd7fdc4f-ae7d-4b25-b313-76aede6e61ea.png)Components are hardwired'
  prefs: []
  type: TYPE_NORMAL
- en: While this might work very well in the context of a monolithic application that
    runs on one or only a few well-known and curated servers, it totally falls apart
    in a distributed application architecture. First of all, in this scenario, we
    have many components, and keeping track of them manually becomes a nightmare.
    This is definitely not scalable. Furthermore, **Service A** typically should or
    will never know on which node of the cluster the other components run. Their location
    may not even be stable as component B could be moved from node X to another node
    Y, due to various reasons external to the application. Thus, we need another way
    in which **Service A** can locate **Service B**, or any other service, for that
    matter. What is most commonly used is an external authority that is aware of the
    topology of the system at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This external authority or service knows all the nodes and their IP addresses
    that currently pertain to the cluster; it knows about all the services that are
    running and where they are running. Often, this kind of service is called a **DNS
    service**, where **DNS** stands for **Domain Name System**. As we will see, Docker has a
    DNS service implemented as part of the underlying engine. Kubernetes – the number
    one container orchestration system, which we''ll discuss in [Chapter 12](27c0d9ce-fab6-4ce9-9034-4f2fb62931e8.xhtml),
    *Orchestrators* – also uses a **DNS service** to facilitate communication between
    components running in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3aa74eff-0e86-46e9-bb4b-dd356a97a816.png)Components consulting an
    external locator service'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see how **Service A** wants to communicate
    with **Service B**, but it can't do this directly. First, it has to query the external authority,
    a registry service (here, called a **DNS Service**), about the whereabouts of
    **Service B**. The registry service will answer with the requested information
    and hand out the IP address and port number that **Service A** can use to reach
    **Service B**. **Service** **A** then uses this information and establishes a
    communication with **Service B**. Of course, this is a naive picture of what's
    really happening on a low level, but it is a good picture to help us understand
    the architectural pattern of service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Routing is the mechanism of sending packets of data from a source component
    to a target component. Routing is categorized into different types. The so-called
    OSI model (see the reference to this in the *Further reading* section of this
    chapter for more information) is used to distinguish between different types of
    routing. In the context of containers and container orchestration, routing at
    layers 2, 3, 4, and 7 is relevant. We will dive into more detail about routing
    in subsequent chapters. Here, let's just say that layer 2 routing is the most
    low-level type of routing, which connects a MAC address to another MAC address,
    while layer 7 routing, which is also called application-level routing, is the
    most high-level one. The latter is, for example, used to route requests that have
    a target identifier, that is, a URL such as [https://acme.com/pets](https://acme.com/pets),
    to the appropriate target component in our system.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load balancing is used whenever **Service A** needs to communicate with **Service
    B**, say in a request-response pattern, but the latter is running in more than
    one instance, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cee74fda-2b24-485d-b10f-ac9c17feedef.png)The request of Service
    A being load balanced to Service B'
  prefs: []
  type: TYPE_NORMAL
- en: If we have multiple instances of a service such as **Service B** running in
    our system, we want to make sure that every one of those instances gets an equal
    amount of workload assigned to it. This task is a generic one, which means that we
    don't want the caller to have to do the load balancing, but rather an external
    service that intercepts the call and takes over the part of deciding which of
    the target service instances to forward the call to. This external service is
    called a load balancer. Load balancers can use different algorithms to decide
    how to distribute incoming calls to target service instances. The most common
    algorithm that's used is called round-robin. This algorithm just assigns requests
    in a repetitive way, starting with instance 1, then 2, until instance n. After
    the last instance has been served, the load balancer starts over with instance
    number 1.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, a **load** **balancer** also facilitates high availability
    since a request from **service A** will be forwarded to a healthy instance of
    **Service B**. The **load balancer** also takes the role of periodically checking
    the health of each instance of B.
  prefs: []
  type: TYPE_NORMAL
- en: Defensive programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing a service for a distributed application, it is important to
    remember that this service is not going to be standalone and that it's dependent
    on other application services or even on external services provided by third parties,
    such as credit card validation services or stock information services, to just
    name two. All these other services are external to the service we are developing.
    We have no control over their correctness or their availability at any given time.
    Thus, when coding, we always need to assume the worst and hope for the best. Assuming
    the worst means that we have to deal with potential failures explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Retries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When there is a possibility that an external service might be temporarily unavailable
    or not responsive enough, then the following procedure can be used. When the call
    to the other service fails or times out, the calling code should be structured
    in such a way that the same call is repeated after a short wait time. If the call
    fails again, the wait should be a bit longer before the next trial. The calls
    should be repeated up until a maximum number of times, each time increasing the
    wait time. After that, the service should give up and provide a degraded service,
    which could mean returning some stale cached data or no data at all, depending
    on the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Important operations that are performed on a service should always be logged.
    Logging information needs to be categorized to be of any real value. A common
    list of categories includes debug, info, warning, error, and fatal. Logging information
    should be collected by a central log aggregation service and not be stored on
    an individual node of the cluster. Aggregated logs are easy to parse and filter
    for relevant information. This information is essential to quickly pinpoint the
    root cause of a failure or unexpected behavior in a distributed system consisting
    of many moving parts, running in production.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, each application service in a distributed application
    is dependent on other services. As developers, we should always expect the worst
    and have appropriate error handling in place. One of the most important best practices
    is to fail fast. Code the service in such a way that unrecoverable errors are
    discovered as early as possible and, if such an error is detected, have the service
    fail immediately. But don't forget to log meaningful information to `STDERR` or
    `STDOUT`, which can be used by developers or system operators later to track malfunctions
    of the system. Also, return a helpful error to the caller, indicating as precisely
    as possible why the call failed.
  prefs: []
  type: TYPE_NORMAL
- en: One sample of fail fast is to always check the input values provided by the
    caller. Are the values in the expected ranges and complete? If not, then do not
    try to continue processing; instead, immediately abort the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A mission-critical system has to be available at all times, around the clock,
    365 days a year. Downtime is not acceptable since it might result in a huge loss
    of opportunities or reputation for the company. In a highly distributed application,
    the likelihood of a failure of at least one of the many involved components is
    non-neglectable. We can say that the question is not whether a component will
    fail, but rather when a failure will occur.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid downtime when one of the many components in the system fails, each
    individual part of the system needs to be redundant. This includes the application
    components, as well as all infrastructure parts. What that means is that if we,
    say, have a payment service as part of our application, then we need to run this
    service redundantly. The easiest way to do that is to run multiple instances of
    this very service on different nodes of our cluster. The same applies, say, for
    an edge router or a load balancer. We cannot afford for this to ever go down.
    Thus, the router or load balancer must be redundant.
  prefs: []
  type: TYPE_NORMAL
- en: Health checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have mentioned various times that in a distributed application architecture,
    with its many parts, the failure of an individual component is highly likely and
    that it is only a matter of time until it happens. For that reason, we run every
    single component of the system redundantly. Proxy services then load balance the
    traffic across the individual instances of a service.
  prefs: []
  type: TYPE_NORMAL
- en: But now, there is another problem. How does the proxy or router know whether
    a certain service instance is available? It could have crashed or it could be
    unresponsive. To solve this problem, we can use so-called health checks. The proxy,
    or some other system service on behalf of the proxy, periodically polls all the
    service instances and checks their health. The questions are basically, Are you
    still there? Are you healthy? The answer to each service is either Yes or No,
    or the health check times out if the instance is not responsive anymore.
  prefs: []
  type: TYPE_NORMAL
- en: If the component answers with No or a timeout occurs, then the system kills
    the corresponding instance and spins up a new instance in its place. If all this
    happens in a fully automated way, then we say that we have an auto-healing system
    in place.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the proxy periodically polling the status of the components, responsibility
    can also be turned around. The components could be required to periodically send
    live signals to the proxy. If a component fails to send live signals over a predefined,
    extended period of time, it is assumed to be unhealthy or dead.
  prefs: []
  type: TYPE_NORMAL
- en: There are situations where either of the described ways is more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A circuit breaker is a mechanism that is used to avoid a distributed application going
    down due to the cascading failure of many essential components. Circuit breakers
    help to avoid one failing component tearing down other dependent services in a
    domino effect. Like circuit breakers in an electrical system, which protect a
    house from burning down due to the failure of a malfunctioning plugged-in appliance
    by interrupting the power line, circuit breakers in a distributed application
    interrupt the connection from **Service A** to **Service B **if the latter is
    not responding or is malfunctioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be achieved by wrapping a protected service call in a circuit breaker
    object. This object monitors for failures. Once the number of failures reaches
    a certain threshold, the circuit breaker trips. All subsequent calls to the circuit
    breaker will return with an error, without the protected call being made at all:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7f511703-df7b-4e5c-af03-f8fc19876a73.png)Circuit breaker pattern'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we have a circuit breaker that tips over after the
    second timeout is received when calling **Service B**.
  prefs: []
  type: TYPE_NORMAL
- en: Running in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To successfully run a distributed application in production, we need to consider a
    few more aspects beyond the best practices and patterns presented in the preceding
    sections. One specific area that comes to mind is introspection and monitoring.
    Let's go through the most important aspects in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once a distributed application is in production, it is not possible to live
    debug it. But how can we then find out what exactly is the root cause of the application
    malfunctioning? The solution to this problem is that the application produces
    abundant and meaningful logging information while running. Developers need to
    instrument their application services in such a way that they output helpful information,
    such as when an error happens or a potentially unexpected or unwanted situation
    is encountered. Often, this information is output to `STDOUT` and `STDERR`, where
    it is then collected by system daemons that write the information to local files
    or forward it to a central log aggregation service.
  prefs: []
  type: TYPE_NORMAL
- en: If there is sufficient information in the logs, developers can use those logs
    to track down the root cause of the errors in the system.
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed application architecture, with its many components, logging
    is even more important than in a monolithic application. The paths of execution
    of a single request through all the components of the application can be very
    complex. Also, remember that the components are distributed across a cluster of
    nodes. Thus, it makes sense to log everything of importance, and to add things
    to each log entry such as the exact time when it happened, the component in which
    it happened, and the node on which the component ran, to name just a few. Furthermore,
    the logging information should be aggregated in a central location so that it
    is readily available for developers and system operators to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing is used to find out how an individual request is funneled through a
    distributed application and how much time is spent overall for the request and
    in every individual component. This information, if collected, can be used as
    one of the sources for dashboards that shows the behavior and health of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operation engineers like to have dashboards showing live key metrics of the
    system, which show them the overall health of the application at a glance. These
    metrics can be non-functional metrics, such as memory and CPU usage, the number
    of crashes of a system or application component, and the health of a node, as
    well as functional and, hence, application-specific metrics, such as the number
    of checkouts in an ordering system or the number of items out of stock in an inventory
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Most often, the base data that's used to aggregate the numbers that are used
    for a dashboard is extracted from logging information. This can either be system
    logs, which will mostly be used for non-functional metrics, or application-level
    logs, for functional metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Application updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the competitive advantages for a company is to be able to react in a
    timely manner to changing market situations. Part of this is to be able to quickly
    adjust an application to fulfill new and changed needs or to add new functionality.
    The faster we can update our applications, the better. Many companies these days
    roll out new or changed features multiple times per day.
  prefs: []
  type: TYPE_NORMAL
- en: Since application updates are so frequent, these updates have to be non-disruptive.
    We cannot allow the system to go down for maintenance when upgrading. It all has
    to happen seamlessly and transparently.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way of updating an application or an application service is to use rolling
    updates. The assumption here is that the particular piece of software that has
    to be updated runs in multiple instances. Only then can we use this type of update.
  prefs: []
  type: TYPE_NORMAL
- en: What happens is that the system stops one instance of the current service and
    replaces it with an instance of the new service. As soon as the new instance is
    ready, it will be served traffic. Usually, the new instance is monitored for some
    time to see whether or not it works as expected and, if it does, the next instance
    of the current service is taken down and replaced with a new instance. This pattern
    is repeated until all the service instances have been replaced.
  prefs: []
  type: TYPE_NORMAL
- en: Since there are always a few instances running at any given time, current or
    new, the application is operational all the time. No downtime is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In blue-green deployments, the current version of the application service, called **blue**,
    handles all the application traffic. We then install the new version of the application
    service, called **green**, on the production system. The new service is not wired
    with the rest of the application yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once **green** is installed, we can execute **smoke tests** against this new
    service and, if those succeed, the router can be configured to funnel all traffic
    that previously went to **blue** to the new service, **green**. The behavior of
    **green** is then observed closely and, if all success criteria are met, **blue**
    can be decommissioned. But if, for some reason, **green** shows some unexpected
    or unwanted behavior, the router can be reconfigured to return all traffic to
    blue. Green can then be removed and fixed, and a new blue-green deployment can
    be executed with the corrected version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ec84720c-9431-4c3a-90fb-bcffe0d5eaac.png)Blue-green deployment'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at canary releases.
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Canary releases are releases where we have the current version of the application
    service and the new version installed on the system in parallel. As such, they
    resemble blue-green deployments. At first, all traffic is still routed through
    the current version. We then configure a router so that it funnels a small percentage,
    say 1%, of the overall traffic to the new version of the application service.
    Subsequently, the behavior of the new service is monitored closely to find out
    whether it works as expected. If all the criteria for success are met, then the
    router is configured to funnel more traffic, say 5% this time, through the new
    service. Again, the behavior of the new service is closely monitored and, if it
    is successful, more and more traffic is routed to it until we reach 100%. Once
    all the traffic has been routed to the new service and it has been stable for
    some time, the old version of the service can be decommissioned.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we call this a canary release? It is named after the coal miners who
    would use canary birds as an early warning system in the mines. Canary birds are
    particularly sensitive to toxic gas and if such a canary bird died, the miners
    knew they had to abandon the mine immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Irreversible data changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If part of our update process is to execute an irreversible change in our state,
    such as an irreversible schema change in a backing relational database, then we
    need to address this with special care. It is possible to execute such changes
    without downtime if we use the right approach. It is important to recognize that,
    in such a situation, we cannot deploy the code changes that require the new data
    structure in the data store at the same time as the changes to the data. Rather,
    the whole update has to be separated into three distinct steps. In the first step,
    we roll out a backward-compatible schema and data change. If this is successful,
    then we roll out the new code in the second step. Again, if that is successful,
    we clean up the schema in the third step and remove the backward compatibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/987dac84-2498-490e-af50-2f9bb870d341.png)Rolling out an irreversible
    data or schema change'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows how the data and its structure are updated, then
    how the application code is updated, and finally, in the third step, how the data
    and data structure are cleaned up.
  prefs: []
  type: TYPE_NORMAL
- en: Rollback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have frequent updates for our application services that run in production,
    sooner or later, there will be a problem with one of those updates. Maybe a developer,
    while fixing a bug, introduced a new one, which was not caught by all the automated,
    and maybe manual, tests, so the application is misbehaving and it is imperative
    that we roll back the service to the previous good version. In this regard, a
    rollback is a recovery from a disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Again, in a distributed application architecture, it is not a question of whether
    a rollback will ever be needed, but rather when a rollback will have to occur.
    Thus, we need to be absolutely sure that we can always roll back to a previous
    version of any service that makes up our application. Rollbacks cannot be an afterthought;
    they have to be a tested and proven part of our deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: If we are using blue-green deployments to update our services, then rollbacks
    should be fairly simple. All we need to do is switch the router from the new green
    version of the service back to the previous blue version.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what a distributed application architecture is and
    what patterns and best practices are helpful or needed to successfully run a distributed
    application. Lastly, we discussed what more is needed to run such an application
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into networking limited to a single host.
    We're going to discuss how containers living on the same host can communicate
    with each other and how external clients can access containerized applications
    if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please answer the following questions to assess your understanding of this
    chapter''s content:'
  prefs: []
  type: TYPE_NORMAL
- en: When and why does every part in a distributed application architecture have
    to be redundant? Explain in a few short sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need DNS services? Explain in three to five sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a circuit breaker and why is it needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the important differences between a monolithic application
    and a distributed or multi-service application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a blue-green deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following articles provide more in-depth information regarding what was
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breakers: [http://bit.ly/1NU1sgW](https://bit.ly/2pBENyP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OSI model explained: [http://bit.ly/1UCcvMt](https://bit.ly/2BIRpJY)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue-green deployments: [http://bit.ly/2r2IxNJ](http://bit.ly/2r2IxNJ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

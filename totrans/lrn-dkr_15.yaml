- en: Orchestrators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced Docker Compose, a tool that allows us
    to work with multi-service applications that are defined in a declarative way
    on a single Docker host.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces the concept of orchestrators. It teaches us why orchestrators
    are needed, and how they work conceptually. This chapter will also provide an
    overview of the most popular orchestrators and list a few of their pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are orchestrators and why do we need them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tasks of an orchestrator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of popular orchestrators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After finishing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Name three to four tasks for which an orchestrator is responsible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List two to three of the most popular orchestrators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain to an interested layman, in your own words, and with appropriate analogies,
    why we need container orchestrators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are orchestrators and why do we need them?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml)*, Distributed Application
    Architecture*, we learned which patterns and best practices are commonly used
    to successfully build, ship, and run a highly distributed application. Now, if
    our distributed application is containerized, then we're facing the exact same
    problems or challenges that a non-containerized distributed application faces.
    Some of these challenges are those that were discussed in [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml), *Distributed
    Application Architecture*—service discovery, load balancing, scaling, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to what Docker did with containers—standardizing the packaging and shipping
    of software with the introduction of those containers—we would like to have some
    tool or infrastructure software that handles all or most of the challenges mentioned.
    This software turns out to be what we call container orchestrators or, as we also
    call them, orchestration engines.
  prefs: []
  type: TYPE_NORMAL
- en: 'If what I just said doesn''t make much sense to you yet, then let''s look at
    it from a different angle. Take an artist who plays an instrument. They can play
    wonderful music to an audience all on their own—just the artist and their instrument.
    But now take an orchestra of musicians. Put them all in a room, give them the
    notes of a symphony, ask them to play it, and leave the room. Without any director,
    this group of very talented musicians would not be able to play this piece in
    harmony; it would more or less sound like a cacophony. Only if the orchestra has
    a conductor, who orchestrates the group of musicians, will the resulting music
    of the orchestra be enjoyable to our ears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a0425d38-20e7-4cc7-b2f5-5680c5ae5b2a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A container orchestrator is like the conductor of an orchestraSource: https://it.wikipedia.org/wiki/Giuseppe_Lanzetta#/media/File:UMB_5945.JPGLicense: https://creativecommons.org/licenses/by-sa/3.0/deed.en'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of musicians, we now have containers, and instead of different instruments,
    we have containers that have different requirements to the container hosts to
    run. And instead of the music being played at varying tempi, we have containers
    that communicate with each other in particular ways, and have to scale up and
    scale down. In this regard, a container orchestrator has very much the same role
    as a conductor in an orchestra. It makes sure that the containers and other resources in a
    cluster play together in harmony.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you can now see more clearly what a container orchestrator is, and
    why we need one. Assuming that you confirm this question, we can now ask ourselves
    how the orchestrator is going to achieve the expected outcome, namely, to make
    sure that all the containers in the cluster play with each other in harmony. Well,
    the answer is, the orchestrator has to execute very specific tasks, similar to
    the way in which the conductor of an orchestra also has a set of tasks that they
    execute in order to tame and, at the same time, elevate the orchestra.
  prefs: []
  type: TYPE_NORMAL
- en: The tasks of an orchestrator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*So, **what are the tasks that we expect an orchestrator worth its money to
    execute for us?* Let''s look at them in detail. The following list shows the most
    important tasks that, at the time of writing, enterprise users typically expect
    from their orchestrator.'
  prefs: []
  type: TYPE_NORMAL
- en: Reconciling the desired state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using an orchestrator, you tell it, in a declarative way, how you want
    it to run a given application or application service. We learned  what declarative
    versus imperative means in [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml)*,
    Docker Compose*. Part of this declarative way of describing the application service
    that we want to run includes elements such as which container image to use, how
    many instances of this service to run, which ports to open, and more. This declaration
    of the properties of our application service is what we call the *desired state*.
  prefs: []
  type: TYPE_NORMAL
- en: So, when we now tell the orchestrator for the first time to create such a new
    application service based on the declaration, then the orchestrator makes sure
    to schedule as many containers in the cluster as requested. If the container image
    is not yet available on the target nodes of the cluster where the containers are
    supposed to run, then the scheduler makes sure that they're first downloaded from
    the image registry. Next, the containers are started with all the settings, such
    as networks to which to attach, or ports to expose. The orchestrator works as
    hard as it can to exactly match, in reality, the cluster to the declaration.
  prefs: []
  type: TYPE_NORMAL
- en: Once our service is up and running as requested, that is, it is running in the
    desired state, then the orchestrator continues to monitor it. Each time the orchestrator
    discovers a discrepancy between the actual state of the service and its desired
    state, it again tries its best to reconcile the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: 'What could such a discrepancy between the actual and desired states of an application
    service be? Well, let''s say one of the replicas of the service, that is, one
    of the containers, crashes due to, say, a bug, then the orchestrator will discover
    that the actual state differs from the desired state in the number of replicas:
    there is one replica missing. The orchestrator will immediately schedule a new
    instance to another cluster node, which replaces the crashed instance. Another
    discrepancy could be that there are too many instances of the application service
    running, if the service has been scaled down. In this case, the orchestrator will
    just randomly kill as many instances as needed in order to achieve parity between
    the actual and the desired number of instances. Yet another discrepancy could
    be when the orchestrator discovers that there is an instance of the application
    service running a wrong (maybe old) version of the underlying container image.
    By now, you should get the picture, right?'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, instead of us actively monitoring our application's services that are
    running in the cluster and correcting any deviation from the desired state, we
    delegate this tedious task to the orchestrator. This works very well provided
    we use a declarative and not an imperative way of describing the desired state
    of our application services.
  prefs: []
  type: TYPE_NORMAL
- en: Replicated and global services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two quite different types of services that we might want to run in
    a cluster that is managed by an orchestrator. They are *replicated *and *global *services.
    A replicated service is a service that is required to run in a specific number
    of instances, say 10\. A global service, in turn, is a service that is required
    to have exactly one instance running on every single worker node of the cluster.
    I have used the term *worker node* here. In a cluster that is managed by an orchestrator,
    we typically have two types of nodes, *managers* and *workers.* A manager node is usually exclusively
    used by the orchestrator to manage the cluster and does not run any other workload.
    Worker nodes, in turn, run the actual applications.
  prefs: []
  type: TYPE_NORMAL
- en: So, the orchestrator makes sure that, for a global service, an instance of it
    is running on every single worker node, no matter how many there are. We do not
    need to care about the number of instances, but only that on each node, it is
    guaranteed to run a single instance of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we can fully rely on the orchestrator to handle this. In a replicated
    service, we will always be guaranteed to find the exact desired number of instances,
    while for a global service, we can be assured that on every worker node, there
    will always run exactly one instance of the service. The orchestrator will always
    work as hard as it can to guarantee this desired state.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, a global service is also called a **DaemonSet**.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we describe an application service in a declarative way, we are never supposed
    to tell the orchestrator on which cluster nodes the different instances of the
    service have to run. We leave it up to the orchestrator to decide which nodes
    best fit this task.
  prefs: []
  type: TYPE_NORMAL
- en: It is, of course, technically possible to instruct the orchestrator to use very
    deterministic placement rules, but this would be an anti-pattern, and is not recommended
    at all, other than in very special edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: So, if we now assume that the orchestration engine has complete and free will
    as to where to place individual instances of the application service and, furthermore,
    that instances can crash and be rescheduled by the orchestrator to different nodes,
    then we will realize that it is a futile task for us to keep track of where the
    individual instances are running at any given time. Even better, we shouldn't
    even try to know this, since it is not important.
  prefs: []
  type: TYPE_NORMAL
- en: OK, you might say, but what about if I have two services, A and B, and Service
    A relies on Service B; *shouldn't any given instance of Service A know where it
    can find an instance of Service B? *
  prefs: []
  type: TYPE_NORMAL
- en: Here, I have to say loudly and clearly—no, it shouldn't. This kind of knowledge
    is not desirable in a highly distributed and scalable application. Rather, we
    should rely on the orchestrator to provide us with the information that we need
    in order to reach the other service instances that we depend on. It is a bit like
    in the old days of telephony, when we could not directly call our friends, but
    had to call the phone company's central office, where some operator would then
    route us to the correct destination. In our case, the orchestrator plays the role
    of the operator, routing a request coming from an instance of Service A to an
    available instance of Service B. This whole process is called **service discovery**.
  prefs: []
  type: TYPE_NORMAL
- en: Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned so far that in a distributed application, we have many interacting
    services. When Service A interacts with Service B, it happens through the exchange
    of data packets. These data packets need to somehow be funneled from Service A
    to Service B. This process of funneling the data packets from a source to a destination
    is also called **routing**. As authors or operators of an application, we do expect
    the orchestrator to take over this task of routing. As we will see in later chapters,
    routing can happen on different levels. It is like in real life. Suppose you're
    working in a big company in one of their office buildings. Now, you have a document
    that needs to be forwarded to another employee of the company. The internal post
    service will pick up the document from your outbox, and take it to the post office
    located in the same building. If the target person works in the same building,
    the document can then be directly forwarded to that person. If, on the other hand,
    the person works in another building of the same block, the document will be forwarded
    to the post office in that target building, from where it is then distributed
    to the receiver through the internal post service. Thirdly, if the document is
    targeted at an employee working in another branch of the company that is located
    in a different city or even country, then the document is forwarded to an external
    postal service such as UPS, which will transport it to the target location, from
    where, once again, the internal post service takes over and delivers it to the
    recipient.
  prefs: []
  type: TYPE_NORMAL
- en: Similar things happen when routing data packets between application services
    that are running in containers. The source and target containers can be located on
    the same cluster node, which corresponds to the situation where both employees
    work in the same building. The target container can be running on a different
    cluster node, which corresponds to the situation where the two employees work
    in different buildings of the same block. Finally, the third situation is when
    a data packet comes from outside of the cluster and has to be routed to the target
    container that is running inside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: All these situations, and more, have to be handled by the orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a highly available distributed application, all components have to be redundant.
    That means that every application service has to be run in multiple instances,
    so that if one instance fails, the service as a whole is still operational.
  prefs: []
  type: TYPE_NORMAL
- en: To make sure that all instances of a service are actually doing work and are
    not just sitting around idle, you have to make sure that the requests for service
    are distributed equally to all the instances. This process of distributing workload
    among service instances is called **load balancing**. Various algorithms exist
    for how the workload can be distributed. Usually, a load balancer works using
    the so-called round robin algorithm, which makes sure that the workload is distributed
    equally to the instances using a cyclic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we expect the orchestrator to take care of the load balancing requests
    from one service to another, or from external sources to internal services.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running our containerized, distributed application in a cluster that is
    managed by an orchestrator, we additionally want an easy way to handle expected
    or unexpected increases in workload. To handle an increased workload, we usually
    just schedule additional instances of a service that is experiencing this increased
    load. Load balancers will then automatically be configured to distribute the workload
    over more available target instances.
  prefs: []
  type: TYPE_NORMAL
- en: But in real-life scenarios, the workload varies over time. If we look at a shopping
    site such as Amazon, it might have a high load during peak hours in the evening,
    when everyone is at home and shopping online; it may experience extreme loads
    during special days such as Black Friday; and it may experience very little traffic
    early in the morning. Thus, services need to not just be able to scale up, but
    also to scale down when the workload goes down.
  prefs: []
  type: TYPE_NORMAL
- en: We also expect orchestrators to distribute the instances of a service in a meaningful
    way when scaling up or down. It would not be wise to schedule all instances of
    the service on the same cluster node, since, if that node goes down, the whole
    service goes down. The scheduler of the orchestrator, which is responsible for
    the placement of the containers, needs to also consider not placing all instances
    into the same rack of computers, since, if the power supply of the rack fails,
    again, the whole service is affected. Furthermore, service instances of critical
    services should even be distributed across data centers in order to avoid outages.
    All these decisions, and many more, are the responsibility of the orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: In the cloud, instead of computer racks, the term '**availability zones**' is
    often used.
  prefs: []
  type: TYPE_NORMAL
- en: Self-healing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days, orchestrators are very sophisticated and can do a lot for us to
    maintain a healthy system. Orchestrators monitor all containers that are running
    in the cluster, and they automatically replace crashed or unresponsive ones with
    new instances. Orchestrators monitor the health of cluster nodes, and take them
    out of the scheduler loop if a node becomes unhealthy or is down. A workload that
    was located on those nodes is automatically rescheduled to different available
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: All these activities, where the orchestrator monitors the current state and
    automatically repairs the damage or reconciles the desired state, lead to a so-called **self-healing** system.
    We do not, in most cases, have to actively engage and repair damage. The orchestrator
    will do this for us automatically.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are a few situations that the orchestrator cannot handle without
    our help. Imagine a situation where we have a service instance running in a container.
    The container is up and running and, from the outside, looks perfectly healthy.
    But, the application running inside it is in an unhealthy state. The application
    did not crash, it just is not able to work as it was originally designed anymore. *How
    could the orchestrator possibly know about this without us giving it a hint?* It
    can't! Being in an unhealthy or invalid state means something completely different
    for each application service. In other words, the health status is service dependent.
    Only the authors of the service, or its operators, know what health means in the
    context of a service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, orchestrators define seams or probes, over which an application service
    can communicate to the orchestrator about what state it is in. Two fundamental
    types of probe exist:'
  prefs: []
  type: TYPE_NORMAL
- en: The service can tell the orchestrator that it is healthy or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service can tell the orchestrator that it is ready or temporarily unavailable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the service determines either of the preceding answers is totally up to
    the service. The orchestrator only defines how it is going to ask, for example,
    through an `HTTP GET` request, or what type of answers it is expecting, for example,
    `OK` or `NOT OK.`
  prefs: []
  type: TYPE_NORMAL
- en: If our services implement logic in order to answer the preceding health or availability
    questions, then we have a truly self-healing system, since the orchestrator can
    kill unhealthy service instances and replace them with new healthy ones, and it
    can take service instances that are temporarily unavailable out of the load balancer's
    round robin.
  prefs: []
  type: TYPE_NORMAL
- en: Zero downtime deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days, it gets harder and harder to justify a complete downtime for a mission-critical
    application that needs to be updated. Not only does that mean missed opportunities,
    but it can also result in a damaged reputation for the company. Customers using
    the application are no longer prepared to accept such an inconvenience, and will
    turn away quickly. Furthermore, our release cycles get shorter and shorter. Where,
    in the past, we would have one or two new releases per year, these days, a lot
    of companies update their applications multiple times a week, or even multiple
    times per day.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to that problem is to come up with a zero downtime application
    update strategy. The orchestrator needs to be able to update individual application
    services, batch-wise. This is also called **rolling updates**. At any given time,
    only one or a few of the total number of instances of a given service are taken
    down and replaced by the new version of the service. Only if the new instances
    are operational, and do not produce any unexpected errors or show any misbehavior,
    will the next batch of instances be updated. This is repeated until all instances
    are replaced with their new version. If, for some reason, the update fails, then
    we expect the orchestrator to automatically roll the updated instances back to
    their previous version.
  prefs: []
  type: TYPE_NORMAL
- en: Other possible zero downtime deployments are blue–green deployments and canary
    releases. In both cases, the new version of a service is installed in parallel
    with the current, active version. But initially, the new version is only accessible
    internally. Operations can then run smoke tests against the new version, and when
    the new version seems to be running just fine, then, in the case of a blue–green
    deployment, the router is switched from the current blue version, to the new green
    version. For some time, the new green version of the service is closely monitored
    and, if everything is fine, the old blue version can be decommissioned. If, on
    the other hand, the new green version does not work as expected, then it is only
    a matter of setting the router back to the old blue version in order to achieve
    a complete rollback.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a canary release, the router is configured in such a way that
    it funnels a tiny percentage, say 1%, of the overall traffic through the new version
    of the service, while 99% of the traffic is still routed through the old version.
    The behavior of the new version is closely monitored and compared to the behavior
    of the old version. If everything looks good, then the percentage of the traffic
    that is funneled through the new service is slightly increased. This process is
    repeated until 100% of the traffic is routed through the new service. If the new
    service has run for a while and everything looks good, then the old service can
    be decommissioned.
  prefs: []
  type: TYPE_NORMAL
- en: Most orchestrators support at least the rolling update type of zero downtime
    deployment out of the box. Blue–green deployments and canary releases are often
    quite easy to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity and location awareness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, certain application services require the availability of dedicated
    hardware on the nodes on which they run. For example, I/O-bound services require
    cluster nodes with an attached high-performance **solid-state drive** (**SSD**),
    or some services that are used for machine learning, or similar, require an **Accelerated
    Processing Unit** (**APU**). Orchestrators allow us to define node affinities
    per application service. The orchestrator will then make sure that its scheduler
    only schedules containers on cluster nodes that fulfill the required criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an affinity to a particular node should be avoided; this would introduce
    a single point of failure and thus compromise high availability. Always define
    a set of multiple cluster nodes as the target for an application service.
  prefs: []
  type: TYPE_NORMAL
- en: Some orchestration engines also support what is called **location awareness** or **geo
    awareness**. What this means is that you can ask the orchestrator to equally distribute
    instances of a service over a set of different locations. You could, for example,
    define a `datacenter` label, with the possible `west`, `center`, and `east` values,
    and apply the label to all of the cluster nodes with the value that corresponds
    to the geographical region in which the respective node is located. Then, you
    instruct the orchestrator to use this label for the geo awareness of a certain
    application service. In this case, if you request nine replicas of the service,
    then the orchestrator would make sure that three instances are deployed to the
    nodes in each of the three data centers—west, center, and east.
  prefs: []
  type: TYPE_NORMAL
- en: Geo awareness can even be defined hierarchically; for example, you can have
    a data center as the top-level discriminator, followed by the availability zone.
  prefs: []
  type: TYPE_NORMAL
- en: Geo awareness, or location awareness, is used to decrease the probability of
    outages due to power supply failures or data center outages. If the application
    instances are distributed across nodes, availability zones, or even data centers,
    it is extremely unlikely that everything will go down at once. One region will
    always be available.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days, security in IT is a very hot topic. Cyber warfare is at an all-time
    high. Most high-profile companies have been victims of hacker attacks, with very
    costly consequences. One of the worst nightmares of each **chief information officer** (**CIO**)
    or **chief technology officer** (**CTO**) is to wake up in the morning and hear
    in the news that their company has become a victim of a hacker attack, and that
    sensitive information has been stolen or compromised.
  prefs: []
  type: TYPE_NORMAL
- en: To counter most of these security threats, we need to establish a secure software
    supply chain, and enforce security defense in depth. Let's look at some of the
    tasks that you can expect from an enterprise-grade orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: Secure communication and cryptographic node identity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First and foremost, we want to make sure that our cluster that is managed by
    the orchestrator is secure. Only trusted nodes can join the cluster. Each node
    that joins the cluster gets a cryptographic node identity, and all communication
    between the nodes must be encrypted. For this, nodes can use **M****utual Transport
    Layer Security** (**MTLS**). In order to authenticate nodes of the cluster with
    each other, certificates are used. These certificates are automatically rotated
    periodically, or on request, to protect the system in case a certificate is leaked.
  prefs: []
  type: TYPE_NORMAL
- en: 'The communication that happens in a cluster can be separated into three types.
    You talk about communication planes—management, control, and data planes:'
  prefs: []
  type: TYPE_NORMAL
- en: The management plane is used by the cluster managers, or masters, to, for example,
    schedule service instances, execute health checks, or create and modify any other
    resources in the cluster, such as data volumes, secrets, or networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The control plane is used to exchange important state information between all
    nodes of the cluster. This kind of information is, for example, used to update
    the local IP tables on clusters, which are used for routing purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data plane is where the actual application services communicate with each
    other and exchange data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally, orchestrators mainly care about securing the management and control
    plane. Securing the data plane is left to the user, although the orchestrator
    may facilitate this task.
  prefs: []
  type: TYPE_NORMAL
- en: Secure networks and network policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running application services, not every service needs to communicate with
    every other service in the cluster. Thus, we want the ability to sandbox services
    from each other, and only run those services in the same networking sandbox that
    absolutely need to communicate with each other. All other services and all network
    traffic coming from outside of the cluster should have no possibility of accessing
    the sandboxed services.
  prefs: []
  type: TYPE_NORMAL
- en: There are at least two ways in which this network-based sandboxing can happen.
    We can either use a **software-defined network** (**SDN**) to group application
    services, or we can have one flat network, and use network policies to control
    who does and does not have access to a particular service or group of services.
  prefs: []
  type: TYPE_NORMAL
- en: Role-based access control (RBAC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important tasks (next to security) that an orchestrator must
    fulfill in order to make it enterprise-ready is to provide role-based access to
    the cluster and its resources. RBAC defines how subjects, users, or groups of
    users of the system, organized into teams, and so on, can access and manipulate
    the system. It makes sure that unauthorized personnel cannot do any harm to the
    system, nor can they see any of the available resources in the system that they're
    not supposed to know of or see.
  prefs: []
  type: TYPE_NORMAL
- en: A typical enterprise might have user groups such as Development, QA, and Prod,
    and each of those groups can have one or many users associated with it. John Doe,
    the developer, is a member of the Development group and, as such, can access resources
    that are dedicated to the development team, but he cannot access, for example,
    the resources of the Prod team, of which Ann Harbor is a member. She, in turn,
    cannot interfere with the Development team's resources.
  prefs: []
  type: TYPE_NORMAL
- en: One way of implementing RBAC is through the definition of **grants**. A grant
    is an association between a subject, a role, and a resource collection. Here,
    a role is comprised of a set of access permissions to a resource. Such permissions
    can be to create, stop, remove, list, or view containers; to deploy a new application
    service; to list cluster nodes or view the details of a cluster node; and many
    more.
  prefs: []
  type: TYPE_NORMAL
- en: A resource collection is a group of logically related resources of the cluster,
    such as application services, secrets, data volumes, or containers.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our daily life, we have loads of secrets. Secrets are information that is
    not meant to be publicly known, such as the username and password combination
    that you use to access your online bank account, or the code to your cell phone
    or your locker at the gym.
  prefs: []
  type: TYPE_NORMAL
- en: When writing software, we often need to use secrets, too. For example, we need
    a certificate to authenticate our application service with the external service
    that we want to access, or we need a token to authenticate and authorize our service
    when accessing some other API. In the past, developers, for convenience, have
    just hardcoded those values, or put them in clear text in some external configuration
    files. There, this very sensitive information has been accessible to a broad audience,
    which, in reality, should never have had the opportunity to see those secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, these days, orchestrators offer what's called secrets to deal with
    such sensitive information in a highly secure way. Secrets can be created by authorized
    or trusted personnel. The values of those secrets are then encrypted and stored
    in the highly available cluster state database. The secrets, since they are encrypted,
    are now secure at rest. Once a secret is requested by an authorized application
    service, the secret is only forwarded to the cluster nodes that actually run an
    instance of that particular service, and the secret value is never stored on the
    node but mounted into the container in a `tmpfs` RAM-based volume. Only inside
    the respective container is the secret value available in clear text.
  prefs: []
  type: TYPE_NORMAL
- en: We already mentioned that the secrets are secure at rest. Once they are requested
    by a service, the cluster manager, or master, decrypts the secret and sends it
    over the wire to the target nodes. *So, what about the secrets being secure in
    transit?* Well, we learned earlier that the cluster nodes use MTLS for their communication,
    and so the secret, although transmitted in clear text, is still secure, since
    data packets will be encrypted by MTLS. Thus, secrets are secure both at rest
    and in transit. Only services that are authorized to use secrets will ever have
    access to those secret values.
  prefs: []
  type: TYPE_NORMAL
- en: Content trust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For added security, we want to make sure that only trusted images run in our
    production cluster. Some orchestrators allow us to configure a cluster so that
    it can only ever run signed images. Content trust and signing images is all about
    making sure that the authors of the image are the ones that we expect them to
    be, namely, our trusted developers or, even better, our trusted CI server. Furthermore,
    with content trust, we want to guarantee that the image that we get is fresh,
    and is not an old and maybe vulnerable image. And finally, we want to make sure
    that the image cannot be compromised by malicious hackers in transit. The latter
    is often called a **man-in-the-middle** (**MITM**) attack.
  prefs: []
  type: TYPE_NORMAL
- en: By signing images at the source, and validating the signature at the target,
    we can guarantee that the images that we want to run are not compromised.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse uptime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last point I want to discuss in the context of security is reverse uptime. *What
    do we mean by that?* Imagine that you have configured and secured a production
    cluster. On this cluster, you're running a few mission-critical applications of
    your company. Now, a hacker has managed to find a security hole in one of your
    software stacks, and has gained root access to one of your cluster nodes. That
    alone is already bad enough but, even worse, this hacker could now mask their
    presence on this node, on which they have root access, after all, and then use
    it as a base to attack other nodes in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Root access in Linux or any Unix-type operating system means that you can do
    anything on this system. It is the highest level of access that someone can have.
    In Windows, the equivalent role is that of an administrator.
  prefs: []
  type: TYPE_NORMAL
- en: But *what if we leverage the fact that containers are ephemeral and cluster
    nodes are quickly provisioned, usually in a matter of minutes if fully automated?* We
    just kill each cluster node after a certain uptime of, say, 1 day. The orchestrator
    is instructed to drain the node and then exclude it from the cluster. Once the
    node is out of the cluster, it is torn down and replaced by a freshly provisioned
    node.
  prefs: []
  type: TYPE_NORMAL
- en: That way, the hacker has lost their base and the problem has been eliminated.
    This concept is not yet broadly available, though, but to me it seems to be a
    huge step toward increased security and, as far as I have discussed it with engineers
    who are working in this area, it is not difficult to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Introspection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed a lot of tasks for which the orchestrator is responsible,
    and that it can execute in a completely autonomous way. But there is also the
    need for human operators to be able to see and analyze what's currently running
    on the cluster, and in what state or health the individual applications are. For
    all this, we need the possibility of introspection. The orchestrator needs to
    surface crucial information in a way that is easily consumable and understandable.
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator should collect system metrics from all the cluster nodes and
    make them accessible to the operators. Metrics include CPU, memory and disk usage,
    network bandwidth consumption, and more. The information should be easily available
    on a node-per-node basis, as well as in an aggregated form.
  prefs: []
  type: TYPE_NORMAL
- en: We also want the orchestrator to give us access to logs that are produced by
    service instances or containers. Even more, the orchestrator should provide us
    with `exec` access to each and every container if we have the correct authorization
    to do so. With `exec` access to containers, you can then debug misbehaving containers.
  prefs: []
  type: TYPE_NORMAL
- en: In highly distributed applications, where each request to the application goes
    through numerous services until it is completely handled, tracing requests is
    a really important task. Ideally, the orchestrator supports us in implementing
    a tracing strategy, or gives us some good guidelines to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, human operators can best monitor a system when working with a graphical
    representation of all the collected metrics and logging and tracing information.
    Here, we are speaking about dashboards. Every decent orchestrator should offer
    at least some basic dashboard with a graphical representation of the most critical
    system parameters.
  prefs: []
  type: TYPE_NORMAL
- en: However, human operators are not the only ones concerned about introspection.
    We also need to be able to connect external systems with the orchestrator in order
    to consume this information. There needs to be an API available, over which external
    systems can access data such as cluster state, metrics, and logs, and use this
    information to make automated decisions, such as creating pager or phone alerts,
    sending out emails, or triggering an alarm siren if some thresholds are exceeded
    by the system.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of popular orchestrators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, there are many orchestration engines out there and in
    use, but there are a few clear winners. The number one spot is clearly held by
    Kubernetes, which reigns supreme. A distant second is Docker's own SwarmKit, followed by others
    such as Apache Mesos, AWS **Elastic Container Service** (**ECS**), or Microsoft **Azure
    Container Service** (**ACS**).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes was originally designed by Google and later donated to the **Cloud
    Native Computing Foundation** (**CNCF**). Kubernetes was modeled after Google's
    proprietary Borg system, which has been running containers on a super massive
    scale for years. Kubernetes was Google's attempt to go back to the drawing board,
    and completely start over and design a system that incorporates all the lessons
    that were learned with Borg.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to Borg, which is proprietary technology, Kubernetes was open sourced
    early on. This was a very wise choice by Google, since it attracted a huge number
    of contributors from outside of the company and, over only a couple of years,
    an even more massive ecosystem evolved around Kubernetes. You can rightfully say
    that Kubernetes is the darling of the community in the container orchestration
    space. No other orchestrator has been able to produce so much hype and attract
    so many talented people who are willing to contribute in a meaningful way to the
    success of the project as a contributor or an early adopter.
  prefs: []
  type: TYPE_NORMAL
- en: In that regard, Kubernetes in the container orchestration space looks to me
    very much like what Linux has become in the server operating system space. Linux
    has become the *de facto* standard of server operating systems. All relevant companies,
    such as Microsoft, IBM, Amazon, Red Hat, and even Docker, have embraced Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'And there is one thing that cannot be denied: Kubernetes was designed from
    the very beginning for massive scalability. After all, it was designed with Google
    Borg in mind.'
  prefs: []
  type: TYPE_NORMAL
- en: One negative aspect that you could be voiced against Kubernetes is that it is
    still complex to set up and manage, at least at the time of writing. There is
    a significant hurdle to overcome for newcomers. The first step is steep, but once
    you have worked with this orchestrator for a while, it all makes sense. The overall
    design is carefully thought through and executed very well.
  prefs: []
  type: TYPE_NORMAL
- en: In release 1.10 of Kubernetes, whose **general availability** (**GA**) was in
    March 2018, most of the initial shortcomings compared to other orchestrators such
    as Docker Swarm have been eliminated. For example, security and confidentiality
    is now not only an afterthought, but an integral part of the system.
  prefs: []
  type: TYPE_NORMAL
- en: New features are implemented at a tremendous speed. New releases are happening
    every 3 months or so, more precisely, about every 100 days. Most of the new features
    are demand-driven, that is, companies using Kubernetes to orchestrate their mission-critical
    applications can voice their needs. This makes Kubernetes enterprise-ready. It
    would be wrong to assume that this orchestrator is only for start-ups and not
    for risk-averse enterprises. The contrary is the case. *On what do I base this
    claim?* Well, my claim is justified by the fact that companies such as Microsoft,
    Docker, and Red Hat, whose clients are mostly big enterprises, have fully embraced
    Kubernetes, and provide enterprise-grade support for it if it is used and integrated
    into their enterprise offerings.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports both Linux and Windows containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is well known that Docker popularized and commoditized software containers.
    Docker did not invent containers, but standardized them and made them broadly
    available, not least by offering the free image registry—Docker Hub. Initially,
    Docker focused mainly on the developer and the development life cycle. However,
    companies that started to use and love containers soon also wanted to use them
    not just during the development or testing of new applications, but also to run
    those applications in production.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, Docker had nothing to offer in that space, so other companies jumped
    into that vacuum and offered help to the users. But it didn't take long, and Docker
    recognized that there was a huge demand for a simple yet powerful orchestrator.
    Docker's first attempt was a product called classic swarm. It was a standalone
    product that enabled users to create a cluster of Docker host machines that could
    be used to run and scale their containerized applications in a highly available
    and self-healing way.
  prefs: []
  type: TYPE_NORMAL
- en: The setup of a classic Docker swarm, though, was hard. A lot of complicated
    manual steps were involved. Customers loved the product, but struggled with its
    complexity. So, Docker decided it could do better. It went back to the drawing
    board and came up with SwarmKit. SwarmKit was introduced at DockerCon 2016 in
    Seattle, and was an integral part of the newest version of the Docker engine.
    Yes, you got that right; SwarmKit was, and still is to this day, an integral part
    of the Docker engine. Thus, if you install a Docker host, you automatically have
    SwarmKit available with it.
  prefs: []
  type: TYPE_NORMAL
- en: SwarmKit was designed with simplicity and security in mind. The mantra was,
    and still is, that it has to be almost trivial to set up a swarm, and that the
    swarm has to be highly secure out of the box. Docker Swarm operates on the assumption
    of least privilege.
  prefs: []
  type: TYPE_NORMAL
- en: Installing a complete, highly available Docker swarm is literally as simple
    as starting with `docker swarm init` on the first node in the cluster, which becomes
    the so-called leader, and then `docker swarm join <join-token>` on all other nodes.
    `join-token` is generated by the leader during initialization. The whole process
    takes fewer that 5 minutes on a swarm with up to 10 nodes. If it is automated,
    it takes even less time.
  prefs: []
  type: TYPE_NORMAL
- en: As I already mentioned, security was top on the list of must-haves when Docker
    designed and developed SwarmKit. Containers provide security by relying on Linux kernel namespaces
    and cgroups, as well as Linux syscall whitelisting (seccomp), and the support
    of Linux capabilities and the **Linux security module** (**LSM**). Now, on top
    of that, SwarmKit adds MTLS and secrets that are encrypted at rest and in transit.
    Furthermore, Swarm defines the so-called **container network model** (**CNM**),
    which allows for SDNs that provide sandboxing for application services that are
    running on the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Docker SwarmKit supports both Linux and Windows containers.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos and Marathon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Mesos is an open source project, and was originally designed to make
    a cluster of servers or nodes look like one single big server from the outside.
    Mesos is software that makes the management of computer clusters simple. Users
    of Mesos should not have to care about individual servers, but just assume they
    have a gigantic pool of resources at their disposal, which corresponds to the
    aggregate of all the resources of all the nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos, in IT terms, is already pretty old, at least compared to the other orchestrators.
    It was first publicly presented in 2009, but at that time, of course, it wasn't
    designed to run containers, since Docker didn't even exist yet. Similar to what
    Docker does with containers, Mesos uses Linux cgroups to isolate resources such
    as CPU, memory, or disk I/O for individual applications or services.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos is really the underlying infrastructure for other interesting services
    built on top of it. From the perspective of containers specifically, Marathon is
    important. Marathon is a container orchestrator that runs on top of Mesos, which
    is able to scale to thousands of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Marathon supports multiple container runtimes, such as Docker or its own Mesos
    containers. It supports not only stateless, but also stateful, application services,
    for example, databases such as PostgreSQL or MongoDB. Similar to Kubernetes and
    Docker SwarmKit, it supports many of the features that were described earlier
    in this chapter, such as high availability, health checks, service discovery,
    load balancing, and location awareness, to name but a few of the most important
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Although Mesos and, to a certain extent, Marathon, are rather mature projects,
    their reach is relatively limited. It seems to be most popular in the area of
    big data, that is, to run data-crunching services such as Spark or Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ECS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are looking for a simple orchestrator and have already heavily bought
    into the AWS ecosystem, then Amazon''s ECS might be the right choice for you.
    It is important to point out one very important limitation of ECS: if you buy
    into this container orchestrator, then you lock yourself into AWS. You will not
    be able to easily port an application that is running on ECS to another platform
    or cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon promotes its ECS service as a highly scalable, fast container management
    service that makes it easy to run, stop, and manage Docker containers on a cluster.
    Next to running containers, ECS gives direct access to many other AWS services
    from the application services that run inside the containers. This tight and seamless
    integration with many popular AWS services is what makes ECS compelling for users
    who are looking for an easy way to get their containerized applications up and
    running in a robust and highly scalable environment. Amazon also provides its
    own private image registry.
  prefs: []
  type: TYPE_NORMAL
- en: With AWS ECS, you can use Fargate to have it fully manage the underlying infrastructure,
    allowing you to concentrate exclusively on deploying containerized applications,
    and you do not have to care about how to create and manage a cluster of nodes.
    ECS supports both Linux and Windows containers.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, ECS is simple to use, highly scalable, and well integrated with
    other popular AWS services; but it is not as powerful as, say, Kubernetes or Docker
    SwarmKit, and it is only available on Amazon AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft ACS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to what we said about ECS, we can claim the same for Microsoft''s ACS.
    It is a simple container orchestration service that makes sense if you are already
    heavily invested in the Azure ecosystem. I should say the same as I have pointed
    out for Amazon ECS: if you buy into ACS, then you lock yourself in to the offerings
    of Microsoft. It will not be easy to move your containerized applications from
    ACS to any other platform or cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: ACS is Microsoft's container service, which supports multiple orchestrators
    such as Kubernetes, Docker Swarm, and Mesos DC/OS. With Kubernetes becoming more
    and more popular, the focus of Microsoft has clearly shifted to that orchestrator.
    Microsoft has even rebranded its service and called it **Azure Kubernetes Service** (**AKS**)
    in order to put the focus on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'AKS manages, for you, a hosted Kubernetes or Docker Swarm or DC/OS environment
    in Azure, so that you can concentrate on the applications that you want to deploy,
    and you don''t have to care about configuring the infrastructure. Microsoft, in
    its own words, claims the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"AKS makes it quick and easy to deploy and manage containerized applications
    without container orchestration expertise. It also eliminates the burden of ongoing
    operations and maintenance by provisioning, upgrading, and scaling resources on
    demand, without taking your applications offline."'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated why orchestrators are needed in the first place, and
    how they conceptually work. It pointed out which orchestrators are the most prominent
    ones at the time of writing, and discussed the main commonalities and differences
    between the various orchestrators.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will introduce Docker’s native orchestrator, SwarmKit. It will
    elaborate on all the concepts and objects that SwarmKit uses to deploy and run
    a distributed, resilient, robust, and highly available application in a cluster—on-premises
    or in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to assess your learning progress:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need an orchestrator? Provide two or three reasons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name three to four typical responsibilities of an orchestrator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name at least two container orchestrators, as well as the main sponsors behind
    them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following links provide some deeper insight into orchestration-related
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes—production-grade orchestration:[https://kubernetes.io/.](https://kubernetes.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of Docker Swarm mode:[https://docs.docker.com/engine/swarm/.](https://docs.docker.com/engine/swarm/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marathon, A container orchestration platform for Mesos and DC/OS: [https://](https://mesosphere.github.io/marathon/)[mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers and orchestration are explained:[http://bit.ly/2DFoQgx.](https://bit.ly/2npjrEl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

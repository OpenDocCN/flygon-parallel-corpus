- en: Zero-Downtime Deployments and Secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored Docker Swarm and its resources in detail.
    We learned how to build a highly available swarm locally and in the cloud. Then,
    we discussed Swarm services and stacks in depth. Finally, we created services
    and stacks in the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will show you how we can update services and stacks running
    in Docker Swarm without interrupting their availability. This is called zero-downtime
    deployment. We are also going to introduce swarm secrets as a means to securely
    provide sensitive information to containers of a service using those secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing configuration data in the swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting sensitive data with Docker Secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After finishing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: List two to three different deployment strategies commonly used to update a
    service without downtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update a service in batches without causing a service interruption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a rollback strategy for a service that is used if an update fails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store non-sensitive configuration data using Docker configs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a Docker secret with a service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the value of a secret without causing downtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code files for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition).
    If you have checked out the repository as indicated in [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml),
    *Setting up a Working Environment*, then you'll find the code at `~/fod-solution/ch14`.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important aspects of a mission-critical application that needs
    frequent updates is the ability to do updates in a fashion that requires no outage
    at all. We call this a zero-downtime deployment. At all times, the application
    that is updated must be fully operational.
  prefs: []
  type: TYPE_NORMAL
- en: Popular deployment strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various ways to achieve this. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm supports rolling updates out of the box. The other two types of
    deployments can be achieved with some extra effort from our side.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a mission-critical application, each application service has to run in multiple
    replicas. Depending on the load, that can be as few as two to three instances
    and as many as dozens, hundreds, or thousands of instances. At any given time,
    we want to have a clear majority when it comes to all the service instances running.
    So, if we have three replicas, we want to have at least two of them up and running
    at all times. If we have 100 replicas, we can be content with a minimum of, say,
    90 replicas, being available. By doing this, we can define a batch size of replicas
    that we may take down to upgrade. In the first case, the batch size would be 1
    and in the second case, it would be 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we take replicas down, Docker Swarm will automatically take those instances
    out of the load balancing pool and all traffic will be load balanced across the
    remaining active instances. Those remaining instances will thus experience a slight
    increase in traffic. In the following diagram, prior to the start of the rolling
    update, if **Task A3** wanted to access **Service B**, it could have been load
    balanced to any of the three tasks of **Service B** by SwarmKit. Once the rolling
    update started, SwarmKit took down **Task B1** for updates. Automatically, this
    task is then taken out of the pool of targets. So, if **Task A3** now requests
    to connect to **Service B**, load balancing will only select from the remaining
    tasks, that is, **B2** and **B3**. Thus, those two tasks might experience a higher
    load temporarily:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b5692dbe-f8b2-4050-bc4b-04147a063825.png)'
  prefs: []
  type: TYPE_IMG
- en: Task B1 is taken down to be updated
  prefs: []
  type: TYPE_NORMAL
- en: The stopped instances are then replaced by an equivalent number of new instances
    of the new version of the application service. Once the new instances are up and
    running, we can have the Swarm observe them for a given period of time and make
    sure they're healthy. If all is well, then we can continue by taking down the
    next batch of instances and replacing them with instances of the new version.
    This process is repeated until all the instances of the application service have
    been replaced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see that **Task B1** of **Service B** has
    been updated to version 2\. The container of **Task B1** was assigned a new **IP**
    address, and it was deployed to another worker node with free resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2e0094c4-5dce-4763-8401-394a87cc79b3.png)'
  prefs: []
  type: TYPE_IMG
- en: The first batch being updated in a rolling update
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that when the task of a service is updated, in
    most cases, it gets deployed to a different worker node than the one it used to
    live on. But that should be fine as long as the corresponding service is stateless.
    If we have a stateful service that is location- or node-aware and we'd like to
    update it, then we have to adjust our approach, but this is outside of the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how we can actually instruct the Swarm to perform a rolling
    update of an application service. When we declare a service in a stack file, we
    can define multiple options that are relevant in this context. Let''s look at
    a snippet of a typical stack file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we can see a section, `update_config`, with the `parallelism`
    and `delay` properties. `parallelism` defines the batch size of how many replicas
    are going to be updated at a time during a rolling update. `delay` defines how
    long Docker Swarm is going to wait between updating individual batches. In the
    preceding case, we have `10` replicas that are being updated in two instances
    at a time and, between each successful update, Docker Swarm waits for `10` seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Let's test such a rolling update. Navigate to the `ch14` subfolder of our `labs`
    folder and use the `stack.yaml` file to create a web service that's been configured
    for a rolling update. The service uses an Alpine-based Nginx image whose version
    is `1.12-alpine`. We will update the service to a newer version, that is, `1.13-alpine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will deploy this service to our swarm that we created locally
    in VirtualBox. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to make sure that we have our Terminal window configured so
    that we can access one of the master nodes of our cluster. Let''s take the leader,
    that is, `node-1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can deploy the service using the stack file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1f326e7f-883f-4cc7-b643-3844164cc739.png)Deployment of the web stack'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the service has been deployed, we can monitor it using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/909a831e-a9a3-4ae8-98b1-addeb1ac75a7.png)Service web of the web
    stack running in Swarm with 10 replicasIf you''re working on a macOS machine,
    you need to make sure your watch tool is installed. Use the `brew install watch`
    command to do so.'
  prefs: []
  type: TYPE_NORMAL
- en: The previous command will continuously update the output and provide us with
    a good overview of what happens during the rolling update.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to open a second Terminal and configure it for remote access for
    the manager node of our swarm. Once we have done that, we can execute the `docker`
    command, which will update the image of the `web` service of the stack, also called
    `web`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command leads to the following output, indicating the progress
    of the rolling update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/80e17241-6cbe-414b-b393-b874ba9f475a.png)Screen showing the progress
    of the rolling update'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output indicates that the first two batches, each with two tasks,
    have been successful and that the third batch is preparing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first Terminal window, where we''re watching the stack, we should now
    see how Docker Swarm updates the service batch by batch with an interval of `10
    seconds`. After the first batch, it should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0c203143-48d9-4eb6-8207-bf098224f2d8.png)Rolling update for a service
    in Docker Swarm'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see that the first batch of the two tasks,
    `8` and `9`, has been updated. Docker Swarm is waiting for `10 seconds` to proceed
    with the next batch.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to note that in this particular case, SwarmKit deploys the
    new version of the task to the same node as the previous version. This is accidental
    since we have five nodes and two tasks on each node. SwarmKit always tries to
    balance the workload evenly across the nodes. So, when SwarmKit takes down a task,
    the corresponding node has a smaller workload than all the others, so the new
    instance is scheduled to it. Normally, you cannot expect to find the new instance
    of a task on the same node. Just try it out yourself by deleting the stack with
    `docker stack rm web` and changing the number of replicas to say, seven, and then
    redeploy and update it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the tasks have been updated, the output of our `docker stack ps web`
    command will look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4def2ed1-1be6-4416-9e98-041b0ec8d8d6.png)All tasks have been updated
    successfully'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that SwarmKit does not immediately remove the containers of the
    previous versions of the tasks from the corresponding nodes. This makes sense
    as we might want to, for example, retrieve the logs from those containers for
    debugging purposes, or we might want to retrieve their metadata using `docker
    container inspect`. SwarmKit keeps the four latest terminated task instances around
    before it purges older ones so that it doesn't clog the system with unused resources.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `--update-order` parameter to instruct Docker to start the new
    container replica before stopping the old one. This can improve application availability.
    Valid values are `"start-first"` and `"stop-first"`. The latter is the default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''re done, we can tear down the stack using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Although using stack files to define and deploy applications is the recommended
    best practice, we can also define the update behavior in a service `create` statement.
    If we just want to deploy a single service, this might be the preferred way of
    doing things. Let''s look at such a `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command defines the same desired state as the preceding stack file. We
    want the service to run with `10` replicas and we want a rolling update to happen
    in batches of two tasks at a time, with a 10-second interval between consecutive
    batches.
  prefs: []
  type: TYPE_NORMAL
- en: Health checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make informed decisions, for example, during a rolling update of a Swarm
    service regarding whether or not the just-installed batch of new service instances
    is running OK or if a rollback is needed, the SwarmKit needs a way to know about
    the overall health of the system. On its own, SwarmKit (and Docker) can collect
    quite a bit of information. But there is a limit. Imagine a container containing
    an application. The container, as seen from the outside, can look absolutely healthy
    and carry on just fine. But that doesn't necessarily mean that the application
    running inside the container is also doing well. The application could, for example,
    be in an infinite loop or be in a corrupt state, yet still running. However, as
    long as the application runs, the container runs and from outside, everything
    looks perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, SwarmKit provides a seam where we can provide it with some help. We, the
    authors of the application services running inside the containers in the swarm,
    know best as to whether or not our service is in a healthy state. SwarmKit gives
    us the opportunity to define a command that is executed against our application
    service to test its health. What exactly this command does is not important to
    Swarm; the command just needs to return `OK`, `NOT OK`, or `time out`. The latter
    two situations, namely `NOT OK` or `timeout`, will tell SwarmKit that the task
    it is investigating is potentially unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I am writing potentially on purpose and later, we will see why:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet from a `Dockerfile`, we can see the keyword `HEALTHCHECK`.
    It has a few options or parameters and an actual command, that is, `CMD`. Let''s
    discuss the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--interval`: Defines the wait time between health checks. Thus, in our case,
    the orchestrator executes a check every `30` seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--timeout`: This parameter defines how long Docker should wait if the health
    check does not respond until it times out with an error. In our sample, this is
    `10` seconds. Now, if one health check fails, SwarmKit retries a couple of times
    until it gives up and declares the corresponding task as unhealthy and opens the
    door for Docker to kill this task and replace it with a new instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of retries is defined with the `--retries` parameter. In the preceding
    code, we want to have three retries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have the start period. Some containers take some time to start up (not
    that this is a recommended pattern, but sometimes it is inevitable). During this
    startup time, the service instance might not be able to respond to health checks.
    With the start period, we can define how long SwarmKit should wait before it executes
    the very first health check and thus give the application time to initialize.
    To define the startup time, we use the `--start-period` parameter. In our case,
    we do the first check after `60` seconds. How long this start period needs to
    be depends on the application and its startup behavior. The recommendation is
    to start with a relatively low value and if you have a lot of false positives
    and tasks that are restarted many times, you might want to increase the time interval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we define the actual probing command on the last line with the `CMD`
    keyword. In our case, we are defining a request to the `/health` endpoint of `localhost`
    at port `3000` as a probing command. This call is expected to have three possible
    outcomes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command succeeds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command fails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command times out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latter two are treated the same way by SwarmKit. This is the orchestrator
    telling us that the corresponding task might be unhealthy. I did say *might *with
    intent since SwarmKit does not immediately assume the worst-case scenario but
    assumes that this might just be a temporary fluke of the task and that it will
    recover from it. This is the reason why we have a `--retries` parameter. There,
    we can define how many times SwarmKit should retry before it can assume that the
    task is indeed unhealthy, and consequently kill it and reschedule another instance
    of this task on another free node to reconcile the desired state of the service.
  prefs: []
  type: TYPE_NORMAL
- en: '*Why can we use localhost in our probing command?* This is a very good question,
    and the reason is because SwarmKit, when probing a container running in the Swarm,
    executes this `probing` command inside the container (that is, it does something
    like `docker container exec <containerID> <probing command>`). Thus, the command
    executes in the same network namespace as the application running inside the container.
    In the following diagram, we can see the life cycle of a service task from its
    beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/04607fa9-4a95-4188-9437-5db991b5d3b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Service task with transient health failure
  prefs: []
  type: TYPE_NORMAL
- en: First, SwarmKit waits to probe until the start period is over. Then, we have
    our first health check. Shortly thereafter, the task fails when probed. It fails
    two consecutive times but then it recovers. Thus, **health check 4** is successful
    and SwarmKit leaves the task running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see a task that is permanently failing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/69eb3697-f61f-446c-b5cc-3c5d36bbe6d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Permanent failure of a task
  prefs: []
  type: TYPE_NORMAL
- en: 'We have just learned how we can define a health check for a service in the
    `Dockerfile` of its image. But this is not the only way we can do this. We can
    also define the health check in the stack file that we use to deploy our application
    into Docker Swarm. Here is a short snippet of what such a stack file would look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we can see how the health check-related information
    is defined in the stack file. First and foremost, it is important to realize that
    we have to define a health check for every service individually. There is no health
    check at an application or global level.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to what we defined previously in the `Dockerfile`, the command that
    is used to execute the health check by SwarmKit is `curl -f http://localhost:3000/health`.
    We also have definitions for `interval`, `timeout`, `retries`, and `start_period`.
    These four key-value pairs have the same meaning as the corresponding parameters
    we used in the `Dockerfile`. If there are health check-related settings defined
    in the image, then the ones defined in the stack file override the ones from the
    `Dockerfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to use a service that has a health check defined. In our `lab`
    folder, we have a file called `stack-health.yaml` with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s deploy this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can find out where the single task was deployed to using `docker stack ps
    myapp`. On that particular node, we can list all the containers to find one of
    our stacks. In my example, the task had been deployed to `node-3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b39744ac-ae71-456f-b8d1-3b34c99837e2.png)Displaying the health status
    of a running task instance'
  prefs: []
  type: TYPE_NORMAL
- en: The interesting thing in this screenshot is the `STATUS` column. Docker, or
    more precisely, SwarmKit, has recognized that the service has a health check function
    defined and is using it to determine the health of each task of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Rollback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, things don't go as expected. A last-minute fix in an application
    release may have inadvertently introduced a new bug, or the new version significantly
    decreases the throughput of the component, and so on. In such cases, we need to
    have a plan B, which in most cases means the ability to roll back the update to
    the previous good version.
  prefs: []
  type: TYPE_NORMAL
- en: As with the update, the rollback has to happen in such a way that it does not
    cause any outages in terms of the application; it needs to cause zero-downtime.
    In that sense, a rollback can be looked at as a reverse update. We are installing
    a new version, yet this new version is actually the previous version.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the update behavior, we can declare, either in our stack files or in
    the Docker service `create` command, how the system should behave in case it needs
    to execute a rollback. Here, we have the stack file that we used previously, but
    this time with some rollback-relevant attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this stack file, which is available in our lab as `stack-rollback.yaml`,
    we defined the details about the rolling update, the health checks, and the behavior
    during rollback. The health check is defined so that after an initial wait time
    of `2` seconds, the orchestrator starts to poll the service on `http://localhost`
    every `2` seconds and it retries `3` times before it considers a task as unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: If we do the math, then it takes at least 8 seconds until a task will be stopped
    if it is unhealthy due to a bug. So, now under deploy, we have a new entry called
    `monitor`. This entry defines how long newly deployed tasks should be monitored
    for health and whether or not to continue with the next batch in the rolling update.
    Here, in this sample, we have given it `10` seconds. This is slightly more than
    the 8 seconds we calculated it takes to discover that a defective service has
    been deployed, so this is good.
  prefs: []
  type: TYPE_NORMAL
- en: We also have a new entry, `failure_action`, which defines what the orchestrator
    will do if it encounters a failure during the rolling update, such as that the
    service is unhealthy. By default, the action is just to stop the whole update
    process and leave the system in an intermediate state. The system is not down
    since it is a rolling update and at least some healthy instances of the service
    are still operational, but an operations engineer would be better at taking a
    look and fixing the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have defined the action to be a `rollback`. Thus, in case of
    failure, SwarmKit will automatically revert all tasks that have been updated back
    to their previous version.
  prefs: []
  type: TYPE_NORMAL
- en: Blue–green deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml), *Distributed Application
    Architecture*, we discussed what blue-green deployments are, in an abstract way.
    It turns out that, on Docker Swarm, we cannot really implement blue-green deployments
    for arbitrary services. The service discovery and load balancing between two services
    running in Docker Swarm are part of the Swarm routing mesh and cannot be (easily)
    customized.
  prefs: []
  type: TYPE_NORMAL
- en: 'If **Service A** wants to call **Service B**, then Docker does this implicitly.
    Docker, given the name of the target service, will use the Docker **DNS** service
    to resolve this name to a **virtual IP** (**VIP**) address. When the request is
    then targeted at the **VIP**, the Linux **IPVS** service will do another lookup
    in the Linux kernel IP tables with the **VIP** and load balance the request to
    one of the physical IP addresses of the tasks of the service represented by the
    **VIP**, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f7103312-f96e-4301-8f61-6a75c5c74a43.png)'
  prefs: []
  type: TYPE_IMG
- en: How service discovery and load balancing work in Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no easy way to intercept this mechanism and replace
    it with a custom behavior. But this would be needed to allow for a true blue-green
    deployment of **Service B**, which is the target service in our example. As we
    will see in [Chapter 16](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml), *Deploying,
    Updating, and Securing an Application with Kubernetes,* Kubernetes is more flexible
    in this area.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, we can always deploy the public-facing services in a blue-green
    fashion. We can use interlock 2 and its layer 7 routing mechanism to allow for
    a true blue-green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technically speaking, rolling updates are a kind of canary release. But due
    to their lack of seams, where you could plug customized logic into the system,
    rolling updates are only a very limited version of canary releases.
  prefs: []
  type: TYPE_NORMAL
- en: True canary releases require us to have more fine-grained control over the update
    process. Also, true canary releases do not take down the old version of the service
    until 100% of the traffic has been funneled through the new version. In that regard,
    they are treated like blue-green deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In a canary release scenario, we don't just want to use things such as health
    checks as deciding factors regarding whether or not to funnel more and more traffic
    through the new version of the service; we also want to consider external input
    in the decision-making process, such as metrics that are collected and aggregated
    by a log aggregator or tracing information. An example that could be used as a
    decision-maker includes conformance to **service-level agreements** (**SLAs**),
    namely if the new version of the service shows response times that are outside
    of the tolerance band. This can happen if we add new functionality to an existing
    service, yet this new functionality degrades the response time.
  prefs: []
  type: TYPE_NORMAL
- en: Storing configuration data in the swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to store non-sensitive data such as configuration files in Docker
    Swarm, then we can use Docker configs. Docker configs are very similar to Docker
    secrets, which we will discuss in the next section. The main difference is that
    config values are not encrypted at rest, while secrets are. Docker configs can
    only be used in Docker Swarm, that is, they cannot be used in your non-Swarm development
    environment. Docker configs are mounted directly into the container's filesystem.
    Configuration values can either be strings or binary values up to a size of 500
    KB.
  prefs: []
  type: TYPE_NORMAL
- en: With the use of Docker configs, you can separate the configuration from Docker
    images and containers. This way, your services can easily be configured with environment-specific
    values. The production swarm environment has different configuration values than
    the staging swarm, which in turn has different config values than the development
    or integration environment.
  prefs: []
  type: TYPE_NORMAL
- en: We can add configs to services and also remove them from running services. Configs
    can even be shared among different services running in the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create some Docker configs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start with a simple string value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command creates the `Hello world` configuration value and uses
    it as input to the config named `hello-config`. The output of this command is
    the unique `ID` of this new config that's being stored in the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we got and use the list command to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the list command shows the `ID` and the `NAME` of the config
    we just created, as well as its `CREATED` and (last) updated time. But since configs
    are non-confidential, we can do more and even output the content of a config,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmmm, interesting. In the `Spec` subnode of the preceding JSON-formatted output,
    we have the `Data` key with a value of `SGVsbG8gd29ybGQK`. Didn''t we just say
    that the config data is not encrypted at rest? It turns out that the value is
    just our string encoded as `base64`, as we can easily verify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So far, so good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define a somewhat more complicated Docker config. Let''s assume
    we are developing a Java application. Java''s preferred way of passing configuration
    data to the application is the use of so-called `properties` files. A `properties`
    file is just a text file containing a list of key-value pairs. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a file called `my-app.properties` and add the following content
    to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and create a Docker config called `app.properties` from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use this (somewhat contrived) command to get the clear text value
    of the config we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a Docker service that uses the preceding config. For simplicity,
    we will be using the nginx image to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The interesting part in the preceding service `create` command is the line that
    contains `--config`. With this line, we're telling Docker to use the config named
    `app.properties` and mount it as a file at `/etc/my-app/conf/app.properties` inside
    the container. Furthermore, we want that file to have the mode `0440` assigned
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we can see that the only instance of the service is
    running on node `node-1`. On this node, I can now list the containers to get the
    `ID` of the nginx instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can `exec` into that container and output the value of the `/etc/my-app/conf/app.properties`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: No surprise here; this is exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker configs can, of course, also be removed from the swarm, but only if
    they are not being used. If we try to remove the config we were just using previously,
    without first stopping and removing the service, we would get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We get an error message in which Docker is nice enough to tell us that the config
    is being used by our service called `nginx`. This behavior is somewhat similar
    to what we are used to when working with Docker volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, first, we need to remove the service and then we can remove the config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note once more that Docker configs should never be used to
    store confidential data such as secrets, passwords, or access keys and key secrets.In
    the next section, we will discuss how to handle confidential data.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting sensitive data with Docker secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Secrets are used to work with confidential data in a secure way. Swarm secrets
    are secure at rest and in transit. That is, when a new secret is created on a
    manager node, and it can only be created on a manager node, its value is encrypted
    and stored in the raft consensus storage. This is why it is secure at rest. If
    a service gets a secret assigned to it, then the manager reads the secret from
    storage, decrypts it, and forwards it to all the containers who are instances
    of the swarm service that requested the secret. Since node-to-node communication
    in Docker Swarm uses mutual **transport layer security** (**TLS**), the secret
    value, although decrypted, is still secure in transit. The manager forwards the
    secret only to the worker nodes that a service instance is running on. Secrets
    are then mounted as files into the target container. Each secret corresponds to
    a file. The name of the secret will be the name of the file inside the container,
    and the value of the secret is the content of the respective file. Secrets are
    never stored on the filesystem of a worker node and are instead mounted using
    `tmpFS` into the container. By default, secrets are mounted into the container
    at `/run/secrets`, but you can change that to any custom folder.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that secrets will not be encrypted on Windows nodes
    since there is no concept similar to `tmpfs`. To achieve the same level of security
    that you would get on a Linux node, the administrator should encrypt the disk
    of the respective Windows node.
  prefs: []
  type: TYPE_NORMAL
- en: Creating secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s see how we can actually create a secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a secret called `sample-secret` with the `sample secret
    value` value. Please note the hyphen at the end of the `docker secret create`
    command. This means that Docker expects the value of the secret from standard
    input. This is exactly what we're doing by piping the `sample secret value` value
    into the `create` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use a file as the source for the secret value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the value of the secret with the name `other-secret` is read from a file
    called `~/my-secrets/secret-value.txt`. Once a secret has been created, there
    is no way to access the value of it. We can, for example, list all our secrets
    to get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2b31dcdd-9f1d-44eb-ac20-1d8263bf8f1c.png)List of all secrets'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this list, we can only see the `ID` and `NAME` of the secret, plus some
    other metadata, but the actual value of the secret is not visible. We can also
    use `inspect` on a secret, for example, to get more information about `other-secret`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/82cad794-8a35-44db-81d7-51a9bbd67b29.png)Inspecting a swarm secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even here, we do not get the value of the secret back. This is, of course,
    intentional: a secret is a secret and thus needs to remain confidential. We can
    assign labels to secrets if we want and we can even use a different driver to
    encrypt and decrypt the secret if we''re not happy with what Docker delivers out
    of the box.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a secret
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Secrets are used by services that run in the swarm. Usually, secrets are assigned
    to a service at creation time. Thus, if we want to run a service called `web`
    and assign it a secret, say, `api-secret-key`, the syntax would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a service called `web` based on the `fundamentalsofdocker/whoami:latest`
    image, publishes the container port `8000` to port `8000` on all swarm nodes,
    and assigns it the secret called `api-secret-key`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will only work if the secret called `api-secret-key` is defined in the
    swarm; otherwise, an error will be generated with the text `secret not found:
    api-secret-key`. Thus, let''s create this secret now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we rerun the service `create` command, it will succeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4dd798d2-4a33-41b6-ae25-39f71d55386e.png)Creating a service with
    a secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use `docker service ps web` to find out on which node the sole
    service instance has been deployed, and then `exec` into this container. In my
    case, the instance has been deployed to `node-3`, so I need to `SSH` into that
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, I list all my containers on that node to find the one instance belonging
    to my service and copy its `container ID`. We can then run the following command
    to make sure that the secret is indeed available inside the container under the
    expected filename containing the secret value in clear text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, in my case, this looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/397cb02b-4760-4f21-9a10-0364294c20b9.png)A secret as a container
    sees it'
  prefs: []
  type: TYPE_NORMAL
- en: 'If, for some reason, the default location where Docker mounts the secrets inside
    the container is not acceptable to you, you can define a custom location. In the
    following command, we mount the secret to `/app/my-secrets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In this command, we are using the extended syntax to define a secret that includes
    the destination folder.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating secrets in a development environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working in development, we usually don't have a local swarm on our machine.
    But secrets only work in a swarm. So, *what can we do*? Well, luckily, this answer
    is really simple. Due to the fact that secrets are treated as files, we can easily
    mount a volume that contains the secrets into the container to the expected location,
    which by default is at `/run/secrets`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a folder called `./dev-secrets` on our local workstation.
    For each secret, we have a file named the same as the secret name and with the
    unencrypted value of the secret as the content of the file. For example, we can
    simulate a secret called `demo-secret` with a secret value of `demo secret value`
    by executing the following command on our workstation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create a container that mounts this folder, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The process running inside the container will be unable to distinguish these
    mounted files from the ones originating from a secret. So, for example, `demo-secret`
    is available as a file called `/run/secrets/demo-secret` inside the container
    and has the expected value `demo secret value`. Let''s take a look at this in
    more detail in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this, we can `exec` a shell inside the preceding container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can navigate to the `/run/secrets` folder and display the content of
    the `demo-secret` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will be looking at secrets and legacy applications.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets and legacy applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we want to containerize a legacy application that we cannot easily,
    or do not want to, change. This legacy application might expect a secret value
    to be available as an environment variable. *How are we going to deal with this
    now?* Docker presents us with the secrets as files but the application is expecting
    them in the form of environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this situation, it is helpful to define a script that runs when the container
    is started (a so-called entry point or startup script). This script will read
    the secret value from the respective file and define an environment variable with
    the same name as the file, assigning the new variable the value read from the
    file. In the case of a secret called `demo-secret` whose value should be available
    in an environment variable called `DEMO_SECRET,` the necessary code snippet in
    this startup script could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, let''s say we have a legacy application that expects the secret
    values to be present as an entry in, say, a YAML configuration file located in
    the `/app/bin` folder and called `app.config`, whose relevant part looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Our initialization script now needs to read the secret value from the `secret`
    file and replace the corresponding placeholder in the config file with the `secret`
    value. For `demo-secret`, this could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we're using the `sed` tool to replace a placeholder
    with a value in place. We can use the same technique for the other two secrets
    in the config file.
  prefs: []
  type: TYPE_NORMAL
- en: We put all the initialization logic into a file called `entrypoint.sh`, make
    this file executable and, for example, add it to the root of the container's filesystem.
    Then, we define this file as `ENTRYPOINT` in the `Dockerfile`, or we can override
    the existing `ENTRYPOINT` of an image in the `docker container run` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make a sample. Let''s assume that we have a legacy application running
    inside a container defined by the `fundamentalsofdocker/whoami:latest` image that
    expects a secret called `db_password` to be defined in a file, `whoami.conf`,
    in the application folder. Let''s take a look at these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a file, `whoami.conf`, on our local machine that contains the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The important part is line 3 of this snippet. It defines where the secret value
    has to be put by the startup script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a file called `entrypoint.sh` to the local folder that contains
    the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The last line in the preceding script stems from the fact that this is the start
    command that was used in the original `Dockerfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, change the mode of this file to an executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, we define a `Dockerfile` that inherits from the `fundamentalsofdocker/whoami:latest`
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a file called `Dockerfile` to the current folder that contains the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build the image from this `Dockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the image has been built, we can run a service from it. But before we
    can do that, we need to define the secret in Swarm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a service that uses the following secret:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Updating secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At times, we need to update a secret in a running service since secrets could
    be leaked out to the public or be stolen by malicious people, such as hackers.
    In this case, we need to change our confidential data since the moment it is leaked
    to a non-trusted entity, it has to be considered as insecure.
  prefs: []
  type: TYPE_NORMAL
- en: Updating secrets, like any other update, has to happen in a way that requires
    zero-downtime. Docker SwarmKit supports us in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new secret in the swarm. It is recommended to use a versioning
    strategy when doing so. In our example, we use a version as a postfix of the secret
    name. We originally started with the secret named `db-password` and now the new
    version of this secret is called `db-password-v2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that the original service that used the secret had been created
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The application running inside the container was able to access the secret
    at `/run/secrets/db-password`. Now, SwarmKit does not allow us to update an existing
    secret in a running service, so we have to remove the now obsolete version of
    the secret and then add the new one. Let''s start with removal with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can add the new secret with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Please note the extended syntax of `--secret-add` with the `source` and `target`
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how SwarmKit allows us to update services without
    requiring downtime. We also discussed the current limits of SwarmKit in regard
    to zero-downtime deployments. In the second part of this chapter, we introduced
    secrets as a means to provide confidential data to services in a highly secure
    way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the currently most popular container
    orchestrator, Kubernetes. We'll discuss the objects that are used to define and
    run a distributed, resilient, robust, and highly available application in a Kubernetes
    cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that's
    used to locally deploy a Kubernetes application, and also demonstrate the integration
    of Kubernetes with Docker for macOS and Docker for Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your understanding of the topics that were discussed in this chapter,
    please answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: In a few simple sentences, explain to an interested layman what zero-downtime
    deployment means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does SwarmKit achieve zero-downtime deployments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contrary to traditional (non-containerized) systems, why does a rollback in
    Docker Swarm just work? Explain this in a few short sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe two to three characteristics of a Docker secret.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You need to roll out a new version of the `inventory` service. What does your
    command look like? Here is some more information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new image is called `acme/inventory:2.1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to use a rolling update strategy with a batch size of two tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want the system to wait for one minute after each batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to update an existing service named `inventory` with a new password
    that is provided through a Docker secret. The new secret is called `MYSQL_PASSWORD_V2`.
    The code in the service expects the secret to be called `MYSQL_PASSWORD`. What
    does the update command look like? (Note that we do not want the code of the service
    to be changed!)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some links to external sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply rolling updates to a service, at [https://dockr.ly/2HfGjlD](https://dockr.ly/2HfGjlD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing sensitive data with Docker secrets, at [https://dockr.ly/2vUNbuH](https://dockr.ly/2vUNbuH)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Docker secrets management, at [https://dockr.ly/2k7zwzE](https://dockr.ly/2k7zwzE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From env variables to Docker secrets, at [https://bit.ly/2GY3UUB](https://bit.ly/2GY3UUB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

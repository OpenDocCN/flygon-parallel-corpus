- en: Deploying, Updating, and Securing an Application with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the basics of the container orchestrator,
    Kubernetes. We got a high-level overview of the architecture of Kubernetes and
    learned a lot about the important objects used by Kubernetes to define and manage
    a containerized application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to deploy, update, and scale applications
    into a Kubernetes cluster. We will also explain how zero downtime deployments
    are achieved to enable disruption-free updates and rollbacks of mission-critical
    applications. Finally, we will introduce Kubernetes secrets as a means to configure
    services and protect sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a first application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining liveness and readiness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero downtime deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After working through this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy a multi-service application into a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a liveness and readiness probe for your Kubernetes application service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update an application service running in Kubernetes without causing downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define secrets in a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure an application service to use Kubernetes secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to use Minikube on our local computer. Please refer
    to [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting Up a Working
    Environment*, for more information on how to install and use Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found here: [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch16/probes](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch16/probes).'
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure you have cloned this book's GitHub repository, as described
    in [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting Up a Working
    Environment*.
  prefs: []
  type: TYPE_NORMAL
- en: In your Terminal, navigate to the `~/fod/ch16` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a first application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take our pets application, which we first introduced in [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml), *Docker
    Compose*, and deploy it into a Kubernetes cluster. Our cluster will be Minikube,
    which, as you know, is a single-node cluster. However, from the perspective of
    a deployment, it doesn't really matter how big the cluster is and where the cluster
    is located in the cloud, in your company's data center, or on your personal workstation.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the web component
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just as a reminder, our application consists of two application services: the
    Node-based web component and the backing PostgreSQL database. In the previous
    chapter, we learned that we need to define a Kubernetes Deployment object for
    each application service we want to deploy. Let''s do this first for the web component.
    As always in this book, we will choose the declarative way of defining our objects.
    Here is the YAML defining a Deployment object for the web component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f43630a9-a410-44cf-a9f1-bcd87d583f54.png)Kubernetes deployment definition
    for the web component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding deployment definition can be found in the `web-deployment.yaml` file
    in the `~/fod/ch16` folder. The lines of code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On line `4`: We define the name for our `Deployment` object as `web`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On line `6`: We declare that we want to have one instance of the `web` component
    running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From line `8` to `10`: We define which pods will be part of our deployment,
    namely those that have the `app` and `service` labels with values of `pets` and `web`,
    respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On line `11`: In the template for the pods starting at line `11`, we define
    that each pod will have the `app` and `service` labels applied to them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From line `17`: We define the single container that will be running in the
    pod. The image for the container is our well-known `fundamentalsofdocker/ch11-web:2.0` image
    and the name of the container will be `web`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ports`: Finally, we declare that the container exposes port `3000` for TCP-type
    traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please make sure that you have set the context of kubectl to Minikube. See [Chapter
    2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting Up a Working Environment*, for
    details on how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can deploy this Deployment object using kubectl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can double-check that the deployment has been created again using our Kubernetes
    CLI. We should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b631548b-c83b-4421-a037-a931a77b9ba7.png)Listing all resources running
    in Minikube'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see that Kubernetes created three objects –
    the deployment, a pertaining ReplicaSet, and a single pod (remember that we specified
    that we want one replica only). The current state corresponds to the desired state
    for all three objects, so we are fine so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the web service needs to be exposed to the public. For this, we need to
    define a Kubernetes Service object of the `NodePort` type. Here is the definition,
    which can be found in the `web-service.yaml` file in the `~/fod/ch16` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aa181850-9aab-492a-a132-e99ecbb7f102.png)Definition of the Service
    object for our web component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding lines of codes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On line `4`: We set the `name` of this Service object to `web`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On line `6`: We define the `type` of Service object we''re using. Since the web component
    has to be accessible from outside of the cluster, this cannot be a Service object
    of the `ClusterIP` type and must be either of the `NodePort` or `LoadBalancer` type.
    We discussed the various types of Kubernetes services in the previous chapter,
    so will not go into further detail about this. In our sample, we''re using a `NodePort` type
    of service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On lines `8` and `9`: We specify that we want to expose port `3000` for access
    through the `TCP` protocol. Kubernetes will map container port `3000` automatically
    to a free host port in the range of 30,000 to 32,768\. Which port Kubernetes effectively
    chooses can be determined using the `kubectl` get service or `kubectl` describe command for
    the service after it has been created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From line `10` to `12`: We define the filter criteria for the pods that this
    service will be a stable endpoint for. In this case, it is all the pods that have
    the `app` and `service` labels with the `pets` and `web` values, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have this specification for a Service object, we can create it
    using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list all the services to see the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/68357689-66d7-4587-97d9-369552a5fe75.png)The Service object created
    for the web component'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see that a service called `web` has been created.
    A unique clusterIP of `10.99.99.133` has been assigned to this service, and the
    container port `3000` has been published on port `31331` on all cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to test this deployment, we need to find out what IP address Minikube
    has, and then use this IP address to access our web service. The following is
    the command that we can use to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: OK, the response is `Pets Demo Application`, which is what we expected. The
    web service is up and running in the Kubernetes cluster. Next, we want to deploy
    the database.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A database is a stateful component and has to be treated differently to stateless
    components, such as our web component. We discussed the difference between stateful
    and stateless components in a distributed application architecture in detail in [Chapter
    9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml), *Distributed Application Architecture*, and [Chapter
    12](27c0d9ce-fab6-4ce9-9034-4f2fb62931e8.xhtml), *Orchestrators*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has defined a special type of `ReplicaSet` object for stateful components.
    The object is called a `StatefulSet`. Let''s use this kind of object to deploy
    our database. The definition can be found in the `~fod/ch16/db-stateful-set.yaml` file.
    The details are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a0e35643-c85e-4f8d-8e9c-b62a372a42dd.png)A StatefulSet for the DB
    component'
  prefs: []
  type: TYPE_NORMAL
- en: OK, this looks a bit scary, but it isn't. It is a bit longer than the definition
    of the deployment for the `web` component due to the fact that we also need to
    define a volume where the PostgreSQL database can store the data. The volume claim
    definition is on lines `25` to `33`. We want to create a volume with the name `pets-data` that
    has a maximum size equal to `100 MB`. On lines `22` to `24`, we use this volume and mount
    it into the container at `/var/lib/postgresql/data`, where PostgreSQL expects
    it. On line `21`, we also declare that PostgreSQL is listening at port `5432`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we use kubectl to deploy the `StatefulSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we list all the resources in the cluster, we will be able to see the
    additional objects that were created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/65326383-101f-44f5-a370-5e936b3933fd.png)'
  prefs: []
  type: TYPE_IMG
- en: The StatefulSet and its pod
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that a `StatefulSet` and a pod have been created. For both,
    the current state corresponds to the desired state and thus the system is healthy.
    But that doesn't mean that the web component can access the database at this time.
    Service discovery won't work so far. Remember that the web component wants to
    access the `db` service under the name `db`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make service discovery work inside the cluster, we have to define a Kubernetes Service object
    for the database component too. Since the database should only ever be accessible
    from within the cluster, the type of Service object we need is `ClusterIP`. Here
    is the specification, which can be found in the `~/fod/ch16/db-service.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/50834c86-6427-4c33-b811-089547e1aef2.png)Definition of the Kubernetes
    Service object for the database'
  prefs: []
  type: TYPE_NORMAL
- en: 'The database component will be represented by this Service object and it can
    be reached by the name `db`, which is the name of the service, as defined on line
    `4`. The database component does not have to be publicly accessible, so we decided
    to use a Service object of the `ClusterIP` type. The selector on lines `10` to
    `12` defines that this service represents a stable endpoint for all the pods that
    have the according labels defined, that is, `app: pets` and `service: db`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy this service with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we should be ready to test the application. We can use the browser this
    time to enjoy the beautiful animal images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5a43f3ff-7ac5-4b0e-af24-e2ceb59bf180.png)Testing the pets application
    running in Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: '`172.29.64.78` is the IP address of my Minikube. Verify your address using
    the `minikube ip` command. Port number `32722` is the number that Kubernetes automatically
    selected for my `web` Service object. Replace this number with the port that Kubernetes
    assigned to your service. You can get the number by using the `kubectl get services` command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have successfully deployed the pets application to Minikube, which
    is a single-node Kubernetes cluster. We had to define four artifacts to do so,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A Deployment and a Service object for the web component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A StatefulSet and a Service object for the database component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To remove the application from the cluster, we can use the following small
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will be streamlining the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Streamlining the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have created four artifacts that needed to be deployed to the cluster.
    This is only a very simple application, consisting of two components. Imagine
    having a much more complex application. It would quickly become a maintenance
    nightmare. Luckily, we have several options as to how we can simplify the deployment.
    The method that we are going to discuss here is the possibility of defining all
    the components that make up an application in Kubernetes in a single file.
  prefs: []
  type: TYPE_NORMAL
- en: Other solutions that lie outside of the scope of this book would include the
    use of a package manager, such as Helm.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have an application consisting of many Kubernetes objects such as `Deployment` and `Service` objects,
    then we can keep them all in one single file and separate the individual object definitions by
    three dashes. For example, if we wanted to have the `Deployment` and the `Service`
    definition for the `web` component in a single file, this would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have collected all four object definitions for the `pets` application
    in the `~/fod/ch16/pets.yaml` file, and we can deploy the application in one go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/76a4ab56-40ed-4d1b-b372-5267d4702597.png)Using a single script to
    deploy the pets application'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we have created a script called `~/fod/ch16/remove-pets.sh` to remove
    all the artifacts of the pets application from the Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5670017a-1e32-4999-8ee1-79d171f3830b.png)Removing pets from the
    Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have taken our pets application we introduced in [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml), *Docker
    Compose*, and defined all the Kubernetes objects that are necessary to deploy
    this application into a Kubernetes cluster. In each step, we have made sure that
    we got the expected result, and once all the artifacts existed in the cluster,
    we showed the running application.
  prefs: []
  type: TYPE_NORMAL
- en: Defining liveness and readiness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Container orchestration systems such as Kubernetes and Docker swarm make it
    significantly easier to deploy, run, and update highly distributed, mission-critical
    applications. The orchestration engine automates many of the cumbersome tasks
    such as scaling up or down, asserting that the desired state is maintained at
    all times, and more.
  prefs: []
  type: TYPE_NORMAL
- en: But, the orchestration engine cannot just do everything automagically. Sometimes,
    we developers need to support the engine with some information that only we can
    know about. So, what do I mean by that?
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a single application service. Let's assume it is a microservice
    and let's call it **service A**. If we run service A containerized on a Kubernetes
    cluster, then Kubernetes can make sure that we have the five instances that we
    require in the service definition running at all times. If one instance crashes,
    Kubernetes can quickly launch a new instance and thus maintain the desired state.
    But, what if an instance of the service does not crash, but is unhealthy or just
    not ready yet to serve requests? It is evident that Kubernetes should know about
    both situations. But it can't, since healthy or not from an application service
    perspective is outside of the knowledge of the orchestration engine. Only we application
    developers can know when our service is healthy and when it is not.
  prefs: []
  type: TYPE_NORMAL
- en: The application service could, for example, be running, but its internal state
    could have been corrupted due to some bug, it could be in an endless loop, or
    in a deadlock situation. Similarly, only we application developers know if our
    service is ready to work, or if it is still initializing. Although it is highly
    recommended to keep the initialization phase of a microservice as short as possible,
    it often cannot be avoided if there is a significant time span needed by a particular
    service so that it's ready to operate. Being in this state of initialization is
    not the same thing as being unhealthy, though. The initialization phase is an
    expected part of the life cycle of a microservice or any other application service.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, Kubernetes should not try to kill our microservice if it is in the initialization
    phase. If our microservice is unhealthy, though, Kubernetes should kill it as
    quickly as possible and replace it with a fresh instance.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has a concept of probes to provide the seam between the orchestration
    engine and the application developer. Kubernetes uses these probes to find out
    more about the inner state of the application service at hand. Probes are executed
    locally, inside each container. There is a probe for the health – also called
    liveness – of the service, a startup probe, and a probe for the readiness of the
    service. Let's look at them in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes liveness probe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes uses the liveness probe to decide when a container needs to be killed
    and when another instance should be launched instead. Since Kubernetes operates
    at a pod level, the respective pod is killed if at least one of its containers
    reports as being unhealthy. Alternatively, we can say it the other way around:
    only if all the containers of a pod report to be healthy, is the pod considered
    to be healthy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the liveness probe in the specification for a pod as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The relevant part is in the `livenessProbe` section. First, we define a command
    that Kubernetes will execute as a probe inside the container. In our case, we
    have a PostresSQL container and use the `netcat` Linux tool to probe port `5432`
    over TCP. The `nc localhost 5432` command is successful once Postgres listens
    at it.
  prefs: []
  type: TYPE_NORMAL
- en: The other two settings, `initialDelaySeconds` and `periodSeconds`, define how
    long Kubernetes should wait after starting the container until it first executes
    the probe and how frequently the probe should be executed thereafter. In our case,
    Kubernetes waits for 10 seconds prior to executing the first probe and then executes
    a probe every 5 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to probe an HTTP endpoint instead of using a command. Let''s
    assume we''re running a microservice from an image, `acme.com/my-api:1.0`, with
    an API that has an endpoint called `/api/health` that returns status `200 (OK)`
    if the microservice is healthy, and `50x (Error)` if it is unhealthy. Here, we
    can define the liveness probe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, I have defined the liveness probe so that it uses
    the HTTP protocol and executed a `GET` request to the `/api/health` endpoint on
    port `5000` of localhost. Remember, the probe is executed inside the container,
    which means I can use localhost.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also directly use the TCP protocol to probe a port on the container.
    But wait a second – didn''t we just do that in our first sample, where we used
    the generic liveness probe based on an arbitrary command? Yes, you''re right,
    we did. But we had to rely on the presence of the `netcat` tool in the container
    to do so. We cannot assume that this tool is always there. Thus, it is favorable
    to rely on Kubernetes to do the TCP-based probing for us out of the box. The modified
    pod spec looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This looks very similar. The only change is that the type of probe has been
    changed from `exec` to `tcpSocket` and that, instead of providing a command, we
    provide the `port` to probe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try this out:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `~/fod/ch16/probes` folder and build the Docker image with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `kubectl` to deploy the sample pod that''s defined in `probes-demo.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Describe the pod and specifically analyze the log part of the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'During the first half minute or so, you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fe42b23f-d21f-4e2a-bd16-6f14c31d71b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Log output of the healthy pod
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait at least 30 seconds and then describe the pod again. This time, you should
    see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/bb3c27de-375c-4cf1-8e23-81762ff9447f.png)Log output of the pod after
    it has changed its state to `Unhealthy`'
  prefs: []
  type: TYPE_NORMAL
- en: The last two lines are indicating the failure of the probe and the fact that
    the pod is going to be restarted.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you get the list of pods, you will see that the pod has been restarted a
    number of times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When you''re done with the sample, delete the pod with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will have a look at the Kubernetes readiness probe.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes readiness probe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes uses a readiness probe to decide when a service instance, that is,
    a container, is ready to accept traffic. Now, we all know that Kubernetes deploys
    and runs pods and not containers, so it only makes sense to talk about the readiness
    of a pod. Only if all containers in a pod report to be ready is the pod considered
    to be ready itself. If a pod reports not to be ready, then Kubernetes removes
    it from the service load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Readiness probes are defined exactly the same way as liveness probes: just
    switch the `livenessProbe` key in the pod spec to `readinessProbe`. Here is an
    example using our prior pod spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in this example, we don't really need an initial delay for the liveness
    probe anymore since we now have a readiness probe. Thus, I have replaced the initial
    delay entry for the liveness probe with an entry called `failureThreshold`, which
    is indicating how many times Kubernetes should repeat probing in case of a failure
    until it assumes that the container is unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes startup probe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often helpful for Kubernetes to know when a service instance has started.
    If we define a startup probe for a container, then Kubernetes does not execute
    the liveness or readiness probes, as long as the container's startup probe does
    not succeed. Once again, Kubernetes looks at pods and starts executing liveness
    and readiness probes on its containers if the startup probes of all the pod's
    containers succeed.
  prefs: []
  type: TYPE_NORMAL
- en: When would we use a startup probe, given the fact that we already have the liveness
    and readiness probes? There might be situations where we have to account for exceptionally
    long startup and initialization times, such as when containerizing a legacy application.
    We could technically configure the readiness or the liveness probes to account
    for this fact, but that would defeat the purpose of these probes. The latter probes
    are meant to provide quick feedback to Kubernetes on the health and availability
    of the container. If we configure for long initial delays or periods, then this
    would counter the desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsurprisingly, the startup probe is defined exactly the same way as the readiness
    and liveness probes. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that you define the `failureThreshold * periodSeconds` product so
    that it's big enough to account for the worst startup time.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the max startup time should not exceed 150 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Zero downtime deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a mission-critical environment, it is important that the application is
    always up and running. These days, we cannot afford any downtime anymore. Kubernetes
    gives us various means of achieving this. Performing an update on an application
    in the cluster that causes no downtime is called a zero downtime deployment. In
    this section, we will present two ways of achieving this. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by discussing rolling updates.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that the Kubernetes Deployment object distinguishes
    itself from the ReplicaSet object in that it adds rolling updates and rollbacks
    on top of the latter's functionality. Let's use our web component to demonstrate
    this. Evidently, we will have to modify the manifest or description of the deployment
    for the web component.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the same deployment definition as in the previous section, with
    one important difference – we will have five replicas of the web component running.
    The following definition can also be found in the `~/fod/ch16/web-deploy-rolling-v1.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create this deployment as usual and also, at the same time, the
    service that makes our component accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have deployed the pods and the service, we can test our web component
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the application is up and running and returns the expected message, `Pets
    Demo Application`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now. our developers have created a new version, 2.1, of the `web` component.
    The code of the new version of the `web` component can be found in the `~/fod/ch16/web` folder, and
    the only change is located on line `12` of the `server.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/70fa34d3-69f4-4f3a-a7ae-593b02df74f6.png)Code change for version
    2.0 of the web component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The developers have built the new image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Subsequently, they pushed the image to Docker Hub, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to update the image that''s used by our pods that are part of
    the web Deployment object. We can do this by using the `set image` command of `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If we test the application again, we''ll get a confirmation that the update
    has indeed happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, how do we know that there hasn''t been any downtime during this update? Did
    the update really happen in a rolling fashion? What does rolling update mean at
    all? Let''s investigate. First, we can get a confirmation from Kubernetes that
    the deployment has indeed happened and was successful by using the `rollout status` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we describe the deployment web with `kubectl describe deploy/web`, we get
    the following list of events at the end of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5fd7c6ba-dbce-4d6e-8d77-f6296c87dc06.png) List of events found in
    the output of the deployment description of the web component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first event tells us that, when we created the deployment, a ReplicaSet
    called `web-769b88f67` with five replicas was created. Then, we executed the update command.
    The second event in the list tells us that this meant creating a new ReplicaSet called `web-55cdf67cd` with,
    initially, one replica only. Thus, at that particular moment, six pods existed
    on the system: the five initial pods and one pod with the new version. But, since
    the desired state of the Deployment object states that we want five replicas only,
    Kubernetes now scales down the old ReplicaSet to four instances, which we can
    see in the third event.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, again, the new ReplicaSet is scaled up to two instances and, subsequently,
    the old ReplicaSet scaled was down to three instances, and so on, until we had
    five new instances and all the old instances were decommissioned. Although we
    cannot see any precise time (other than 3 minutes) when that happened, the order
    of the events tells us that the whole update happened in a rolling fashion.
  prefs: []
  type: TYPE_NORMAL
- en: During a short time period, some of the calls to the web service would have
    had an answer from the old version of the component, and some calls would have
    received an answer from the new version of the component, but, at no time would
    the service have been down.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also list the ReplicaSet objects in the cluster and will get confirmation
    of what I said in the preceding section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3a04d23d-d8d2-4f3b-a4d2-4235bac45448.png)Listing all the ReplicaSet
    objects in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the new ReplicaSet has five instances running and that
    the old one has been scaled down to zero instances. The reason that the old ReplicaSet object
    is still lingering is that Kubernetes provides us with the possibility of rolling
    back the update and, in that case, will reuse that ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To roll back the update of the image in case some undetected bug sneaked into
    the new code, we can use the `rollout undo` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'I have also listed the test command using `curl` in the preceding snippet to
    verify that the rollback indeed happened. If we list the ReplicaSets, we will
    see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7ef8b322-07e3-43e1-81a1-9bd7af4d1a3d.png)Listing ReplicaSet objects
    after rollback'
  prefs: []
  type: TYPE_NORMAL
- en: This confirms that the old ReplicaSet (`web-769b88f67`) object has been reused
    and that the new one has been scaled down to zero instances.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, we cannot, or do not want to, tolerate the mixed state of
    an old version coexisting with the new version. We want an *all-or-nothing* strategy.
    This is where blue-green deployments come into play, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to do a blue-green style deployment for our component web of the
    pets application, then we can do so by using labels creatively. First, let''s
    remind ourselves how blue-green deployments work. Here is a rough step-by-step
    instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy the first version of the `web` component as `blue`. We will label the
    pods with a label of `color: blue` to do so.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the Kubernetes service for these pods with the `color: blue` label in
    the selector section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can deploy version 2 of the web component, but, this time, the pods
    have a label of `color: green`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can test the green version of the service to check that it works as expected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we flip traffic from blue to green by updating the Kubernetes service
    for the web component. We modify the selector so that it uses the `color: green` label.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s define a Deployment object for version 1, blue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9f7a8ee5-54bb-4e8d-99f6-63deae81e7bd.png)Specification of the blue
    deployment for the web component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding definition can be found in the `~/fod/ch16/web-deploy-blue.yaml` file.
    Please take note of line `4`, where we define the name of the deployment as `web-blue` to
    distinguish it from the upcoming deployment, `web-green`. Also, note that we have
    added the label `color: blue` on lines `11` and `17`. Everything else remains
    the same as before.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the Service object for the web component. It will be the
    same as the one we used before but with a minor change, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7a1661a0-137e-402c-a664-a8ad9ea1df56.png)Kubernetes service for
    the web component supporting blue-green deployments'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference regarding the definition of the service we used earlier
    in this chapter is line `13`, which adds the `color: blue` label to the selector.
    We can find the preceding definition in the `~/fod/ch16/web-svc-blue-green.yaml` file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can deploy the blue version of the web component with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the service is up and running, we can determine its IP address and port
    number and test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, we get the response `Pets Demo Application`. Now, we can deploy
    the green version of the web component. The definition of its Deployment object
    can be found in the `~/fod/ch16/web-deploy-green.yaml` file and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b87b509f-c2ac-4dd6-bc19-5b7f55c9143a.png)Specification of the deployment
    green for the web component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting lines are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Line `4`: Named `web-green` to distinguish it from `web-blue` and allow for
    parallel installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lines `11` and `17`: Have the color `green`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Line `20`: Now using version `2.1` of the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we''re ready to deploy this green version of the service. It should run
    separately from the blue service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can make sure that both deployments coexist like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4bd95b70-62d5-4697-a3d4-e67b4383a4ae.png)Displaying the list of
    Deployment objects running in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, we have both blue and green running. We can verify that blue is
    still the active service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the interesting part. We can flip traffic from blue to green by editing
    the existing service for the web component. To do so, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the value of the label color from `blue` to `green`. Then, save and
    quit the editor. The Kubernetes CLI will automatically update the service. When
    we now query the web service again, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This confirms that the traffic has indeed switched to the green version of the
    web component (note the `v2` at the end of the response to the `curl` command).
  prefs: []
  type: TYPE_NORMAL
- en: If we realize that something went wrong with our green deployment and the new
    version has a defect, we can easily switch back to the blue version by editing
    the service web again and replacing the value of the label color with blue. This
    rollback is instantaneous and should always work. Then, we can remove the buggy
    green deployment and fix the component. When we have corrected the problem, we
    can deploy the green version once again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the green version of the component is running as expected and performing
    well, we can decommission the blue version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: When we're ready to deploy a new version, 3.0, this one becomes the blue version.
    We update the `~/fod/ch16/web-deploy-blue.yaml` file accordingly and deploy it.
    Then, we flip the service web from `green` to `blue`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully demonstrated, with our component web of the pets application,
    how blue-green deployment can be achieved in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, services that we want to run in the Kubernetes cluster have to use
    confidential data such as passwords, secret API keys, or certificates, to name
    just a few. We want to make sure that this sensitive information can only ever
    be seen by the authorized or dedicated service. All other services running in
    the cluster should not have any access to this data.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, Kubernetes secrets have been introduced. A secret is a key-value
    pair where the key is the unique name of the secret and the value is the actual
    sensitive data. Secrets are stored in etcd. Kubernetes can be configured so that
    secrets are encrypted at rest, that is, in etcd, and in transit, that is, when
    the secrets are going over the wire from a master node to the worker nodes that
    the pods of the service using this secret are running on.
  prefs: []
  type: TYPE_NORMAL
- en: Manually defining secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create a secret declaratively the same way as we can create any other
    object in Kubernetes. Here is the YAML for such a secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding definition can be found in the `~/fod/ch16/pets-secret.yaml` file.
    Now, you might be wondering what the values are. Are these the real (unencrypted)
    values? No, they are not. And they are also not really encrypted values, but just base64-encoded
    values. Thus, they are not really secure, since base64-encoded values can be easily
    reverted to clear text values. How did I get these values? That''s easy: follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `base64` tool as follows to encode the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/bbd59476-831a-40c9-b69e-96b1cacfe2f1.png)Creating base64-encoded
    values for the secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding values, we can create the secret and describe it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/df5c56b3-47a4-4ad5-9724-dc08beca28e5.png)Creating and describing
    the Kubernetes secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the description of the secret, the values are hidden and only their length
    is given. So, maybe the secrets are safe now? No, not really. We can easily decode
    this secret using the `kubectl get` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/75e781a7-6783-47aa-afe0-d26ff4e7e7b6.png)Kubernetes secret decoded'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding screenshot, we have our original secret values
    back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decode the values you got previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the consequences are that this method of creating a Kubernetes is not
    to be used in any environment other than development, where we deal with non-sensitive
    data. In all other environments, we need a better way to deal with secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Creating secrets with kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A much safer way to define secrets is to use `kubectl`. First, we create files containing
    the base64-encoded secret values similar to what we did in the preceding section,
    but, this time, we store the values in temporary files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use `kubectl` to create a secret from those files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The secret can then be used the same way as the manually created secret.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this method more secure than the other one, you might ask? Well, first
    of all, there is no YAML that defines a secret and is stored in some source code
    version control system, such as GitHub, which many people have access to and so
    can see and decode the secrets. Only the admin that is authorized to know the
    secrets ever sees their values and uses them to directly create the secrets in
    the (production) cluster. The cluster itself is protected by role-based access
    control so that no unauthorized persons have access to it, nor can they possibly
    decode the secrets defined in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can actually use the secrets that we have defined.
  prefs: []
  type: TYPE_NORMAL
- en: Using secrets in a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say we want to create a Deployment object where the web component uses
    our secret, `pets-secret`, that we introduced in the preceding section. We can
    use the following command to create the secret in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `~/fod/ch16/web-deploy-secret.yaml` file, we can find the definition
    of the `Deployment` object. We had to add the part starting from line `23` to
    the original definition of the `Deployment` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f52d1a33-412d-4270-8f9c-8bc86346e616.png)Deployment object for the web
    component with a secret'
  prefs: []
  type: TYPE_NORMAL
- en: On lines `27` through `30`, we define a volume called `secrets` from our secret, `pets-secret`.
    Then, we use this volume in the container, as described on lines `23` through
    `26`. We mount the secrets in the container filesystem at `/etc/secrets` and we
    mount the volume in read-only mode. Thus, the secret values will be available
    to the container as files in the said folder. The names of the files will correspond
    to the key names, and the content of the files will be the values of the corresponding
    keys. The values will be provided in unencrypted form to the application running
    inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, since we have the `username` and `password` keys in the secret,
    we will find two files, named `username `and `password`, in the `/etc/secrets` folder
    in the container filesystem. The `username` file should contain the value `john.doe` and
    the `password` file should contain the value `sEcret-pasSw0rD`. Here is the confirmation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1c911dec-f905-45e6-adff-8df72c62b729.png)Confirming that secrets
    are available inside the container'
  prefs: []
  type: TYPE_NORMAL
- en: On line `1` of the preceding output, we `exec` into the container where the
    web component runs. Then, on lines `2` to `5`, we list the files in the `/etc/secrets` folder,
    and, finally, on lines `6` to `8`, we show the content of the two files, which,
    unsurprisingly, show the secret values in clear text.
  prefs: []
  type: TYPE_NORMAL
- en: Since any application written in any language can read simple files, this mechanism
    of using secrets is very backward compatible. Even an old Cobol application can
    read clear text files from the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, applications expect secrets to be available in environment
    variables. Let's look at what Kubernetes offers us in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Secret values in environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say our web component expects the username in the environment variable, `PETS_USERNAME`,
    and the password in `PETS_PASSWORD`. If this is the case, we can modify our deployment
    YAML so that it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/30bcf71f-93a0-4fd9-855c-808380d22767.png)Deployment mapping secret
    values to environment variables'
  prefs: []
  type: TYPE_NORMAL
- en: On lines `23` through `33`, we define the two environment variables, `PETS_USERNAME` and `PETS_PASSWORD`,
    and map the corresponding key-value pair of `pets-secret` to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we don''t need a volume anymore; instead, we directly map the individual
    keys of our `pets-secret` into the corresponding environment variables that are
    valid inside the container. The following sequence of commands shows that the
    secret values are indeed available inside the container in the respective environment
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e8372fc5-eb69-49c9-be2d-3fe7570f0b74.png)Secret values are mapped
    to environment variables'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have shown you how to define secrets in a Kubernetes cluster
    and how to use those secrets in containers running as part of the pods of a deployment.
    We have shown two variants of how secrets can be mapped inside a container, the
    first using files and the second using environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to deploy an application into a Kubernetes
    cluster and how to set up application-level routing for this application. Furthermore,
    we have learned how to update application services running in a Kubernetes cluster
    without causing any downtime. Finally, we used secrets to provide sensitive information
    to application services running in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn about different techniques that are
    used to monitor an individual service or a whole distributed application running
    on a Kubernetes cluster. We will also learn how we can troubleshoot an application
    service that is running in production without altering the cluster or the cluster
    nodes that the service is running on. Stay tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your learning progress, please answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: You have an application consisting of two services, the first one being a web
    API and the second one being a DB, such as Mongo DB. You want to deploy this application
    into a Kubernetes cluster. In a few short sentences, explain how you would proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe in your own words what components you need in order to establish layer
    7 (or application level) routing for your application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the main steps needed to implement a blue-green deployment for a simple
    application service. Avoid going into too much detail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name three or four types of information that you would provide to an application
    service through Kubernetes secrets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the sources that Kubernetes accepts when creating a secret.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few links that provide additional information on the topics that
    were discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing a rolling update: [https://bit.ly/2o2okEQ](https://bit.ly/2o2okEQ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue-green deployment: [https://bit.ly/2r2IxNJ](https://bit.ly/2r2IxNJ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secrets in Kubernetes: [https://bit.ly/2C6hMZF](https://bit.ly/2C6hMZF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

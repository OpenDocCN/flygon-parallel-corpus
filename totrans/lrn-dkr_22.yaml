- en: Running a Containerized App in the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to deploy, monitor, and troubleshoot
    an application in production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will give an overview of some of the most popular ways of
    running containerized applications in the cloud. We will explore self-hosting
    and hosted solutions and discuss their pros and cons. Fully managed offerings
    from vendors such as Microsoft Azure and Google Cloud Engine will be briefly discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the topics we will be discussing in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and using Docker **Enterprise Edition** (**EE**) on **Amazon Web Services**
    (**AWS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Microsoft's **Azure Kubernetes Service** (**AKS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding **Google Kubernetes Engine** (**GKE**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After reading this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Kubernetes cluster in AWS using Docker EE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy and run a simple distributed application in a Docker EE cluster in AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy and run a simple distributed application on Microsoft's AKS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy and run a simple distributed application on GKE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to use AWS, Microsoft Azure, and Google Cloud in this chapter.
    Therefore, it is necessary to have an account for each platform. If you do not
    have an existing account, you can ask for a trial account for all of these cloud
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also use the files in the `~/fod-solution/ch18` folder of our `labs` repository
    from GitHub at [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch18](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch18).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and using Docker EE on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to install Docker **Universal Control Plane** (**UCP**)
    version 3.0\. UCP is part of Docker's enterprise offering and supports two orchestration engines,
    Docker Swarm and Kubernetes. UCP can be installed in the cloud or on-premises.
    Even hybrid clouds are possible with UCP.
  prefs: []
  type: TYPE_NORMAL
- en: To try this, you need a valid license for Docker EE or you can claim a free
    test license on Docker Store.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning the infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this first section, we are going to set up the infrastructure needed to
    install Docker UCP. This is relatively straightforward if you are somewhat familiar
    with AWS. Let''s do this by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an **A****uto Scaling group** (**ASG**) in AWS using the Ubuntu 16.04
    server AMI. Configure the ASG to contain three instances of size `t2.xlarge`.
    Here is the result of this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/3aef6cd3-8664-4019-8eff-5ef9712222f7.png)ASG on AWS ready for Docker
    EE'
  prefs: []
  type: TYPE_NORMAL
- en: Once the ASG has been created, and before we continue, we need to open the **security
    group** (**SG**) a bit (which our ASG is part of) so that we can access it through
    SSH from our laptop and also so that the **virtual machines** (**VMs**) can communicate
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to your SG and add two new inbound rules, which are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/00df1c32-a142-4c54-9890-8f69a6fa1a88.png)AWS SG settings'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, the first rule allows any traffic from my personal
    laptop (with the IP address `70.113.114.234`) to access any resource in the SG. The
    second rule allows any traffic inside the SG itself. These settings are not meant
    to be used in a production-like environment as they are way too permissive. However,
    for this demo environment, they work well.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will show you how to install Docker on the VMs we just prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After having provisioned the cluster nodes, we need to install Docker on each
    of them. This can be easily achieved by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSH into all three instances and install Docker. Using the downloaded key,
    SSH into the first machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, `<IP address>` is the public IP address of the VM we want to SSH into.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can install Docker. For detailed instructions, refer to [https://dockr.ly/2HiWfBc](https://dockr.ly/2HiWfBc).
    We have a script in the `~/fod/ch18/aws` folder called `install-docker.sh` that
    we can use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we need to clone the `labs` GitHub repository to the VM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run the script to install Docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once the script is finished, we can verify that Docker is indeed installed using `sudo
    docker version`. Repeat the preceding code for the two other VMs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo` is only necessary until the next SSH session is opened to this VM since
    we have added the `ubuntu` user to the `docker` group. So, we need to exit the
    current SSH session and connect again. This time, `sudo` should not be needed
    in conjunction with `docker`.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will show how to install Docker UCP on the infrastructure we just prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker UCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to set a few environment variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, `<IP address>` and `<FQDN>` are the public IP address and the public DNS
    name of the AWS EC2 instance we're installing in UCP.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we can use the following command to download all the images that
    UCP needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can install UCP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cd7dc52c-302c-4bee-8746-89a1bde6a8da.png)Installing UCP 3.0.0-beta2
    on a VM in AWS'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can open a browser window and navigate to `https://<IP address>`. Log
    in with your username, `admin`, and password, `adminadmin`. When asked for the
    license, upload your license key or follow the link to procure a trial license.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once logged in, on the left-hand side under the Shared Resources section, select Nodes and
    then click on the Add Node button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fbd3044c-9c09-4899-ab0b-8dd5ba14112b.png)Adding a new node to UCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the subsequent Add Node dialog box, make sure that the node type is Linux and
    the Worker node role is selected. Then, copy the `docker swarm join` command at
    the bottom of the dialog box. SSH into the other two VMs you created and run this
    command to have the respective node join the Docker swarm as a worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a45a4a6d-6e7d-4c02-be40-c3175c9511b8.png)Joining a node as a worker
    to the UCP cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the web UI of UCP, you should see that we now have three nodes ready,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ee9b3da4-5b1e-4a53-be1a-72c52291bd58.png)List of nodes in the UCP
    cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, worker nodes are configured so that they can only run the Docker
    Swarm workload. This can be changed in the node details, though. In this, three settings are
    possible: Swarm only, Kubernetes only, or mixed workload. Let''s start with Docker
    Swarm as the orchestration engine and deploy our pets application.'
  prefs: []
  type: TYPE_NORMAL
- en: Using remote admin for the UCP cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to manage our UCP cluster remotely from our laptop, we need to create and
    download a so-called **client bundle** from UCP. Proceed with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the UCP web UI, on the left-hand side under admin, select the My Profile option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the subsequent dialog, select the New Client Bundle option and then Generate
    Client Bundle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/e6dd0c9c-f925-4ae3-a78f-5113da2f806d.png)Generating and downloading
    a UCP client bundle'
  prefs: []
  type: TYPE_NORMAL
- en: Locate the downloaded bundle on your disk and unzip it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a new Terminal window, navigate to that folder and source the `env.sh` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get an output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can verify that we can indeed remotely access the UCP cluster by, for
    example, listing all the nodes of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8d630be3-12cd-4a63-9a0b-d9b2a8aab377.png)Listing all the nodes of
    our remote UCP cluster'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at how to deploy the pets application as a
    stack using Docker Swarm as the orchestration engine.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is now time to deploy our distributed application to our cluster orchestrated
    by Docker Swarm. Follow these steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Terminal, navigate to the `~/fod/ch18/ucp` folder and create the `pets` stack
    using the `stack.yml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/b83cbe73-2b3b-4b9b-9812-f81f645566b5.png)Deploying the pets stack
    into the UCP cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the UCP web UI, we can verify that the stack has been created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/a520aaa1-33d0-4ec8-9365-8f17118a6a54.png)The pets stack listing
    in the UCP web UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the application, we can navigate to Services under the main menu, Swarm.
    The list of services running in the cluster will be displayed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/738af281-8a2d-4e01-a299-c9580ae9e18c.png)Details of the ''web''
    services of the pets stack'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we see our two services, `web` and `db`, of the `pets` stack.
    If we click on the `web` service, its details are displayed on the right-hand
    side. There we find an entry, Published Endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the link and our `pets` application should be displayed in the browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When done, remove the stack from the console with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can try to remove that stack from within the UCP web UI.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the same Terminal that you used to remotely access the UCP cluster to deploy
    the pets application as a stack using Docker Swarm as the orchestration engine,
    we can now try to deploy the pets application to the UCP cluster using Kubernetes
    as the orchestration engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you''re still in the `~/fod/ch18/ucp` folder. Use `kubectl` to deploy
    the pets application. First, we need to test that we can get all the nodes of
    the cluster with the Kubernetes CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cf58df9f-5abd-4502-8992-0b22ae829ff9.png)Getting all the nodes of
    the UCP cluster using the Kubernetes CLI'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apparently, my environment is configured correctly and `kubectl` can indeed
    list all the nodes in the UCP cluster. That means I can now deploy the pets application
    using the definitions in the `pets.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ea01cf48-9dc5-4983-8d4d-b0febc33cb13.png)Creating the pets application
    in the UCP cluster using the Kubernetes CLI'
  prefs: []
  type: TYPE_NORMAL
- en: We can list the objects created by using `kubectl get all`. In a browser, we
    can then navigate to `http://<IP address>:<port>` to access the pets application,
    where `<IP address>` is the public IP address of one of the UCP cluster nodes
    and `<port>` is the port published by the `web` Kubernetes service.
  prefs: []
  type: TYPE_NORMAL
- en: We have created a cluster of three VMs in an AWS ASG and have installed Docker
    and UCP 3.0 on them. We then deployed our famous pets application into the UCP
    cluster, once using Docker Swarm as the orchestration engine and once Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Docker UCP is a platform-agnostic container platform that offers a secure enterprise-grade
    software supply chain in any cloud and on-premises, on bare metal, or in virtualized
    environments. It even offers freedom of choice when it comes to orchestration
    engines. The user can choose between Docker Swarm and Kubernetes. It is also possible
    to run applications in both orchestrators in the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Microsoft's Azure Kubernetes Service (AKS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To experiment with Microsoft's container-related offerings in Azure, we need an
    account on Azure. You can create a trial account or use an existing account. You
    can get a free trial account here: [https://azure.microsoft.com/en-us/free/](https://azure.microsoft.com/en-us/free/).
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft offers different container-related services on Azure. The easiest one
    to use is probably Azure Container Instances, which promises the fastest and simplest
    way to run a container in Azure, without having to provision any virtual machines
    and without having to adopt a higher-level service. This service is only really
    useful if you want to run a single container in a hosted environment. The setup
    is quite easy. In the Azure portal ([portal.azure.com](http://portal.azure.com)),
    you first create a new resource group and then create an Azure container instance.
    You only need to fill out a short form with properties such as the name of the
    container, the image to use, and the port to open. The container can be made available
    on a public or private IP address and will be automatically restarted if it crashes.
    There is a decent management console available, for example, to monitor resource
    consumption such as CPU and memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second choice is **Azure Container Service** (**ACS**), which provides
    a way to simplify the creation, configuration, and management of a cluster of
    VMs that are preconfigured to run containerized applications. ACS uses Docker
    images and provides a choice between three orchestrators: Kubernetes, Docker Swarm,
    and DC/OS (powered by Apache Mesos). Microsoft claims that their service can be
    scaled to tens of thousands of containers. ACS is free and you are only charged
    for computing resources.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will concentrate on the most popular offering, based on
    Kubernetes. It is called AKS and can be found here: [https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/).
    AKS makes it easy for you to deploy applications into the cloud and run them on
    Kubernetes. All the difficult and tedious management tasks are handled by Microsoft
    and you can concentrate fully on your applications. What that means is that you
    will never have to deal with tasks such as installing and managing Kubernetes,
    upgrading Kubernetes, or upgrading the operating system of the underlying Kubernetes
    nodes. All this is handled by the experts at Microsoft Azure. Furthermore, you
    will never have to deal with `etc` or Kubernetes master nodes. This is all hidden
    from you, and the only things you will interact with are the Kubernetes worker
    nodes that run your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Azure CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That said, let's start. We assume that you have created a free trial account
    or that you are using an existing account on Azure. There are various ways to
    interact with your Azure account. We will use the Azure CLI running on our local
    computer. We can either download and install the Azure CLI natively on our computer
    or run it from within a container running on our local Docker for Desktop. Since
    this book is all about containers, let's select the latter approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest version of the Azure CLI can be found on Docker Hub. Let''s pull
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be running a container from this CLI and executing all subsequent commands
    from within the shell running inside this container. Now, there is a little problem
    we need to overcome. This container will not have a Docker client installed. But
    we will also run some Docker commands, so we have to create a custom image derived
    from the preceding image, which contains a Docker client. The `Dockerfile` that''s
    needed to do so can be found in the `~/fod/ch18` folder and has this content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'On line 2, we are just using the Alpine package manager, `apk`, to install
    Docker. We can then use Docker Compose to build and run this custom image. The
    corresponding `docker-compose.yml` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Please note the command that is used to keep the container running, as well
    as the mounting of the Docker socket and the current folder in the `volumes` section. If
    you are running Docker for Desktop on Windows, then you need to define the `COMPOSE_CONVERT_WINDOWS_PATHS`
    environment variable to be able to mount the Docker socket. Use
  prefs: []
  type: TYPE_NORMAL
- en: '`export COMPOSE_CONVERT_WINDOWS_PATHS=1` from a Bash shell or `$Env:COMPOSE_CONVERT_WINDOWS_PATHS=1`
    when running PowerShell. Please refer to the following link for more details: [https://github.com/docker/compose/issues/4240](https://github.com/docker/compose/issues/4240).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s build and run this container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s execute into the `az` container and run a Bash shell in it with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will find ourselves running in a Bash shell inside the container. Let''s
    first check the version of the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in an output similar to this (shortened):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, we''re running on version `2.0.78`. Next, we need to log in to our account.
    Execute this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You will be presented with the following message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the instructions and log in through the browser. Once you have successfully
    authenticated your Azure account, you can go back to your Terminal and you should
    be logged in, as indicated by the output you''ll get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to first move our container images to Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a container registry on Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we create a new resource group named `animal-rg`. In Azure, resource
    groups are used to logically group a collection of associated resources. To have
    an optimal cloud experience and keep latency low, it is important that you select
    a data center located in a region near you. You can use the following command
    to list all regions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will give you a rather long list of all possible regions you can select
    from. Use the `name`, for example, `eastasia`, to identify the region of your
    choice. In my case, I will be selecting `westeurope`. Please note that not all
    locations listed are valid for resource groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to create a resource group is simple; we just need a name for the
    group and the location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that your output shows `"provisioningState": "Succeeded"`.'
  prefs: []
  type: TYPE_NORMAL
- en: When running a containerized application in production, we want to make sure
    that we can freely download the corresponding container images from a container
    registry. So far, we have always downloaded our images from Docker Hub. But this
    is often not possible. For security reasons, the servers of a production system
    often have no direct access to the internet and thus are not able to reach out
    to Docker Hub. Let's follow this best practice and assume the same for our Kubernetes
    cluster that we are going to create in an instant.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what can we do? Well, the solution is to use a container image registry
    that is close to our cluster and that is in the same security context. In Azure,
    we can create an **Azure container registry** (**ACR**) and host our images there.
    Let''s first create such a registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `<acr-name>` needs to be unique. In my case, I have chosen the name
    `fodanimalsacr`. The (shortened) output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After successfully creating the container registry, we need to log in to that
    registry using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once we are successfully logged in to the container registry on Azure, we need
    to tag our containers correctly so that we can then push them to ACR. Tagging
    and pushing images to ACR will be described next.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing our images to ACR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have successfully logged in to ACR, we can tag our images such that
    they can be pushed to the registry. For this, we need to get the URL of our ACR
    instance. We can do so with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use the preceding URL to tag our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can push them to our ACR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To double-check that our images are indeed in our ACR, we can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, the two images we just pushed are listed. With that, we are ready to
    create our Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once again, we will be using our custom Azure CLI to create the Kubernetes
    cluster. We will have to make sure that the cluster can access our ACR instance,
    which we just created and is where our container images reside. So, the command
    to create a cluster named `animals-cluster` with two worker nodes looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This command takes a while, but after a few minutes, we should receive some
    JSON-formatted output with all the details about the newly created cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the cluster, we need `kubectl`. We can easily get it installed in
    our Azure CLI container using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Having installed `kubectl`, we need the necessary credentials to use the tool
    to operate on our new Kubernetes cluster in Azure. We can get the necessary credentials
    with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After the success of the preceding command, we can list all the nodes in our
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we have two worker nodes up and running. The version of Kubernetes
    that is running on those nodes is `1.14.8`.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to deploy our application to this cluster. In the next section,
    we are going to learn how we can do this.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our application to the Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To deploy the application, we can use the `kubectl apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to test the application. Remember that we had created a service
    of type `LoadBalancer` for the web component. This service exposes the application
    to the internet. This process can take a moment, as AKS, among other tasks, needs
    to assign a public IP address to this service. We can observe this with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note the `--watch` parameter in the preceding command. It allows us
    to monitor the progress of the command over time. Initially, we should see output
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The public IP address is marked as pending. After a few minutes, that should
    change to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Our application is now ready at the IP address `51.105.229.192` and port number
    `3000`. Note that the load balancer maps the internal port `32618` to the external
    port `3000`; this was not evident to me the first time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check it out. In a new browser tab, navigate to `http://51.105.229.192:3000/pet`
    and you should see our familiar application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f45ab4b8-610f-4909-b6ce-4fce569200c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Our sample application running on AKS
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully deployed our distributed application to Kubernetes
    hosted in Azure. We did not have to worry about installing or managing Kubernetes;
    we could concentrate on the application itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are done experimenting with the application, we should not forget
    to delete all resources on Azure to avoid incurring unnecessary costs. We can
    delete all resources created by deleting the resource group as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Azure has a few compelling offerings regarding the container workload, and the
    lock-in is not as evident as it is on AWS due to the fact that Azure does mainly offer open
    source orchestration engines, such as Kubernetes, Docker Swarm, DC/OS, and Rancher.
    Technically, we remain mobile if we initially run our containerized applications
    in Azure and later decide to move to another cloud provider. The cost should be
    limited.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that, when you delete your resource group, the Azure Active
    Directory service principal used by the AKS cluster is not removed. Refer to the
    online help for details on how to delete the service principal.
  prefs: []
  type: TYPE_NORMAL
- en: Next on the list is Google with their Kubernetes Engine.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GKE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google is the inventor of Kubernetes and, to this date, the driving force behind
    it. You would therefore expect that Google has a compelling offering around hosted
    Kubernetes. Let''s have a peek into it now. To continue, you need to either have
    an existing account with Google Cloud or create a test account here: [https://console.cloud.google.com/freetrial](https://console.cloud.google.com/freetrial).
    Proceed with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the main menu, select Kubernetes Engine. The first time you do that, it will
    take a few moments until the Kubernetes engine is initialized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, create a new project and name it `massai-mara`; this may take a moment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once this is ready, we can create a cluster by clicking on Create Cluster in
    the popup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Your first cluster** template on the left-hand side of the form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the cluster `animals-cluster`, select the region or zone that's closest
    to you, leave all other settings in the Create a Kubernetes Cluster form with
    their default values, and click on Create at the bottom of the form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It will again take a few moments to provision the cluster for us. Once the
    cluster has been created, we can open Cloud Shellby clicking on the shell icon
    in the upper-right corner of the view. This should look similar to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f1bd8565-7c02-4260-bb8e-0dc482fc8051.png)The first Kubernetes cluster
    ready and Cloud Shell open in GKE'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now clone our `labs` GitHub repository to this environment with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now find an `animals.yaml` file in the current folder, which we can
    use to deploy the animals application into our Kubernetes cluster. Have a look
    at the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'It has pretty much the same content as the same file we used in the previous
    chapter. The two differences are these:'
  prefs: []
  type: TYPE_NORMAL
- en: We use a service of type `LoadBalancer` (instead of `NodePort`) to publicly
    expose the `web` component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not use volumes for the PostgreSQL database since configuring StatefulSets
    correctly on GKE is a bit more involved than in Minikube. The consequence of this
    is that our animals application will not persist the state if the `db` pod crashes.
    How to use persistent volumes on GKE lies outside the scope of this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, note that we are not using Google Container Registry to host the container
    images but are instead directly pulling them from Docker Hub. It is very easy,
    and similar to what we have learned in the section about AKS, to create such a
    container registry in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can continue, we need to set up `gcloud` and `kubectl` credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Having done that, it''s time to deploy the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the objects have been created, we can observe the `LoadBalancer` service `web` until
    it is assigned a public IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The second line in the output is showing the situation while the creation of
    the load balancer is still pending, and the third one gives the final state. Press
    *Ctrl* + *C* to quit the `watch` command. Apparently, we got the public IP address
    `146.148.23.70` assigned and the port is `3000`.
  prefs: []
  type: TYPE_NORMAL
- en: We can then use this IP address and navigate to `http://<IP address>:3000/pet`, and
    we should be greeted by the familiar animal image.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done playing with the application, delete the cluster and the project
    in the Google Cloud console to avoid any unnecessary costs.
  prefs: []
  type: TYPE_NORMAL
- en: We have created a hosted Kubernetes cluster in GKE. We have then used Cloud
    Shell, provided through the GKE portal, to first clone our `labs` GitHub repository
    and then the `kubectl` tool to deploy the animals application into the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When looking into a hosted Kubernetes solution, GKE is a compelling offering.
    It makes it very easy to start, and since Google is the main driving force behind
    Kubernetes, we can rest assured that we will always be able to leverage the full
    functionality of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter of the book, you first got a quick introduction to how
    to install and use Docker's UCP, which is part of Docker's enterprise offering
    on AWS. Then, you learned how to create a hosted Kubernetes cluster in AKS and
    run the animals application on it, followed by the same for Google's own hosted
    Kubernetes offering, GKE.
  prefs: []
  type: TYPE_NORMAL
- en: I am honored that you selected this book, and I want to thank you for accompanying
    me on this journey, where we explored Docker containers and container orchestration
    engines. I hope that this book has served as a valuable resource on your learning
    journey. I wish you all the best and much success when using containers in your
    current and future projects.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your knowledge, please answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Give a high-level description of the tasks needed to provision and run Docker
    UPC on AWS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List a few reasons why you would select a hosted Kubernetes offering, such as
    Microsoft's AKS or Google's GKE, to run your applications on Kubernetes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two reasons when using a hosted Kubernetes solution, such as AKS or GKE,
    to consider hosting your container images in the container registry of the respective
    cloud provider.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following articles give you some more information related to the topics
    we discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Install individual Docker EE components on Linux servers at [https://dockr.ly/2vH5dpN](https://dockr.ly/2vH5dpN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Container Service (AKS) at [https://bit.ly/2JglX9d](https://bit.ly/2JglX9d)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Kubernetes Engine at [https://bit.ly/2I8MjJx](https://bit.ly/2I8MjJx)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

["```cpp\nSEC(\"xdp\")\ninthello(structxdp_md*ctx){\nbooldrop; `drop``=``<``examine``packet``and``decide``whether``to``drop``it``>``;` ``if``(``drop``)`\n`return``XDP_DROP``;` ``else` ``return``XDP_PASS``;` ``}```", "```cpp\n\n ```", "```cpp  ```", "```cpp\nstruct`xdp_md`{`__u32`data;`__u32``data_end`;`__u32``data_meta`;/* Below access go through struct xdp_rxq_info */`__u32``ingress_ifindex`;/* rxq->dev->ifindex */`__u32``rx_queue_index`;/* rxq->queue_index  */`__u32``egress_ifindex`;/* txq->dev->ifindex */};\n```", "```cpp\nSEC(\"xdp\") `int``ping``(``struct``xdp_md``*``ctx``)``{` ``long``protocol``=``lookup_protocol``(``ctx``);` ``if``(``protocol``==``1``)``// ICMP`\n`{` ``bpf_printk``(``\"Hello ping\"``);` ``}` ``return``XDP_PASS``;` ``}```", "```cpp`\n```", "```cppYou can see this program in action by following these steps:\n\n1.  Run `make` in the *chapter8* directory. This doesn\u2019t just build the code; it also attaches the XDP program to the loopback interface (called `lo`).\n\n2.  Run `ping localhost` in one terminal window.\n\n3.  In another terminal window, watch the output generated in the trace pipe by running `cat /sys/kernel/tracing/trace_pipe`.\n\nYou should see two lines of trace being generated approximately every second, and they should look like this:\n\n```", "```cpp\n\nThere are two lines of trace per second because the loopback interface is receiving both the ping requests and the ping responses.\n\nYou can easily modify this code to drop ping packets by adding a line of code to return `XDP_DROP` when the protocol matches, like this:\n\n```", "```cpp`\n```", "```cppIf you try this, you\u2019ll see that output resembling the following is only generated in the trace output once per second:\n\n```", "```cpp\n\nThe loopback interface receives a ping request, and the XDP program drops it, so the request never gets far enough through the network stack to elicit a response.\n\nMost of the work in this XDP program is being done in a function called `lookup_protocol()` that determines the Layer 4 protocol type. It\u2019s just an example, not a production-quality implementation of parsing a network packet! But it\u2019s sufficient to give you an idea of how parsing in eBPF works.\n\nThe network packet that has been received consists of a string of bytes that are laid out as shown in [Figure\u00a08-1](#layout_of_an_ip_network_packetcomma_sta).\n\n![Layout of an IP network packet, starting with an Ethernet header, followed by an IP header, and then the Layer 4 data](assets/lebp_0801.png)\n\n###### Figure 8-1\\. Layout of an IP network packet, starting with an Ethernet header, followed by an IP header, and then the Layer 4 data\n\nThe `lookup_protocol()` function takes the `ctx` structure that holds information about where this network packet is in memory and returns the protocol type that it finds in the IP header. The code is as follows:\n\n```", "```cpp\n\n[![1](assets/1.png)](#code_id_8_1)\n\nThe local variables `data` and `data_end` point to the start and end of the network packet.\n\n[![2](assets/2.png)](#code_id_8_2)\n\nThe network packet should start with an Ethernet header.\n\n[![3](assets/3.png)](#code_id_8_3)\n\nBut you can\u2019t simply assume this network packet is big enough to hold that Ethernet header! The verifier requires that you check this explicitly.\n\n[![4](assets/4.png)](#code_id_8_4)\n\nThe Ethernet header contains a 2-byte field that tells us the Layer 3 protocol.\n\n[![5](assets/5.png)](#code_id_8_5)\n\nIf the protocol type indicates that it\u2019s an IP packet, the IP header immediately follows the Ethernet header.\n\n[![6](assets/6.png)](#code_id_8_6)\n\nYou can\u2019t just assume there\u2019s enough room for that IP header in the network packet. Again the verifier requires that you check explicitly.\n\n[![7](assets/7.png)](#code_id_8_7)\n\nThe IP header contains the protocol byte the function will return to its caller.\n\nThe `bpf_ntohs()` function used by this program ensures that the two bytes are in the order expected on this host. Network protocols are big-endian, but most processors are little-endian, meaning they hold multibyte values in a different order. This function converts (if necessary) from network ordering to host ordering. You should use this function whenever you extract a value from a field in a network packet that\u2019s more than one byte long.\n\nThe simple example here shows how just a few lines of eBPF code can have a dramatic impact on networking functionality. It\u2019s not hard to imagine how more complex rules about which packets to pass and which packets to drop could result in the features I described at the start of this section: firewalling, DDoS protection, and packet-of-death vulnerability mitigation. Now let\u2019s consider how even more functionality can be provided given the power to modify network packets within eBPF programs.```", "```cpp```", "```\nSEC(\"xdp_lb\")intxdp_load_balancer(structxdp_md*ctx){void*data=(void*)(long)ctx->data;![1](assets/1.png)void*data_end=(void*)(long)ctx->data_end;structethhdr*eth=data;if(data+sizeof(structethhdr)>data_end)returnXDP_ABORTED;if(bpf_ntohs(eth->h_proto)!=ETH_P_IP)returnXDP_PASS;structiphdr*iph=data+sizeof(structethhdr);if(data+sizeof(structethhdr)+sizeof(structiphdr)>data_end)returnXDP_ABORTED;if(iph->protocol!=IPPROTO_TCP)![2](assets/2.png)returnXDP_PASS;if(iph->saddr==IP_ADDRESS(CLIENT))![3](assets/3.png){charbe=BACKEND_A;![4](assets/4.png)if(bpf_get_prandom_u32()%2)be=BACKEND_B;iph->daddr=IP_ADDRESS(be);![5](assets/5.png)eth->h_dest[5]=be;}else{iph->daddr=IP_ADDRESS(CLIENT);![6](assets/6.png)eth->h_dest[5]=CLIENT;}iph->saddr=IP_ADDRESS(LB);![7](assets/7.png)eth->h_source[5]=LB;iph->check=iph_csum(iph);![8](assets/8.png)returnXDP_TX;}\n```", "```\nxdp: $(BPF_OBJ)\n   bpftool net detach xdpgeneric dev eth0\n   rm -f /sys/fs/bpf/$(TARGET)\n   bpftool prog load $(BPF_OBJ) /sys/fs/bpf/$(TARGET)\n   bpftool net attach xdpgeneric pinned /sys/fs/bpf/$(TARGET) dev eth0\n```", "```\ninttc_drop(struct__sk_buff*skb){ `bpf_trace_printk``(``\"[tc] dropping packet``\\n``\"``);` ``return``TC_ACT_SHOT``;` ``}```", "```\n\n ```", "```\ninttc(struct__sk_buff*skb){ `void``*``data``=``(``void``*``)(``long``)``skb``->``data``;` ``void``*``data_end``=``(``void``*``)(``long``)``skb``->``data_end``;` ``if``(``is_icmp_ping_request``(``data``,``data_end``))``{` ``struct``iphdr``*``iph``=``data``+``sizeof``(``struct``ethhdr``);` ``struct``icmphdr``*``icmp``=``data``+``sizeof``(``struct``ethhdr``)``+``sizeof``(``struct``iphdr``);` ``bpf_trace_printk``(``\"[tc] ICMP request for %x type %x``\\n``\"``,``iph``->``daddr``,` ``icmp``->``type``);` ``return``TC_ACT_SHOT``;` ``}` ``return``TC_ACT_OK``;` ``}```", "```````cpp`\n```\n\n ```cppThe `sk_buff` structure has pointers to the start and end of the packet data, very much like the `xdp_md` structure, and packet parsing proceeds in very much the same way. Again, to pass verification you have to explicitly check that any access to data is within the range between `data` and `data_end`.\n\nYou might be wondering why you would want to implement something like this at the TC layer when you have already seen the same kind of functionality implemented with XDP. One good reason is that you can use TC programs for egress traffic, where XDP can only process ingress traffic. Another is that because XDP is triggered as soon as the packet arrives, there is no `sk_buff` kernel data structure related to the packet at that point. If the eBPF program is interested in or wants to manipulate the `sk_buff` the kernel creates for this packet, the TC attachment point is suitable.\n\n###### Note\n\nTo better understand the differences between XDP and TC eBPF programs, read the \u201cProgram Types\u201d section in the [BPF and XDP Reference Guide](https://oreil.ly/MWAJL) from the Cilium project.\n\nNow let\u2019s consider an example that doesn\u2019t just drop certain packets. This example identifies a ping request being received and responds with a ping response:\n\n```\ninttc_pingpong(struct__sk_buff*skb){void*data=(void*)(long)skb->data;void*data_end=(void*)(long)skb->data_end;if(!is_icmp_ping_request(data,data_end)){![1](assets/1.png)returnTC_ACT_OK;}structiphdr*iph=data+sizeof(structethhdr);structicmphdr*icmp=data+sizeof(structethhdr)+sizeof(structiphdr);swap_mac_addresses(skb);![2](assets/2.png)swap_ip_addresses(skb);// Change the type of the ICMP packet to 0 (ICMP Echo Reply) \n// (was 8 for ICMP Echo request)\nupdate_icmp_type(skb,8,0);![3](assets/3.png)// Redirecting a clone of the modified skb back to the interface \n// it arrived on\nbpf_clone_redirect(skb,skb->ifindex,0);![4](assets/4.png)returnTC_ACT_SHOT;![5](assets/5.png)}\n```cpp\n\n[![1](assets/1.png)](#code_id_8_16)\n\nThe `is_icmp_ping_request()` function parses the packet and checks not only that it\u2019s an ICMP message, but also that it\u2019s an echo (ping) request.\n\n[![2](assets/2.png)](#code_id_8_17)\n\nSince this function is going to send a response to the sender, the source and destination addresses need to be swapped. (You can read the example code if you want to see the nitty-gritty details of this, which also includes updating the IP header checksum.)\n\n[![3](assets/3.png)](#code_id_8_18)\n\nThis is converted to an echo response by changing the type field in the ICMP header.\n\n[![4](assets/4.png)](#code_id_8_19)\n\nThis helper function sends a clone of the packet back through the interface (`skb->ifindex`) on which it was received.\n\n[![5](assets/5.png)](#code_id_8_20)\n\nSince the helper function cloned the packet before sending out the response, the original packet should be dropped.\n\nIn normal circumstances, a ping request would be handled later by the kernel\u2019s network stack, but this small example demonstrates how network functionality more generally can be replaced by an eBPF implementation.\n\nLots of networking capabilities today are handled by user space services, but where they can be replaced by eBPF programs, it\u2019s likely to be great for performance. A packet that\u2019s processed within the kernel doesn\u2019t have to complete its journey through the rest of the stack; there is no need for it to transition to user space for processing, and the response doesn\u2019t require a transition back into the kernel. What\u2019s more, the two could run in parallel\u2014an eBPF program can return `TC_ACT_OK` for any packet that requires complex processing that it can\u2019t handle so that it gets passed up to the user space service as normal.\n\nFor me, this is an important aspect of implementing network functionality in eBPF. As the eBPF platform develops (e.g., more recent kernels allowing programs of one million instructions), it\u2019s possible to implement increasingly complex aspects of networking in the kernel. The parts that are not yet implemented in eBPF can still be handled either by the traditional stack within the kernel or in user space. Over time, more and more features can be moved from user space into the kernel, with the flexibility and dynamic nature of eBPF meaning you won\u2019t have to wait for them to be part of the kernel distribution itself. You can load eBPF implementations immediately, just as I discussed in [Chapter\u00a01](ch01.html#what_is_ebpf_and_why_is_it_importantque).\n\nI\u2019ll return to the implementation of networking features in [\u201ceBPF and Kubernetes Networking\u201d](#ebpf_and_kubernetes_networking). But first, let\u2019s consider another use case that eBPF enables: inspecting the decrypted contents of encrypted traffic.``````", "```\nstaticintprocess_SSL_data(structpt_regs*ctx,uint64_tid,enum\nssl_data_event_typetype,constchar*buf){ `...` ``bpf_probe_read``(``event``->``data``,``event``->``data_len``,``buf``);` ``tls_events``.``perf_submit``(``ctx``,``event``,``sizeof``(``struct``ssl_data_event_t``));` ``return``0``;` ``}```", "```\n\n ```", "```\n// Function signature being probed: // int SSL_read(SSL *s, void *buf, int num) intprobe_entry_SSL_read(structpt_regs*ctx){uint64_tcurrent_pid_tgid=bpf_get_current_pid_tgid();...constchar*buf=(constchar*)PT_REGS_PARM2(ctx);![1](assets/1.png)active_ssl_read_args_map.update(&current_pid_tgid,&buf);![2](assets/2.png)return0;}\n```", "```\nintprobe_ret_SSL_read(structpt_regs*ctx){uint64_tcurrent_pid_tgid=bpf_get_current_pid_tgid();...constchar**buf=active_ssl_read_args_map.lookup(&current_pid_tgid);![1](assets/1.png)if(buf!=NULL){process_SSL_data(ctx,current_pid_tgid,kSSLRead,*buf);![2](assets/2.png)}active_ssl_read_args_map.delete(&current_pid_tgid);![3](assets/3.png)return0;}\n```", "```  ``# eBPF and Kubernetes Networking\n\nAlthough this book isn\u2019t about Kubernetes, eBPF is so widely used for Kubernetes networking that it\u2019s a great illustration of using the platform to customize the networking stack.\n\nIn Kubernetes environments, applications are deployed in *pods*. Each pod is a group of one or more containers that share kernel namespaces and cgroups, isolating pods from each other and from the host machine they are running on.\n\nIn particular (for the purposes of this chapter), a pod typically has its own network namespace and its own IP address.^([9](ch08.html#ch08fn9)) This means the kernel has a set of network stack structures for that namespace, separated from the host\u2019s and from other pods. As shown in [Figure\u00a08-5](#network_path_in_kubernetes), the pod is connected to the host by a virtual Ethernet connection, and it is allocated its own IP address.\n\n![Network path in Kubernetes](assets/lebp_0805.png)\n\n###### Figure 8-5\\. Network path in Kubernetes\n\nYou can see from [Figure\u00a08-5](#network_path_in_kubernetes) that a packet coming from outside the machine destined for an application pod has to travel through the network stack on the host, across the virtual Ethernet connection, and into the pod\u2019s network namespace, and then it has to traverse the network stack again to reach the application.\n\nThose two network stacks are running in the same kernel, so the packet is really running through the same processing twice. The more code a network packet has to pass through, the higher the latency, so if it\u2019s possible to shorten the network path, that will likely bring about performance improvements.\n\nAn eBPF-based networking solution like Cilium can hook into the network stack to override the kernel\u2019s native networking behavior, as shown in [Figure\u00a08-6](#bypassing_iptables_and_conntrack_proces).\n\n![Bypassing iptables and conntrack processing with eBPF](assets/lebp_0806.png)\n\n###### Figure 8-6\\. Bypassing iptables and conntrack processing with eBPF\n\nIn particular, eBPF enables replacing iptables and conntrack with a more efficient solution for managing network rules and connection tracking. Let\u2019s discuss why this results in a significant performance improvement in Kubernetes.\n\n## Avoiding iptables\n\nKubernetes has a component called kube-proxy that implements load balancing behavior, allowing multiple pods to fulfill requests to a service. This has been implemented using iptables rules.\n\nKubernetes offers users the choice of which networking solution to use through the use of the Container Network Interface (CNI). Some CNI plug-ins use iptables rules to implement L3/L4 network policy in Kubernetes; that is, the iptables rules indicate whether to drop a packet because it doesn\u2019t meet the network policy.\n\nAlthough iptables was effective for traditional (precontainer) networking, it has some weaknesses when it\u2019s used in Kubernetes. In this environment, pods\u2014and their IP addresses\u2014come and go dynamically, and each time a pod is added or removed, the iptables rules have to be rewritten in their entirety, and this impacts performance at scale. (A [talk](https://oreil.ly/BO0-8) by Haibin Xie and Quinton Hoole at KubeCon in 2017 described how making a single rule update to iptables rules for 20,000 services could take five hours.)\n\nUpdates to iptables aren\u2019t the only performance issues: looking up a rule requires a linear search through the table, which is an O(n) operation, growing linearly with the number of rules.\n\nCilium uses eBPF hash table maps to store network policy rules, connection tracking, and load balancer lookup tables, which can replace iptables for kube-proxy. Both looking up an entry in a hash table and inserting a new one are approximately O(1) operations, which means they scale much, much better.\n\nYou can read about the benchmarked performance improvements this achieves on the Cilium [blog](https://oreil.ly/9NV99). In the same post you\u2019ll see that Calico, another CNI that has an eBPF option, also achieves better performance when you pick its eBPF implementation over iptables. eBPF offers the most performant mechanisms for scalable, dynamic Kubernetes deployments.\n\n## Coordinated Network Programs\n\nA complex networking implementation like Cilium can\u2019t be written as a single eBPF program. As shown in [Figure\u00a08-7](#cilium_consists_of_multiple_coordinated), it provides several different eBPF programs that are hooked into different parts of the kernel and its network stack.\n\n![Cilium consists of multiple coordinated eBPF programs that hook into different points in the kernel](assets/lebp_0807.png)\n\n###### Figure 8-7\\. Cilium consists of multiple coordinated eBPF programs that hook into different points in the kernel\n\nAs a general principle, Cilium intercepts traffic as soon as it can in order to shorten the processing path for each packet. Messages flowing out from an application pod are intercepted at the socket layer, as close to the application as possible. Inbound packets from the external network are intercepted using XDP. But what about the additional attachment points?\n\nCilium supports different networking modes that suit different environments. A full description of this is beyond the scope of this book (you can find more information at [Cilium.io](https://cilium.io)), but I\u2019ll give a brief overview here so that you can see why there are so many different eBPF programs!\n\nThere is a simple, flat networking mode, in which Cilium allocates IP addresses for all the pods in a cluster from the same CIDR and directly routes traffic between them. There are also a couple of different tunneling modes, in which traffic intended for a pod on a different node gets encapsulated in a message addressed to that destination node\u2019s IP address and decapsulated on that destination node for the final hop into the pod. Different eBPF programs get invoked to handle traffic depending on whether a packet is destined for a local container, the local host, another host on this network, or a tunnel.\n\nIn [Figure\u00a08-7](#cilium_consists_of_multiple_coordinated) you can see multiple TC programs that handle traffic to and from different devices. These devices represent the possible different real and virtual network interfaces where a packet might be flowing:\n\n*   The interface to a pod\u2019s network (one end of the virtual Ethernet connection between the pod and the host)\n\n*   The interface to a network tunnel\n\n*   The interface to a physical network device on the host\n\n*   The host\u2019s own network interface\n\n###### Note\n\nIf you\u2019re interested in learning more about how packets flow through Cilium, Arthur Chiao wrote this detailed and interesting blog post: [\u201cLife of a Packet in Cilium: Discovering the Pod-to-Service Traffic Path and BPF Processing Logics\u201d](https://oreil.ly/toxsM).\n\nThe different eBPF programs attached at these various points in the kernel communicate using eBFP maps and using the metadata that can be attached to network packets as they flow through the stack (which I mentioned when I discussed accessing network packets in the XDP example). These programs don\u2019t just route packets to their destination; they\u2019re also used to drop packets\u2014just like you saw in earlier examples\u2014based on network policies.\n\n## Network Policy Enforcement\n\nYou saw at the start of this chapter how eBPF programs can drop packets, and that means they simply won\u2019t reach their destination. This is the basis of network policy enforcement, and conceptually it\u2019s essentially the same whether we are thinking about \u201ctraditional\u201d or cloud native firewalling. A policy determines whether a packet should be dropped or not, based on information about its source and/or destination.\n\nIn traditional environments, IP addresses are assigned to a particular server for a long period of time, but in Kubernetes, IP addresses come and go dynamically, and the address assigned today for a particular application pod might very well be reused for a completely different application tomorrow. This is why traditional firewalling isn\u2019t terribly effective in cloud native environments. It would be impractical to redefine firewall rules manually every time IP addresses change.\n\nInstead, Kubernetes supports the concept of a NetworkPolicy resource, which defines firewalling rules based on the labels applied to particular pods rather than based on their IP address. Although the resource type is native to Kubernetes, it\u2019s not implemented by Kubernetes itself. Instead, this functionality is delegated to whatever CNI plug-in you\u2019re using. If you choose a CNI that doesn\u2019t support NetworkPolicy resources, any rules you might configure are simply ignored. On the flip side, CNIs are free to configure custom resources that allow for more sophisticated network policy configurations than the native Kubernetes definition allows. For example, Cilium supports features like DNS-based network policy rules, so you can define whether traffic is or isn\u2019t allowed not based on an IP address but based on the DNS name (e.g., \u201c*example.com*\u201d). You can also define policies for various Layer 7 protocols, for example, allowing or denying traffic for HTTP GET calls but not for POST calls to a particular URL.\n\n###### Note\n\nIsovalent\u2019s free hands-on lab [\u201cGetting Started with Cilium\u201d](https://oreil.ly/afdeh) walks you through defining network policies at Layers 3/4 and Layer 7\\. Another very useful resource is the Network Policy Editor at [*networkpolicy.io*](http://networkpolicy.io), which visually presents the effects of a network policy.\n\nAs I discussed earlier in this chapter, it\u2019s possible to use iptables rules to drop traffic, and that\u2019s an approach some CNIs have taken to implement Kubernetes NetworkPolicy rules. Cilium uses eBPF programs to drop traffic that doesn\u2019t match the set of rules currently in place. Having seen examples of dropping packets earlier in this chapter, I hope you have a rough mental model for how this would work.\n\nCilium uses Kubernetes identities to determine whether a given network policy rule applies. In the same way labels define which pods are part of a service in Kubernetes, labels also define Cilium\u2019s security identity for the pod. eBPF hash tables, indexed by these service identities, make for very efficient rule lookups.\n\n## Encrypted Connections\n\nMany organizations have requirements to protect their deployments and their users\u2019 data by encrypting traffic between applications. This can be achieved by writing code in each application to ensure that it sets up secure connections, typically using mutual Traffic Layer Security (mTLS) underpinning an HTTP or gRPC connection. Setting up these connections requires first establishing the identities of the apps at either end of the connection (which is usually achieved by exchanging certificates) and then encrypting the data that flows between them.\n\nIn Kubernetes, it\u2019s possible to offload the requirement from the application, either to a service mesh layer or to the underlying network itself. A full discussion of service mesh is beyond the scope of this book, but you might be interested in a piece I wrote on the new stack: [\u201cHow eBPF Streamlines the Service Mesh\u201d](https://oreil.ly/5ayvF). Let\u2019s concentrate here on the network layer and how eBPF makes it possible to push the encryption requirement into the kernel.\n\nThe simplest option to ensure that traffic is encrypted within a Kubernetes cluster is to use *transparent encryption*. It\u2019s called \u201ctransparent\u201d because it takes place entirely at the network layer and it\u2019s extremely lightweight from an operational point of view. The applications themselves don\u2019t need to be aware of the encryption at all, and they don\u2019t need to set up HTTPS connections; nor does this approach require any additional infrastructure components running under Kubernetes.\n\nThere are two in-kernel encryption protocols in common usage, IPsec and WireGuard^((R)), and they\u2019re both supported in Kubernetes networking by Cilium and Calico CNIs. It\u2019s beyond the scope of this book to discuss the differences between these two protocols, but the key point is that they set up a secure tunnel between two machines. The CNI can choose to connect the eBPF endpoint for a pod via this secure tunnel.\n\n###### Note\n\nThere is a nice write-up on the [Cilium blog](https://oreil.ly/xjpGP) of how Cilium uses WireGuard^((R)) as well as IPsec to provide encrypted traffic between nodes. The post also gives a brief overview of the performance characteristics of both.\n\nThe secure tunnel is set up using the identities of the nodes at either end. These identities are managed by Kubernetes anyway, so the administrative burden for an operator is minimal. For many purposes this is sufficient as it ensures that all network traffic in a cluster is encrypted. Transparent encryption can also be used unmodified with NetworkPolicy that uses Kubernetes identities to manage whether traffic can flow between different endpoints in the cluster.\n\nSome organizations operate a multitenant environment where there\u2019s a need for strong multitenant boundaries and where it\u2019s essential to use certificates to identify every application endpoint. Handling this within every application is a significant burden, so it\u2019s something that more recently has been offloaded to a service mesh layer, but this requires a whole extra set of components to be deployed, causing additional resource consumption, latency, and operational complexity.\n\neBPF is now enabling a [new approach](https://oreil.ly/DSnLZ) that builds on transparent encryption but uses TLS for the initial certificate exchange and endpoint authentication so that the identities can represent individual applications rather than the nodes they are running on, as depicted in [Figure\u00a08-8](#transparent_encryption_between_authenti).\n\n![Transparent encryption between authenticated application identities](assets/lebp_0808.png)\n\n###### Figure 8-8\\. Transparent encryption between authenticated application identities\n\nOnce the authentication step has taken place, IPsec or WireGuard^((R)) within the kernel is used to encrypt the traffic that flows between those applications. This has a number of advantages. It allows third-party certificate and identity management tools like cert-manager or SPIFFE/SPIRE to handle the identity part, and the network takes care of encryption so that it\u2019s all entirely transparent to the application. Cilium supports NetworkPolicy definitions that specify endpoints by their SPIFFE ID rather than just by their Kubernetes labels. And perhaps most importantly, this approach can be used with any protocol that travels in IP packets. That\u2019s a big step up from mTLS, which works only for TCP-based connections.\n\nThere\u2019s not enough room in this book to dive deep into all the internals of Cilium, but I hope this section helped you understand how eBPF is a powerful platform for building complex networking functionality like a fully featured Kubernetes CNI.\n\n# Summary\n\nIn this chapter you saw eBPF programs attached at a variety of different points in the network stack. I showed examples of basic packet processing, and I hope these gave you an indication of how eBPF can create powerful networking features. You also saw some real-life examples of these networking features, including load balancing, firewalling, security mitigation, and Kubernetes networking.\n\n# Exercises and Further Reading\n\nHere are some ways to learn more about the range of networking use cases for eBPF:\n\n1.  Modify the example XDP program `ping()` so that it generates different trace messages for ping responses and ping requests. The ICMP header immediately follows the IP header in the network packet (just like the IP header follows the Ethernet header). You\u2019ll likely want to use `struct icmphdr` from *linux/icmp.h* and look at whether the type field shows `ICMP_ECHO` or `ICMP_ECHOREPLY`.\n\n2.  If you want to dive further into XDP programming, I recommend the xdp-project\u2019s [xdp-tutorial](https://oreil.ly/UmJMF).\n\n3.  Use [sslsniff](https://oreil.ly/Zuww7) from the BCC project to view the contents of encrypted traffic.\n\n4.  Explore Cilium by using tutorials and labs linked to from the [Cilium website](https://cilium.io/get-started).\n\n5.  Use the editor at [*networkpolicy.io*](https://networkpolicy.io) to visualize the effect of network policies in a Kubernetes deployment.\n\n^([1](ch08.html#ch08fn1-marker)) At the time of this writing, around 100 organizations have publicly announced their use of Cilium in its [*USERS.md* file](https://oreil.ly/PC7-G), though this number is growing quickly. Cilium has also been adopted by AWS, Google, and Microsoft.\n\n^([2](ch08.html#ch08fn2-marker)) This example is based on a talk I gave at eBPF Summit 2021 called [\u201cA Load Balancer from scratch\u201d](https://oreil.ly/mQxtT). Build an eBPF load balancer in just over 15 minutes!\n\n^([3](ch08.html#ch08fn3-marker)) If you want to explore this, try [CTF Challenge 3 from eBPF Summit 2022](https://oreil.ly/YIh_t). I won\u2019t give spoilers here in the book, but you can see the solution in [a walkthrough given by Duffie Cooley and me here](https://oreil.ly/_51rC).\n\n^([4](ch08.html#ch08fn4-marker)) See Daniel Borkmann\u2019s presentation [\u201cLittle Helper Minions for Scaling Microservices\u201d](https://oreil.ly/_8ZuF) that includes a history of eBPF, where he tells this anecdote.\n\n^([5](ch08.html#ch08fn5-marker)) Cilium maintains a [list of drivers that support XDP](https://oreil.ly/wCMjB) within the [BPF and XDP Reference Guide](https://oreil.ly/eB7vL).\n\n^([6](ch08.html#ch08fn6-marker)) Ceznam shared data about the performance boost its team saw when experimenting with an eBPF-based load balancer in [this blog post](https://oreil.ly/0cbCx).\n\n^([7](ch08.html#ch08fn7-marker)) For a more complete overview of TC and its concepts, I recommend Quentin Monnet\u2019s post [\u201cUnderstanding tc \u201cdirect action\u201d mode for BPF\u201d](https://oreil.ly/7gU2A).\n\n^([8](ch08.html#ch08fn8-marker)) There is also a blog post that accompanies this example at [*https://blog.px.dev/ebpf-openssl-tracing*](https://blog.px.dev/ebpf-openssl-tracing).\n\n^([9](ch08.html#ch08fn9-marker)) It\u2019s possible for pods to be run in the host\u2019s network namespace so that they share the IP address of the host, but this isn\u2019t usually done unless there\u2019s a good reason for an application running in the pod to require it.```"]
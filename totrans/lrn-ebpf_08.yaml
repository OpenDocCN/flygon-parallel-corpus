- en: Chapter 8\. eBPF for Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。网络的eBPF
- en: As you saw in [Chapter 1](ch01.html#what_is_ebpf_and_why_is_it_importantque),
    the dynamic nature of eBPF allows us to customize the behavior of the kernel.
    In the world of networking, there is a huge range of desirable behavior that depends
    on the application. For example, a telecommunications operator might have to interface
    with telco-specific protocols like SRv6; a Kubernetes environment might need to
    be integrated with legacy applications; dedicated hardware load balancers can
    be replaced with XDP programs running on commodity hardware. eBPF allows programmers
    to build networking features to meet specific needs, without having to force them
    on all upstream kernel users.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第1章](ch01.html#what_is_ebpf_and_why_is_it_importantque)中看到的，eBPF的动态特性允许我们定制内核的行为。在网络世界中，有许多取决于应用程序的理想行为。例如，电信运营商可能需要与SRv6等电信特定协议进行接口交互；Kubernetes环境可能需要与传统应用程序集成；专用硬件负载均衡器可以被运行在通用硬件上的XDP程序所取代。eBPF允许程序员构建网络功能以满足特定需求，而无需强加给所有上游内核用户。
- en: Network tools based on eBPF are now widely used and have proven to be effective
    at prolific scale. The CNCF’s [Cilium project](http://cilium.io), for example,
    uses eBPF as a platform for Kubernetes networking, standalone load balancing,
    and much more, and it’s used by cloud native adopters in every conceivable industry
    vertical.^([1](ch08.html#ch08fn1)) Meta has been using eBPF at a vast scale—every
    packet to and from Facebook since 2017 has been through an XDP program. Another
    public and hyper-scaled example is Cloudflare’s use of eBPF for DDoS (distributed
    denial-of-service) protection.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于eBPF的网络工具现在被广泛使用，并已被证明在大规模下非常有效。例如，CNCF的[Cilium项目](http://cilium.io)将eBPF用作Kubernetes网络、独立负载均衡等的平台，并且被云原生采用者在各种行业垂直领域广泛使用。^([1](ch08.html#ch08fn1))
    Meta自2017年以来一直在大规模使用eBPF——自Facebook以来的每个数据包都经过了XDP程序。另一个公开且超大规模的例子是Cloudflare使用eBPF进行DDoS（分布式拒绝服务）防护。
- en: These are complex, production-ready solutions, and their details are far beyond
    the scope of this book, but by reading the examples in this chapter you can get
    a feel for how eBPF networking solutions like these are built.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是复杂的、可投入生产的解决方案，它们的细节远远超出了本书的范围，但通过阅读本章的示例，您可以感受到像这样的eBPF网络解决方案是如何构建的。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: The code examples for this chapter are in the *chapter8* directory of the repository
    at [*github.com/lizrice/learning-ebpf*](https://github.com/lizrice/learning-ebpf).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码示例位于[*github.com/lizrice/learning-ebpf*](https://github.com/lizrice/learning-ebpf)存储库的*chapter8*目录中。
- en: Packet Drops
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据包丢弃
- en: 'There are several network security features that involve dropping certain incoming
    packets and allowing others. These features include firewalling, DDoS protection,
    and mitigating packet-of-death vulnerabilities:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个网络安全功能涉及丢弃某些传入数据包并允许其他数据包。这些功能包括防火墙、DDoS防护和减轻致命数据包漏洞：
- en: Firewalling involves deciding on a per-packet basis whether to allow a packet,
    based on the source and destination IP addresses and/or port numbers.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防火墙涉及基于每个数据包的源和目标IP地址和/或端口号决定是否允许数据包通过。
- en: DDoS protection adds some complexity, perhaps keeping track of the rate at which
    packets are arriving from a particular source and/or detecting certain characteristics
    of the packet contents to determine that an attacker or set of attackers is trying
    to flood the interface with traffic.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDoS防护增加了一些复杂性，也许需要跟踪从特定来源到达的数据包的速率和/或检测数据包内容的某些特征，以确定攻击者或一组攻击者是否试图用流量淹没接口。
- en: A packet-of-death vulnerability is a class of kernel vulnerability in which
    the kernel fails to safely process a packet crafted in a particular way. An attacker
    who sends packets with this particular format can exploit the vulnerability, which
    could potentially cause the kernel to crash. Traditionally, when a kernel vulnerability
    like this is found, it requires installing a new kernel with the fix, which in
    turn requires machine downtime. But an eBPF program that detects and drops these
    malicious packets can be installed dynamically, instantly protecting that host
    without affecting any applications running on the machine.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据包致命漏洞是一类内核漏洞，内核在处理特定方式构造的数据包时无法安全处理。发送具有这种特定格式的数据包的攻击者可以利用这个漏洞，这可能导致内核崩溃。传统上，当发现这样的内核漏洞时，需要安装带有修复的新内核，这又需要机器停机。但是，可以动态安装检测并丢弃这些恶意数据包的eBPF程序，立即保护主机，而不影响机器上运行的任何应用程序。
- en: The decision-making algorithms for features like these are beyond the scope
    of this book, but let’s explore how eBPF programs attached to the XDP hook on
    a network interface drop certain packets, which is the basis for implementing
    these use cases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能的决策算法超出了本书的范围，但让我们探讨一下如何通过附加到网络接口的XDP挂钩的eBPF程序丢弃某些数据包，这是实现这些用例的基础。
- en: XDP Program Return Codes
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XDP程序返回代码
- en: 'An XDP program is triggered by the arrival of a network packet. The program
    examines the packet, and when it’s done, the return code gives a *verdict* that
    indicates what to do next with that packet:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: XDP程序是由网络数据包到达时触发的。程序检查数据包，当完成时，返回代码给出了一个*决定*，指示下一步该如何处理该数据包：
- en: '`XDP_PASS` indicates that the packet should be sent to the network stack in
    the normal way (as it would have done if there were no XDP program).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XDP_PASS`表示数据包应该以正常方式发送到网络堆栈（如果没有XDP程序的话）。'
- en: '`XDP_DROP` causes the packet to be discarded immediately.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XDP_DROP`导致数据包立即被丢弃。'
- en: '`XDP_TX` sends the packet back out of the same interface it arrived on.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XDP_TX`将数据包发送回到它到达的同一接口。'
- en: '`XDP_REDIRECT` is used to send it to a different network interface.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XDP_REDIRECT`用于将数据包发送到不同的网络接口。'
- en: '`XDP_ABORTED` results in the packet being dropped, but its use implies an error
    case or something unexpected, rather than a “normal” decision to discard a packet.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XDP_ABORTED`导致数据包被丢弃，但其使用意味着错误情况或意外情况，而不是“正常”决定丢弃数据包。'
- en: 'For some use cases (like firewalling), the XDP program simply has to decide
    between passing the packet on or dropping it. An outline for an XDP program that
    decides whether to drop packets looks something like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些用例（如防火墙），XDP程序只需决定是传递数据包还是丢弃数据包。决定是否丢弃数据包的XDP程序概述如下：
- en: '[PRE0]``'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]``'
- en: '[PRE1]An XDP program can also manipulate the packet contents, but I’ll come
    to that later in this chapter.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]XDP程序还可以操作数据包内容，但我会在本章后面讨论这个问题。'
- en: XDP programs get triggered whenever an inbound network packet arrives on the
    interface to which it is attached. The `ctx` parameter is a pointer to an `xdp_md`
    structure, which holds metadata about the incoming packet. Let’s see how you can
    use this structure to examine the packet’s contents in order to reach a verdict.[PRE2]##
    XDP Packet Parsing
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每当入站网络数据包到达其所连接的接口时，XDP程序就会被触发。`ctx`参数是指向`xdp_md`结构的指针，该结构保存了有关传入数据包的元数据。让我们看看如何使用这个结构来检查数据包的内容，以便做出判断。[PRE2]##
    XDP数据包解析
- en: 'Here’s the definition of the `xdp_md` structure:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`xdp_md`结构的定义：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Don’t be fooled by the `__u32` type for the first three fields, as they are
    really pointers. The `data` field indicates the location in memory where the packet
    starts, and `data_end` shows where it ends. As you saw in [Chapter 6](ch06.html#the_ebpf_verifier),
    to pass the eBPF verifier you will have to explicitly check that any reads or
    writes to the packet’s contents are within the range `data` to `data_end`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被前三个字段的`__u32`类型所迷惑，因为它们实际上是指针。`data`字段指示数据包开始的内存位置，`data_end`显示数据包结束的位置。正如您在[第6章](ch06.html#the_ebpf_verifier)中看到的，为了通过eBPF验证器，您必须明确检查对数据包内容的任何读取或写入是否在`data`到`data_end`范围内。
- en: There is also an area in memory ahead of the packet, between `data_meta` and
    `data`, for storing metadata about this packet. This can be used for coordination
    between multiple eBPF programs that might process the same packet at various places
    on its journey through the stack.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包之前的内存区域，即`data_meta`和`data`之间，用于存储有关此数据包的元数据。这可用于协调多个eBPF程序之间的协作，这些程序可能会在数据包通过堆栈的各个位置进行处理。
- en: 'To illustrate the basics of parsing a network packet, there is an XDP program
    called `ping()` in the example code, which will simply generate a line of trace
    whenever it detects a ping (ICMP) packet. Here’s the code for that program:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明解析网络数据包的基础知识，示例代码中有一个名为`ping()`的XDP程序，它在检测到ping（ICMP）数据包时会生成一行跟踪。以下是该程序的代码：
- en: '[PRE4][PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4][PRE5]'
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'ping-26622   [000] d.s11 276880.862408: bpf_trace_printk: Hello ping'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26622   [000] d.s11 276880.862408: bpf_trace_printk: 你好ping'
- en: 'ping-26622   [000] d.s11 276880.862459: bpf_trace_printk: Hello ping'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26622   [000] d.s11 276880.862459: bpf_trace_printk: 你好ping'
- en: 'ping-26622   [000] d.s11 276881.889575: bpf_trace_printk: Hello ping'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26622   [000] d.s11 276881.889575: bpf_trace_printk: 你好ping'
- en: 'ping-26622   [000] d.s11 276881.889676: bpf_trace_printk: Hello ping'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26622   [000] d.s11 276881.889676: bpf_trace_printk: 你好ping'
- en: 'ping-26622   [000] d.s11 276882.910777: bpf_trace_printk: Hello ping'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26622   [000] d.s11 276882.910777: bpf_trace_printk: 你好ping'
- en: 'ping-26622   [000] d.s11 276882.910930: bpf_trace_printk: Hello ping'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26622   [000] d.s11 276882.910930: bpf_trace_printk: 你好ping'
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: if(protocol==1)// ICMP
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: if(protocol==1)// ICMP
- en: '{ `bpf_printk``(``"Hello ping"``);` ``return``XDP_DROP``;` ``}` ``return``XDP_PASS``;[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '{ `bpf_printk``(``"Hello ping"``);` ``return``XDP_DROP``;` ``}` ``return``XDP_PASS``;[PRE8]'
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'ping-26639   [002] d.s11 277050.589356: bpf_trace_printk: Hello ping'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26639   [002] d.s11 277050.589356: bpf_trace_printk: 你好ping'
- en: 'ping-26639   [002] d.s11 277051.615329: bpf_trace_printk: Hello ping'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26639   [002] d.s11 277051.615329: bpf_trace_printk: 你好ping'
- en: 'ping-26639   [002] d.s11 277052.637708: bpf_trace_printk: Hello ping'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'ping-26639   [002] d.s11 277052.637708: bpf_trace_printk: 你好ping'
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: unsignedcharlookup_protocol(structxdp_md*ctx){unsignedcharprotocol=0;void*data=(void*)(long)ctx->data;![1](assets/1.png)void*data_end=(void*)(long)ctx->data_end;structethhdr*eth=data;![2](assets/2.png)if(data+sizeof(structethhdr)>data_end)![3](assets/3.png)return0;//
    Check that it's an IP packet
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: unsignedcharlookup_protocol(structxdp_md*ctx){unsignedcharprotocol=0;void*data=(void*)(long)ctx->data;![1](assets/1.png)void*data_end=(void*)(long)ctx->data_end;structethhdr*eth=data;![2](assets/2.png)if(data+sizeof(structethhdr)>data_end)![3](assets/3.png)return0;//
    检查是否为IP数据包
- en: if(bpf_ntohs(eth->h_proto)==ETH_P_IP)![4](assets/4.png){// Return the protocol
    of this packet
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: if(bpf_ntohs(eth->h_proto)==ETH_P_IP)![4](assets/4.png){// 返回此数据包的协议}}
- en: // 1 = ICMP
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: // 1 = ICMP
- en: // 6 = TCP
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: // 6 = TCP
- en: // 17 = UDP
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: // 17 = UDP
- en: structiphdr*iph=data+sizeof(structethhdr);![5](assets/5.png)if(data+sizeof(structethhdr)+sizeof(structiphdr)<=data_end)![6](assets/6.png)protocol=iph->protocol;![7](assets/7.png)}returnprotocol;}
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: structiphdr*iph=data+sizeof(structethhdr);![5](assets/5.png)if(data+sizeof(structethhdr)+sizeof(structiphdr)<=data_end)![6](assets/6.png)protocol=iph->protocol;![7](assets/7.png)}returnprotocol;}
- en: '[PRE11][PRE12]  ``# Load Balancing and Forwarding'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11][PRE12]  ``# 负载均衡和转发'
- en: XDP programs aren’t limited to inspecting the contents of a packet. They can
    also modify the packet’s contents. Let’s consider what’s involved if you want
    to build a simple load balancer that takes packets sent to a given IP address
    and fans those requests to a number of backends that can fulfill the request.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: XDP程序不仅限于检查数据包的内容。它们还可以修改数据包的内容。如果您想构建一个简单的负载均衡器，以便接收发送到特定IP地址的数据包，并将这些请求传递给能够满足请求的多个后端，那么就需要考虑其中的内容。
- en: There’s an example of this in the GitHub repo.^([2](ch08.html#ch08fn2)) The
    setup here is a set of containers that run on the same host. There’s a client,
    a load balancer, and two backends, each running in their own container. As illustrated
    in [Figure 8-2](#example_load_balancer_setup), the load balancer receives traffic
    from the client and forwards it to one of the two backend containers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub存储库中有一个示例。这里的设置是一组运行在同一主机上的容器。有一个客户端、一个负载均衡器和两个后端，每个后端都在自己的容器中运行。如[图8-2](#example_load_balancer_setup)所示，负载均衡器接收来自客户端的流量，并将其转发到两个后端容器中的一个。
- en: '![Example load balancer setup](assets/lebp_0802.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![示例负载均衡器设置](assets/lebp_0802.png)'
- en: Figure 8-2\. Example load balancer setup
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2。示例负载均衡器设置
- en: The load balancing function is implemented as an XDP program attached to the
    load balancer’s eth0 network interface. The return code from this program is `XDP_TX`,
    indicating that the packet should be sent back out of the interface it came in
    on. But before that happens, the program has to update the address information
    in the packet headers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡功能被实现为一个附加到负载均衡器eth0网络接口的XDP程序。这个程序的返回代码是`XDP_TX`，表示数据包应该被发送回来自的接口。但在这之前，程序必须更新数据包头部中的地址信息。
- en: Although I think it’s useful as a learning exercise, this example code is very,
    very far from being production ready; for example, it uses hard-coded addresses
    that assume the exact setup of IP addresses shown in [Figure 8-2](#example_load_balancer_setup).
    It assumes that the only TCP traffic it will ever receive is requests from the
    client or responses to the client. It also cheats by taking advantage of the way
    Docker sets up virtual MAC addresses, using each container’s IP address as the
    last four bytes of the MAC address for the virtual Ethernet interface for each
    container. That virtual Ethernet interface is called eth0 from the perspective
    of the container.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我认为这是一个很有用的学习练习，但这个示例代码离生产就非常非常远；例如，它使用了硬编码的地址，假设IP地址的确切设置如[图8-2](#example_load_balancer_setup)所示。它假设它将收到的TCP流量只是来自客户端的请求或者发往客户端的响应。它还通过利用Docker设置虚拟MAC地址的方式作弊，使用每个容器的IP地址作为每个容器的虚拟以太网接口的MAC地址的最后四个字节。从容器的角度来看，这个虚拟以太网接口被称为eth0。
- en: 'Here’s the XDP program from the example load balancer code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是示例负载均衡器代码中的XDP程序：
- en: '[PRE13]cpp'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]cpp'
- en: '[![1](assets/1.png)](#code_id_8_8)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#code_id_8_8)'
- en: 'The first part of this function is practically the same as in the previous
    example: it locates the Ethernet header and then the IP header in the packet.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的第一部分实际上与前面的例子几乎相同：它在数据包中定位以太网头部，然后是IP头部。
- en: '[![2](assets/2.png)](#code_id_8_9)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#code_id_8_9)'
- en: This time it will process only TCP packets, passing anything else it receives
    on up the stack as if nothing had happened.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这次它只处理TCP数据包，将接收到的其他任何内容传递到堆栈上，就好像什么都没有发生过一样。
- en: '[![3](assets/3.png)](#code_id_8_10)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#code_id_8_10)'
- en: Here the source IP address is checked. If this packet didn’t come from the client,
    I will assume it is a response going to the client.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里检查源IP地址。如果这个数据包不是来自客户端的，我会假设它是发往客户端的响应。
- en: '[![4](assets/4.png)](#code_id_8_11)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#code_id_8_11)'
- en: This code generates a pseudorandom choice between backends A and B.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码在后端A和B之间生成一个伪随机选择。
- en: '[![5](assets/5.png)](#code_id_8_12)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#code_id_8_12)'
- en: The destination IP and MAC addresses are updated to match whichever backend
    was chosen…
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 目标IP和MAC地址将被更新以匹配所选择的后端…
- en: '[![6](assets/6.png)](#code_id_8_13)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#code_id_8_13)'
- en: …or if this is a response from a backend (which is the assumption here if it
    didn’t come from a client), the destination IP and MAC addresses are updated to
    match the client.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: …或者如果这是来自后端的响应（如果不是来自客户端的话，这里是假设），目标IP和MAC地址将被更新以匹配客户端。
- en: '[![7](assets/7.png)](#code_id_8_14)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#code_id_8_14)'
- en: Wherever this packet is going, the source addresses need to be updated so that
    it looks as though the packet originated from the load balancer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这个数据包去哪里，源地址都需要更新，以便看起来像是由负载均衡器发起的。
- en: '[![8](assets/8.png)](#code_id_8_15)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#code_id_8_15)'
- en: The IP header includes a checksum calculated over its contents, and since the
    source and destination IP addresses have both been updated, the checksum also
    needs to be recalculated and replaced in this packet.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: IP头部包括对其内容计算的校验和，由于源和目标IP地址都已经被更新，校验和也需要被重新计算并替换在这个数据包中。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since this is a book on eBPF and not networking, I haven’t delved into details
    such as why the IP and MAC addresses need to be updated or what happens if they
    aren’t. If you’re interested, I cover this some more in my [YouTube video of the
    eBPF Summit talk](https://oreil.ly/mQxtT) where I originally wrote this example
    code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本关于eBPF而不是网络的书，我没有深入探讨IP和MAC地址需要更新的原因，或者如果它们没有被更新会发生什么。如果你感兴趣，我在我的[eBPF峰会演讲的YouTube视频](https://oreil.ly/mQxtT)中对此进行了更多的介绍，我最初是在那里编写了这个示例代码。
- en: 'Much like the previous example, the Makefile includes instructions to not only
    build the code but also use `bpftool` to load and attach the XDP program to the
    interface, like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子类似，Makefile包括了指令，不仅构建代码，还使用`bpftool`将XDP程序加载并附加到接口上，就像这样：
- en: '[PRE14]cpp'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE14]cpp'
- en: 'This `make` instruction needs to be run *inside* the load balancer container
    so that eth0 corresponds to its virtual Ethernet interface. This leads to an interesting
    point: an eBPF program is loaded into the kernel, of which there is only one;
    yet the attachment point may be within a particular network namespace and visible
    only within that network namespace.^([3](ch08.html#ch08fn3))'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`make`指令需要在负载均衡器容器*内部*运行，以便eth0对应其虚拟以太网接口。这导致一个有趣的问题：一个eBPF程序被加载到内核中，只有一个；然而附着点可能在特定的网络命名空间内，并且只在该网络命名空间内可见。^([3](ch08.html#ch08fn3))
- en: XDP Offloading
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XDP卸载
- en: The idea for XDP originated from a conversation speculating how useful it would
    be if you could run eBPF programs on a network card to make decisions about individual
    packets before they even reach the kernel’s networking stack.^([4](ch08.html#ch08fn4))
    There are some network interface cards that support this full *XDP offload* capability
    where they can indeed run eBPF programs on inbound packets on their own processor.
    This is illustrated in [Figure 8-3](#network_interface_cards_that_support_xd).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: XDP的想法源自一次对话，推测如果可以在网络卡上运行eBPF程序来在数据包到达内核网络堆栈之前做出决策，那将会有多么有用。^([4](ch08.html#ch08fn4))
    有一些网络接口卡支持完整的*XDP卸载*功能，它们确实可以在自己的处理器上运行入站数据包的eBPF程序。这在[图8-3](#network_interface_cards_that_support_xd)中有所说明。
- en: '![Network interface cards that support XDP offload can process, drop, and retransmit
    packets without any work required from the host CPU](assets/lebp_0803.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![支持XDP卸载的网络接口卡可以处理、丢弃和重传数据包，而无需主机CPU执行任何工作](assets/lebp_0803.png)'
- en: Figure 8-3\. Network interface cards that support XDP offload can process, drop,
    and retransmit packets without any work required from the host CPU
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。支持XDP卸载的网络接口卡可以处理、丢弃和重传数据包，而无需主机CPU执行任何工作
- en: This means a packet that gets dropped or redirected back out of the same physical
    interface—like the packet drop and load balancing examples earlier in this chapter—is
    never seen by the host’s kernel, and no CPU cycles on the host machine are ever
    spent processing them, as all the work is done on the network card.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着被丢弃或重定向回同一物理接口的数据包——就像本章前面的数据包丢弃和负载均衡示例一样——从不会被主机的内核看到，并且主机机器上的CPU周期从未被用于处理它们，因为所有的工作都是在网络卡上完成的。
- en: Even if the physical network interface card doesn’t support full XDP offload,
    many NIC drivers support XDP hooks, which minimizes the memory copying required
    for an eBPF program to process a packet.^([5](ch08.html#ch08fn5))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 即使物理网络接口卡不支持完整的XDP卸载，许多网卡驱动程序支持XDP钩子，这最小化了eBPF程序处理数据包所需的内存复制。^([5](ch08.html#ch08fn5))
- en: This can result in significant performance benefits and allows functionality
    like load balancing to run very efficiently on commodity hardware.^([6](ch08.html#ch08fn6))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以带来显著的性能优势，并允许像负载均衡这样的功能在通用硬件上运行得非常高效。^([6](ch08.html#ch08fn6))
- en: You’ve seen how XDP can be used to process inbound network packets, accessing
    them as soon as possible as they arrive on a machine. eBPF can also be used to
    process traffic at other points in the network stack, in whatever direction it
    is flowing. Let’s move on and think about eBPF programs attached within the TC
    subsystem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到了XDP如何用于处理入站网络数据包，尽快访问它们，因为它们到达机器。eBPF也可以用于处理网络堆栈中的其他点的流量，无论流向如何。让我们继续思考一下附加在TC子系统中的eBPF程序。
- en: Traffic Control (TC)
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流量控制（TC）
- en: I mentioned traffic control in the previous chapter. By the time a network packet
    reaches this point it will be in kernel memory in the form of an [`sk_buff`](https://oreil.ly/TKDCF).
    This is a data structure that’s used throughout the kernel’s network stack. eBPF
    programs attached within the TC subsystem receive a pointer to the `sk_buff` structure
    as the context parameter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一章中提到了流量控制。当网络数据包到达这一点时，它将以[`sk_buff`](https://oreil.ly/TKDCF)的形式存在于内核内存中。这是一个在整个内核网络堆栈中使用的数据结构。附加在TC子系统中的eBPF程序接收`sk_buff`结构的指针作为上下文参数。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You might be wondering why XDP programs don’t also use this same structure for
    their context. The answer is that the XDP hook happens before the network data
    reaches the network stack and before the `sk_buff` structure has been set up.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么XDP程序不也使用相同的结构来进行上下文。答案是XDP钩子发生在网络数据到达网络堆栈之前，也发生在`sk_buff`结构被设置之前。
- en: The TC subsystem is intended to regulate how network traffic is scheduled. For
    example, you might want to limit the bandwidth available to each application so
    that they all get a fair chance. But when you’re looking at scheduling individual
    packets, *bandwidth* isn’t a terribly meaningful term, as it’s used for the average
    amount of data being sent or received. A given application might be very bursty,
    or another application might be very sensitive to network latency, so TC gives
    much finer control over the way packets are handled and prioritized.^([7](ch08.html#ch08fn7))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TC子系统旨在调节网络流量的调度。例如，您可能希望限制每个应用程序可用的带宽，以便它们都有公平的机会。但是，当您查看调度单个数据包时，“带宽”并不是一个非常有意义的术语，因为它用于发送或接收的平均数据量。给定的应用程序可能非常突发，或者另一个应用程序可能对网络延迟非常敏感，因此TC可以更精细地控制数据包的处理和优先级。^([7](ch08.html#ch08fn7))
- en: eBPF programs were introduced here to give custom control over the algorithms
    used within TC. But with the power to manipulate, drop, or redirect packets, eBPF
    programs attached within TC can also be used as the building blocks for complex
    network behaviors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF程序被引入这里是为了对TC中使用的算法进行自定义控制。但是，通过操纵、丢弃或重定向数据包的能力，附加在TC中的eBPF程序也可以用作复杂网络行为的构建模块。
- en: 'A given piece of network data in the stack flows in one of two directions:
    *ingress* (inbound from the network interface) or *egress* (outbound toward the
    network interface). eBPF programs can be attached in either direction and will
    affect traffic only in that direction. Unlike XDP, it’s possible to attach multiple
    eBPF programs that will be processed in sequence.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈中的给定网络数据流向两个方向之一：*入口*（从网络接口进入）或*出口*（向网络接口外发出）。eBPF程序可以附加在任一方向，并且只会影响该方向的流量。与XDP不同，可以附加多个eBPF程序，这些程序将按顺序处理。
- en: Traditional traffic control is split into *classifiers*, which classify packets
    based on some rule, and separate *actions*, which are taken based on the output
    from a classifier and determine what to do with a packet. There can be a series
    of classifiers, all defined as part of a *qdisc* or queuing discipline.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的流量控制分为*分类器*，根据某些规则对数据包进行分类，以及单独的*操作*，根据分类器的输出采取操作，并确定对数据包的处理方式。可以有一系列分类器，所有这些都被定义为*qdisc*或排队规则的一部分。
- en: 'eBPF programs are attached as a classifier, but they can also determine what
    action to take within the same program. The action is indicated by the program’s
    return code (whose values are defined in *linux/pkt_cls.h*):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF程序被附加为分类器，但它们也可以在同一个程序中确定要采取的操作。操作由程序的返回代码指示（其值在*linux/pkt_cls.h*中定义）：
- en: '`TC_ACT_SHOT` tells the kernel to drop the packet.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TC_ACT_SHOT`告诉内核丢弃数据包。'
- en: '`TC_ACT_UNSPEC` behaves as if the eBPF program hadn’t been run on this packet
    (so it would be passed to the next classifier in the sequence, if there is one).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TC_ACT_UNSPEC`的行为就像eBPF程序没有运行在这个数据包上一样（因此它将被传递到序列中的下一个分类器，如果有的话）。'
- en: '`TC_ACT_OK` tells the kernel to pass the packet to the next layer in the stack.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TC_ACT_OK` 告诉内核将数据包传递给堆栈中的下一层。'
- en: '`TC_ACT_REDIRECT` sends the packet to the ingress or egress path of a different
    network device.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TC_ACT_REDIRECT` 将数据包发送到不同网络设备的入口或出口路径。'
- en: 'Let’s take a look at a few simple examples of programs that can be attached
    within TC. The first simply generates a line of trace and then tells the kernel
    to drop the packet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些可以附加在 TC 中的简单程序的示例。第一个简单地生成一行跟踪，然后告诉内核丢弃数据包：
- en: '[PRE15]cpp'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE15]cpp'
- en: '[PRE16]cppNow let’s consider how to drop only a subset of packets. This example
    drops ICMP (ping) request packets and is very similar to the XDP example you saw
    earlier in this chapter:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16]cpp现在让我们考虑如何仅丢弃数据包的子集。此示例丢弃 ICMP（ping）请求数据包，与本章前面看到的 XDP 示例非常相似：'
- en: '[PRE17]cpp[PRE18]cpp  ``# Packet Encryption and Decryption'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE17]cpp[PRE18]cpp  ``# 数据包加密和解密'
- en: If an application uses encryption to secure data it sends or receives, there
    will be a point before it’s encrypted or after it’s decrypted where the data is
    in the clear. Recall that eBPF can attach programs pretty much anywhere on a machine,
    so if you can hook into a point where data is being passed and isn’t yet encrypted,
    or just after it has been decrypted, that would allow your eBPF program to observe
    that data in the clear. There’s no need to supply any certificates to decrypt
    the traffic, as you would in a traditional SSL inspection tool.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序使用加密来保护其发送或接收的数据，则在加密之前或解密之后将存在数据处于明文状态。回想一下，eBPF 可以几乎在机器的任何地方附加程序，因此如果您可以钩入数据传递并且尚未加密的点，或者刚刚解密后，那么您的
    eBPF 程序就可以观察到明文数据。无需提供任何证书来解密流量，就像传统的 SSL 检查工具中需要的那样。
- en: In many cases an application will encrypt data using a library like OpenSSL
    or BoringSSL that lives in user space. In this case the traffic will already be
    encrypted by the time it reaches the socket, which is the user space/kernel boundary
    for network traffic. If you want to trace out this data in its unencrypted form,
    you can use an eBPF program attached to the right place in the user space code.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，应用程序将使用像 OpenSSL 或 BoringSSL 这样的库对数据进行加密，这些库位于用户空间。在这种情况下，流量在到达套接字之前已经被加密，而套接字是网络流量的用户空间/内核边界。如果您想以未加密形式跟踪这些数据，可以使用附加到用户空间代码中正确位置的
    eBPF 程序。
- en: User Space SSL Libraries
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户空间 SSL 库
- en: One common way to trace out the decrypted content of encrypted packets is to
    hook into calls made to user space libraries like OpenSSL or BoringSSL. An application
    using OpenSSL sends data to be encrypted by making a call to a function called
    `SSL_write()` and retrieves cleartext data that was received over the network
    in encrypted form using `SSL_read()`. Hooking eBPF programs into these functions
    with uprobes allows an application to observe the data *from any application that
    uses this shared library* in the clear, before it is encrypted or after it has
    been decrypted. And there is no need for any keys, because those are already being
    provided by the application.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪加密数据包的解密内容的一种常见方法是钩入对用户空间库（如 OpenSSL 或 BoringSSL）的调用。使用 OpenSSL 的应用程序通过调用名为
    `SSL_write()` 的函数发送要加密的数据，并使用 `SSL_read()` 从以加密形式接收的网络中检索明文数据。使用 uprobes 将 eBPF
    程序钩入这些函数，允许应用程序观察*使用此共享库的任何应用程序*中的明文数据，在其加密之前或解密之后。而且无需任何密钥，因为这些已经由应用程序提供。
- en: 'There is a fairly straightforward example called [openssl-tracer in the Pixie
    project](https://oreil.ly/puDp9),^([8](ch08.html#ch08fn8)) within which the eBPF
    programs are in a file called *openssl_tracer_bpf_funcs.c*. Here’s the part of
    that code that sends data to user space, using a perf buffer (similar to examples
    you have seen earlier in this book):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pixie 项目中有一个相当简单的示例称为 [Pixie 项目中的 openssl-tracer](https://oreil.ly/puDp9)，其中
    eBPF 程序位于名为 *openssl_tracer_bpf_funcs.c* 的文件中。以下是该代码的一部分，它使用性能缓冲区将数据发送到用户空间（类似于本书中之前看到的示例）：
- en: '[PRE19]cpp``'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE19]cpp``'
- en: '[PRE20]cppYou can see that data from `buf` gets read into an `event` structure
    using the helper function `bpf_probe_read()`, and then that `event` structure
    is submitted to a perf buffer.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE20]cpp您可以看到使用辅助函数 `bpf_probe_read()` 将 `buf` 中的数据读入 `event` 结构，然后将该 `event`
    结构提交到性能缓冲区。'
- en: 'If this data is being sent to user space, it’s reasonable to assume this must
    be the data in unencrypted format. So where is this buffer of data obtained? You
    can work that out by seeing where the `process_SSL_data()` function is called.
    It’s called in two places: one for data being read and one for data being written.
    [Figure 8-4](#ebpf_programs_are_hooked_to_uprobes_at_) illustrates what is happening
    in the case of reading data that arrives on this machine in encrypted form.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此数据被发送到用户空间，可以合理地假设这必须是未加密格式的数据。那么这个数据缓冲区是在哪里获取的？您可以通过查看 `process_SSL_data()`
    函数的调用位置来解决这个问题。它在两个位置被调用：一个用于读取数据，一个用于写入数据。[图 8-4](#ebpf_programs_are_hooked_to_uprobes_at_)
    说明了在加密形式到达此机器的数据被读取时发生了什么。
- en: When you’re reading data, you supply a pointer to a buffer to `SSL_read()`,
    and when the function returns, that buffer will contain the unencrypted data.
    Much like kprobes, the input parameters to a function—including that buffer pointer—are
    only available to a uprobe attached to the entry point, as the registers they’re
    held in might well get overwritten during the function’s execution. But the data
    won’t be available in the buffer until the function exits, when you can read it
    using a uretprobe.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当您读取数据时，您向 `SSL_read()` 提供一个指向缓冲区的指针，当函数返回时，该缓冲区将包含未加密的数据。与 kprobes 类似，函数的输入参数（包括缓冲区指针）仅在附加到入口点的
    uprobe 中可用，因为它们所在的寄存器在函数执行期间可能会被覆盖。但是在函数退出时，数据将不会在缓冲区中可用，此时您可以使用 uretprobe 读取它。
- en: '![eBPF programs are hooked to uprobes at the entry to and exit from SSL_read()
    so that the unencrypted data can be read from the buffer pointer](assets/lebp_0804.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![eBPF 程序在 SSL_read() 的入口和出口处被钩入 uprobes，以便从缓冲区指针中读取未加密数据](assets/lebp_0804.png)'
- en: Figure 8-4\. eBPF programs are hooked to uprobes at the entry to and exit from
    `SSL_read()` so that the unencrypted data can be read from the buffer pointer
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。eBPF程序在`SSL_read()`的入口和出口处挂钩，以便从缓冲指针中读取未加密数据
- en: 'So this example follows a common pattern for kprobes and uprobes, illustrated
    in [Figure 8-4](#ebpf_programs_are_hooked_to_uprobes_at_), where the entry probe
    temporarily stores input parameters using a map, from which the exit probe can
    retrieve them. Let’s look at the code that does this, starting with the eBPF program
    attached to the start of `SSL_read()`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个例子遵循了kprobes和uprobes的常见模式，如[图8-4](#ebpf_programs_are_hooked_to_uprobes_at_)所示，入口探针临时使用映射存储输入参数，退出探针可以从中检索这些参数。让我们看看执行此操作的代码，从附加到`SSL_read()`开头的eBPF程序开始：
- en: '[PRE21]cpp'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE21]cpp'
- en: '[![1](assets/1.png)](#code_id_8_21)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#code_id_8_21)'
- en: As described in the comment for this function, the buffer pointer is the second
    parameter passed into the `SSL_read()` function to which this probe will be attached.
    The `PT_REGS_PARM2` macro gets this parameter from the context.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如此函数的注释所述，缓冲指针是传递给`SSL_read()`函数的第二个参数，该探针将附加到该函数。`PT_REGS_PARM2`宏从上下文中获取此参数。
- en: '[![2](assets/2.png)](#code_id_8_22)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#code_id_8_22)'
- en: The buffer pointer is stored in a hash map, for which the key is the current
    process and thread ID, obtained at the start of the function using the helper
    `bpf_get_current_pid_tgif()`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲指针存储在哈希映射中，其键是在函数开始时使用辅助函数`bpf_get_current_pid_tgid()`获取的当前进程和线程ID。
- en: 'Here’s the corresponding program for the exit probe:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是退出探针的相应程序：
- en: '[PRE22]cpp'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22]cpp'
- en: '[![1](assets/1.png)](#code_id_8_23)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#code_id_8_23)'
- en: Having looked up the current process and thread ID, use this as the key to retrieve
    the buffer pointer from the hash map.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 查找当前进程和线程ID，将其用作键从哈希映射中检索缓冲指针。
- en: '[![2](assets/2.png)](#code_id_8_24)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#code_id_8_24)'
- en: If this isn’t a null pointer, call `process_SSL_data()`, which is the function
    you saw earlier that sends the data from that buffer to user space using the perf
    buffer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不是空指针，则调用`process_SSL_data()`，这是您之前看到的将数据从该缓冲区发送到用户空间的函数，使用perf缓冲区。
- en: '[![3](assets/3.png)](#code_id_8_25)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#code_id_8_25)'
- en: Clean up the entry in the hash map, since every entry call should be paired
    with an exit.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 清理哈希映射中的条目，因为每个条目调用都应该与一个退出配对。
- en: This example shows how to trace out the cleartext version of encrypted data
    that gets sent and received by a user space application. The tracing itself is
    attached to a user space library, and there’s no guarantee that every application
    will use a given SSL library. The BCC project includes a utility called [*sslsniff*](https://oreil.ly/tFT9p)
    that also supports GnuTLS and NSS. But if someone’s application uses some other
    encryption library (or even, heaven forbid, they chose to “roll their own crypto”),
    the uprobes simply won’t have the right places to hook to and these tracing tools
    won’t work.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了如何跟踪用户空间应用程序发送和接收的加密数据的明文版本。跟踪本身附加到用户空间库，不能保证每个应用程序都会使用给定的SSL库。BCC项目包括一个名为[*sslsniff*](https://oreil.ly/tFT9p)的实用程序，还支持GnuTLS和NSS。但是，如果某人的应用程序使用其他加密库（甚至，天哪，他们选择“自己编写加密”），uprobes就无法找到正确的挂钩位置，这些跟踪工具就无法工作。
- en: There are even more common reasons why this uprobe-based approach might not
    be successful. Unlike the kernel (of which there is only one per [virtual] machine),
    there can be multiple copies of user space library code. If you’re using containers,
    each one is likely to have its own set of all library dependencies. You can hook
    into uprobes in these libraries, but you’d have to identify the right copy for
    the particular container you want to trace. Another possibility is that rather
    than using a shared, dynamically linked library, an application might be statically
    linked so that it’s a single standalone executable.[PRE23]```
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至有更常见的原因可能导致这种基于uprobes的方法不成功。与内核不同（每台[虚拟]机器只有一个内核），用户空间库代码可以有多个副本。如果您使用容器，每个容器可能都有自己的所有库依赖项。您可以在这些库中挂钩uprobes，但您必须识别要跟踪的特定容器的正确副本。另一种可能性是，应用程序可能不是使用共享的动态链接库，而是静态链接，因此它是一个单独的可执行文件。[PRE23]```

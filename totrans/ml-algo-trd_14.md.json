["```py\nnlp = spacy.load('en')\nnlp.pipe_names\n['tagger', 'parser', 'ner'] \n```", "```py\nsample_text = 'Apple is looking at buying U.K. startup for $1 billion'\ndoc = nlp(sample_text) \n```", "```py\npd.DataFrame([[t.text, t.lemma_, t.pos_, t.tag_, t.dep_, t.shape_, \n               t.is_alpha, t.is_stop]\n              for t in doc],\n             columns=['text', 'lemma', 'pos', 'tag', 'dep', 'shape', \n                      'is_alpha', is_stop']) \n```", "```py\ndisplacy.render(doc, style='dep', options=options, jupyter=True) \n```", "```py\nspacy.explain(\"VBZ\") \nverb, 3rd person singular present \n```", "```py\n    files = Path('..', 'data', 'bbc').glob('**/*.txt')\n    bbc_articles = []\n    for i, file in enumerate(sorted(list(files))):\n        with file.open(encoding='latin1') as f:\n            lines = f.readlines()\n            body = ' '.join([l.strip() for l in lines[1:]]).strip()\n            bbc_articles.append(body)\n    len(bbc_articles)\n    2225 \n    ```", "```py\ndoc = nlp(bbc_articles[0])\ntype(doc)\nspacy.tokens.doc.Doc \n```", "```py\nsentences = [s for s in doc.sents]\nsentences[:3]\n[Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (\u00c2\u00a3600m) for the three months to December, from $639m year-earlier.  ,\n The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.,\n TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.] \n```", "```py\nfor t in sentences[0]:\n    if t.ent_type_:\n        print('{} | {} | {}'.format(t.text, t.ent_type_, spacy.explain(t.ent_type_)))\nQuarterly | DATE | Absolute or relative dates or periods\nUS | GPE | Countries, cities, states\nTimeWarner | ORG | Companies, agencies, institutions, etc. \n```", "```py\nentities = [e.text for e in entities(doc)]\npd.Series(entities).value_counts().head()\nTimeWarner        7\nAOL               5\nfourth quarter    3\nyear-earlier      2\none               2 \n```", "```py\npd.Series([n.text for n in ngrams(doc, n=2, min_freq=2)]).value_counts()\nfourth quarter     3\nquarter profits    2\nTime Warner        2\ncompany said       2\nAOL Europe         2 \n```", "```py\niter_texts = (bbc_articles[i] for i in range(len(bbc_articles)))\nfor i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=8)):\n    assert doc.is_parsed \n```", "```py\nmodel = {}\nfor language in ['en', 'es']:\n    model[language] = spacy.load(language) \n```", "```py\ntext = {}\npath = Path('../data/TED')\nfor language in ['en', 'es']:\n    file_name = path /  'TED2013_sample.{}'.format(language)\n    text[language] = file_name.read_text() \n```", "```py\nparsed, sentences = {}, {}\nfor language in ['en', 'es']:\n    parsed[language] = model[language](text[language])\n    sentences[language] = list(parsed[language].sents)\n    print('Sentences:', language, len(sentences[language]))\nSentences: en 22\nSentences: es 22 \n```", "```py\npos = {}\nfor language in ['en', 'es']:\n    pos[language] = pd.DataFrame([[t.text, t.pos_, spacy.explain(t.pos_)] \n                                  for t in sentences[language][0]],\n                                 columns=['Token', 'POS Tag', 'Meaning'])\npd.concat([pos['en'], pos['es']], axis=1).head() \n```", "```py\nfrom textblob import TextBlob\narticle = docs.sample(1).squeeze()\nparsed_body = TextBlob(article.body) \n```", "```py\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\n[(word, stemmer.stem(word)) for i, word in enumerate(parsed_body.words) \n if word.lower() != stemmer.stem(parsed_body.words[i])]\n[('Manchester', 'manchest'),\n ('United', 'unit'),\n ('reduced', 'reduc'),\n ('points', 'point'),\n ('scrappy', 'scrappi') \n```", "```py\nparsed_body.sentiment\nSentiment(polarity=0.088031914893617, subjectivity=0.46456433637284694) \n```", "```py\nbinary_vectorizer = CountVectorizer(max_df=1.0,\n                                    min_df=1,\n                                    binary=True)\nbinary_dtm = binary_vectorizer.fit_transform(docs.body)\n<2225x29275 sparse matrix of type '<class 'numpy.int64'>'\n   with 445870 stored elements in Compressed Sparse Row format> \n```", "```py\nm = binary_dtm.todense()        # pdist does not accept sparse format\npairwise_distances = pdist(m, metric='cosine')\nclosest = np.argmin(pairwise_distances)  # index that minimizes distance\nrows, cols = np.triu_indices(n_docs)      # get row-col indices\nrows[closest], cols[closest]\n(6, 245) \n```", "```py\nnlp = spacy.load('en')\ndef tokenizer(doc):\n    return [w.lemma_ for w in nlp(doc) \n                if not w.is_punct | w.is_space]\nvectorizer = CountVectorizer(tokenizer=tokenizer, binary=True)\ndoc_term_matrix = vectorizer.fit_transform(docs.body) \n```", "```py\nsample_docs = ['call you tomorrow',\n               'Call me a taxi',\n               'please call me... PLEASE!'] \n```", "```py\nvectorizer = CountVectorizer()\ntf_dtm = vectorizer.fit_transform(sample_docs).todense()\ntokens = vectorizer.get_feature_names()\nterm_frequency = pd.DataFrame(data=tf_dtm,\n                             columns=tokens)\n  call  me  please  taxi  tomorrow  you\n0     1   0       0     0         1    1\n1     1   1       0     1         0    0\n2     1   1       2     0         0    0 \n```", "```py\nvectorizer = CountVectorizer(binary=True)\ndf_dtm = vectorizer.fit_transform(sample_docs).todense().sum(axis=0)\ndocument_frequency = pd.DataFrame(data=df_dtm,\n                                  columns=tokens)\n   call  me  please  taxi  tomorrow  you\n0     3   2       1     1         1    1 \n```", "```py\ntfidf = pd.DataFrame(data=tf_dtm/df_dtm, columns=tokens)\n   call   me  please  taxi  tomorrow  you\n0  0.33 0.00    0.00  0.00      1.00 1.00\n1  0.33 0.50    0.00  1.00      0.00 0.00\n2  0.33 0.50    2.00  0.00      0.00 0.00 \n```", "```py\nvect = TfidfVectorizer(smooth_idf=True,\n                      norm='l2',  # squared weights sum to 1 by document\n                      sublinear_tf=False,  # if True, use 1+log(tf)\n                      binary=False)\npd.DataFrame(vect.fit_transform(sample_docs).todense(),\n            columns=vect.get_feature_names())\n   call   me  please  taxi  tomorrow  you\n0  0.39 0.00    0.00  0.00      0.65 0.65\n1  0.43 0.55    0.00  0.72      0.00 0.00\n2  0.27 0.34    0.90  0.00      0.00 0.00 \n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2225 entries, 0 to 2224\nData columns (total 3 columns):\ntopic      2225 non-null object\nheading    2225 non-null object\nbody       2225 non-null object \n```", "```py\ny = pd.factorize(docs.topic)[0] # create integer class values\nX = docs.body\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,\n                                                    stratify=y) \n```", "```py\nvectorizer = CountVectorizer()\nX_train_dtm = vectorizer.fit_transform(X_train)\nX_test_dtm = vectorizer.transform(X_test)\nX_train_dtm.shape, X_test_dtm.shape\n((1668, 25919), (557, 25919)) \n```", "```py\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\ny_pred_class = nb.predict(X_test_dtm) \n```", "```py\naccuracy_score(y_test, y_pred_class)\n0.97666068222621 \n```", "```py\nvectorizer = CountVectorizer(min_df=.001, max_df=.8, stop_words='english')\ntrain_dtm = vectorizer.fit_transform(train.text)\n<1566668x934 sparse matrix of type '<class 'numpy.int64'>'\n    with 6332930 stored elements in Compressed Sparse Row format> \n```", "```py\nnb = MultinomialNB()\nnb.fit(train_dtm, train.polarity)\npredicted_polarity = nb.predict(test_dtm) \n```", "```py\naccuracy_score(test.polarity, predicted_polarity)\n0.7768361581920904 \n```", "```py\ntrain_numeric = sparse.csr_matrix(train_dummies.astype(np.uint))\ntrain_dtm_numeric = sparse.hstack((train_dtm, train_numeric)) \n```", "```py\ntest['predicted'] = train.stars.mode().iloc[0]\naccuracy_score(test.stars, test.predicted)\n0.5196950594793454 \n```", "```py\nnb = MultinomialNB()\nnb.fit(train_dtm,train.stars)\npredicted_stars = nb.predict(test_dtm) \n```", "```py\naccuracy_score(test.stars, predicted_stars)\n0.6465164206691094 \n```", "```py\ndef evaluate_model(model, X_train, X_test, name, store=False):\n    start = time()\n    model.fit(X_train, train.stars)\n    runtime[name] = time() \u2013 start\n    predictions[name] = model.predict(X_test)\n    accuracy[result] = accuracy_score(test.stars, predictions[result])\n    if store:\n        joblib.dump(model, f'results/{result}.joblib')\nCs = np.logspace(-5, 5, 11)\nfor C in Cs:\n    model = LogisticRegression(C=C, multi_class='multinomial', solver='lbfgs')\n    evaluate_model(model, train_dtm, test_dtm, result, store=True) \n```", "```py\nparam = {'objective':'multiclass', 'num_class': 5}\nbooster = lgb.train(params=param, \n                    train_set=lgb_train, \n                    num_boost_round=500, \n                    early_stopping_rounds=20,\n                    valid_sets=[lgb_train, lgb_test]) \n```"]
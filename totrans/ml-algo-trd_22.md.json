["```py\ngrid_size = (3, 4)\nblocked_cell = (1, 1)\nbaseline_reward = -0.02\nabsorbing_cells = {(0, 3): 1, (1, 3): -1}\nactions = ['L', 'U', 'R', 'D']\nnum_actions = len(actions)\nprobs = [.1, .8, .1, 0] \n```", "```py\nto_1d = lambda x: np.ravel_multi_index(x, grid_size)\nto_2d = lambda x: np.unravel_index(x, grid_size) \n```", "```py\nnum_states = np.product(grid_size)\ncells = list(np.ndindex(grid_size))\nstates = list(range(len(cells)))\ncell_state = dict(zip(cells, states))\nstate_cell= dict(zip(states, cells))\nabsorbing_states = {to_1d(s):r for s, r in absorbing_cells.items()}\nblocked_state = to_1d(blocked_cell) \n```", "```py\nstate_rewards = np.full(num_states, baseline_reward)\nstate_rewards[blocked_state] = 0\nfor state, reward in absorbing_states.items():\n    state_rewards[state] = reward\nstate_rewards\narray([-0.02, -0.02, -0.02,  1\\.  , -0.02,  0\\.  , -0.02, -1\\.  , -0.02,\n       -0.02, -0.02, -0.02]) \n```", "```py\naction_outcomes = {}\nfor i, action in enumerate(actions):\n    probs_ = dict(zip([actions[j % 4] for j in range(i, \n                                               num_actions + i)], probs))\n    action_outcomes[actions[(i + 1) % 4]] = probs_\nAction_outcomes\n{'U': {'L': 0.1, 'U': 0.8, 'R': 0.1, 'D': 0},\n 'R': {'U': 0.1, 'R': 0.8, 'D': 0.1, 'L': 0},\n 'D': {'R': 0.1, 'D': 0.8, 'L': 0.1, 'U': 0},\n 'L': {'D': 0.1, 'L': 0.8, 'U': 0.1, 'R': 0}} \n```", "```py\ndef get_new_cell(state, move):\n    cell = to_2d(state)\n    if actions[move] == 'U':\n        return cell[0] - 1, cell[1]\n    elif actions[move] == 'D':\n        return cell[0] + 1, cell[1]\n    elif actions[move] == 'R':\n        return cell[0], cell[1] + 1\n    elif actions[move] == 'L':\n        return cell[0], cell[1] - 1 \n```", "```py\ndef update_transitions_and_rewards(state, action, outcome):\n    if state in absorbing_states.keys() or state == blocked_state:\n        transitions[action, state, state] = 1\n    else:\n        new_cell = get_new_cell(state, outcome)\n        p = action_outcomes[actions[action]][actions[outcome]]\n        if new_cell not in cells or new_cell == blocked_cell:\n            transitions[action, state, state] += p\n            rewards[action, state, state] = baseline_reward\n        else:\n            new_state= to_1d(new_cell)\n            transitions[action, state, new_state] = p\n            rewards[action, state, new_state] = state_rewards[new_state] \n```", "```py\nrewards = np.zeros(shape=(num_actions, num_states, num_states))\ntransitions = np.zeros((num_actions, num_states, num_states))\nactions_ = list(range(num_actions))\nfor action, outcome, state in product(actions_, actions_, states):\n    update_transitions_and_rewards(state, action, outcome)\nrewards.shape, transitions.shape\n((4,12,12), (4,12,12)) \n```", "```py\nskip_states = list(absorbing_states.keys())+[blocked_state]\nstates_to_update = [s for s in states if s not in skip_states] \n```", "```py\nV = np.random.rand(num_states)\nV[skip_states] = 0\ngamma = .99\nepsilon = 1e-5 \n```", "```py\nwhile True:\n    V_ = np.copy(V)\n    for state in states_to_update:\n        q_sa = np.sum(transitions[:, state] * (rewards[:, state] + gamma* V), \n                      axis=1)\n        V[state] = np.max(q_sa)\n    if np.sum(np.fabs(V - V_)) < epsilon:\n        break \n```", "```py\npd.DataFrame(V.reshape(grid_size))\n         0         1         2         3\n0.884143  0.925054  0.961986  0.000000\n1  0.848181  0.000000  0.714643  0.000000\n2  0.808344  0.773327  0.736099  0.516082 \n```", "```py\ndef policy_improvement(value, transitions):\n    for state, reward in absorbing_states.items():\n        value[state] = reward\n    return np.argmax(np.sum(transitions * value, 2),0) \n```", "```py\npi = np.random.choice(list(range(num_actions)), size=num_states) \n```", "```py\niterations = 0\nconverged = False\nwhile not converged:\n    pi_ = np.copy(pi)\n    for state in states_to_update:\n        action = policy[state]\n        V[state] = np.dot(transitions[action, state], \n                                      rewards[action, state] + gamma* V)\n        pi = policy_improvement(V.copy(), transitions)\n    if np.array_equal(pi_, pi):\n        converged = True\n    iterations += 1 \n```", "```py\nvi = mdp.ValueIteration(transitions=transitions,\n                        reward=rewards,\n                        discount=gamma,\n                        epsilon=epsilon)\nvi.run() \n```", "```py\nnp.allclose(V.reshape(grid_size), np.asarray(vi.V).reshape(grid_size)) \n```", "```py\npi = mdp.PolicyIteration(transitions=transitions,\n                        reward=rewards,\n                        discount=gamma,\n                        max_iter=1000)\npi.run() \n```", "```py\nmax_episodes = 2500\nalpha = .1\nepsilon = .05 \n```", "```py\nQ = np.random.rand(num_states, num_actions)\nQ[skip_states] = 0 \n```", "```py\nfor episode in range(max_episodes):\n    state = np.random.choice([s for s in states if s not in skip_states])\n    while not state in absorbing_states.keys():\n        if np.random.rand() < epsilon:\n            action = np.random.choice(num_actions)\n        else:\n            action = np.argmax(Q[state])\n        next_state = np.random.choice(states, p=transitions[action, state])\n        reward = rewards[action, state, next_state]\n        Q[state, action] += alpha * (reward + \n                            gamma * np.max(Q[next_state])-Q[state, action])\n        state = next_state \n```", "```py\n    class DDQNAgent:\n        def __init__(self, state_dim, num_actions, gamma,\n                     epsilon_start, epsilon_end, epsilon_decay_steps,\n                     epsilon_exp_decay,replay_capacity, learning_rate,\n                     architecture, l2_reg, tau, batch_size,\n                     log_dir='results'): \n    ```", "```py\n def build_model(self, trainable=True):\n        layers = []\n        for i, units in enumerate(self.architecture, 1):\n            layers.append(Dense(units=units,\n                                input_dim=self.state_dim if i == 1 else None,\n                                activation='relu',\n                                kernel_regularizer=l2(self.l2_reg),\n                                trainable=trainable))\n        layers.append(Dense(units=self.num_actions, \n                            trainable=trainable))\n        model = Sequential(layers)\n        model.compile(loss='mean_squared_error',\n                      optimizer=Adam(lr=self.learning_rate))\n        return model \n```", "```py\n def memorize_transition(self, s, a, r, s_prime, not_done):\n        if not_done:\n            self.episode_reward += r\n            self.episode_length += 1\n        else:\n            self.episodes += 1\n            self.rewards_history.append(self.episode_reward)\n            self.steps_per_episode.append(self.episode_length)\n            self.episode_reward, self.episode_length = 0, 0\n        self.experience.append((s, a, r, s_prime, not_done)) \n```", "```py\n def experience_replay(self):\n        if self.batch_size > len(self.experience):\n            return\n        # sample minibatch from experience\n        minibatch = map(np.array, zip(*sample(self.experience, \n                                              self.batch_size)))\n        states, actions, rewards, next_states, not_done = minibatch\n        # predict next Q values to select best action\n        next_q_values = self.online_network.predict_on_batch(next_states)\n        best_actions = tf.argmax(next_q_values, axis=1)\n        # predict the TD target\n        next_q_values_target = self.target_network.predict_on_batch(\n            next_states)\n        target_q_values = tf.gather_nd(next_q_values_target,\n                                       tf.stack((self.idx, tf.cast(\n                                          best_actions, tf.int32)), axis=1))\n        targets = rewards + not_done * self.gamma * target_q_values\n        # predict q values\n        q_values = self.online_network.predict_on_batch(states)\n        q_values[[self.idx, actions]] = targets\n        # train model\n        loss = self.online_network.train_on_batch(x=states, y=q_values)\n        self.losses.append(loss)\n        if self.total_steps % self.tau == 0:\n            self.update_target()\n    def update_target(self):\n        self.target_network.set_weights(self.online_network.get_weights()) \n```", "```py\nenv = gym.make('LunarLander-v2')\nstate_dim = env.observation_space.shape[0]  # number of dimensions in state\nnum_actions = env.action_space.n  # number of actions\nmax_episode_steps = env.spec.max_episode_steps  # max number of steps per episode\nenv.seed(42) \n```", "```py\nfrom gym import wrappers\nenv = wrappers.Monitor(env,\n                       directory=monitor_path.as_posix(),\n                       video_callable=lambda count: count % video_freq == 0,\n                      force=True) \n```", "```py\ngamma=.99,  # discount factor\nlearning_rate=1e-4  # learning rate \n```", "```py\ntau=100  # target network update frequency\nreplay_capacity=int(1e6)\nbatch_size = 1024 \n```", "```py\nepsilon_start=1.0\nepsilon_end=0.01\nepsilon_linear_steps=250\nepsilon_exp_decay=0.99 \n```", "```py\nclass DataSource:\n    \"\"\"Data source for TradingEnvironment\n    Loads & preprocesses daily price & volume data\n    Provides data for each new episode.\n    \"\"\"\n    def __init__(self, trading_days=252, ticker='AAPL'):\n        self.ticker = ticker\n        self.trading_days = trading_days\n    def load_data(self):\n        idx = pd.IndexSlice\n        with pd.HDFStore('../data/assets.h5') as store:\n            df = (store['quandl/wiki/prices']\n                  .loc[idx[:, self.ticker],\n                       ['adj_close', 'adj_volume', 'adj_low', 'adj_high']])\n        df.columns = ['close', 'volume', 'low', 'high']\n        return df \n```", "```py\ndef preprocess_data(self):\n\"\"\"calculate returns and percentiles, then removes missing values\"\"\"\n   self.data['returns'] = self.data.close.pct_change()\n   self.data['ret_2'] = self.data.close.pct_change(2)\n   self.data['ret_5'] = self.data.close.pct_change(5)\n   self.data['rsi'] = talib.STOCHRSI(self.data.close)[1]\n   self.data['atr'] = talib.ATR(self.data.high, \n                                self.data.low, self.data.close)\n   self.data = (self.data.replace((np.inf, -np.inf), np.nan)\n                .drop(['high', 'low', 'close'], axis=1)\n                .dropna())\n   if self.normalize:\n       self.data = pd.DataFrame(scale(self.data),\n                                columns=self.data.columns,\n                                index=self.data.index) \n```", "```py\ndef take_step(self):\n    \"\"\"Returns data for current trading day and done signal\"\"\"\n    obs = self.data.iloc[self.offset + self.step].values\n    self.step += 1\n    done = self.step > self.trading_days\n    return obs, done \n```", "```py\ndef take_step(self, action, market_return):\n    \"\"\" Calculates NAVs, trading costs and reward\n        based on an action and latest market return\n        returns the reward and an activity summary\"\"\"\n    start_position = self.positions[max(0, self.step - 1)]\n    start_nav = self.navs[max(0, self.step - 1)]\n    start_market_nav = self.market_navs[max(0, self.step - 1)]\n    self.market_returns[self.step] = market_return\n    self.actions[self.step] = action\n    end_position = action - 1 # short, neutral, long\n    n_trades = end_position \u2013 start_position\n    self.positions[self.step] = end_position\n    self.trades[self.step] = n_trades\n    time_cost = 0 if n_trades else self.time_cost_bps\n    self.costs[self.step] = abs(n_trades) * self.trading_cost_bps + time_cost\n    if self.step > 0:\n        reward = start_position * market_return - self.costs[self.step-1]\n        self.strategy_returns[self.step] = reward\n        self.navs[self.step] = start_nav * (1 + \n                                            self.strategy_returns[self.step])\n        self.market_navs[self.step] = start_market_nav * (1 + \n                                            self.market_returns[self.step])\n    self.step += 1\n    return reward \n```", "```py\nclass TradingEnvironment(gym.Env):\n    \"\"\"A simple trading environment for reinforcement learning.\n    Provides daily observations for a stock price series\n    An episode is defined as a sequence of 252 trading days with random start\n    Each day is a 'step' that allows the agent to choose one of three actions.\n    \"\"\"\n    def __init__(self, trading_days=252, trading_cost_bps=1e-3,\n                 time_cost_bps=1e-4, ticker='AAPL'):\n        self.data_source = DataSource(trading_days=self.trading_days,\n                                      ticker=ticker)\n        self.simulator = TradingSimulator(\n                steps=self.trading_days,\n                trading_cost_bps=self.trading_cost_bps,\n                time_cost_bps=self.time_cost_bps)\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(self.data_source.min_values,\n                                            self.data_source.max_values) \n```", "```py\ndef reset(self):\n    \"\"\"Resets DataSource and TradingSimulator; returns first observation\"\"\"\n    self.data_source.reset()\n    self.simulator.reset()\n    return self.data_source.take_step()[0] \n```", "```py\ndef step(self, action):\n    \"\"\"Returns state observation, reward, done and info\"\"\"\n    assert self.action_space.contains(action), \n      '{} {} invalid'.format(action, type(action))\n    observation, done = self.data_source.take_step()\n    reward, info = self.simulator.take_step(action=action,\n                                            market_return=observation[0])\n    return observation, reward, done, info \n```", "```py\nfrom gym.envs.registration import register\nregister(\n        id='trading-v0',\n        entry_point='trading_env:TradingEnvironment',\n        max_episode_steps=252) \n```", "```py\ntrading_environment = gym.make('trading-v0')\ntrading_environment.env.trading_cost_bps = 1e-3\ntrading_environment.env.time_cost_bps = 1e-4\ntrading_environment.env.ticker = 'AAPL'\ntrading_environment.seed(42) \n```", "```py\nLayer (type)                 Output Shape              Param #   \nDense_1 (Dense)              (None, 64)                704       \nDense_2 (Dense)              (None, 64)                4160      \ndropout (Dropout)            (None, 64)                0         \nOutput (Dense)               (None, 3)                 195       \nTotal params: 5,059\nTrainable params: 5,059 \n```", "```py\nfor episode in range(1, max_episodes + 1):\n    this_state = trading_environment.reset()\n    for episode_step in range(max_episode_steps):\n        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, \n                                                               state_dim))\n        next_state, reward, done, _ = trading_environment.step(action)\n\n        ddqn.memorize_transition(this_state, action,\n                                 reward, next_state,\n                                 0.0 if done else 1.0)\n        ddqn.experience_replay()\n        if done:\n            break\n        this_state = next_state\ntrading_environment.close() \n```"]
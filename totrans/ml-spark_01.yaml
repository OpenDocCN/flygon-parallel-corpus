- en: Getting Up and Running with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Spark** is a framework for distributed computing; this framework aims
    to make it simpler to write programs that run in parallel across many nodes in
    a cluster of computers or virtual machines. It tries to abstract the tasks of
    resource scheduling, job submission, execution, tracking, and communication between
    nodes as well as the low-level operations that are inherent in parallel data processing.
    It also provides a higher level API to work with distributed data. In this way,
    it is similar to other distributed processing frameworks such as Apache Hadoop;
    however, the underlying architecture is somewhat different.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark began as a research project at the AMP lab in University of California,
    Berkeley ([https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/](https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/)).
    The university was focused on the use case of distributed machine learning algorithms.
    Hence, it is designed from the ground up for high performance in applications
    of an iterative nature, where the same data is accessed multiple times. This performance
    is achieved primarily through caching datasets in memory combined with low latency
    and overhead to launch parallel computation tasks. Together with other features
    such as fault tolerance, flexible distributed-memory data structures, and a powerful
    functional API, Spark has proved to be broadly useful for a wide range of large-scale
    data processing tasks, over and above machine learning and iterative analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, you can visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/community.html](http://spark.apache.org/community.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/community.html#history](http://spark.apache.org/community.html#history)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance wise, Spark is much faster than Hadoop for related workloads. Refer
    to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://amplab.cs.berkeley.edu/wp-content/uploads/2011/11/spark-lr.png'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark runs in four modes:'
  prefs: []
  type: TYPE_NORMAL
- en: The standalone local mode, where all Spark processes are run within the same
    **Java Virtual Machine** (**JVM**) process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standalone cluster mode, using Spark's own built-in, job-scheduling framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Mesos**, a popular open source cluster-computing framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using YARN (commonly referred to as NextGen MapReduce), Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Spark binaries and set up a development environment that runs in
    Spark's standalone local mode. This environment will be used throughout the book
    to run the example code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore Spark's programming model and API using Spark's interactive console.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write our first Spark program in Scala, Java, R, and Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a Spark cluster using Amazon's **Elastic Cloud Compute** (**EC2**) platform,
    which can be used for large-sized data and heavier computational requirements,
    rather than running in the local mode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a Spark Cluster using Amazon Elastic Map Reduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have previous experience in setting up Spark and are familiar with the
    basics of writing a Spark program, feel free to skip this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and setting up Spark locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can be run using the built-in standalone cluster scheduler in the local
    mode. This means that all the Spark processes are run within the same JVM-effectively,
    a single, multithreaded instance of Spark. The local mode is very used for prototyping,
    development, debugging, and testing. However, this mode can also be useful in
    real-world scenarios to perform parallel computation across multiple cores on
    a single computer.
  prefs: []
  type: TYPE_NORMAL
- en: As Spark's local mode is fully compatible with the cluster mode; programs written
    and tested locally can be run on a cluster with just a few additional steps.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in setting up Spark locally is to download the latest version
    [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html),
    which contains links to download various versions of Spark as well as to obtain
    the latest source code via GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: The documents/docs available at [http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)
    are a comprehensive resource to learn more about Spark. We highly recommend that
    you explore it!
  prefs: []
  type: TYPE_NORMAL
- en: Spark needs to be built against a specific version of Hadoop in order to access
    **Hadoop Distributed File System** (**HDFS**) as well as standard and custom Hadoop
    input sources Cloudera's Hadoop Distribution, MapR's Hadoop distribution, and
    Hadoop 2 (YARN). Unless you wish to build Spark against a specific Hadoop version,
    we recommend that you download the prebuilt Hadoop 2.7 package from an Apache
    mirror from [http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz).
  prefs: []
  type: TYPE_NORMAL
- en: Spark requires the Scala programming language (version 2.10.x or 2.11.x at the
    time of writing this book) in order to run. Fortunately, the prebuilt binary package
    comes with the Scala runtime packages included, so you don't need to install Scala
    separately in order to get started. However, you will need to have a **Java Runtime
    Environment** (**JRE**) or **Java Development Kit** (**JDK**).
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the software and hardware list in this book's code bundle for installation
    instructions. R 3.1+ is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the Spark binary package, unpack the contents of the
    package and change it to the newly created directory by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark places user scripts to run Spark in the `bin` directory. You can test
    whether everything is working correctly by running one of the example programs
    included in Spark. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run the example in Spark''s local standalone mode. In this mode,
    all the Spark processes are run within the same JVM, and Spark uses multiple threads
    for parallel processing. By default, the preceding example uses a number of threads
    equal to the number of cores available on your system. Once the program is executed,
    you should see something similar to the following lines toward the end of the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command calls class `org.apache.spark.examples.SparkPi` class.
  prefs: []
  type: TYPE_NORMAL
- en: This class takes parameter in the `local[N]` form, where `N` is the number of
    threads to use. For example, to use only two threads, run the following command
    `instead:N` is the number of threads to use. Giving `local[*]` will use all of
    the cores on the local machine--that is a common usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use only two threads, run the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Spark clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Spark cluster is made up of two types of processes: a driver program and
    multiple executors. In the local mode, all these processes are run within the
    same JVM. In a cluster, these processes are usually run on separate nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a typical cluster that runs in Spark''s standalone mode (that
    is, using Spark''s built-in cluster management modules) will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A master node that runs the Spark standalone master process as well as the driver
    program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number of worker nodes, each running an executor process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While we will be using Spark''s local standalone mode throughout this book
    to illustrate concepts and examples, the same Spark code that we write can be
    run on a Spark cluster. In the preceding example, if we run the code on a Spark
    standalone cluster, we could simply pass in the URL for the master node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, `IP` is the IP address and `PORT` is the port of the Spark master. This
    tells Spark to run the program on the cluster where the Spark master process is
    running.
  prefs: []
  type: TYPE_NORMAL
- en: A full treatment of Spark's cluster management and deployment is beyond the
    scope of this book. However, we will briefly teach you how to set up and use an
    Amazon EC2 cluster later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an overview of the Spark cluster-application deployment, take a look at
    the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/cluster-overview.html](http://spark.apache.org/docs/latest/cluster-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark programming model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we delve into a high-level overview of Spark's design, we will introduce
    the `SparkContext` object as well as the Spark shell, which we will use to interactively
    explore the basics of the Spark programming model.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this section provides a brief overview and examples of using Spark, we
    recommend that you read the following documentation to get a detailed understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: For the Spark Quick Start refer to, [http://spark.apache.org/docs/latest/quick-start](http://spark.apache.org/docs/latest/quick-start)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the Spark Programming guide, which covers Scala, Java, Python and R--, refer
    to, [http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkContext and SparkConf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The starting point of writing any Spark program is `SparkContext` (or `JavaSparkContext`
    in Java). `SparkContext` is initialized with an instance of a `SparkConf` object,
    which contains various Spark cluster-configuration settings (for example, the
    URL of the master node).
  prefs: []
  type: TYPE_NORMAL
- en: It is a main entry point for Spark functionality. A `SparkContext` is a connection
    to a Spark cluster. It can be used to create RDDs, accumulators, and broadcast
    variables on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Only one `SparkContext` is active per JVM. You must call `stop()`, which is
    the active `SparkContext`, before creating a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once initialized, we will use the various methods found in the `SparkContext`
    object to create and manipulate distributed datasets and shared variables. The
    Spark shell (in both Scala and Python, which is unfortunately not supported in
    Java) takes care of this context initialization for us, but the following lines
    of code show an example of creating a context running in the local mode in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a context running in the local mode with four threads, with the
    name of the application set to `Test Spark App`. If we wish to use the default
    configuration values, we could also call the following simple constructor for
    our `SparkContext` object, which works in the exact same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the example code
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book from any other source, you can visit[http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: SparkSession
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`SparkSession` allows programming with the `DataFrame` and Dataset APIs. It
    is a single point of entry for these APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create an instance of the `SparkConf` class and use it to
    create the `SparkSession` instance. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we can use spark object to create a `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Spark shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark supports writing programs interactively using the Scala, Python, or R
    **REPL** (that is, the **Read-Eval-Print-Loop**, or interactive shell). The shell
    provides instant feedback as we enter code, as this code is immediately evaluated.
    In the Scala shell, the return result and type is also displayed after a piece
    of code is run.
  prefs: []
  type: TYPE_NORMAL
- en: To use the Spark shell with Scala, simply run `./bin/spark-shell` from the Spark
    base directory. This will launch the Scala shell and initialize `SparkContext`,
    which is available to us as the Scala value, `sc`. With Spark 2.0, a `SparkSession`
    instance in the form of Spark variable is available in the console as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your console output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the Python shell with Spark, simply run the `./bin/pyspark` command.
    Like the Scala shell, the Python `SparkContext` object should be available as
    the Python variable, `sc`. Your output should be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**R** is a language and has a runtime environment for statistical computing
    and graphics. It is a GNU project. R is a different implementation of **S** (a
    language developed by Bell Labs).'
  prefs: []
  type: TYPE_NORMAL
- en: R provides statistical (linear and nonlinear modeling, classical statistical
    tests, time-series analysis, classification, and clustering) and graphical techniques.
    It is considered to be highly extensible.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Spark using R, run the following command to open Spark-R shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Resilient Distributed Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core of Spark is a concept called the **Resilient Distributed Dataset**
    (**RDD**). An RDD is a collection of *records* (strictly speaking, objects of
    some type) that are distributed or partitioned across many nodes in a cluster
    (for the purposes of the Spark local mode, the single multithreaded process can
    be thought of in the same way). An RDD in Spark is fault-tolerant; this means
    that if a given node or task fails (for some reason other than erroneous user
    code, such as hardware failure, loss of communication, and so on), the RDD can
    be reconstructed automatically on the remaining nodes and the job will still be
    completed.
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RDDs can be Scala Spark shells that you launched earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: RDDs can also be created from Hadoop-based input sources, including the local
    filesystem, HDFS, and Amazon S3\. A Hadoop-based RDD can utilize any input format
    that implements the Hadoop `InputFormat` interface, including text files, other
    standard Hadoop formats, HBase, Cassandra, tachyon, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is an example of creating an RDD from a text file located
    on the local filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `textFile` method returns an RDD where each record is a `String`
    object that represents one line of the text file. The output of the preceding
    command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is an example of how to create an RDD from a text file located
    on the HDFS using `hdfs://` protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is an example of how to create an RDD from a text file located
    on the Amazon S3 using `s3n://` protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Spark operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have created an RDD, we have a distributed collection of records that
    we can manipulate. In Spark's programming model, operations are split into transformations
    and actions. Generally speaking, a transformation operation applies some function
    to all the records in the dataset, changing the records in some way. An action
    typically runs some computation or aggregation operation and returns the result
    to the driver program where `SparkContext` is running.
  prefs: []
  type: TYPE_NORMAL
- en: Spark operations are functional in style. For programmers familiar with functional
    programming in Scala, Python, or Lambda expressions in Java 8, these operations
    should seem natural. For those without experience in functional programming, don't
    worry; the Spark API is relatively easy to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common transformations that you will use in Spark programs
    is the map operator. This applies a function to each record of an RDD, thus *mapping*
    the input to some new output. For example, the following code fragment takes the
    RDD we created from a local text file and applies the `size` function to each
    record in the RDD. Remember that we created an RDD of Strings. Using `map`, we
    can transform each string to an integer, thus returning an RDD of `Ints`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to the following line in your shell; this indicates
    the type of the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we saw the use of the `=>` syntax. This is the Scala
    syntax for an anonymous function, which is a function that is not a named method
    (that is, one defined using the `def` keyword in Scala or Python, for example).
  prefs: []
  type: TYPE_NORMAL
- en: While a detailed treatment of anonymous functions is beyond the scope of this
    book, they are used extensively in Spark code in Scala and Python, as well as
    in Java 8 (both in examples and real-world applications), so it is useful to cover
    a few practicalities.
  prefs: []
  type: TYPE_NORMAL
- en: The line `=> line.size` syntax means that we are applying a function where `=>`
    is the operator, and the output is the result of the code to the right of the
    `=>` operator. In this case, the input is line, and the output is the result of
    calling `line.size`. In Scala, this function that maps a string to an integer
    is expressed as `String => Int`.
  prefs: []
  type: TYPE_NORMAL
- en: This syntax saves us from having to separately define functions every time we
    use methods such as map; this is useful when the function is simple and will only
    be used once, as in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can apply a common action operation, count, to return the number of
    records in our RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result should look something like the following console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Perhaps we want to find the average length of each line in this text file.
    We can first use the `sum` function to add up all the lengths of all the records
    and then divide the sum by the number of records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark operations, in most cases, return a new RDD, with the exception of most
    actions, which return the result of a computation (such as `Long` for count and
    `Double` for sum in the preceding example). This means that we can naturally chain
    together operations to make our program flow more concise and expressive. For
    example, the same result as the one in the preceding line of code can be achieved
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: An important point to note is that Spark transformations are lazy. That is,
    invoking a transformation on an RDD does not immediately trigger a computation.
    Instead, transformations are chained together and are effectively only computed
    when an action is called. This allows Spark to be more efficient by only returning
    results to the driver when necessary so that the majority of operations are performed
    in parallel on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that if your Spark program never uses an action operation, it will
    never trigger an actual computation, and you will not get any results. For example,
    the following code will simply return a new RDD that represents the chain of transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following result in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that no actual computation happens and no result is returned. If we
    now call an action, such as sum, on the resulting RDD, the computation will be
    triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You will now see that a Spark job is run, and it results in the following console
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The complete list of transformations and actions possible on RDDs, as well as
    a set of more detailed examples, are available in the Spark programming guide
    (located at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html)),
    and the API documentation (the Scala API documentation) is located at ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)).
  prefs: []
  type: TYPE_NORMAL
- en: Caching RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most powerful features of Spark is the ability to cache data in
    memory across a cluster. This is achieved through the use of the cache method
    on an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Calling `cache` on an RDD tells Spark that the RDD should be kept in memory.
    The first time an action is called on the RDD that initiates a computation, the
    data is read from its source and put into memory. Hence, the first time such an
    operation is called, the time it takes to run the task is partly dependent on
    the time it takes to read the data from the input source. However, when the data
    is accessed the next time (for example, in subsequent queries in analytics or
    iterations in a machine learning model), the data can be read directly from memory,
    thus avoiding expensive I/O operations and speeding up the computation, in many
    cases, by a significant factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now call the `count` or `sum` function on our cached RDD, the RDD is
    loaded into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark also allows more fine-grained control over caching behavior. You can
    use the persist method to specify what approach Spark uses to cache data. More
    information on RDD caching can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence](http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence)'
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables and accumulators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another core feature of Spark is the ability to create two special types of
    variables--broadcast variables and accumulators.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **broadcast variable** is a *read-only* variable that is created from the
    driver program object and made available to the nodes that will execute the computation.
    This is very useful in applications that need to make the same data available
    to the worker nodes in an efficient manner, such as distributed systems. Spark
    makes creating broadcast variables as simple as calling a method on `SparkContext`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'A broadcast variable can be accessed from nodes other than the driver program
    that created it (that is, the worker nodes) by calling `value` on the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a new RDD with three records from a collection (in this case,
    a Scala `List`) of `("1", "2", "3")`. In the map function, it returns a new collection
    with the relevant rom our new RDD appended to the `broadcastAList` that is our
    broadcast variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `collect` method in the preceding code. This is a Spark *action*
    that returns the entire RDD to the driver as a Scala (or Python or Java) collection.
  prefs: []
  type: TYPE_NORMAL
- en: We will often use when we wish to apply further processing to our results locally
    within the driver program.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `collect` should generally only be used in cases where we really want
    to return the full result set to the driver and perform further processing. If
    we try to call `collect` on a very large dataset, we might run out of memory on
    the driver and crash our program.
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to perform as much heavy-duty processing on our Spark cluster
    as possible, preventing the driver from becoming a bottleneck. In many cases,
    however, such as during iterations in many machine learning models, collecting
    results to the driver is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'On inspecting the result, we will see that for each of the three records in
    our new RDD, we now have a record that is our original broadcasted `List`, with
    the new element appended to it (that is, there is now `"1"`, `"2"`, or `"3"` at
    the end):'
  prefs: []
  type: TYPE_NORMAL
- en: An **accumulator** is also a variable that is broadcasted to the worker nodes.
    The key difference between a broadcast variable and an accumulator is that while
    the `broadcast` variable is read-only, the accumulator can be added to. There
    are limitations to this, that is, in particular, the addition must be an associative
    operation so that the global accumulated value can be correctly computed in parallel
    and returned to the driver program. Each worker node can only access and add to
    its own local accumulator value, and only the driver program can access the global
    value. Accumulators are also accessed within the Spark code using the value method.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on broadcast variables and accumulators, refer to the *Shared
    Variables* section of the *Spark Programming Guide* at [http://spark.apache.org/docs/latest/programming-guide.html#shared-variables](http://spark.apache.org/docs/latest/programming-guide.html#shared-variables).
  prefs: []
  type: TYPE_NORMAL
- en: SchemaRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SchemaRDD** is a combination of RDD and schema information. It also offers
    many rich and easy-to-use APIs (that is, the `DataSet` API). SchemaRDD is not
    used with 2.0 and is internally used by `DataFrame` and `Dataset` APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: A schema is used to describe how structured data is logically organized. After
    obtaining the schema information, the SQL engine is able to provide the structured
    query capability for the corresponding data. The `DataSet` API is a replacement
    for Spark SQL parser's functions. It is an API to achieve the original program
    logic tree. Subsequent processing steps reuse Spark SQL's core logic. We can safely
    consider `DataSet` API's processing functions as completely equivalent to that
    of SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: SchemaRDD is an RDD subclass. When a program calls the `DataSet` API, a new
    SchemaRDD object is created, and a logic plan attribute of the `new` object is
    created by adding a new logic operation node on the original logic plan tree.
    Operations of the `DataSet` API (like RDD) are of two types--**Transformation**
    and **Action**.
  prefs: []
  type: TYPE_NORMAL
- en: APIs related to the relational operations are attributed to the Transformation
    type.
  prefs: []
  type: TYPE_NORMAL
- en: Operations associated with data output sources are of Action type. Like RDD,
    a Spark job is triggered and delivered for cluster execution, only when an Action
    type operation is called.
  prefs: []
  type: TYPE_NORMAL
- en: Spark data frame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Apache Spark, a `Dataset` is a distributed collection of data. The `Dataset`
    is a new interface added since Spark 1.6\. It provides the benefits of RDDs with
    the benefits of Spark SQL's execution engine. A `Dataset` can be constructed from
    JVM objects and then manipulated using functional transformations (`map`, `flatMap`,
    `filter`, and so on). The `Dataset` API is available only for in Scala and Java.
    It is not available for Python or R.
  prefs: []
  type: TYPE_NORMAL
- en: A `DataFrame` is a dataset with named columns. It is equivalent to a table in
    a relational database or a data frame in R/Python, with richer optimizations.
    `DataFrame` is constructed from structured data files, tables in Hive, external
    databases, or existing RDDs. The `DataFrame` API is available in Scala, Python,
    Java, and R.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Spark `DataFrame` needs the Spark session instantiated first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a `DataFrame` from a Json file using the `spark.read.json`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that Spark `Implicits` are being used to implicitly convert RDD to Data
    Frame types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Implicit methods available in Scala for converting common Scala objects into
    `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Output will be similar to the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we want to see how this is actually loaded in the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The first step to a Spark program in Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now use the ideas we introduced in the previous section to write a basic
    Spark program to manipulate a dataset. We will start with Scala and then write
    the same program in Java and Python. Our program will be based on exploring some
    data from an online store, about which users have purchased which products. The
    data is contained in a **Comma-Separated-Value** (**CSV**) file called `UserPurchaseHistory.csv`.
    This file is expected to be in the `data` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contents are shown in the following snippet. The first column of the CSV
    is the username, the second column is the product name, and the final column is
    the price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: For our Scala program, we need to create two files-our Scala code and our project
    build configuration file-using the build tool **Scala Build Tool** (**SBT**).
    For ease of use, we recommend that you use -spark-app for this chapter. This code
    also contains the CSV file under the data directory. You will need SBT installed
    on your system in order to run this example program (we use version 0.13.8 at
    the time of writing this book).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up SBT is beyond the scope of this book; however, you can find more
    information at [http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html](http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our SBT configuration file, `build.sbt`, looks like this (note that the empty
    lines between each line of code are required):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The last line adds the dependency on Spark to our project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Scala program is contained in the `ScalaApp.scala` file. We will walk through
    the program piece by piece. First, we need to import the required Spark classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In our main method, we need to initialize our `SparkContext` object and use
    this to access our CSV data file with the `textFile` method. We will then map
    the raw text by splitting the string on the delimiter character (a comma in this
    case) and extracting the relevant records for username, product, and price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have an RDD, where each record is made up of (`user`, `product`,
    `price`), we can compute various interesting metrics for our store, such as the
    following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of purchases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of unique users who purchased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our total revenue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our most popular product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s compute the preceding metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This last piece of code to compute the most popular product is an example of
    the *Map/Reduce* pattern made popular by Hadoop. First, we mapped our records
    of (`user`, `product`, `price`) to the records of `(product, 1)`. Then, we performed
    a `reduceByKey` operation, where we summed up the 1s for each unique product.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have this transformed RDD, which contains the number of purchases for
    each product, we will call `collect`, which returns the results of the computation
    to the driver program as a local Scala collection. We will then sort these counts
    locally (note that in practice, if the amount of data is large, we will perform
    the sorting in parallel, usually with a Spark operation such as `sortByKey`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will print out the results of our computations to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run this program by running `sbt run` in the project''s base directory
    or by running the program in your Scala IDE if you are using one. The output should
    look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we have `5` purchases from four different users with total revenue
    of `39.91`. Our most popular product is an `iPhone cover with 2 purchases`.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to a Spark program in Java
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Java API is very similar in principle to the Scala API. However, while Scala
    can call the Java code quite easily, in some cases, it is not possible to call
    the Scala code from Java. This is particularly the case when Scala code makes
    use of Scala features such as implicit conversions, default parameters, and the
    Scala reflection API.
  prefs: []
  type: TYPE_NORMAL
- en: Spark makes heavy use of these features in general, so it is necessary to have
    a separate API specifically for Java that includes Java versions of the common
    classes. Hence, `SparkContext` becomes `JavaSparkContext` and RDD becomes JavaRDD.
  prefs: []
  type: TYPE_NORMAL
- en: Java versions prior to version 8 do not support anonymous functions and do not
    have succinct syntax for functional-style programming, so functions in the Spark
    Java API must implement a `WrappedFunction` interface with the `call` method signature.
    While it is significantly more verbose, we will often create one-off anonymous
    classes to pass to our Spark operations, which implement this interface and the
    `call` method to achieve much the same effect as anonymous functions in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides support for Java 8's anonymous function (or *lambda*) syntax.
    Using this syntax makes a Spark program written in Java 8 look very close to the
    equivalent Scala program.
  prefs: []
  type: TYPE_NORMAL
- en: In Scala, an RDD of key/value pairs provides special operators (such as `reduceByKey`
    and `saveAsSequenceFile`, for example) that are accessed automatically via implicit
    conversions. In Java, special types of `JavaRDD` classes are required in order
    to access similar functions. These include `JavaPairRDD` to work with key/value
    pairs and `JavaDoubleRDD` to work with numerical records.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the standard Java API syntax. For more details and
    examples related to working RDDs in Java, as well as the Java 8 lambda syntax,
    refer to the Java sections of the *Spark Programming Guide* found at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: We will see examples of most of these differences in the following Java program,
    which is included in the example code of this chapter in the directory named `java-spark-app`.
    The `code` directory also contains the CSV data file under the `data` subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: We will build and run this project with the **Maven** build tool, which we assume
    you have installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and setting up Maven is beyond the scope of this book. Usually, Maven
    can easily be installed using the package manager on your Linux system or HomeBrew
    or MacPorts on Mac OS X.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed installation instructions can be found at [http://maven.apache.org/download.cgi](http://maven.apache.org/download.cgi).
  prefs: []
  type: TYPE_NORMAL
- en: 'The project contains a Java file called `JavaApp.java`, which contains our
    program code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As in our Scala example, we first need to initialize our context. Note that
    we will use the `JavaSparkContext` class here instead of the `SparkContext` class
    that we used earlier. We will use the `JavaSparkContext` class in the same way
    to access our data using `textFile` and then split each row into the required
    fields. Note how we used an anonymous class to define a split function that performs
    the string processing in the highlighted code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the same metrics as we did in our Scala example. Note how
    some methods are the same (for example, `distinct` and `count`) for the Java and
    Scala APIs. Also note the use of anonymous classes that we pass to the map function.
    This code is highlighted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following lines of code, we can see that the approach to compute the
    most popular product is the same as that in the Scala example. The extra code
    might seem complex, but it is mostly related to the Java code required to create
    the anonymous functions (which we have highlighted here). The actual functionality
    is the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, the general structure is similar to the Scala version, apart
    from the extra boilerplate code used to declare variables and functions via anonymous
    inner classes. It is a good exercise to work through both examples and compare
    lines of Scala code to those in Java to understand how the same result is achieved
    in each language.
  prefs: []
  type: TYPE_NORMAL
- en: 'This program can be run with the following command executed from the project''s
    base directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see output that looks very similar to the Scala version with identical
    results of the computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The first step to a Spark program in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark's Python API exposes virtually all the functionalities of Spark's Scala
    API in the Python language. There are some features that are not yet supported
    (for example, graph processing with GraphX and a few API methods here and there).
    Refer to the Python section of *Spark Programming Guide* ([http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html))
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '**PySpark** is built using Spark''s Java API. Data is processed in native Python,
    cached, and shuffled in JVM. Python driver program''s `SparkContext` uses Py4J
    to launch a JVM and create a `JavaSparkContext`. The driver uses Py4J for local
    communication between the Python and Java `SparkContext` objects. RDD transformations
    in Python map to transformations on `PythonRDD` objects in Java. `PythonRDD` object
    launches Python sub-processes on remote worker machines, communicate with them
    using pipes. These sub-processes are used to send the user''s code and to process
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Following on from the preceding examples, we will now write a Python version.
    We assume that you have Python version 2.6 and higher installed on your system
    (for example, most Linux and Mac OS X systems come with Python preinstalled).
  prefs: []
  type: TYPE_NORMAL
- en: The example program is included in the sample code for this chapter, in the
    directory named `python-spark-app`, which also contains the CSV data file under
    the `data` subdirectory. The project contains a script, `pythonapp.py`, provided
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple Spark app in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If you compare the Scala and Python versions of our program, you will see that
    generally, the syntax looks very similar. One key difference is how we express
    anonymous functions (also called `lambda` functions; hence, the use of this keyword
    for the Python syntax). In Scala, we''ve seen that an anonymous function mapping
    an input `x` to an output `y` is expressed as `x => y`, while in Python, it is
    `lambda x: y`. In the highlighted line in the preceding code, we are applying
    an anonymous function that maps two inputs, `a` and `b`, generally of the same
    type, to an output. In this case, the function that we apply is the plus function;
    hence, `lambda a, b: a + b`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to run the script is to run the following command from the base
    directory of the sample project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `SPARK_HOME` variable should be replaced with the path of the directory
    in which you originally unpacked the Spark prebuilt binary package at the start
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon running the script, you should see output similar to that of the Scala
    and Java examples, with the results of our computation being the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The first step to a Spark program in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SparkR** is an R package which provides a frontend to use Apache Spark from
    R. In Spark 1.6.0; SparkR provides a distributed data frame on large datasets.
    SparkR also supports distributed machine learning using MLlib. This is something
    you should try out while reading machine learning chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: SparkR DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DataFrame` is a collection of data organized into names columns that are distributed.
    This concept is very similar to a relational database or a data frame of R but
    with much better optimizations. Source of these data frames could be a CSV, a
    TSV, Hive tables, local R data frames, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark distribution can be run using the `./bin/sparkR shell`.
  prefs: []
  type: TYPE_NORMAL
- en: Following on from the preceding examples, we will now write an R version. We
    assume that you have R (R version 3.0.2 (2013-09-25)-*Frisbee Sailing*), R Studio
    and higher installed on your system (for example, most Linux and Mac OS X systems
    come with Python preinstalled).
  prefs: []
  type: TYPE_NORMAL
- en: The example program is included in the sample code for this chapter, in the
    directory named `r-spark-app`, which also contains the CSV data file under the
    `data` subdirectory. The project contains a script, `r-script-01.R`, which is
    provided in the following. Make sure you change `PATH` to appropriate value for
    your environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script with the following command on the bash terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output will be similar to the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Getting Spark running on Amazon EC2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Spark project provides scripts to run a Spark cluster in the cloud on Amazon''s
    EC2 service. These scripts are located in the `ec2` directory. You can run the
    `spark-ec2` script contained in this directory with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Running it in this way without an argument will show the help output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Before creating a Spark EC2 cluster, you will need to ensure that you have an
  prefs: []
  type: TYPE_NORMAL
- en: Amazon account.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have an Amazon Web Services account, you can sign up at [http://aws.amazon.com/](http://aws.amazon.com/).
  prefs: []
  type: TYPE_NORMAL
- en: The AWS console is available at [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need to create an Amazon EC2 key pair and retrieve the relevant
    security credentials. The Spark documentation for EC2 (available at [http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html))
    explains the requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an Amazon EC2 key pair for yourself. This can be done by logging into
    your Amazon Web Services account through the AWS console, clicking on Key Pairs
    on the left sidebar, and creating and downloading a key. Make sure that you set
    the permissions for the private key file to 600 (that is, only you can read and
    write it) so that ssh will work.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you want to use the spark-ec2 script, set the environment variables
    `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to your Amazon EC2 access key
    `ID` and secret access key, respectively. These can be obtained from the AWS homepage
    by clicking Account | Security Credentials | Access Credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating a key pair, choose a name that is easy to remember. We will simply
    use the name *spark* for the key pair. The key pair file itself will be called
    `spark.pem`. As mentioned earlier, ensure that the key pair file permissions are
    set appropriately and that the environment variables for the AWS credentials are
    exported using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: You should also be careful to keep your downloaded key pair file safe and not
    lose it, as it can only be downloaded once when it is created!
  prefs: []
  type: TYPE_NORMAL
- en: Note that launching an Amazon EC2 cluster in the following section will *incur
    costs* to your AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: Launching an EC2 Spark cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re now ready to launch a small Spark cluster by changing into the `ec2`
    directory and then running the cluster launch command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This will launch a new Spark cluster called test-cluster with one master and
    one slave node of instance type `m3.medium`. This cluster will be launched with
    a Spark version built for Hadoop 2\. The key pair name we used is spark, and the
    key pair file is `spark.pem` (if you gave the files different names or have an
    existing AWS key pair, use that name instead).
  prefs: []
  type: TYPE_NORMAL
- en: 'It might take quite a while for the cluster to fully launch and initialize.
    You should see something like the following immediately after running the launch
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'If the cluster has launched successfully, you should eventually see a console
    output similar to the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create two VMs - Spark Master and Spark Slave of type m1.large as
    shown in the following screenshot :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To test whether we can connect to our new cluster, we can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Remember to replace the public domain name of the master node (the address after
    `root@` in the preceding command) with the correct Amazon EC2 public domain name
    that will be shown in your console output after launching the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also retrieve your cluster''s master public domain name by running
    this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After successfully running the `ssh` command, you will be connected to your
    Spark master node in EC2, and your terminal output should match the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can test whether our cluster is correctly set up with Spark by changing
    into the `Spark` directory and running an example in the local mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to what you would get on running the same command
    on your local computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have an actual cluster with multiple nodes, we can test Spark in
    the cluster mode. We can run the same example on the cluster, using our one slave
    node by passing in the master URL instead of the local version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Note that you will need to substitute the preceding master domain name with
    the correct domain name for your specific cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the output should be similar to running the example locally; however,
    the log messages will show that your driver program has connected to the Spark
    master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Feel free to experiment with your cluster. Try out the interactive console
    in Scala, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve finished, type `exit` to leave the console. You can also try the
    PySpark console by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: You can use the Spark Master web interface to see the applications registered
    with the master. To load the Master Web UI, navigate to `ec2-52-90-110-128.compute-1.amazonaws.com:8080`
    (again, remember to replace this domain name with your own master domain name).
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that *you will be charged by Amazon* for usage of the cluster. Don''t
    forget to stop or terminate this test cluster once you''re done with it. To do
    this, you can first exit the `ssh` session by typing `exit` to return to your
    own local system and then run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Hit *Y* and then *Enter* to destroy the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You've just set up a Spark cluster in the cloud, run a fully
    parallel example program on this cluster, and terminated it. If you would like
    to try out any of the example code in the subsequent chapters (or your own Spark
    programs) on a cluster, feel free to experiment with the Spark EC2 scripts and
    launch a cluster of your chosen size and instance profile. (Just be mindful of
    the costs and remember to shut it down when you're done!)
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and running Spark on Amazon Elastic Map Reduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Launch a Hadoop cluster with Spark installed using the Amazon Elastic Map Reduce.
    Perform the following steps to create an EMR cluster with Spark installed:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch an Amazon EMR Cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the Amazon EMR UI console at [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose Create cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_01_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Choose appropriate Amazon AMI Version 3.9.0 or later as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_01_006.png)'
  prefs: []
  type: TYPE_IMG
- en: For the applications to be installed field, choose Spark 1.5.2 or later from
    the list shown on the User Interface and click on Add.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select other hardware options as necessary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Instance Type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The keypair to be used with SSH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IAM roles (Default orCustom)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Create cluster. The cluster will start instantiating as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_01_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Log in into the master. Once the EMR cluster is ready, you can SSH into the
    master:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be similar to following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark Shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Run Basic Spark sample from the EMR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: UI in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides a web interface which can be used to monitor jobs, see the environment,
    and run SQL commands.
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkContext` launches a web UI on port `4040` that displays useful information
    about the application. This includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of scheduler stages and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of RDD sizes and memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the running executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This interface can be accessed by going to `http://<driver-node>:4040` in a
    web browser. If multiple `SparkContexts` are running on the same host, they will
    bind to ports beginning with port `4040` (`4041`, `4042`, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshots display some of the information provided by the Web
    UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_009.png)'
  prefs: []
  type: TYPE_IMG
- en: UI showing the Environment of the Spark Content
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_010.png)'
  prefs: []
  type: TYPE_IMG
- en: UI table showing Executors available
  prefs: []
  type: TYPE_NORMAL
- en: Supported machine learning algorithms by Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following algorithms are supported by Spark ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaborative filtering**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternating Least Squares (ALS):** Collaborative filtering is often used
    for recommender systems. These techniques aim to fill the missing entries of a
    user-item association matrix. The `spark.mllib` currently supports model-based
    collaborative filtering. In this implementation, users and products are described
    by a small set of latent factors that can be used to predict missing entries.
    The `spark.mllib` uses the ALS algorithm to learn these latent factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: This is an unsupervised learning problem where the aim is to
    group subsets of entities with one another based on the notion of similarity.
    Clustering is used for exploratory analysis and as a component of a hierarchical
    supervised learning pipeline. When used in a learning pipeline, distinct classifiers
    or regression models are trained for each cluster. The following clustering techniques
    are implemented in Spark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-means**: This is one of the commonly used clustering algorithms that cluster
    the data points into a predefined number of clusters. It is up to the user to
    choose the number of clusters. The `spark.mllib` implementation includes a parallelized
    variant of the k-means++ method ([http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian mixture**: A **Gaussian Mixture Model** (**GMM**) represents a composite
    distribution where points are taken from one of the k Gaussian sub-distributions.
    Each of these distributions has its own probability. The `spark.mllib` implementation
    uses the expectation-maximization algorithm to induce the maximum-likelihood model
    given a set of samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Iteration Clustering (PIC)**: This is a scalable algorithm for clustering
    vertices of a graph given pairwise similarities as edge properties. It computes
    a pseudo-eigenvector of the (affinity matrix which is normalized) of the graph
    using power iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power iteration is an eigenvalue algorithm. Given a matrix *X*, the algorithm
    will produce a number*λ* ( eigenvalue) and a non-zero vector*v* (the eigenvector),
    such that*Xv = λv*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pseudo eigenvectors of a matrix can be thought of as the eigenvectors of a
    nearby matrix. More specifically, pseudo eigenvectors are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: Let *A* be an *n* by *n* matrix. Let *E* be any matrix such that *||E|| = €*.
    Then the eigenvectors of *A + E* are defined to be pseudo-eigenvectors of *A*.
    This eigenvector uses it to cluster graph vertices.
  prefs: []
  type: TYPE_NORMAL
- en: The `spark.mllib` includes an implementation of PIC using *GraphX*. It takes
    an RDD of tuples and outputs a model with the clustering assignments. The similarities
    must be non-negative. PIC makes the assumption that the similarity measure is
    symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: (In statistics, a similarity measure or similarity function is a real-valued
    function that quantifies the similarity between two objects. Such measures are
    inverse of distance metrics; an example of this is the Cosine similarity)
  prefs: []
  type: TYPE_NORMAL
- en: A pair (`srcId`, `dstId`) regardless of the ordering should appear at the most
    once in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**): This is a form of a topic model
    that infers topics from a collection of text documents. LDA is a form clustering
    algorithm. The following points explain the topics:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Topics are cluster centers and documents correspond to examples in a dataset
    Topics and documents both exist in a feature space, where feature vectors are
    vectors of word counts ( also known as bag of words)
  prefs: []
  type: TYPE_NORMAL
- en: Instead of estimating a clustering using a traditional distance approach, LDA
    uses a function based on a model of how text documents are generated
  prefs: []
  type: TYPE_NORMAL
- en: '**Bisecting k-means**: This is a type of hierarchical clustering. **Hierarchical
    Cluster Analysis** (**HCA**) is a method of cluster analysis that builds a hierarchy
    of clusters*top down*. In this approach, all observations start in one cluster
    and splits are performed recursively as one moves down the hierarchy.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering is one of the commonly used methods of cluster analysis
    that seek to build a hierarchy of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Streaming k-means**: When data arrives in a stream, we want to estimate clusters
    dynamically and update them as new data arrives. The `spark.mllib` supports streaming
    k-means clustering, with parameters to control the decay of the estimates. The
    algorithm uses a generalization of the mini-batch k-means update rule.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Trees:** Decision trees and their ensembles are one of the methods
    for classification and regression. Decision trees are popular as they are easy
    to interpret, handle categorical features, and extend to the multiclass classification
    setting. They do not require feature scaling and are also able to capture non-linearities
    and feature interactions. Tree ensemble algorithms, random forests and boosting
    are among the top performers for classification and regression scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `spark.mllib` implements decision trees for binary and multiclass classification
    and regression. It supports both continuous and categorical features. The implementation
    partitions data by rows, which allows distributed training with millions of instances.
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Bayes**: Naive Bayes classifiers are a family of simple probabilistic
    classifiers based on applying Bayes'' theorem ([https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem))
    with strong (naive) independence assumptions between the features.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes is a multiclass classification algorithm with the assumption of
    independence between every pair of features. In a single pass of training data,
    the algorithm computes the conditional probability distribution of each feature
    given the label, and then it applies Bayes' theorem to compute the conditional
    probability distribution of a label given an observation, which is then used for
    prediction. The `spark.mllib` supports multinomial naive Bayes and Bernoulli Naive
    Bayes. These models are generally used for document classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Probabil****ity Classifier**: In machine learning, a probabilistic classifier
    is a classifier that can predict, given an input, a probability distribution over
    a set of classes, rather than outputting the most likely class that the sample
    should belong to. Probabilistic classifiers provide classification with some certainty,
    which can be useful on its own or when combining classifiers into ensembles.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistical Regression**: This is a method used to predict a binary response.
    Logistic regression measures the relationship between the categorical dependent
    variable and independent variables by estimating probabilities using a logistical
    function. This function is a cumulative logistic distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a special case of **Generalized Linear Models** (**GLM**) that predicts
    the probability of the outcome. For more background and more details about the
    implementation, refer to the documentation on the logistic regression in `spark.mllib`.
  prefs: []
  type: TYPE_NORMAL
- en: GLM is considered a generalization of linear regression that allows for response
    variables that have an error distribution other than a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**: This algorithms use ensembles of decision trees to decide
    decision boundaries. Random forests combine many decision trees. This reduces
    the risk of overfitting the result.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark ML supports random forest for binary and multi-class classification as
    well as regression. It can use used for continuous or categorical values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: This is the process of reducing the number of
    variables on which machine learning will be done. It can be used to extract latent
    features from raw features or to compress data while maintaining the overall structure.
    MLlib provides support dimensionality reduction on top of the `RowMatrix` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular value decomposition (SVD)**: Singular value decomposition of a matrix
    *M: m x n* (real or complex) is a factorization of the form *UΣV**, where *U*
    is an*m x R* matrix. *Σ* is an *R x R* rectangular diagonal matrix with non-negative
    real numbers on the diagonal, and *V* is an *n x r* unitary matrix. *r* is equal
    to the rank of the matrix *M*.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal component analysis (PCA)**: This is a statistical method used to
    find a rotation to find largest variance in the first coordinate. Each succeeding
    coordinate, in turn, has the largest variance possible. The columns of the rotation
    matrix are called principal components. PCA is used widely in dimensionality reduction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLlib supports PCA for tall-and-skinny matrices stored in row-oriented format
    using `RowMatrix`.
  prefs: []
  type: TYPE_NORMAL
- en: Spark supports features extraction and transforation using TF-IDF, ChiSquare,
    Selector, Normalizer, and Word2Vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequent pattern mining**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP-growth**: FP stands for frequent pattern. Algorithm first counts item
    occurrences (attribute and value pairs) in the dataset and stores them in the
    header table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second pass, the algorithm builds the FP-tree structure by inserting
    instances (made of items). Items in each instance are sorted by descending order
    of their frequency in the dataset; this ensures that the tree can be processed
    quickly. Items in each instance that do not meet minimum coverage threshold are
    discarded. For a use case where many instances share most frequent items, the
    FP-tree provides high compression close to the tree root.
  prefs: []
  type: TYPE_NORMAL
- en: '**Association rules**: Association rule learning is a mechanism for discovering
    interesting relations between variables in large databases.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: It implements a parallel rule generation algorithm for constructing rules that
    have a single item as the consequent.
  prefs: []
  type: TYPE_NORMAL
- en: '**PrefixSpan**: This is a sequential pattern mining algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: The `spark.mllib` comes with a suite of metrics for
    evaluating the algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****PMML model export****: The **Predictive Model Markup Language** (**PMML**)
    is an XML-based predictive model interchange format. PMML provides a mechanism
    for analytic applications to describe and exchange predictive models produced
    by machine learning algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `spark.mllib` allows the export of its machine learning models to PMML and
    their equivalent PMML models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization (Developer)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent**: This is used to optimize gradient descent
    to minimize an objective function; this function is a sum of differentiable functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent methods and the **Stochastic Subgradient Descent** (**SGD**)
    are included as a low-level primitive in MLlib, on top of which various ML algorithms
    are developed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited-Memory BFGS (L-BFGS)**: This is an optimization algorithm and belongs
    to the family of quasi-Newton methods that approximates the **Broyden-Fletcher-Goldfarb-Shanno**
    (**BFGS**) algorithm. It uses a limited amount of computer memory. It is used
    for parameter estimation in machine learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BFGS method approximates Newton's method, which is a class of hill-climbing
    optimization techniques that seeks a stationary point of a function. For such
    problems, a necessary optimal condition is that the gradient should be zero**.**
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of using Spark ML as compared to existing libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AMQ Lab at Berkley Evaluated Spark, and RDDs were evaluated through a series
    of experiments on Amazon EC2 as well as benchmarks of user applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms used**: Logistical Regression and k-means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use case**: First iteration, multiple iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the tests used `m1.xlarge` EC2 nodes with 4 cores and 15 GB of RAM. HDFS
    was for storage with 256 MB blocks. Refer to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding graph shows the comparison between the performance of Hadoop
    and Spark for the first and subsequent iteration for **Logistical Regression**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_012.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows the comparison between the performance of Hadoop and
    Spark for the first and subsequent iteration for K Means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall results show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark outperforms Hadoop by up to 20 times in iterative machine learning and
    graph applications. The speedup comes from avoiding I/O and deserialization costs
    by storing data in memory as Java objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The applications written perform and scale well. Spark can speed up an analytics
    report that was running on Hadoop by 40 times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When nodes fail, Spark can recover quickly by rebuilding only the lost RDD partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark was be used to query a 1-TB dataset interactively with latencies of 5-7
    seconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, go to [http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Spark versus Hadoop for a SORT Benchmark--In 2014, the Databricks team participated
    in a SORT benchmark test ([http://sortbenchmark.org/](http://sortbenchmark.org/)).
    This was done on a 100-TB dataset. Hadoop was running in a dedicated data center
    and a Spark cluster of over 200 nodes was run on EC2\. Spark was run on HDFS distributed
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark was 3 times faster than Hadoop and used 10 times fewer machines. Refer
    to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark Cluster on Google Compute Engine - DataProc
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cloud Dataproc** is a Spark and Hadoop service running on Google Compute
    Engine. It is a managed service. Cloud Dataproc automation helps create clusters
    quickly, manage them easily, and save money by turning clusters off when you don''t
    need them.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to create a Spark cluster using DataProc
    and running a Sample app on it.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you have created a Google Compute Engine account and installed
    Google Cloud SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)).
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop and Spark Versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataProc supports the following Hadoop and Spark versions. Note that this will
    change with time as new versions come out:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark 1.5.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 2.7.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pig 0.15.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive 1.2.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCS connector 1.4.3-hadoop2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BigQuery connector 0.7.3-hadoop2 ([https://github.com/GoogleCloudPlatform/bigdata-interop](https://github.com/GoogleCloudPlatform/bigdata-interop))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, go to [http://cloud.google.com/dataproc-versions](http://cloud.google.com/dataproc-versions).
  prefs: []
  type: TYPE_NORMAL
- en: In the following steps, we will use Google Cloud Console (the user interface
    used to create a Spark Cluster and submit a job).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can create a Spark cluster by going to the Cloud Platform Console. Select
    the project, and then click Continue to open the Clusters page. You would see
    the Cloud Dataproc clusters that belong to your project, if you have created any.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Create a cluster button to open the Create a Cloud Data pros cluster
    page. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click on Create a cluster, a detailed form, which is as shown in the
    following screenshot, shows up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot shows the Create a Cloud Dataproc cluster page with
    the default fields automatically filled in for a new cluster-1 cluster. Take a
    look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_016.png)'
  prefs: []
  type: TYPE_IMG
- en: You can expand the workers, bucket, network, version, initialization, and access
    options panel to specify one or more worker nodes, a staging bucket, network,
    initialization, the Cloud Dataproc image version, actions, and project-level access
    for your cluster. Providing these values is optional.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default cluster is created with no worker nodes, an auto-created staging
    bucket, and a default network It also has the latest released Cloud Dataproc image
    version. You can change these default settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_017.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you have configured all fields on the page, click on the Create button
    to create the cluster. The cluster name created appears on the Clusters page.
    The status is updated to Running once the spark cluster is created.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the cluster name created earlier to open the cluster details page.
    It also has a Overview tab and the CPU utilization graph selected.
  prefs: []
  type: TYPE_NORMAL
- en: You can examine jobs, instances, and so on for the cluster from the other tabs.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting a Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To submit a job from the Cloud Platform Console to the cluster, go to the Cloud
    Platform UI. Select the appropriate project and then click on Continue. The first
    time you submit a job, the following dialog appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Submit a job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To submit a Spark sample job, fill the fields on the Submit a job page, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a cluster name from the cluster list on the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set Job type toSpark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `file:///usr/lib/spark/lib/spark-examples.jar` to Jar files. Here, `file:///`
    denotes a Hadoop `LocalFileSystem` scheme; Cloud Dataproc installs `/usr/lib/spark/lib/spark-examples.jar`
    on the cluster's master node when it creates the cluster. Alternatively, you can
    specify a Cloud Storage path (`gs://my-bucket/my-jarfile.jar`) or an `HDFS` path
    (`hdfs://examples/myexample.jar`) to one of the custom jars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `Main` class or jar to `org.apache.spark.examples.SparkPi`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set Arguments to the single argument `1000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Submit to start the job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the job starts, it is added to the Jobs list. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the job is complete, its status changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_021.png)'
  prefs: []
  type: TYPE_IMG
- en: Take a look at the `job` output as listed here.
  prefs: []
  type: TYPE_NORMAL
- en: Execute the command from the terminal with the appropriate Job ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the Job ID was `1ed4d07f-55fc-45fe-a565-290dcd1978f7` and project-ID
    was `rd-spark-1`; hence, the command looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The (abridged) output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: You can also SSH into the Spark Instance and run spark-shell in the interactive
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered how to set up Spark locally on our own computer
    as well as in the cloud as a cluster running on Amazon EC2\. You learned how to
    run Spark on top of Amazon's **Elastic Map Reduce** (**EMR**). You also learned
    how to use Google Compute Engine's Spark Service to create a cluster and run a
    simple job. We discussed the basics of Spark's programming model and API using
    the interactive Scala console, and we wrote the same basic Spark program in Scala,
    Java, R, and Python. We also compared the performance metrics of Hadoop versus
    Spark for different machine learning algorithms as well as SORT benchmark tests.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will consider how to go about using Spark to create
    a machine learning system.
  prefs: []
  type: TYPE_NORMAL

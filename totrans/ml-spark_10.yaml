- en: Advanced Text Processing with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml), *Obtaining, Processing,
    and Preparing Data with Spark*, we covered various topics related to feature extraction
    and data processing, including the basics of extracting features from text data.
    In this chapter, we will introduce more advanced text processing techniques available
    in Spark ML to work with large-scale text datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Work through detailed examples that illustrate data processing, feature extraction,
    and the modeling pipeline, as they relate to text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the similarity between two documents based on the words in the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the extracted text features as inputs for a classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cover a recent development in natural language processing to model words themselves
    as vectors and illustrate the use of Spark's Word2Vec model to evaluate the similarity
    between two words, based on their meaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at how to use Spark's MLlib as well as Spark ML for text processing
    examples as well clustering of documents.
  prefs: []
  type: TYPE_NORMAL
- en: What's so special about text data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data can be complex to work with for two main reasons. First, text and
    language have an inherent structure that is not easily captured using the raw
    words as is (for example, meaning, context, different types of words, sentence
    structure, and different languages, to highlight a few). Therefore, naive feature
    extraction is usually relatively ineffective.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the effective dimensionality of text data is extremely large and potentially
    limitless. Think about the number of words in the English language alone and add
    all kinds of special words, characters, slang, and so on to this. Then, throw
    in other languages and all the types of text one might find across the Internet.
    The dimension of text data can easily exceed tens or even hundreds of millions
    of words, even in relatively small datasets. For example, the Common Crawl dataset
    of billions of websites contains over 840 billion individual words.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with these issues, we need ways of extracting more structured features
    and methods to handle the huge dimensionality of text data.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the right features from your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The field of **Natural Language Processing** (**NLP**) covers a wide range
    of techniques to work with text, from text processing and feature extraction through
    to modeling and machine learning. In this chapter, we will focus on two feature
    extraction techniques available within Spark MLlib and Spark ML: the **term frequency-inverse
    document frequency** (**tf-idf**) term weighting scheme and feature hashing.'
  prefs: []
  type: TYPE_NORMAL
- en: Working through an example of tf-idf, we will also explore the ways in which
    processing, tokenization, and filtering during feature extraction can help reduce
    the dimensionality of our input data as well as improve the information content
    and usefulness of the features we extract.
  prefs: []
  type: TYPE_NORMAL
- en: Term weighting schemes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](6e72c765-ca1d-4959-91f4-6b741ff2f7cb.xhtml), *Obtaining, Processing,
    and Preparing Data with Spark*, we looked at vector representation, where text
    features are mapped to a simple binary vector called the **bag-of-words** model.
    Another representation used commonly in practice is called Term Frequency-Inverse
    Document Frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'tf-idf weights each term in a piece of text (referred to as a **document**)
    based on its frequency in the document (the **term frequency**). A global normalization,
    called the **inverse document frequency**, is then applied based on the frequency
    of this term among all documents (the set of documents in a dataset is commonly
    referred to as a **corpus**). The standard definition of tf-idf is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tf-idf(t,d) = tf(t,d) x idf(t)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *tf(t,d)* is the frequency (number of occurrences) of term *t* in document
    *d* and *idf(t)* is the inverse document frequency of term *t* in the corpus;
    this is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*idf(t) = log(N / d)*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *N* is the total number of documents, and *d* is the number of documents
    in which the term *t* occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The tf-idf formulation means that terms occurring many times in a document receive
    a higher weighting in the vector representation relative to those that occur few
    times in the document. However, the IDF normalization has the effect of reducing
    the weight of terms that are very common across all documents. The end result
    is that truly rare or important terms should be assigned higher weighting, while
    more common terms (which are assumed to have less importance) should have less
    impact in terms of weighting.
  prefs: []
  type: TYPE_NORMAL
- en: A good resource to learn more about the bag-of-words model (or vector space
    model) is the book Introduction to Information Retrieval, Christopher D. Manning,
    Prabhakar Raghavan and Hinrich Schütze, Cambridge University Press (available
    in HTML form at [http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html](http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)).
  prefs: []
  type: TYPE_NORMAL
- en: It contains sections on text processing techniques, including tokenization,
    stop word removal, stemming, and the vector space model, as well as weighting
    schemes such as tf-idf.
  prefs: []
  type: TYPE_NORMAL
- en: An overview can also be found at [http://en.wikipedia.org/wiki/Tf%E2%80%93idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf).
  prefs: []
  type: TYPE_NORMAL
- en: Feature hashing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Feature hashing** is a technique to deal with high-dimensional data and is
    often used with text and categorical datasets where the features can take on many
    unique values (often many millions of values). In the previous chapters, we often
    used the *1-of-K* encoding approach for categorical features, including text.
    While this approach is simple and effective, it can break down in the face of
    extremely high-dimensional data.'
  prefs: []
  type: TYPE_NORMAL
- en: Building and using *1-of-K* feature encoding requires us to keep a mapping of
    each possible feature value to an index in a vector. Furthermore, the process
    of creating the mapping itself requires at least one additional pass through the
    dataset and can be tricky to do in parallel scenarios. Up until now, we have often
    used a simple approach of collecting the distinct feature values and zipping this
    collection with a set of indices to create a map of feature value to index. This
    mapping is then broadcast (either explicitly in our code or implicitly by Spark)
    to each worker.
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with huge feature dimensions in the tens of millions or
    more that are common when working with text, this approach can be slow and can
    require significant memory and network resources, both on the Spark master (to
    collect the unique values) and workers (to broadcast the resulting mapping to
    each worker, which keeps it in memory to allow it to apply the feature encoding
    to its local piece of the input data).
  prefs: []
  type: TYPE_NORMAL
- en: Feature hashing works by assigning the vector index for a feature based on the
    value obtained by hashing this feature to a number (usually, an integer value)
    using a hash function. For example, let's say the hash value of a categorical
    feature for the geolocation of `United States` is `342`. We will use the hashed
    value as the vector index, and the value at this index will be `1.0` to indicate
    the presence of the `United States` feature. The hash function used must be consistent
    (that is, for a given input, it returns the same output each time).
  prefs: []
  type: TYPE_NORMAL
- en: This encoding works the same way as mapping-based encoding, except that we choose
    a size for our feature vector upfront. As the most common hash functions return
    values in the entire range of integers, we will use a *modulo* operation to restrict
    the index values to the size of our vector, which is typically much smaller (a
    few tens of thousands to a few million, depending on our requirements).
  prefs: []
  type: TYPE_NORMAL
- en: Feature hashing has the advantage that we do not need to build a mapping and
    keep it in memory. It is also easy to implement, very fast, and can be done online
    and in real time, thus not requiring a pass through our dataset first. Finally,
    because we selected a feature vector dimension that is significantly smaller than
    the raw dimensionality of our dataset, we bound the memory usage of our model
    both in training and production; hence, memory usage does not scale with the size
    and dimensionality of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are two important drawbacks, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: As we don't create a mapping of features to index values, we also cannot do
    the reverse mapping of feature index to value. This makes it harder to, for example,
    determine which features are most informative in our models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we are restricting the size of our feature vectors, we might experience
    **hash collisions**. This happens when two different features are hashed into
    the same index in our feature vector. Surprisingly, this doesn''t seem to have
    a severe impact on model performance as long as we choose a reasonable feature
    vector dimension relative to the dimension of the input data. If the Hashed vector
    is large the affect of collision is minimal but the gain is still significant.
    Please refer to this paper for more details : [http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf](http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further information on hashing can be found at [http://en.wikipedia.org/wiki/Hash_function](http://en.wikipedia.org/wiki/Hash_function).
  prefs: []
  type: TYPE_NORMAL
- en: 'A key paper that introduced the use of hashing for feature extraction and machine
    learning is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kilian Weinberger*, *Anirban Dasgupta*, *John Langford*, *Alex Smola*, and
    *Josh Attenberg*. *Feature Hashing for Large Scale Multitask Learning*. *Proc.
    ICML 2009*, which is available at [http://alex.smola.org/papers/2009/Weinbergeretal09.pdf](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the tf-idf features from the 20 Newsgroups dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate the concepts in this chapter, we will use a well-known text dataset
    called **20 Newsgroups**; this dataset is commonly used for text-classification
    tasks. This is a collection of newsgroup messages posted across 20 different topics.
    There are various forms of data available. For our purposes, we will use the `bydate`
    version of the dataset, which is available at [http://qwone.com/~jason/20Newsgroups](http://qwone.com/~jason/20Newsgroups).
  prefs: []
  type: TYPE_NORMAL
- en: This dataset splits up the available data into training and test sets that comprise
    60 percent and 40 percent of the original data, respectively. Here, the messages
    in the test set occur after those in the training set. This dataset also excludes
    some of the message headers that identify the actual newsgroup; hence, it is an
    appropriate dataset to test the real-world performance of classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Further information on the original dataset can be found in the *UCI Machine
    Learning Repository* page at [http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, download the data and unzip the file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create two folders: one called `20news-bydate-train` and another
    one called `20news-bydate-test`. Let''s take a look at the directory structure
    under the training dataset folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see that it contains a number of subfolders, one for each newsgroup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of files under each newsgroup folder; each file contains
    an individual message posting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take a look at a part of one of these messages to see the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, each message contains some header fields that contain the sender,
    subject, and other metadata, followed by the raw content of the message.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the 20 Newsgroups data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a Spark Program to load and analyze the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the directory structure, you might recognize that once again, we
    have data contained in individual text files (one text file per message). Therefore,
    we will again use Spark's `wholeTextFiles` method to read the content of each
    file into a record in our RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code that follows, `PATH` refers to the directory in which you extracted
    the `20news-bydate` ZIP file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you put a breakpoint, you will see the following line displayed, indicating
    the total number of files that Spark has detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After the command has finished running, you will see the total record count,
    which should be the same as the preceding `Total input paths to process` screen
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now print first element of the `rdd` into which data has been loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will take a look at the newsgroup topics available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the number of messages is roughly even between the topics.
  prefs: []
  type: TYPE_NORMAL
- en: Applying basic tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in our text processing pipeline is to split up the raw text
    content in each document into a collection of terms (also referred to as **tokens**).
    This is known as **tokenization**. We will start by applying a simple **whitespace**
    tokenization, together with converting each token to lowercase for each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used the `flatMap` function instead of `map`, as for
    now, we want to inspect all the tokens together for exploratory analysis. Later
    in this chapter, we will apply our tokenization scheme on a per-document basis,
    so we will use the `map` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this code snippet, you will see the total number of unique tokens
    after applying our tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, for even a relatively small amount of text, the number of raw
    tokens (and, therefore, the dimensionality of our feature vectors) can be very
    high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a randomly selected document. We will use the sample
    method of RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that we set the third parameter to the `sample` function, which is the
    random seed. We set this function to `42` so that we get the same results from
    each call to `sample` so that your results match those in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will display the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Improving our tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding simple approach results in a lot of tokens and does not filter
    out many nonword characters (such as punctuation). Most tokenization schemes will
    remove these characters. We can do this by splitting each raw document on nonword
    characters using a regular expression pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This reduces the number of unique tokens significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we inspect the first few tokens, we will see that we have eliminated most
    of the less useful characters in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following result displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: While our nonword pattern to split text works fairly well, we are still left
    with numbers and tokens that contain numeric characters. In some cases, numbers
    can be an important part of a corpus. For our purposes, the next step in our pipeline
    will be to filter out numbers and tokens that are words mixed with numbers.
  prefs: []
  type: TYPE_NORMAL
- en: We can do this by applying another regular expression pattern and use this to
    filter out tokens that do not match the pattern, `val regex = """[^0-9]*""".r`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This further reduces the size of the token set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at another random sample of the filtered tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we have removed all the numeric characters. This still leaves
    us with a few strange *words*, but we will not worry about these too much here.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Stop words** refer to common words that occur many times across almost all
    documents in a corpus (and across most corpuses). Examples of typical English
    stop words include and, but, the, of, and so on. It is a standard practice in
    text feature extraction to exclude stop words from the extracted tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: When using tf-idf weighting, the weighting scheme actually takes care of this
    for us. As stop words have a very low idf score, they will tend to have very low
    tf-idf weightings and thus less importance. In some cases, for information retrieval
    and search tasks, it might be desirable to include stop words. However, it can
    still be beneficial to exclude stop words during feature extraction, as it reduces
    the dimensionality of the final feature vectors as well as the size of the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take a look at some of the tokens in our corpus that have the highest
    occurrence across all documents to get an idea about some other stop words to
    exclude:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we took the tokens after filtering out numeric characters
    and generated a count of the occurrence of each token across the corpus. We can
    now use Spark's top function to retrieve the top 20 tokens by count. Notice that
    we need to provide the top function with an ordering that tells Spark how to order
    the elements of our RDD. In this case, we want to order by the count, so we will
    specify the second element of our key-value pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding code snippet will result in the following top tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we might expect, there are a lot of common words in this list that we could
    potentially label as stop words. Let's create a set of stop words with some of
    these
  prefs: []
  type: TYPE_NORMAL
- en: 'as well as other common words. We will then look at the tokens after filtering
    out these stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: You might notice that there are still quite a few common words in this top list.
    In practice, we might have a much larger set of stop words. However, we will keep
    a few (partly to illustrate the impact of common words when using tf-idf weighting
    a little later).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find list of common stop words here : [http://xpo6.com/list-of-english-stop-words/](http://xpo6.com/list-of-english-stop-words/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'One other filtering step that we will use is removing any tokens that are only
    one character in length. The reasoning behind this is similar to removing stop
    words-these single-character tokens are unlikely to be informative in our text
    model and can further reduce the feature dimension and model size. We will do
    this with another filtering step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we will examine the tokens remaining after this filtering step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Apart from some of the common words that we have not excluded, we see that a
    few, potentially more informative words are starting to appear.
  prefs: []
  type: TYPE_NORMAL
- en: Excluding terms based on frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is also a common practice to exclude terms during tokenization when their
    overall occurrence in the corpus is very low. For example, let''s examine the
    least occurring terms in the corpus (notice the different ordering we use here
    to return the results sorted in ascending order):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, there are many terms that only occur once in the entire corpus.
    Since typically, we want to use our extracted features for other tasks such as
    document similarity or machine learning models, tokens that only occur once are
    not useful to learn from, as we will not have enough training data relative to
    these tokens. We can apply another filter to exclude these rare tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we are left with tokens that occur at least twice in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s count the number of unique tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, by applying all the filtering steps in our tokenization pipeline,
    we have reduced the feature dimension from `402,978` to `51,801`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now combine all our filtering logic into one function, which we can
    apply to each document in our RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check whether this function gives us the same result with the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This will output `51801`, giving us the same unique token count as our step-by-step
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can tokenize each document in our RDD as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see output similar to the following, showing the first part of the
    tokenized version of our first document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: A note about stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common step in text processing and tokenization is **stemming**. This is the
    conversion of whole words to a **base form** (called a **word stem**). For example,
    plurals might be converted to singular (*dogs* becomes *dog*), and forms such
    as *walking* and *walker* might become walk. Stemming can become quite complex
    and is typically handled with specialized NLP or search engine software (such
    as NLTK, OpenNLP, and Lucene, for example). We will ignore stemming for the purpose
    of our example here.
  prefs: []
  type: TYPE_NORMAL
- en: A full treatment of stemming is beyond the scope of this book. You can find
    more details at [http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming).
  prefs: []
  type: TYPE_NORMAL
- en: Feature Hashing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First we explain what is feature hashing so that it becomes easier to understand
    the tf-idf model in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature hashing converts a String or a word into a fixed length vector which
    makes it easy to process text.
  prefs: []
  type: TYPE_NORMAL
- en: Spark currently uses Austin Appleby's MurmurHash 3 algorithm (MurmurHash3_x86_32)
    for hashing text into numbers.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the implementation here
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Please note functions hashInt, hasnLong etc are called from Util.scala
  prefs: []
  type: TYPE_NORMAL
- en: Building a tf-idf model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now use Spark ML to transform each document, in the form of processed
    tokens, into a vector representation. The first step will be to use the `HashingTF`
    implementation, which makes use of feature hashing to map each token in the input
    text to an index in the vector of term frequencies. Then, we will compute the
    global IDF and use it to transform the term frequency vectors into tf-idf vectors.
  prefs: []
  type: TYPE_NORMAL
- en: For each token, the index will thus be the hash of the token (mapped in turn
    onto the dimension of the feature vector). The value for each token will be the
    tf-idf weighting for that token (that is, the term frequency multiplied by the
    inverse document frequency).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the classes we need and create our `HashingTF` instance,
    passing in a `dim` dimension parameter. While the default feature dimension is
    2^(20) (or around 1 million), we will choose 2^(18) (or around 260,000), since
    with about 50,000 tokens, we should not experience a significant number of hash
    collisions, and a smaller dimension will be more memory and processing friendly
    for illustrative purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note that we imported MLlib's `SparseVector` using an alias of `SV`. This is
    because later, we will use Breeze's `linalg` module, which itself also imports
    `SparseVector`. This way, we will avoid namespace collisions.
  prefs: []
  type: TYPE_NORMAL
- en: The `transform` function of `HashingTF` maps each input document (that is, a
    sequence of tokens) to an MLlib `Vector`. We will also call `cache` to pin the
    data in memory to speed up subsequent operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect the first element of our transformed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `HashingTF.transform` returns an `RDD[Vector]`, so we will cast the
    result returned to an instance of an MLlib `SparseVector`.
  prefs: []
  type: TYPE_NORMAL
- en: The `transform` method can also work on an individual document by taking an
    `Iterable` argument (for example, a document as a `Seq[String]`). This returns
    a single vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the dimension of each sparse vector of term frequencies is 262,144
    (or 2^(18) as we specified). However, the number on non-zero entries in the vector
    is only 706\. The last two lines of the output show the frequency counts and vector
    indexes for the first few entries in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now compute the inverse document frequency for each term in the corpus
    by creating a new `IDF` instance and calling `fit` with our RDD of term frequency
    vectors as the input. We will then transform our term frequency vectors to tf-idf
    vectors through the `transform` function of `IDF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'When you examine the first element in the RDD of tf-idf transformed vectors,
    you will see output similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the number of non-zero entries hasn't changed (at `706`), nor
    have the vector indices for the terms. What has changed are the values for each
    term. Earlier, these represented the frequency of each term in the document, but
    now, the new values represent the frequencies weighted by the IDF.
  prefs: []
  type: TYPE_NORMAL
- en: IDF weightage came into picture when we executed the following two lines
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing the tf-idf weightings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let's investigate the tf-idf weighting for a few terms to illustrate the
    impact of the commonality or rarity of a term.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can compute the minimum and maximum tf-idf weights across the entire
    corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the minimum tf-idf is zero, while the maximum is significantly
    larger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We will now explore the tf-idf weight attached to various terms. In the previous
    section on stop words, we filtered out many common terms that occur frequently.
    Recall that we did not remove all such potential stop words. Instead, we kept
    a few in the corpus so that we could illustrate the impact of applying the tf-idf
    weighting scheme on these terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tf-idf weighting will tend to assign a lower weighting to common terms. To
    see this, we can compute the tf-idf representation for a few of the terms that
    appear in the list of top occurrences that we previously computed, such as `you`,
    `do`, and `we`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If we form a tf-idf vector representation of this document, we would see the
    following values assigned to each term. Note that because of feature hashing,
    we are not sure exactly which term represents what. However, the values illustrate
    that the weighting applied to these terms is relatively low:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s apply the same transformation to a few less common terms that we
    might intuitively associate with being more linked to specific topics or concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see from the following results that the tf-idf weightings are indeed
    significantly higher than for the more common terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Using a tf-idf model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we often refer to training a tf-idf model, it is actually a feature extraction
    process or transformation rather than a machine learning model. Tf-idf weighting
    is often used as a preprocessing step for other models, such as dimensionality
    reduction, classification, or regression.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the potential uses of tf-idf weighting, we will explore two examples.
    The first is using the tf-idf vectors to compute document similarity, while the
    second involves training a multilabel classification model with the tf-idf vectors
    as input features.
  prefs: []
  type: TYPE_NORMAL
- en: Document similarity with the 20 Newsgroups dataset and tf-idf features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might recall from [Chapter 5](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Recommendation Engine with Spark*, that the similarity between two
    vectors can be computed using a distance metric. The closer two vectors are (that
    is, the lower the distance metric), the more similar they are. One such metric
    that we used to compute similarity between movies is cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Just like we did for movies, we can also compute the similarity between two
    documents. Using tf-idf, we have transformed each document into a vector representation.
    Hence, we can use the same techniques as we used for movie vectors to compare
    two documents.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we might expect two documents to be more similar to each other
    if they share many terms. Conversely, we might expect two documents to be less
    similar if they each contain many terms that are different from each other. As
    we compute cosine similarity by computing a dot product of the two vectors and
    each vector is made up of the terms in each document, we can see that documents
    with a high overlap of terms will tend to have a higher cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can see tf-idf at work. We might reasonably expect that even very different
    documents might contain many overlapping terms that are relatively common (for
    example, our stop words). However, due to a low tf-idf weighting, these terms
    will not have a significant impact on the dot product and, therefore, will not
    have much impact on the similarity computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we might expect two randomly chosen messages from the `hockey`
    newsgroup to be relatively similar to each other. Let''s see if this is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first filtered our raw input RDD to keep only the
    messages within the hockey topic. We then applied our tokenization and term frequency
    transformation functions. Note that the `transform` method used is the version
    that works on a single document (in the form of a `Seq[String]`) rather than the
    version that works on an RDD of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we applied the `IDF` transform (note that we use the same IDF that
    we have already computed on the whole corpus).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our `hockey` document vectors, we can select two of these vectors
    at random and compute the cosine similarity between them (as we did earlier, we
    will use Breeze for the linear algebra functionality, in particular converting
    our MLlib vectors to Breeze `SparseVector` instances first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the cosine similarity between the documents is around 0.06:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: While this might seem quite low, recall that the effective dimensionality of
    our features is high due to the large number of unique terms that is typical when
    dealing with text data. Hence, we can expect that any two documents might have
    a relatively low overlap of terms even if they are about the same topic, and therefore
    would have a lower absolute similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, we can compare this similarity score to the one computed between
    one of our `hockey` documents and another document chosen randomly from the `comp.graphics`
    newsgroup, using the same methodology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The cosine similarity is significantly lower at `0.0047`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it is likely that a document from another sports-related topic might
    be more similar to our `hockey` document than one from a computer-related topic.
    However, we would probably expect a `baseball` document to not be as similar as
    our `hockey` document. Let''s see whether this is the case by computing the similarity
    between a random message from the `baseball` newsgroup and our `hockey` document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, as we expected, we found that the `baseball` and `hockey` documents
    have a cosine similarity of `0.05`, which is significantly higher than the `comp.graphics`
    document, but also somewhat lower than the other `hockey` document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Source Code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala)'
  prefs: []
  type: TYPE_NORMAL
- en: Training a text classifier on the 20 Newsgroups dataset using tf-idf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using tf-idf vectors, we expected that the cosine similarity measure would
    capture the similarity between documents, based on the overlap of terms between
    them. In a similar way, we would expect that a machine learning model, such as
    a classifier, would be able to learn weightings for individual terms; this would
    allow it to distinguish between documents from different classes. That is, it
    should be possible to learn a mapping between the presence (and weighting) of
    certain terms and a specific topic.
  prefs: []
  type: TYPE_NORMAL
- en: In the 20 Newsgroups example, each newsgroup topic is a class, and we can train
    a classifier using our tf-idf transformed vectors as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are dealing with a multiclass classification problem, we will use
    the naive Bayes model in MLlib, which supports multiple classes. As the first
    step, we will import the Spark classes that we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We will keep our clustering code in an object called `Document clustering`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will need to extract the 20 topics and convert them to class mappings.
    We can do this in exactly the same way as we might for *1-of-K* feature encoding,
    by assigning a numeric index to each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we took the `newsgroups` RDD, where each element
    is the topic, and used the `zip` function to combine it with each element in our
    `tfidf` RDD of tf-idf vectors. We then mapped over each key-value element in our
    new zipped RDD and created a `LabeledPoint` instance, where `label` is the class
    index and `features` is the tf-idf vector.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `zip` operator assumes that each RDD has the same number of partitions
    as well as the same number of elements in each partition. It will fail if this
    is not the case. We can make this assumption here because we have effectively
    created both our `tfidf` RDD and `newsgroups` RDD from a series of `map` transformations
    on the same original RDD that preserved the partitioning structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an input RDD in the correct form, we can simply pass it to
    the naive Bayes `train` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s evaluate the performance of the model on the test dataset. We will load
    the raw test data from the `20news-bydate-test` directory, again using `wholeTextFiles`
    to read each message into an RDD element. We will then extract the class labels
    from the file paths in the same way as we did for the `newsgroups` RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Transforming the text in the test dataset follows the same procedure as for
    the training data-we will apply our `tokenize` function followed by the term frequency
    transformation, and we will again use the same IDF computed from the training
    data to transform the TF vectors into tf-idf vectors. Finally, we will zip the
    test class labels with the tf-idf vectors and create our test `RDD[LabeledPoint]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Note that it is important that we use the training set IDF to transform the
    test data, as this creates a more realistic estimation of model performance on
    new data, which might potentially contain terms that the model has not yet been
    trained on. It would be "cheating" to recompute the IDF vector based on the test
    dataset and, more importantly, would potentially lead to incorrect estimates of
    optimal model parameters selected through cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re ready to compute the predictions and true class labels for our
    model. We will use this RDD to compute accuracy and the multiclass weighted F-measure
    for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The weighted F-measure is an overall measure of precision and recall performance
    (where, like area under an ROC curve, values closer to 1.0 indicate better performance),
    which is then combined through a weighted averaged across the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that our simple multiclass naive Bayes model has achieved close
    to 80 percent for both accuracy and F-measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the impact of text processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text processing and tf-idf weighting are examples of feature extraction techniques
    designed to both reduce the dimensionality of, and extract some structure from,
    raw text data. We can see the impact of applying these processing techniques by
    comparing the performance of a model trained on raw text data with one trained
    on processed and tf-idf weighted text data.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing raw features with processed tf-idf features on the 20 Newsgroups dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will simply apply the hashing term frequency transformation
    to the raw text tokens obtained using a simple whitespace splitting of the document
    text. We will train a model on this data and evaluate the performance on the test
    set as we did for the model trained with tf-idf features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Perhaps surprisingly, the raw model does quite well, although both accuracy
    and F-measure are a few percentage points lower than those of the tf-idf model.
    This is also partly a reflection of the fact that the naive Bayes model is well
    suited to data in the form of raw frequency counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Text classification with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the libsvm version of *20newsgroup* data to use
    the Spark DataFrame-based APIs to classify the text documents. In the current
    version of Spark libsvm version 3.22 is supported ([https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/))
  prefs: []
  type: TYPE_NORMAL
- en: Download the libsvm formatted data from the following link and copy output folder
    under Spark-2.0.x.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the following link for the *20newsgroup libsvm* data: [https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D](https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the appropriate packages from `org.apache.spark.ml` and create Wrapper
    Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will load the `libsvm` data into a Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the `NaiveBayes` model from the `org.apache.spark.ml.classification.NaiveBayes`
    class and train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table is the output of the predictions DataFrame `.show()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the accuracy of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Accuracy of this model is above `0.8` as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Word2Vec models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have used a bag-of-words vector, optionally with some weighting
    scheme such as tf-idf to represent the text in a document. Another recent class
    of models that has become popular is related to representing individual words
    as vectors.
  prefs: []
  type: TYPE_NORMAL
- en: These are generally based in some way on the co-occurrence statistics between
    the words in a corpus. Once the vector representation is computed, we can use
    these vectors in ways similar to how we might use tf-idf vectors (such as using
    them as features for other machine learning models). One such common use case
    is computing the similarity between two words with respect to their meanings,
    based on their vector representations.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec refers to a specific implementation of one of these models, often referred
    to as **distributed vector representations**. The MLlib model uses a **skip-gram**
    model, which seeks to learn vector representations that take into account the
    contexts in which words occur.
  prefs: []
  type: TYPE_NORMAL
- en: While a detailed treatment of Word2Vec is beyond the scope of this book, Spark's
    documentation at [http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec)
    contains some further details on the algorithm as well as links to the reference
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main academic papers underlying Word2Vec is *Tomas Mikolov*, *Kai
    Chen*, *Greg Corrado*, and *Jeffrey Dean*. *Efficient Estimation of Word Representations
    in Vector Space*. *In Proceedings of Workshop at ICLR*, *2013*.
  prefs: []
  type: TYPE_NORMAL
- en: It is available at [http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Another recent model in the area of word vector representations is GloVe at
    [http://www-nlp.stanford.edu/projects/glove/](http://www-nlp.stanford.edu/projects/glove/).
  prefs: []
  type: TYPE_NORMAL
- en: You can also leverage third party libraries to do Parts of Speech tagging. For
    example Stanford NLP library could be hooked into scala code. Please refer to
    this discussion thread ([http://stackoverflow.com/questions/18416561/pos-tagging-in-scala](http://stackoverflow.com/questions/18416561/pos-tagging-in-scala))
    for more details on how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec with Spark MLlib on the 20 Newsgroups dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a Word2Vec model in Spark is relatively simple. We will pass in an
    RDD where each element is a sequence of terms. We can use the RDD of tokenized
    documents we have already created as input to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Our code is in the Scala object `Word2VecMllib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us start by loading the text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the tokens created by tf-idf as the starting point for Word2Vec. Let
    us first initialize the object and set a seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's create the model by calling `word2vec.fit()` on the tf-idf tokens:.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: You will see some output while the model is being trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once trained, we can easily find the top 20 synonyms for a given term (that
    is, the most similar term to the input term, computed by cosine similarity between
    the word vectors). For example, to find the 20 most similar terms to `philosopher`,
    use the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the following output, most of the terms relate to hockey
    or others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Word2Vec with Spark ML on the 20 Newsgroups dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we look at how to use the Spark ML DataFrame and newer implementations
    from Spark 2.0.X to create a Word2Vector model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a DataFrame from the dataSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be followed by creating the `Word2Vec` class and training the model
    on the DataFrame `textDF` created above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us try to find some synonyms for `hockey`:'
  prefs: []
  type: TYPE_NORMAL
- en: The following
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Following output will be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the results are quite different from the results we got using
    RDD. This is because the two implementations differ for Word2Vector conversion
    in Spark 1.6 and Spark 2.0/2.1.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a deeper look into more complex text processing and
    explored MLlib's text feature extraction capabilities, in particular the tf-idf
    term weighting schemes. We covered examples of using the resulting tf-idf feature
    vectors to compute document similarity and train a newsgroup topic classification
    model. Finally, you learned how to use MLlib's cutting-edge Word2Vec model to
    compute a vector representation of words in a corpus of text and use the trained
    model to find words with contextual meaning that is similar to a given word. We
    also looked at using Word2Vec with Spark ML
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at online learning, and you will learn
    how Spark Streaming relates to online learning models.
  prefs: []
  type: TYPE_NORMAL

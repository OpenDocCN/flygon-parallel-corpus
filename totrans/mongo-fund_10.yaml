- en: 10\. Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce MongoDB cluster concepts and administration. It
    starts with a discussion on the concepts of high availability and the load sharing
    of a MongoDB database. You will configure and install MongoDB replica sets in
    different environments, manage and monitor MongoDB replica set clusters, and practice
    cluster switchover and failover steps. You will explore high-availability clusters
    in MongoDB and connect to a MongoDB cluster to perform typical administration
    tasks on MongoDB cluster deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a MongoDB developer perspective, it is probably true that the MongoDB database
    server is some sort of black box, living somewhere in the cloud or in a data center
    room. Details are not important if the database is up and running when needed.
    From a business perspective though, things look slightly different. For example,
    when a production application needs to be available online for customers 24/7,
    those details are very important. Any outage can have a negative impact on service
    availability for customers, and ultimately, if the failure is not recovered quickly,
    the business' financial results.
  prefs: []
  type: TYPE_NORMAL
- en: Outages happen from time to time, and they can be attributed to a wide variety
    of reasons. These are often the result of common hardware failures, such as disk
    or memory failures, but they may also be caused by network failures, software
    failures, or even application failures. For example, a software failure such as
    an OS bug can render the server unresponsive to users and applications. Outages
    can also be caused by disasters such as flooding and earthquakes. Even though
    the probability of a disaster is much smaller, they could still have a devastating
    impact on businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting failures and disasters is an impossible task, as it is not possible
    to guess the exact time when they will strike. Therefore, the business strategy
    should focus on solutions for these, by allocating redundant hardware and software
    resources. In the case of MongoDB, the solution to high availability and disaster
    recovery is to deploy MongoDB clusters instead of a single-server database. As
    opposed to other third-party database solutions, MongoDB doesn't require expensive
    hardware to build high-availability clusters, and they are relatively easy to
    deploy. This is where replication comes in handy. This chapter explores the idea
    of replication in detail.
  prefs: []
  type: TYPE_NORMAL
- en: First, it is important to learn about the basics of high-availability clusters.
  prefs: []
  type: TYPE_NORMAL
- en: High-Availability Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we delve into the technical details of MongoDB clusters, let's first
    clarify the basic concepts. There are many different technical implementations
    of high-availability clusters, and it is important to find out how a MongoDB cluster
    solution is different from other third-party cluster implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Computer clusters are a group of computers, connected to provide a common service.
    Compared to single servers, clusters are designed to provide better availability
    and performance. Clusters have redundant hardware and software that permits the
    continuation of services in the event of failures, so that, from the user perspective,
    the cluster appears as a single unified system rather than a group of different
    computers.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A cluster node is a server computer system (or virtual server) that is part
    of the cluster. It takes at least two different servers to make a cluster, with
    each cluster node having its own hostname and IP address. MongoDB 4.2 clusters
    can have a maximum of 50 nodes. In practice, most MongoDB clusters have at least
    3 members and they rarely reach more than 10 nodes, even for very large clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Share-Nothing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In other third-party clusters, cluster nodes share common cluster resources,
    such as disk storage. MongoDB has a "share-nothing" cluster model instead, where
    nodes are independent computers. Cluster nodes are connected only by the MongoDB
    software, and data replication is performed over the internet. The advantage of
    this model is that MongoDB clusters are easier to build with just commodity server
    hardware, which is not expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Names
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A cluster name is defined in the Atlas Console, and it is used to manage the
    cluster from the Atlas web interface. As mentioned in some of the previous chapters,
    in Atlas Free Tier, you can create only one cluster (M0), which has three cluster
    nodes. The default name for a new cluster is `Cluster0`. The name of the cluster
    cannot be changed after the cluster is created.
  prefs: []
  type: TYPE_NORMAL
- en: Replica Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A MongoDB cluster is based on data replication between cluster nodes. Data is
    replicated among nodes or replica set members with the purpose of keeping data
    in sync across all MongoDB database instances.
  prefs: []
  type: TYPE_NORMAL
- en: Primary-Secondary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data replication in MongoDB replica set clusters is a master-slave replication
    architecture. The primary node sends data to secondary nodes. The replication
    is always unidirectional, from primary to secondary. There is no option for multi-master
    replication in MongoDB, so there can be only one primary node at a time. All other
    members of the MongoDB replica set cluster must be secondary nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to have multiple `mongod` processes on the same server. Each
    `mongod` process can be a standalone database instance, or it can be a member
    of a replica set cluster. For production servers, it is recommended to deploy
    just one `mongod` process per server.
  prefs: []
  type: TYPE_NORMAL
- en: The Oplog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One database component that is essential for MongoDB replication is the **Oplog**
    (**Operation Log**). The Oplog is a special circular buffer in which all data
    changes are saved for cluster replication. Data changes are generated by CRUD
    operations (insert/update/delete) on the primary database. Nevertheless, database
    queries don''t generate any Oplog records because queries don''t modify any data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Mongo DB Oplog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.1: Mongo DB Oplog'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, all CRUD database writes are applied to datafiles by changing JSON
    data in database collections (just like on non-clustered databases) and are saved
    in the Oplog buffer for replication. Data change operations are converted into
    a special idempotent format that can be applied multiple times with the same result.
  prefs: []
  type: TYPE_NORMAL
- en: At the database logical level, the Oplog appears as a capped (circular) collection
    in the local system database. The size of the Oplog collection is particularly
    important for cluster operations and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the maximum allocated size for the Oplog is 5% of the server''s
    free disk space. To check the size of the currently allocated Oplog (in bytes),
    use the **local** database to query replication stats, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following JS script will print the size of the Oplog in megabytes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: Output after running the JS script'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.2: Output after running the JS script'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 10.2*, the Oplog size for this Atlas cluster is `3258 MB`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the Oplog is mistaken for WiredTiger journaling. Journaling is also
    a log for database changes, but with a different scope. While the Oplog is designed
    for cluster data replication, database journaling is a low-level log needed for
    database recovery. For example, if MongoDB crashes unexpectedly, datafiles can
    become corrupted because the last changes were not saved. Journal records are
    needed to perform database recovery after the instance restarts.
  prefs: []
  type: TYPE_NORMAL
- en: Replication Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram depicts the architecture diagram of a simple replica
    set cluster with only three server nodes – one primary node and two secondary
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: MongoDB replication'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.3: MongoDB replication'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding model, the PRIMARY database is the only active replica set
    member that receives write operations from database clients. The PRIMARY database
    saves data changes in the Oplog. Changes saved in the Oplog are sequential—that
    is, saved in the order that they are received and executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SECONDARY database is querying the PRIMARY database for new changes in the
    Oplog. If there are any changes, then Oplog entries are copied from PRIMARY to
    SECONDARY as soon as they are created on the PRIMARY node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the SECONDARY database applies changes from the Oplog to its own datafiles.
    Oplog entries are applied in the same order they were inserted in the log. As
    a result, datafiles on SECONDARY are kept in sync with changes on PRIMARY.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, SECONDARY databases copy data changes directly from PRIMARY. Sometimes
    a SECONDARY database can replicate data from another SECONDARY. This type of replication
    is called *Chained Replication* because it is a two-step replication process.
    Chained replication is useful in certain replication topologies, and it is enabled
    by default in MongoDB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that, once a MongoDB instance is part of a replica
    set cluster, all changes are copied to the Oplog for data replication. It is not
    possible to use a replica set to replicate only some parts, such as just a few
    database collections. For this reason, all user data is replicated and kept in
    sync across all cluster members.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster members can have different states, such as PRIMARY and SECONDARY in
    the preceding diagram. Node states can change in time, depending on cluster activity.
    For example, a node can be in the PRIMARY state at one point in time, and in the
    SECONDARY state, another time. PRIMARY and SECONDARY are the most common states
    of a node in the cluster configuration, although other states are possible. To
    understand their possible roles and how they can change, let's explore the technical
    details of cluster election.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Members
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Atlas, you can see the cluster member list from the `Clusters` page, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4: Atlas web interface'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.4: Atlas web interface'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the cluster name `Cluster0` from `SANDBOX`. Then the list of servers
    and their roles will be displayed in the Atlas application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5: Atlas web interface'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.5: Atlas web interface'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 10.5*, this cluster has three cluster members, which are
    named with the same prefix as the Atlas cluster name (in this case, `Cluster0`).
    For MongoDB clusters that are installed without using the Atlas PaaS web interface
    (or that are installed locally, on premises), you can check the cluster members
    using the following mongo shell command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: An example of using the cluster status command will be provided in *Exercise
    10.01*, *Checking Atlas Cluster Members*.
  prefs: []
  type: TYPE_NORMAL
- en: The Election Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One feature specific to all cluster implementations is the ability to survive
    (or fail over) in the event of failures. The MongoDB replica set is protected
    against any type of failure, be it a hardware failure, software failure, or network
    outage. The MongoDB software responsible for this process is called **cluster
    election**—a name derived from the action of electing using votes. The purpose
    of a cluster election is to "elect" a new primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The election process is initiated by an event. For example, consider that the
    primary member is lost. Analogous to political elections, the MongoDB cluster
    members participate in a vote to elect a new primary member. The election is validated
    only if it obtains the majority of all votes in the cluster. The formula is remarkably
    simple: the surviving cluster has a majority of (*N/2 + 1*), where *N* is the
    total number of nodes. Therefore, half plus one of the votes is enough to elect
    a new primary. This majority is necessary to avoid split-brain syndrome:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Split-brain syndrome is the terminology used to define a situation where two
    parts of the same cluster are isolated and they both "believe" that they are the
    only surviving part of the cluster. Enforcing the "half plus one" rule ensures
    that only the largest part of the cluster can elect a new primary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: MongoDB election'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.6: MongoDB election'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the preceding diagram. After a network partition incident, nodes 3
    and 5 are isolated from the rest of the cluster. In this situation, the left side
    (nodes 1, 2, and 4) form a majority, whereas nodes 3 and 5 form a minority. So,
    nodes 1, 2, and 4 can elect a primary, since they form the majority cluster. Nevertheless,
    there are situations where a network partition could split the cluster into halves,
    with identical numbers of nodes. In this case, none of the halves have a majority
    necessary to elect a new primary. Therefore, one of the key factors in MongoDB
    cluster design is that clusters should always be configured with an odd number
    of nodes to avoid a perfect half split.
  prefs: []
  type: TYPE_NORMAL
- en: Not all cluster members can participate in an election. There can be a maximum
    of seven votes, regardless of the total number of members in a MongoDB cluster.
    This is designed to limit the network traffic between cluster nodes during the
    election process. Non-voting members cannot participate in elections, but they
    can replicate data from the primary as secondary nodes. By default, each node
    can have one vote.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10.01: Checking Atlas Cluster Members'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will connect to the Atlas cluster using mongo shell and
    identify the cluster name and all cluster members, together with their current
    state. Use JavaScript to list the cluster members:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to your Atlas database using mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The replica set status function `rs.status()` gives detailed information about
    the cluster that is not visible from the Atlas web interface. A simple JS script
    to list all nodes and their member roles for `rs.status` is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The script can run from any node of the cluster if you are connected to one
    secondary instead of the primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7: Output after running the JS script'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.7: Output after running the JS script'
  prefs: []
  type: TYPE_NORMAL
- en: We have learned about the basic concepts of MongoDB replica set clusters. The
    MongoDB primary-secondary replication technology protects the database from any
    hardware and software failures. In addition to providing high availability and
    disaster recovery for applications and users, MongoDB clusters are also easy to
    deploy and manage. Thanks to the Atlas managed database service, users can easily
    connect to Atlas and test applications, without the need to install and configure
    the cluster locally.
  prefs: []
  type: TYPE_NORMAL
- en: Client Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MongoDB connection string was covered in *Chapter 3*, *Servers and Clients*.
    Database services deployed in Atlas are always replica set clusters, and the connection
    string can be copied from the Atlas interface. In this section, we will explore
    the connections between clients and MongoDB clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to a Replica Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, the same rules apply for the MongoDB connection string. Consider
    the following screenshot, which shows such a connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8: An example of a connection string in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.8: An example of a connection string in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure10.6*, the connection string looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As explained in *Chapter 3*, *Servers and Clients*, this type of string needs
    DNS to resolve the actual server names or IP addresses. In this example, the connection
    string contains the Atlas cluster name `cluster0` and the ID number `u7n6b`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In your case, the connection string could be different. That is because your
    Atlas cluster deployment is likely to have a different ID number and/or a different
    cluster name. Your actual connection string can be copied from your Atlas web
    console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a careful inspection of the text in the shell, we see the following
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to notice is that the second string is significantly longer
    than the first. That is because the original connection string is substituted
    (after a successful DNS SRV lookup) into the equivalent string with the `mongodb://`
    URI prefix. The following table explains the structure of the cluster connection
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9: Structure of the collection string'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.9: Structure of the collection string'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following a successful connection and user authentication, the shell prompt
    will have the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`MongoDB Enterprise` here specifies the version of the MongoDB server running
    in the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`atlas-rzhbg7-shard-0` indicates the MongoDB replica set name. Note that in
    the current version of Atlas, the MongoDB replica set name is different from the
    cluster name, which is `Cluster0` in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PRIMARY` refers to the database instance role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is a clear distinction in MongoDB between a cluster connection and a
    single server connection. The connection shows the MongoDB cluster, in the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify the current connection from mongo shell, use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10: Verifying the connection string in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.10: Verifying the connection string in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The replica set name connection parameter `replicaSet` indicates that the connection
    string is for a cluster instead of a simple MongoDB server instance. In this case,
    the shell will attempt to connect to all server members of the cluster. From the
    application perspective, the replica set is behaving as a single system, rather
    than a collection of separate servers. When connected to a cluster, the shell
    will always indicate the `PRIMARY` read-write instance.
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at single-server connections.
  prefs: []
  type: TYPE_NORMAL
- en: Single-Server Connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the same way we connect to a non-clustered MongoDB database, we have the
    option to connect to individual cluster members separately. In this case, the
    target server name (cluster member) needs to be contained in the connection string.
    Also, the `replicaSet` parameter needs to be removed. Here is an example for the
    Atlas cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The other two parameters, `authSource` and `ssl`, need to be retained for Atlas
    server connections. As described in *Chapter 3*, *Servers and Clients*, Atlas
    has authorization and SSL network encryption activated for cloud security protection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11: Connecting to individual cluster members'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.11: Connecting to individual cluster members'
  prefs: []
  type: TYPE_NORMAL
- en: This time, the shell prompt indicates `SECONDARY`, which indicates that we are
    connected to the secondary node. Also, the `db.getMongo()` function returns a
    simple server and port number connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'As described earlier, data changes are not allowed on secondary members. This
    is because a MongoDB cluster needs to maintain a consistent copy of data across
    all cluster nodes. Therefore, changing data is allowed only on the primary node
    of the cluster. For example, if we try to modify, insert, or update a collection
    while connected on a secondary member, we will get the `not master` error message,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12: Getting the "not master" error message in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.12: Getting the "not master" error message in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: However, read-only operations are allowed on secondary members, and this is
    precisely the scope of the next exercise. In this exercise, you will learn how
    to read collections while connected on secondary cluster members.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To enable read operations while connected to a secondary node, it is necessary
    to run the shell command `rs.slaveOk()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10.02: Checking the Cluster Replication'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will connect to the Atlas cluster database using mongo
    shell and observe the data replication between the primary and secondary cluster
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to your Atlas cluster with mongo shell and user `admindb`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The connection string could be different in your case. You can copy the connection
    string from the Atlas web interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to create a new collection on the primary node
    and insert a few new documents with random numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13: Inserting new documents with random numbers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.13: Inserting new documents with random numbers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to a secondary node by entering the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The connection string could be different in your case. Make sure you edit the
    correct server node in the connection string. The connection should indicate a
    `SECONDARY` member.
  prefs: []
  type: TYPE_NORMAL
- en: 'Query the collection to see whether data is replicated on the secondary nodes.
    To enable the reading of data on the secondary nodes, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14: Reading data on the secondary nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.14: Reading data on the secondary nodes'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you verified the cluster MongoDB replication by inserting
    documents on the primary node and querying them on secondary nodes. You may notice
    that the replication is almost instantaneous, even though MongoDB replication
    is asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: Read Preference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it is possible to read data from a secondary node (as shown in the previous
    exercise), it is not ideal for applications because it requires a separate connection.
    **Read preference** is a term in MongoDB that defines how clients can redirect
    read operations to secondary nodes automatically, without connecting to individual
    nodes. There are a few reasons why the client may choose to redirect read operations
    to secondary nodes. For example, running large queries on the primary node will
    slow down overall performance for all operations. Offloading the primary node
    by running queries on secondary nodes is a good idea to optimize performance for
    inserts and updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, all operations are performed on the primary node. While write operations
    must be executed only on the primary node, read operations can be performed on
    any secondary node (except an arbiter node). The client can set a read preference
    at the session or statement level while connected to a MongoDB cluster. The following
    command helps check the current read preference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the various **read preferences** in MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15: Read preferences in MongoDB'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.15: Read preferences in MongoDB'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows an example of setting the read preference (in this
    case, `secondary`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have a current cluster connection, with DNS SRV or a cluster/server
    list. The read preference setting doesn't work correctly with a single node connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of using a read preference from mongo shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16: Read preference from mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.16: Read preference from mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that once the read preference is set to `secondary`, the shell client
    automatically redirects the read operations to secondary nodes. After the query
    is performed, the shell returns to `primary` (shell prompt: `PRIMARY`). All further
    queries will be redirected to `secondary`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The read preference is lost if the client disconnects from the replica set.
    This is because the read preference is a client-side setting (not server). In
    this case, you will need to set the read preference again, after reconnecting
    to the MongoDB cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The read preference can also be set as an option in the connection string URI,
    with the `?readPreference` parameter. For example, consider the following connection string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB offers even more sophisticated features for setting the read preference
    in a cluster. In more advanced configurations, the administrator can set tag names
    for each cluster member. For example, a tag name can indicate that the cluster
    member is located in a specific geographical region or data center. The tag name
    can then be used as a parameter to the `db.setReadPref()` function to redirect
    reads to a specific geographical region in the proximity of the client's location.
  prefs: []
  type: TYPE_NORMAL
- en: Write Concern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, a Mongo client receives a confirmation for each write operation
    (insert/update/delete) on the primary node. The confirmation return code can be
    used in applications to make sure that data is securely written into the database.
    In the case of replica set clusters, though, the situation is more complex. For
    example, it is possible to insert rows in a primary instance, but if the primary
    node crashes before replication Oplog records are applied to secondary nodes,
    then there is a risk of data loss. Write concern addresses this issue by ensuring
    that the write is confirmed on multiple cluster nodes. Therefore, in the event
    of an unexpected crash of the primary node, the inserted data will not be lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the write concern is `{w: 1}`, which indicates acknowledgment from
    the primary instance only. `{w: 2}` will require confirmation from two nodes for
    each write operation. Multiple node confirmation comes at a cost, however. A large
    number for the write concern can lead to slower write operations on the cluster.
    `(w: "majority")` indicates the majority of cluster nodes. This setting helps
    ensure data safety in unexpected failure scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write concern can be set at the cluster level or at the write statement level.
    In Atlas, we cannot see or configure the write concern, as it is preset by MongoDB
    to `{w: "majority"}`. The following is an example of write concern at the statement
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'All CRUD operations (except queries) have an option for write concern. Optionally,
    a second parameter can be set, `wtimeout: 1000`, to configure the maximum timeout
    in milliseconds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17: Write concern in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.17: Write concern in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: The MongoDB client has many options for replication-set clusters. Understanding
    the basics of a client session in the cluster environment is essential for application
    development. It can lead to mistakes if developers overlook the cluster configuration.
    For example, one common mistake is to run all queries on the primary node or to
    assume that secondary reads are executed by default without any configuration.
    Setting up the read preference can significantly improve the performance of applications
    while reducing the load on the primary cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up a new MongoDB replica set cluster is an operational task that is
    usually required at the start of a new development project. Depending on the complexity
    of the new environment, the deployment of a new replica set cluster can vary from
    a relatively easy, straightforward, simple configuration to more complex and enterprise-grade
    cluster deployments. In general, deploying MongoDB clusters requires more technical
    and operational knowledge than installing a single server database. Planning and
    preparation are essential and should never be overlooked before cluster deployments.
    That is because users need to carefully plan the cluster architecture, the underlying
    infrastructure, and database security to provide the best performance and availability
    for their database.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the method used for MongoDB replica set cluster deployments, there
    are a few tools that can help with the automatization and management of the deployments.
    The most common method is manual deployment. Nevertheless, the manual method is
    probably the most laborious option—especially for complex clusters. Automatization
    tools are available from MongoDB and other third-party software providers. The
    next section looks at the most common methods used for MongoDB cluster deployments
    and the advantages of each method.
  prefs: []
  type: TYPE_NORMAL
- en: Atlas Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying MongoDB clusters on the Atlas cloud is the easiest option available
    for developers as it saves on effort and money. The MongoDB company manages the
    infrastructure, including the server hardware, OS, network, and `mongod` instances.
    As a result, users can focus on application development and DevOps, rather than
    spending time on the infrastructure. In many cases, this is the perfect solution
    for fast-delivery projects.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a cluster on Atlas requires nothing more than a few clicks in the
    Atlas web application. You are already familiar with database deployments in Atlas
    from *Chapter 1*, *Introduction to MongoDB*. The free-tier Atlas M0 cluster is
    a great free-of-charge environment for learning and testing. As a matter of fact,
    all deployments in Atlas are replica set clusters. In the current Atlas version,
    it is not possible to deploy single-server clusters in Atlas.
  prefs: []
  type: TYPE_NORMAL
- en: Atlas offers more cluster options for larger deployments, which are charged
    services. If required, Atlas clusters can scale up easily—both vertically (adding
    server resources) and horizontally (adding more members). It is possible to build
    multi-region, replica set clusters on dedicated Atlas servers M10 and higher.
    Therefore, high availability can extend across geographical regions, between Europe
    and North America. This option is ideal for allocating read-only secondary nodes
    in a remote data center.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of a multi-region cluster configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18: Multi-region cluster configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.18: Multi-region cluster configuration'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the primary database is in London, together with two
    other secondary nodes, while in Sydney, Australia, one additional secondary node
    is configured for read-only access.
  prefs: []
  type: TYPE_NORMAL
- en: Manual Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manual deployment is the most common form of MongoDB cluster deployment. For
    many developers, building a MongoDB cluster manually is also the preferred option
    for database installation because this method gives them full control over the
    infrastructure and cluster configuration. Manual deployment is more laborious
    compared with other methods, however, which makes this method less scalable for
    large environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'You would perform the following steps to manually deploy MongoDB clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the server members of the new cluster. Whether they are physical servers
    or virtual, they must meet the minimum requirements for the MongoDB database.
    Also, all cluster members should have identical hardware and software specifications
    (CPU, memory, disk, and OS).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MongoDB binaries must be installed on each server. Use the same installation
    path on all servers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run one `mongod` instance per server. Servers should be on separate hardware
    with a separate power supply and network connections. For testing, however, it
    is possible to deploy all cluster members on a single physical server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the Mongo server with the `--bind_ip` parameter. By default, `mongod`
    binds only to the localhost IP address (`127.0.0.1`). In order to communicate
    with other cluster members, `mongod` must bind to external private or public IP addresses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the network properly. Each server must be able to communicate freely with
    other members without firewalls. Also, servers' IPs and DNS names must match in
    the DNS domain configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the directory structure for database files and database instance logs.
    Use the same path on all servers. For example, use `/data/db` for database files
    (WiredTiger storage) and `/var/log/mongodb` for log files on Unix/macOS systems,
    and in the case of Windows OSes, use `C:\data\db` directories for datafiles and
    `C:\log\mongo` for log files. Directories must be empty (create a new database
    cluster).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start up the `mongod` instance on each server with the replica set parameter
    `replSet`. To start a `mongod` instance, start an OS Command Prompt or terminal
    and execute the following command for Linux and macOS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows OSes, the command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table lists the parameters and the description for each:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19: Description of the parameters in the commands'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.19: Description of the parameters in the commands'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to the new cluster with mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the cluster config JSON document and save it in a JS variable (`cfg`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding configuration steps are not real commands. `hostname1.domain`
    should be replaced with the real hostname and domain that matches DNS records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate the cluster as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Cluster activation saves the configuration and starts the cluster configuration.
    During the cluster configuration, there is an election process where member nodes
    decide on the new primary instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the configuration is activated, the shell prompt will display the cluster
    name (for example, `cluster0 : PRIMARY>`). Moreover, you can check the cluster
    status with the `rs.status()` command, which gives detailed information about
    the cluster and member servers. In the next exercise, you will set up a MongoDB
    cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10.03: Building Your Own MongoDB Cluster'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will set up a new MongoDB cluster that will have three
    members. All `mongod` instances will be started on the local computer, and you
    need to set different directories for each server so that instances will not clash
    on the same datafiles. You will also need to use a different TCP port for each
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the file directories. For Windows OSes, this should be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`C:\data\inst1`: For instance 1 datafiles'
  prefs: []
  type: TYPE_NORMAL
- en: '`C:\data\inst2`: For instance 2 datafiles'
  prefs: []
  type: TYPE_NORMAL
- en: '`C:\data\inst3`: For instance 3 datafiles'
  prefs: []
  type: TYPE_NORMAL
- en: '`C:\data\log`: Log file destination'
  prefs: []
  type: TYPE_NORMAL
- en: For Linux, the file directories are the following. Note that for MacOS, you
    can use any directory name of your choice instead of `/data`.
  prefs: []
  type: TYPE_NORMAL
- en: '`/data/db/inst1`: For instance 1 datafiles'
  prefs: []
  type: TYPE_NORMAL
- en: '`/data/db/inst2`: For instance 2 datafiles'
  prefs: []
  type: TYPE_NORMAL
- en: '`/data/db/inst3`: For instance 3 datafiles'
  prefs: []
  type: TYPE_NORMAL
- en: '`/var/log/mongodb`: Log file destination'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of this in Windows Explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.20: Directory structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.20: Directory structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the various instances, use the following TCP ports:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance 1: 27001'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance 2: 27002'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance 3: 27003'
  prefs: []
  type: TYPE_NORMAL
- en: Use the replica set name `my_cluster`. The Oplog size should be 50 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Start the `mongod` instances from Windows Command Prompt. Use `start` to run
    the `mongod` startup command. This will create a new window for the process. Otherwise,
    the `start mongod` command might hang, and you will need to use another Command
    Prompt window. Note that you will need to use `sudo` instead of `start` for MacOS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `--logappend` parameter adds log messages at the end of the log file. Otherwise,
    the log file will be truncated each time you start the `mongod` instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the startup messages in the log destination folder (`C:\data\log`). Each
    instance has a separate log file, and at the end of the log, there should be a
    message as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In a separate terminal (or Windows Command Prompt), connect to the cluster
    using mongo shell using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows an example using mongo shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.21: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.21: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the shell command prompt is just `>`, even though you connected
    with the `replicaSet` parameter in the connection string. That is because the
    cluster is not configured yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the cluster configuration JSON document (in the JS variable `cfg`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This code can be typed directly into mongo shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate the cluster configuration as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it usually takes some time for the cluster to activate the configuration
    and elect a new primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.22: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.22: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'The shell prompt should indicate the cluster connection (initially `mycluster:
    SECONDARY` and then `PRIMARY`) after the election process is completed and successful.
    If your prompt still shows `SECONDARY`, then try to reconnect or check the server
    logs for errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the cluster configuration. For this, connect with mongo shell and verify
    that the prompt is `PRIMARY>`, and then run the following command to check the
    cluster status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to verify the current cluster configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Both commands return a long output with many details. The expected results
    are in the following screenshot (which shows a partial output):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.23: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.23: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you manually deployed all members of a replica set cluster
    on your local system. This exercise is for testing purposes only and should not
    be used for real applications. In real life, MongoDB cluster nodes should be deployed
    on separate servers, but the exercise gave a good inside look at a replica set's
    initial configuration and is especially useful for quick tests.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For large-scale enterprise applications, MongoDB provides integrated tools for
    managing deployments. It is easy to imagine why deploying and managing hundreds
    of MongoDB cluster servers could be an incredibly challenging task. Therefore,
    the ability to manage all deployments in an integrated interface is essential
    for large, enterprise-scale MongoDB environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'MongoDB provides two different interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MongoDB OPS Manager** is a package available for MongoDB Enterprise Advanced.
    It typically requires installation on-premises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDB Cloud Manager** is a cloud-hosted service to manage MongoDB Enterprise
    deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Both Cloud Manager and Atlas are cloud applications, but they provide different
    services. While Atlas is a fully managed database service, Cloud Manager is a
    service to manage database deployments, including local server infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Both applications provide similar functionality for enterprise users, with integrated
    automation for deployments, advanced graphical monitoring, and backup management.
    Using Cloud Manager, administrators are able to deploy all types of MongoDB servers
    (both single and clusters), while maintaining full control over the underlying
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the Cloud Manager architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.24: Cloud Manager architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.24: Cloud Manager architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture is based on a central management server and MongoDB Agent.
    Before a server can be managed in Cloud Manager, the MongoDB Agent needs to be
    deployed on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB Agent software should not be confused with MongoDB database software.
    MongoDB Agent software is used for Cloud Manager and OPS Manager centralized management.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to Cloud Manager, users are not actually required to download and
    install MongoDB databases. All MongoDB versions are managed automatically by the
    deployment server once the agent is installed and the server is added to Cloud
    Manager configuration. MongoDB Agent will automatically download, stage, and install
    MongoDB server binaries on the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example from MongoDB Cloud Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.25: Cloud Manager screenshot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.25: Cloud Manager screenshot'
  prefs: []
  type: TYPE_NORMAL
- en: The Cloud Manager web interface is similar to the Atlas application. One major
    difference between them is that Cloud Manager has more features. While Cloud Manager
    can manage your Atlas deployments, it has more complex options available for MongoDB
    Enterprise deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to add a deployment (the `New Replica Set` button), and then
    to add servers to the deployment and install MongoDB agents. Once the MongoDB
    agent is installed on cluster members, the deployment is performed automatically
    by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can test Cloud Manager for free for 30 days on MongoDB Cloud. The registration
    process is similar to the steps were shown in *Chapter 1*, *Introduction to MongoDB*.
  prefs: []
  type: TYPE_NORMAL
- en: The MongoDB Atlas managed DBaaS cloud service is a great platform for quick
    and easy deployments. Most users will find Atlas their preferred choice for database
    deployments because the cloud environment is fully managed, secure, and always
    available. On the downside, the Atlas cloud service has some limitations for users
    when compared with Mongo DB on-premises. For example, Atlas does not allow users
    to access or tune the hardware and software infrastructure. If users want to have
    full control over the infrastructure, they can choose to manually deploy MongoDB
    databases. In the case of large enterprise database deployments, MongoDB provides
    software solutions such as Cloud Manager, which is useful for managing many cluster
    deployments while still having full control of the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a scenario where one of your servers that is running a MongoDB database
    has reported memory errors. You are a bit worried because the computer is running
    the primary active member of your cluster. The server needs maintenance to replace
    the faulty **DIMM** (**Dual In-Line Memory Module**). You decide to switch over
    the primary instance to another server. The maintenance should take less than
    an hour, but you want to make sure that users can use their applications during
    the maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB cluster operations refer to such day-to-day administration tasks that
    are necessary for cluster maintenance and monitoring. This is especially important
    for clusters deployed manually, where users must fully manage and operate replica
    set clusters. In the case of the Atlas DBaaS managed service, the only interaction
    is through the Atlas web application and most of the work is done behind the scenes
    by MongoDB. Therefore, our discussion will be limited to MongoDB clusters deployed
    manually, either in the local infrastructure or in cloud IaaS (Infrastructure
    as a Service).
  prefs: []
  type: TYPE_NORMAL
- en: Adding and Removing Members
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'New members can be added to replica sets with the command `rs.add()`. Before
    we can add a new member, the `mongod` instance needs to be prepared and started
    with the same `—replSet` cluster name option. The same rules apply to new cluster
    members. For example, starting the new `mongod` instance would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we add a new member to an existing replica set, though, we need to decide
    on the type of cluster member. The following options are available for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.26: Descriptions for the member types'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.26: Descriptions for the member types'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Member
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few arguments that can be passed when we add a new cluster member,
    depending on the member type. In its simplest form, the `add` command has only
    one parameter—a string containing the hostname and port of the new instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep in mind the following while adding a member:'
  prefs: []
  type: TYPE_NORMAL
- en: A `SECONDARY` member should be added to the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Priority can be any number between `0` and `1000`. If this instance were to
    be elected as the primary, the priority must be set greater than `0`. Otherwise,
    the instance is considered `READ ONLY`. Moreover, the priority must be `0` for
    the `HIDDEN`, `DELAY`, and `ARBITER` instance types. The default value is `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All nodes have one vote by default. In version 4.4, a node can have either 0
    votes or 1 vote. There can be a maximum of 7 voting members—with one vote each.
    The rest of the nodes are not participating in the election process, having 0
    votes. The default value is 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of adding a member:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.27: Example of adding a member'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.27: Example of adding a member'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, `"ok" : 1` indicates that the add member operation
    was successful. In the new instance logs, the initial sync (database copy) is
    started for the new replica set member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`0` adds a different member type, but the `add` command can be different. For
    example, to add a hidden member with a vote, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If successful, the `add` command will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the cluster configuration by adding the new member node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the initial sync—the database is copied to the new member instance (except
    in the case of `ARBITER`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some situations, adding a new member can change the current primary.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The new member cluster must have an empty database (empty data directory) before
    joining the replica set cluster. Oplog operations that are generated on the primary
    node during the sync process are also copied and applied to the new cluster member.
    The synchronization process may take a long time, especially if synchronization
    is running over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Removing a Member
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cluster members can be removed by connecting to the cluster and running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Removing a cluster member does not remove the instance and datafiles. The instance
    can be started in single-server mode (without the `—replSet` option), and datafiles
    will contain the latest updates from before it was removed.
  prefs: []
  type: TYPE_NORMAL
- en: Reconfiguring a Cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cluster reconfiguration may be necessary if you want to make more complex changes
    to a replica set, such as adding multiple nodes in one step or editing the default
    values for votes and priority. Clusters can be reconfigured by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a step-by-step breakdown of a cluster reconfiguration with
    a different priority for each node:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the configuration in a JS variable as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit `new_conf` to change the default priority by adding the following snippet:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable the new configuration as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows an example of cluster reconfiguration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.28: Example of cluster reconfiguration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.28: Example of cluster reconfiguration'
  prefs: []
  type: TYPE_NORMAL
- en: Failover
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In certain situations, the MongoDB cluster could initiate an election process.
    In data center operations terminology, these types of events are usually called
    **Failover** and **Switchover**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Failover** is always a result of an incident. When one or more cluster members
    become unavailable (usually because of a failure or network outage) the cluster
    fails over. The replica set detects that some of the nodes become unavailable,
    and the replica set election is automatically started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: How does a replica set cluster detect an incident? Member servers regularly
    communicate between themselves—sending/receiving a heartbeat network request every
    couple of seconds. If one member does not reply for a longer time (the default
    is 10 seconds), then the member is declared unavailable and a new cluster election
    is initiated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Switchover** is a user-initiated process (that is, initiated by a server
    command). The purpose of switchover is to perform planned maintenance on the cluster.
    For example, the server running the primary member needs to restart for OS patching,
    and the administrator switches the primary over to another cluster member.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of whether it is a failover or a switchover, the election mechanism
    is started, and the cluster aims to achieve a new majority and, if successful,
    a new primary node. During the election process, there is a transition period
    when writes are not possible on the database and client sessions will reconnect
    to the new primary member. Application coding should be able to handle MongoDB
    failover events transparently for users.
  prefs: []
  type: TYPE_NORMAL
- en: In Atlas, failovers are managed automatically by MongoDB, so no user involvement
    is required. In larger Atlas deployments (such as M10+), the `Test Failover` button
    is available in the Atlas application. The `Test Failover` button will force a
    cluster failover for application testing. If the new cluster majority cannot be
    achieved, then all nodes will stay in the secondary state and no primary will
    be elected. In this situation, the clients will not be able to modify any data
    in the database. However, the read-only operations are still possible on all secondary
    nodes regardless of the cluster status.
  prefs: []
  type: TYPE_NORMAL
- en: Failover (Outage)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the event of outages, usually, messages such as the one in the following
    code snippet can be seen in the instance logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The client session (in other words, the connection pool) will automatically
    reconnect to the remaining nodes, and the activity can continue as normal. Once
    the missing node is restarted, it will rejoin the cluster automatically. If the
    cluster cannot successfully complete election with the available nodes, then the
    failover is not considered successful. In the logs, we can see a message like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the client connection is dropped, and users are not able to reconnect
    unless the read preference is set to `secondary`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if election is not successful, the users are able to connect with a read
    preference `secondary` setting, as in the following connection string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to open the database instance in read-write mode (the primary
    state) unless there are sufficient nodes to form a cluster majority. One typical
    mistake is to reboot many secondary members at the same time. If the cluster detects
    that the majority is lost, then the primary state member will step down to secondary.
  prefs: []
  type: TYPE_NORMAL
- en: Rollback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some situations, failover events could generate rollbacks of writes on the
    former primary node. This may happen if writes on the primary were performed with
    the default write concern (`w:1`), and the former primary crashed before it had
    the chance to replicate changes to any secondary node. The cluster forms a new
    majority, and the activity will continue with a new primary. When the former primary
    is back up, it needs to roll back those (previously un-replicated) transactions
    before it can get in sync with the new primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chances of rollback could be reduced by setting write concern to `majority`
    (`w: ''majority''`)—that is, by obtaining acknowledgment from most cluster nodes
    (the majority) for every database write. On the downside, this could slow down
    the writes for the application.'
  prefs: []
  type: TYPE_NORMAL
- en: Normally, failures and outages are remedied quickly, and the affected nodes
    rejoin the cluster when they are back up. However, if the outage is taking a long
    time (for example, a week), then the secondary instances could become **stale**.
    A stale instance will not be able to resynchronize data with the primary member
    after a restart. In that case, the instance should be added as a new member (empty
    data directory) or from a recent database backup.
  prefs: []
  type: TYPE_NORMAL
- en: Switchover (Stepdown)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For maintenance activities, we often need to transfer the primary state from
    one instance to another. For this, the user admin command to be executed on the
    primary is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `stepDown` command will force the primary node to step down and cause the
    secondary node with the highest priority to step up as the new primary node. The
    primary node will step down only if the secondary node is up to date. Therefore,
    switchover is a safer operation compared to failover. There is no risk of losing
    writes on a former primary member.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.29: Using the stepDown command'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.29: Using the stepDown command'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify the current master node by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note that in order for a switchover to be successful, the target cluster member
    must be configured with a higher priority. A member with a default priority (`priority
    = 0`) will never become a primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10.04: Performing Database Maintenance'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will perform cluster maintenance on a primary node. First,
    you will switch over to the secondary server, `inst2`, so that the current primary
    server will become secondary. Then, you will shut down the former primary server
    for maintenance and restart the former primary and switch over:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before you start this exercise, prepare the cluster script and directories as
    per the steps given in *Exercise 10.02*, *Checking the Cluster Replication*.
  prefs: []
  type: TYPE_NORMAL
- en: Start up all cluster members (if not already started), connect with mongo shell,
    and verify the configuration and the current master node with `rs.isMaster().primary`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reconfigure the cluster. For this, copy the existing cluster configuration into
    a variable, `sw_over`, and set the read-only member priority. For `inst3`, the
    priority should be set to `0` (read-only).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Switch over to `inst2`. On the primary node, run the `stepDown` command as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the new primary is `inst2` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now, `inst1` can be stopped for hardware maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shut down the instance locally using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.30: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15507_10_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.30: Output in mongo shell'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you practiced the switchover steps in a cluster. The commands
    are quite simple. Switchover is a good practice to test how applications handle
    MongoDB cluster events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10.01: Testing a Disaster Recovery Procedure for a MongoDB Database'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Your company is about to become public, and as a result, some certifications
    are necessary to prove that a business continuity plan is in place in case of
    disaster. One of the requirements is to implement and test a disaster recovery
    procedure for a MongoDB database. The cluster architecture is distributed between
    the main office (primary instance) and a remote office (secondary instance), which
    is the disaster recovery location. To help with MongoDB replica set elections
    in case of a network split, an arbiter node is installed in a third separate location.
    Once a year, the DR plan is tested by simulating a crash of all cluster members
    in the main office, and this year, that task falls to you. The following steps
    will help you to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you have multiple computers, it is a good idea to try the activity with two
    or three computers, with each computer emulating a physical location. In the solution,
    however, this activity will be completed by starting all instances on the same
    local computer. All secondary databases (including DR) should be in sync with
    the primary database when the activity is started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure a `sale-cluster` cluster with three members:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sale-prod`: Primary'
  prefs: []
  type: TYPE_NORMAL
- en: '`sale-dr`: Secondary'
  prefs: []
  type: TYPE_NORMAL
- en: '`sale-ab`: Arbiter (third location)'
  prefs: []
  type: TYPE_NORMAL
- en: Insert test data records into the primary collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulate a disaster. Reboot the primary node (that is, kill the current `mongod`
    primary instance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform testing on DR by inserting a few documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shut down the DR instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart all nodes for the main office.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After 10 minutes, start up the DR instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the rollback of inserted test records and re-sync with the primary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After restarting `sales_dr`, you should see a rollback message in the logs.
    The following code snippet shows an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found via [this link](B15507_Solution_Final_SZ_ePub.xhtml#_idTextAnchor479).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned that MongoDB replica sets are essential for providing
    high availability and load sharing in a MongoDB database environment. While Atlas
    transparently provides support for infrastructure and software (including for
    replica set cluster management), not all MongoDB clusters are deployed in Atlas.
    In this chapter, we discussed the concepts and operations of replica set clusters.
    Learning about simple concepts for clusters, such as read preference, can help
    developers build more reliable, high-performance applications in the cloud. In
    the next chapter, you will learn about backup and restore operations in MongoDB.
  prefs: []
  type: TYPE_NORMAL

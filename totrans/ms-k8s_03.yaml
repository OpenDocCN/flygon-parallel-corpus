- en: Monitoring, Logging, and Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](4a72bd61-c0d6-430b-b146-15a1ca3391da.xhtml), *Creating Kubernetes
    Clusters*, you learned how to create Kubernetes clusters in different environments,
    experimented with different tools, and created a couple of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kubernetes cluster is just the beginning of the story. Once the cluster
    is up and running, you need to make sure that it is operational, all the necessary
    components are in place and properly configured, and enough resources are deployed
    to satisfy the requirements. Responding to failures, debugging, and troubleshooting
    is a major part of managing any complicated system, and Kubernetes is no exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with Heapster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance analytics with Kubernetes dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Central logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting problems at the node level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of this chapter, you will have a solid understanding of the various
    options available to monitor Kubernetes clusters, how to access logs, and how
    to analyze them. You will be able to look at a healthy Kubernetes cluster and
    verify that everything is OK. You will also be able to look at an unhealthy Kubernetes
    cluster and methodically diagnose it, locate the problems, and address them.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Kubernetes with Heapster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Heapster is a Kubernetes project that provides a robust monitoring solution
    for Kubernetes clusters. It runs as a pod (of course), so it can be managed by
    Kubernetes itself. Heapster supports Kubernetes and CoreOS clusters. It has a
    very modular and flexible design. Heapster collects both operational metrics and
    events from every node in the cluster, stores them in a persistent backend (with
    a well-defined schema), and allows visualization and programmatic access. Heapster
    can be configured to use different backends (or sinks, in Heapster''s parlance)
    and their corresponding visualization frontends. The most common combination is
    InfluxDB as the backend and Grafana as the frontend. The Google Cloud Platform
    integrates Heapster with the Google monitoring service. There are many other less
    common backends, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Log
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hawkular-Metrics (metrics only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenTSDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monasca (metrics only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka (metrics only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riemann (metrics only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can use multiple backends by specifying sinks on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: cAdvisor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cAdvisor is part of the kubelet, which runs on every node. It collects information
    about the CPU/cores' usage, memory, network, and filesystems of each container.
    It provides a basic UI on port `4194`, but, most importantly for Heapster, it
    provides all this information through the Kubelet. Heapster records the information
    collected by cAdvisor on each node and stores it in its backend for analysis and
    visualization.
  prefs: []
  type: TYPE_NORMAL
- en: The cAdvisor UI is useful if you want to quickly verify that a particular node
    is set up correctly, for example, while creating a new cluster when Heapster is
    not hooked up yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/994b310a-76bb-4d78-baf2-e16507c117e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing Heapster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Heapster components may or may not be installed in your Kubernetes cluster.
    If Heapster is not installed, you can install it with a few simple commands. First,
    let''s clone the Heapster repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In earlier versions of Kubernetes, Heapster exposed the services as `NodePort`
    by default. Now, they are exposed by default as `ClusterIP`, which means that
    they are available only inside the cluster. To make them available locally, I
    added type: `NodePort` to the spec of each service in `deploy/kube-config/influxdb`.
    For example, for `deploy/kube-config/influxdb/influxdb.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'I made a similar change to `deploy/kube-config/influxdb/grafana.yaml`, which
    has `+ type: NodePort` this line commented out, so I just uncommented it. Now,
    we can actually install InfluxDB and Grafana:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: InfluxDB backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: InfluxDB is a modern and robust distributed time-series database. It is very
    well-suited and used broadly for centralized metrics and logging. It is also the
    preferred Heapster backend (outside the Google Cloud Platform). The only thing
    is InfluxDB clustering; high availability is part of enterprise offering.
  prefs: []
  type: TYPE_NORMAL
- en: The storage schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The InfluxDB storage schema defines the information that Heapster stores in
    InfluxDB, and it is available for querying and graphing later. The metrics are
    divided into multiple categories, named measurements. You can treat and query
    each metric separately, or you can query a whole category as one measurement and
    receive the individual metrics as fields. The naming convention is `<category>/<metrics
    name>` (except for uptime, which has a single metric). If you have an SQL background,
    you can think of measurements as tables. Each metric is stored per container.
    Each metric is labeled with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pod_id`: A unique ID of a pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pod_name`: A user-provided name of a pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pod_namespace`: The namespace of a pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container_base_image`: A base image for the container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container_name`: A user-provided name of the container or full `cgroup` name
    for system containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host_id`: A cloud-provider-specified or user-specified identifier of a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hostname`: The hostname where the container ran'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: The comma-separated list of user-provided labels; format is `key:value`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespace_id`: The UID of the namespace of a pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resource_id`: A unique identifier used to differentiate multiple metrics of
    the same type, for example, FS partitions under filesystem/usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are all the metrics grouped by category, as you can see, it is quite extensive.
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CPU metrics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cpu/limit`: CPU hard limit in millicores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/node_capacity`: CPU capacity of a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/node_allocatable`: CPU allocatable of a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/node_reservation`: Share of CPU that is reserved on the node allocatable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/node_utilization`: CPU utilization as a share of node allocatable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/request`: CPU request (the guaranteed amount of resources) in millicores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/usage`: Cumulative CPU usage on all cores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpu/usage_rate`: CPU usage on all cores in millicores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filesystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Filesystem metrics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filesystem/usage`: The total number of bytes consumed on a filesystem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filesystem/limit`: The total size of the filesystem in bytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filesystem/available`: The number of available bytes remaining in the filesystem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The memory metrics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`memory/limit`: Memory hard limit in bytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/major_page_faults`: The number of major page faults'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/major_page_faults_rate`: The number of major page faults per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/node_capacity`: Memory capacity of a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/node_allocatable`: Memory allocatable of a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/node_reservation`: Share of memory that is reserved on the node allocatable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/node_utilization`: Memory utilization as a share of memory allocatable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/page_faults`: The number of page faults'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/page_faults_rate`: The number of page faults per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/request`: Memory request (the guaranteed amount of resources) in bytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/usage`: Total memory usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory/working_set`: Total working set usage; working set is the memory being
    used and is not easily dropped by the kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The network metrics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`network/rx`: Cumulative number of bytes received over the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/rx_errors`: Cumulative number of errors while receiving over'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the network
  prefs: []
  type: TYPE_NORMAL
- en: '`network/rx_errors_rate`: The number of errors per second while receiving over
    the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/rx_rate`: The number of bytes received over the network per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/tx`: Cumulative number of bytes sent over the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/tx_errors`: Cumulative number of errors while sending over the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/tx_errors_rate`: The number of errors while sending over the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/tx_rate`: The number of bytes sent over the network per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uptime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Uptime is the number of milliseconds since the container was started.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can work with InfluxDB directly if you''re familiar with it. You can either
    connect to it using its own API or use its web interface. Type the following command
    to find its port and endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can browse the InfluxDB web interface using the HTTP port. You''ll
    need to configure it to point to the API port. The `Username` and `Password` are
    `root` and `root` by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9ae3efb3-3192-44a7-be2b-3b9174c1d30c.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you're set up, you can select what database to use (see the top-right corner).
    The Kubernetes database is named `k8s`. You can now query the metrics using the
    InfluxDB query language.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Grafana runs in its own container and serves a sophisticated dashboard that
    works well with InfluxDB as a data source. To locate the port, type the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can access the Grafana web interface on that port. The first thing
    you need to do is set up the data source to point to the InfluxDB backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/79a74148-198b-4a7a-ac70-86144ee6ca9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Make sure to test the connection and then go explore the various options in
    the dashboards. There are several default dashboards, but you should be able to
    customize them to your preferences. Grafana is designed to let you adapt it to
    your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery and load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discovery and load balancing category is often where you start from. Services
    are the public interface to your Kubernetes cluster. Serious problems will affect
    your services, which will affect your users:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ac735902-2574-44a9-9489-ae1b94edd493.png)'
  prefs: []
  type: TYPE_IMG
- en: When you drill down by clicking on a service, you get some information about
    the service (most important is the label selector) and a pods view.
  prefs: []
  type: TYPE_NORMAL
- en: Performance analysis with the dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'My favorite tool by far, when I just want to know what''s going on in the cluster,
    is the Kubernetes dashboard. There are a couple of reasons for this, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is built-in (always in sync and tested with Kubernetes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's fast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an intuitive drill-down interface, from the cluster level all the
    way down to individual container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn't require any customization or configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although Heapster, InfluxDB, and Grafana are better for customized and heavy-duty
    views and queries, the Kubernetes dashboard's predefined views can probably answer
    all your questions 80-90% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: You can also deploy applications and create any Kubernetes resource using the
    dashboard by uploading the proper YAML or JSON file, but I will not cover this
    because it is an anti-pattern for manageable infrastructure. It may be useful
    when playing around with a test cluster, but for actually modifying the state
    of the cluster, I prefer the command line. Your mileage may vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s find the port first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Top-level view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dashboard is organized with a hierarchical view on the left (it can be hidden
    by clicking the hamburger menu) and dynamic, context-based content on the right.
    You can drill down into the hierarchical view to get deeper into the information
    that's relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several top-level categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovery and load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Config and storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also filter everything by a particular namespace or choose all namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Cluster view has five sections: Namespaces, Nodes, PersistentVolumes, Roles,
    and Storage Classes. It is mostly about observing the physical resources of the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3dbe8557-cce3-4e5b-9922-e7ed73042915.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You get, in a glance, a lot of information: CPU and memory usage of all the
    nodes, what namespaces are available, their Status, and Age. For each node, you
    can see its Age, Labels, and if it''s ready or not. If there were persistent volumes
    and roles, you would see them as well, then the storage classes (just host path
    in this case).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we drill down the nodes and click on the minikube node itself, we get a
    detailed screen of information about that node and the allocated resources in
    a nice pie chart. This is critical for dealing with performance issues. If a node
    doesn''t have enough resources, then it might not be able to satisfy the needs
    of its pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/8143ad1c-4616-4b6c-b176-b8f28598c4cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you scroll down, you''ll see even more interesting information. The Conditions
    pane is where it''s at. You get a great, concise view of memory and disk pressure
    at the individual node level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/4b7585a5-ab73-4762-9170-f6e87759ad63.png)'
  prefs: []
  type: TYPE_IMG
- en: There are also Pods and Events panes. We'll talk about pods in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Workloads category is the main one. It organizes many types of Kubernetes
    resources, such as CronJobs, Daemon Sets, Deployments, Jobs, Pods, Replica Sets,
    Replication Controllers, and Stateful Sets. You can drill down along any of these
    dimensions. Here is the top-level Workloads view for the default namespace that
    currently has only the echo service deployed. You can see the Deployments, Replica
    Sets, and Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0a80dca8-6665-4f68-ad1f-2e01735b4c88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s switch to all namespaces and dive into the Pods subcategory. This is
    a very useful view. In each row, you can tell if the pod is running or not, how
    many times it restarted, its IP, and the CPU and memory usage histories are even
    embedded as nice little graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/da3368c6-80dc-4056-8fc0-3b186dc40164.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also view the Logs for any pod by clicking the text symbol (second
    from the right). Let''s check the Logs of the InfluxDB pod. It looks like everything
    is in order and Heapster is successfully writing to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0358b581-6b6a-46f9-aa3e-a7995142fc90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is one more level of detail that we haven''t explored yet. We can go
    down to the container level. Let''s click on the kubedns pod. We get the following
    screen, which shows the individual containers and their `run` command; we can
    also view their logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/eb484adb-241d-444b-bcf7-ac80ffb637cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding central logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Central logging or cluster-level logging is a fundamental requirement for any
    cluster with more than a couple of nodes, pods, or containers. First, it is impractical
    to view the logs of each pod or container independently. You can't get a global
    picture of the system, and there will be just too many messages to sift through.
    You need a solution that aggregates the log messages and lets you slice and dice
    them easily. The second reason is that containers are ephemeral. Problematic pods
    will often just die, and their replication controller or replica set will just
    start a new instance, losing all the important log info. By logging to a central
    logging service, you preserve this critical troubleshooting information.
  prefs: []
  type: TYPE_NORMAL
- en: Planning central logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conceptually, central logging is very simple. On each node, you run a dedicated
    agent that intercepts all log messages from all the pods and containers on the
    node, and sends them, along with enough metadata, to a central repository where
    they are stored safely.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, if you run on the Google platform, then GKE's got you covered, and
    there is a Google central-logging service integrated nicely. For other platforms,
    a popular solution is fluentd, Elasticsearch, and Kibana. There is an official
    add-on to set up the proper services for each component. The `fluentd-elasticsearch`
    add-on is at [http://bit.ly/2f6MF5b](http://bit.ly/2f6MF5b).
  prefs: []
  type: TYPE_NORMAL
- en: It is installed as a set of services for Elasticsearch and Kibana, and the fluentd
    agent is installed on each node.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fluentd is a unified logging layer that sits between arbitrary data sources
    and arbitrary data sinks and makes sure that log messages can stream from A to
    B. Kubernetes comes with an add-on that has a Docker image that deploys the fluentd
    agent, which knows how to read various logs that are relevant to Kubernetes, such
    as `Docker` logs, `etcd` logs, and `Kube` logs. It also adds labels to each log
    message to make it easy for users to filter later by label. Here is a snippet
    from the `fluentd-es-configmap.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elasticsearch is a great document store and full-text search engine. It is
    a favorite in the enterprise because it is very fast, reliable, and scalable.
    It is used in the Kubernetes central logging add-on as a Docker image, and it
    is deployed as a service. Note that a fully-fledged production cluster of Elasticsearch
    (which will be deployed on a Kubernetes cluster) requires its own master, client,
    and data nodes. For large-scale and highly-available Kubernetes clusters, the
    central logging itself will be clustered. Elasticsearch can use self-discovery.
    Here is an enterprise grade solution: [https://github.com/pires/kubernetes-elasticsearch-cluster](https://github.com/pires/kubernetes-elasticsearch-cluster).'
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kibana is Elasticsearch's partner in crime. It is used to visualize and interact
    with the data stored and indexed by Elasticsearch. It is also installed as a service
    by the add-on. Here is the Kibana Dockerfile template ([http://bit.ly/2lwmtpc](http://bit.ly/2lwmtpc)).
  prefs: []
  type: TYPE_NORMAL
- en: Detecting node problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Kubernetes'' conceptual model, the unit of work is the pod. However, pods
    are scheduled on nodes. When it comes to monitoring and reliability, the nodes
    are what require the most attention, because Kubernetes itself (the scheduler
    and replication controllers) takes care of the pods. Nodes can suffer from a variety
    of problems that Kubernetes is unaware of. As a result, it will keep scheduling
    pods to the bad nodes and the pods might fail to function properly. Here are some
    of the problems that nodes may suffer while still appearing functional:'
  prefs: []
  type: TYPE_NORMAL
- en: Bad CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel deadlock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corrupt filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the Docker daemon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kubelet and cAdvisor don't detect these issues, another solution is needed.
    Enter the node problem detector.
  prefs: []
  type: TYPE_NORMAL
- en: Node problem detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The node problem detector is a pod that runs on every node. It needs to solve
    a difficult problem. It needs to detect various problems across different environments,
    different hardware, and different OSes. It needs to be reliable enough not to
    be affected itself (otherwise, it can't report the problem), and it needs to have
    relatively-low overhead to avoid spamming the master. In addition, it needs to
    run on every node. Kubernetes recently received a new capability named DaemonSet
    that addresses that last concern.
  prefs: []
  type: TYPE_NORMAL
- en: The source code is at [https://github.com/kubernetes/node-problem-detector](https://github.com/kubernetes/node-problem-detector).
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DaemonSet is a pod for every node. Once you define DaemonSet, every node that's
    added to the cluster automatically gets a pod. If that pod dies, Kubernetes will
    start another instance of that pod on that node. Think about it as a fancy replication
    controller with 1:1 node-pod affinity. Node problem detector is defined as a DaemonSet,
    which is a perfect match for its requirements. It is possible to use affinity,
    anti-affinity, and taints to have more fine-grained control over DaemonSet scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Problem daemons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem with node problem detector (pun intended) is that there are too
    many problems which it needs to handle. Trying to cram all of them into a single
    codebase can lead to a complex, bloated, and never-stabilizing codebase. The design
    of the node problem detector calls for separation of the core functionality of
    reporting node problems to the master from the specific problem detection. The
    reporting API is based on generic conditions and events. The problem detection
    should be done by separate problem daemons (each in its own container). This way,
    it is possible to add and evolve new problem detectors without impacting the core
    node problem detector. In addition, the control plane may have a remedy controller
    that can resolve some node problems automatically, therefore implementing self-healing.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage (Kubernetes 1.10), problem daemons are baked into the node problem
    detector binary, and they execute as Goroutines, so you don't get the benefits
    of the loosely-coupled design just yet.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the important topic of node problems, which can
    get in the way of successful scheduling of workloads, and how the node problem
    detector can help. In the next section, we'll talk about various failure scenarios
    and how to troubleshoot them using Heapster, central logging, the Kubernetes dashboard,
    and node problem detector.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are so many things that can go wrong in a large Kubernetes cluster, and
    they will, this is expected. You can employ best practices and minimize some of
    them (mostly human errors) using stricter processes. However, some issues such
    as hardware failures and networking issues can't be totally avoided. Even human
    errors should not always be minimized if it means slower development time. In
    this section, we'll discuss various categories of failures, how to detect them,
    how to evaluate their impact, and consider the proper response.
  prefs: []
  type: TYPE_NORMAL
- en: Designing robust systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you want to design a robust system, you first need to understand the possible
    failure modes, the risk/probability of each failure, and the impact/cost of each
    failure. Then, you can consider various prevention and mitigation measures, loss-cutting
    strategies, incident-management strategies, and recovery procedures. Finally,
    you can come up with a plan that matches risks to mitigation profiles, including
    cost. A comprehensive design is important and needs to be updated as the system
    evolves. The higher the stakes, the more thorough your plan should be. This process
    has to be tailored for each organization. A corner of error recovery and robustness
    is detecting failures and being able to troubleshoot. The following subsections
    describe common failure categories, how to detect them, and where to collect additional
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hardware failures in Kubernetes can be divided into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: The node is unresponsive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The node is responsive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the node is not responsive, it can be difficult sometimes to determine
    if it's a networking issue, a configuration issue, or actual hardware failure.
    You obviously can't use any information like logs or run diagnostics on the node
    itself. What can you do? First, consider if the node was ever responsive. If it's
    a node that was just added to the cluster, it is more likely a configuration issue.
    If it's a node that was part of the cluster then you can look at historical data
    from the node on Heapster or central logging and see if you detect any errors
    in the logs or degradation in performance that may indicate failing hardware.
  prefs: []
  type: TYPE_NORMAL
- en: When the node is responsive, it may still suffer from the failure of redundant
    hardware, such as non-OS disk or some cores. You can detect the hardware failure
    if the node problem detector is running on the node and raises some event or node
    condition to the attention of master. Alternatively, you may note that pods keep
    getting restarted or jobs take longer to complete. All of these may be signs of
    hardware failure. Another strong hint for hardware failure is if the problems
    are isolated to a single node and standard maintenance operations such as reboot
    don't alleviate the symptoms.
  prefs: []
  type: TYPE_NORMAL
- en: If your cluster is deployed in the cloud, replacing a node which you suspect
    as having hardware problems is trivial. It is simple to just manually provision
    a new VM and remove the bad VM. In some cases, you may want to employ a more automated
    process and employ a remedy controller, as suggested by the node problem detector
    design. Your remedy controller will listen to problems (or missing health checks)
    and can automatically replace bad nodes. This approach can work even for private
    hosting or bare metal if you keep a pool of extra nodes ready to kick in. Large-scale
    clusters can function just fine, even with reduced capacity most of the time.
    Either you can tolerate slightly reduced capacity when a small number of nodes
    are down, or you can over-provision a little bit. This way, you have some headway
    when a node goes down.
  prefs: []
  type: TYPE_NORMAL
- en: Quotas, shares, and limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes is a multitenant system. It is designed to use resources efficiently,
    but it schedules pods and allocates resources based on a system of checks and
    balances between available quotas and limits per namespace, and requests for guaranteed
    resources from pods and containers. We will dive into the details later in the
    book. Here, we''ll just consider what can go wrong and how to detect it. There
    are several bad outcomes you can run into:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Insufficient resources**: If a pod requires a certain amount of CPU or memory,
    and there is no node with available capacity, then the pod can''t be scheduled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Under-utilization**: A pod may declare that it requires a certain amount
    of CPU or memory, and Kubernetes will oblige, but then the pod may only use a
    small percentage of its requested resources. This is just wasteful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mismatched node configuration**: A pod that requires a lot of CPU but very
    little memory may be scheduled to a high-memory node and use all its CPU resources,
    thereby hogging the node, so no other pod can be scheduled but the unused memory
    is wasted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Checking out the dashboard is a great way to look for suspects visually. Nodes
    and pods that are either over-subscribed or under-utilized are candidates for
    quota and resource request mismatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/37fed9ea-0d5c-4666-a98f-cc4e23b9195d.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you detect a candidate, you can dive into using the `describe` command
    at the node or pod level. In a large-scale cluster, you should have automated
    checks that compare the utilization against capacity planning. This is important
    because most large systems have some level of fluctuation and a uniform load is
    not expected. Make sure that you understand the demands on your system and that
    your cluster's capacity is within the normal range or can adjust elastically,
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Bad configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bad configuration is an umbrella term. Your Kubernetes cluster state is configuration;
    your containers' command-line arguments are configuration; all the environment
    variables used by Kubernetes, your application services, and any third-party services
    are configuration; and all the configuration files are configuration. In some
    data-driven systems, configuration is stored in various data stores. Configuration
    issues are very common because, usually, there aren't any established good practices
    to test them. They often have various fallbacks (for example, search path for
    configuration files) and defaults, and the production-environment configuration
    is different to the development or staging environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Kubernetes cluster level, there are many possible configuration problems,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect labeling of nodes, pods, or containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling pods without a replication controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect specification of ports for services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect ConfigMap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these problems can be addressed by having a proper automated deployment
    process, but you must have a deep understanding of your cluster architecture and
    how Kubernetes resources fit together.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration problems typically occur after you change something. It is critical,
    after each deployment or manual change to the cluster, to verify its state.
  prefs: []
  type: TYPE_NORMAL
- en: Heapster and the dashboard are great options here. I suggest starting from the
    services and verifying that they are available, responsive, and functional. Then,
    you can dive deeper and verify that the system also operates within the expected
    performance parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The logs also provide helpful hints and can pinpoint specific configuration
    options.
  prefs: []
  type: TYPE_NORMAL
- en: Cost versus performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large clusters are not cheap. This is especially true if you run in the cloud.
    A major part of operating massive-scale systems is keeping track of the expense.
  prefs: []
  type: TYPE_NORMAL
- en: Managing cost on the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the greatest benefits of the cloud is that it can satisfy elastic demand
    that caters for systems that expand and contract automatically by allocating and
    deallocating resources as needed. Kubernetes fits this model very well and can
    be extended to provision more nodes as necessary. The risk here is that, if not
    constrained properly, a denial-of-service attack (malicious, accidental, or self-inflicted)
    can lead to arbitrary provisioning of expensive resources. This needs to be monitored
    carefully, so it can be caught early on. Quotas on namespaces can avoid it, but
    you still need to be able to dive in and pinpoint the core issue. The root cause
    can be external (a botnet attack), misconfiguration, an internal test gone awry,
    or a bug in the code that detects or allocate resources.
  prefs: []
  type: TYPE_NORMAL
- en: Managing cost on bare metal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On bare metal, you typically don't have to worry about runaway allocation, but
    you can easily run into a wall if you need extra capacity and can't provision
    more resources fast enough. Capacity planning and monitoring your system's performance
    to detect the need early are primary concerns for OPS. Heapster can show historical
    trends and help identify both peak times and overall growth in demand.
  prefs: []
  type: TYPE_NORMAL
- en: Managing cost on hybrid clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hybrid clusters run on both bare metal and the cloud (and possibly on private
    hosting services too). The considerations are similar, but you may need to aggregate
    your analysis. We will discuss hybrid clusters in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: Using Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Heapster and the default monitoring and logging that come in the box with Kubernetes
    are a great starting point. However, the Kubernetes community is bursting with
    innovation and several alternative solutions are available. One of the most popular
    solutions is Prometheus. In this section, we will explore the new world of operators,
    the Prometheus Operator, how to install it, and how to use it to monitor your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: What are operators?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operators are a new class of software that encapsulates the operational knowledge
    needed to develop, manage, and maintain applications on top of Kubernetes. The
    term was introduced by CoreOS in late 2016\. An operator is an application-specific
    controller that extends the Kubernetes API to create, configure, and manage instances
    of complex stateful applications on behalf of a Kubernetes user. It builds upon
    the basic Kubernetes resource and controller concepts, but includes domain or
    application-specific knowledge to automate common tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus Operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prometheus ([https://prometheus.io](https://prometheus.io)) is an open source
    systems monitoring and alerting toolkit for monitoring applications in clusters.
    It was inspired by Google's Borgmon and designed for the Kubernetes model of assigning
    and scheduling units of work. It joined CNCF in 2016, and it has been adopted
    widely across the industry. The primary differences between InfluxDB and Prometheus
    is that Prometheus uses a pull model where anyone can hit the /metrics endpoint,
    and its query language is very expressive, but simpler than the SQL-like query
    language of InfluxDB.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has built-in features to support Prometheus metrics, and Prometheus
    awareness of Kuberneres keeps improving. The Prometheus Operator packages all
    that monitoring goodness into an easy to install and use bundle.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Prometheus with kube-prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to install Prometheus is using kube-prometheus. It uses the
    Prometheus Operator as well as Grafana for dashboarding and `AlertManager` for
    managing alerts. To get started, clone the repo and run the `deploy` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The script creates a monitoring namespace and lots of Kubernetes entities and
    supporting components:'
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus Operator itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Prometheus node_exporter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kube-state metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Prometheus configuration covering monitoring of all Kubernetes core components
    and exporters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A default set of alerting rules on the cluster components' health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Grafana instance serving dashboards on cluster metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A three node highly available Alertmanager cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s verify that everything is in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that `alertmanager-main-2` is pending. I suspect that this is due to Minikube
    running on two cores. It is not causing any problem in practice in my setup.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring your cluster with Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the Prometheus Operator is up and running along with Grafana and the Alertmanager,
    you can access their UIs and interact with the different components:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus UI on node port `30900`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alertmanager UI on node port `30903`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana on node port `30902`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prometheus supports a dizzying array of metrics to choose from. Here is a screenshot
    that shows the duration of HTTP requests in microseconds broken down by container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e55c149a-84ac-4c8c-91a8-1addb887cc63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To limit the view to only the 0.99 quantile for the `prometheus-k8s` service,
    use the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/348f4501-a178-41e3-818c-d69d69a48a33.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/a11d115c-7f30-4aea-871e-8c616f00f73d.png)'
  prefs: []
  type: TYPE_IMG
- en: The Alertmanager is another important part of the Prometheus monitoring story.
    Here is a screenshot of the web UI that lets you define and configure alerts based
    on arbitrary metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at monitoring, logging, and troubleshooting. This
    is a crucial aspect of operating any system and, in particular, a platform such
    as Kubernetes with so many moving pieces. My greatest worry whenever I'm responsible
    for something is that something will go wrong and I will have no systematic way
    to figure out what's wrong and how to fix it. Kubernetes has ample tools and facilities
    built in, such as Heapster, logging, DaemonSets, and node problem detector. You
    can also deploy any kind of monitoring solution you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](0b446f8f-3748-4bb4-8406-78f2af468e14.xhtml), *High Availability
    and Reliability*, we will look at highly available and scalable Kubernetes clusters.
    This is arguably the most important use case for Kubernetes, where it shines compared
    with other orchestration solutions.
  prefs: []
  type: TYPE_NORMAL

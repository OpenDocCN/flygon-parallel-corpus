- en: High Availability and Reliability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性和可靠性
- en: In the previous chapter, we looked at monitoring your Kubernetes cluster, detecting
    problems at the node level, identifying and rectifying performance problems, and
    general troubleshooting.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了监视您的Kubernetes集群，在节点级别检测问题，识别和纠正性能问题以及一般故障排除。
- en: In this chapter, we will dive into the topic of highly available clusters. This
    is a complicated topic. The Kubernetes project and the community haven't settled
    on one true way to achieve high-availability nirvana. There are many aspects to
    highly available Kubernetes clusters, such as ensuring that the control plane
    can keep functioning in the face of failures, protecting the cluster state in
    `etcd`, protecting the system's data, and recovering capacity and/or performance
    quickly. Different systems will have different reliability and availability requirements.
    How to design and implement a highly available Kubernetes cluster will depend
    on those requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨高度可用的集群主题。这是一个复杂的主题。Kubernetes项目和社区尚未就实现高可用性的真正方式达成一致。高度可用的Kubernetes集群有许多方面，例如确保控制平面在面对故障时能够继续运行，保护`etcd`中的集群状态，保护系统的数据，并快速恢复容量和/或性能。不同的系统将有不同的可靠性和可用性要求。如何设计和实现高度可用的Kubernetes集群将取决于这些要求。
- en: By the end of this chapter, you will understand the various concepts associated
    with high availability and be familiar with Kubernetes high availability best
    practices and when to employ them. You will be able to upgrade live clusters using
    different strategies and techniques, and you will be able to choose between multiple
    possible solutions based on trade-offs between performance, cost, and availability.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，您将了解与高可用性相关的各种概念，并熟悉Kubernetes高可用性最佳实践以及何时使用它们。您将能够使用不同的策略和技术升级实时集群，并能够根据性能、成本和可用性之间的权衡选择多种可能的解决方案。
- en: High-availability concepts
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性概念
- en: In this section, we will start our journey into high availability by exploring
    the concepts and building blocks of reliable and highly available systems. The
    million (trillion?) dollar question is how do we build reliable and highly available
    systems from unreliable components? Components will fail, you can take that to
    the bank; hardware will fail; networks will fail; configuration will be wrong;
    software will have bugs; people will make mistakes. Accepting that, we need to
    design a system that can be reliable and highly available even when components
    fail. The idea is to start with redundancy, detect component failure, and replace
    bad components quickly.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过探索可靠和高可用系统的概念和构建模块来开始我们的高可用性之旅。百万（万亿？）美元的问题是，我们如何从不可靠的组件构建可靠和高可用的系统？组件会失败，你可以把它带到银行；硬件会失败；网络会失败；配置会出错；软件会有bug；人会犯错误。接受这一点，我们需要设计一个系统，即使组件失败，也能可靠和高可用。这个想法是从冗余开始，检测组件故障，并快速替换坏组件。
- en: Redundancy
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冗余
- en: '**Redundancy** is the foundation of reliable and highly available systems at
    the hardware and data levels. If a critical component fails and you want the system
    to keep running, you must have another identical component ready to go. Kubernetes
    itself takes care of your stateless pods through replication controllers and replica
    sets. However, your cluster state in `etcd` and the master components themselves
    need redundancy to function when some components fail. In addition, if your system''s
    tasteful components are not backed up by redundant storage (for example, on a
    cloud platform), then you need to add redundancy to prevent data loss.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**冗余**是可靠和高可用系统在硬件和数据级别的基础。如果关键组件失败并且您希望系统继续运行，您必须准备好另一个相同的组件。Kubernetes本身通过复制控制器和副本集来管理无状态的pod。然而，您的`etcd`中的集群状态和主要组件本身需要冗余以在某些组件失败时继续运行。此外，如果您的系统的重要组件没有受到冗余存储的支持（例如在云平台上），那么您需要添加冗余以防止数据丢失。'
- en: Hot swapping
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 热插拔
- en: '**Hot swapping** is the concept of replacing a failed component on the fly
    without taking the system down, with minimal (ideally, zero) interruption to users.
    If the component is stateless (or its state is stored in separate redundant storage),
    then hot swapping a new component to replace it is easy and just involves redirecting
    all clients to the new component. However, if it stores local state, including
    in memory, then hot swapping is important. There are the following two main options:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**热插拔**是指在不关闭系统的情况下替换失败的组件的概念，对用户的中断最小（理想情况下为零）。如果组件是无状态的（或其状态存储在单独的冗余存储中），那么热插拔新组件来替换它就很容易，只需要将所有客户端重定向到新组件。然而，如果它存储本地状态，包括内存中的状态，那么热插拔就很重要。有以下两个主要选项：'
- en: Give up on in-flight transactions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 放弃在飞行中的交易
- en: Keep a hot replica in sync
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持热备份同步
- en: The first solution is much simpler. Most systems are resilient enough to cope
    with failures. Clients can retry failed requests, and the hot-swapped component
    will service them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案要简单得多。大多数系统都足够弹性，可以应对故障。客户端可以重试失败的请求，而热插拔的组件将为它们提供服务。
- en: The second solution is more complicated and fragile, and will incur a performance
    overhead because every interaction must be replicated to both copies (and acknowledged).
    It may be necessary for some parts of the system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个解决方案更加复杂和脆弱，并且会产生性能开销，因为每次交互都必须复制到两个副本（并得到确认）。对于系统的某些部分可能是必要的。
- en: Leader election
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领导者选举
- en: Leader or master election is a common pattern in distributed systems. You often
    have multiple identical components that collaborate and share the load, but one
    component is elected as the leader and certain operations are serialized through
    the leader. You can think of distributed systems with leader election as a combination
    of redundancy and hot swapping. The components are all redundant and, when the
    current leader fails or becomes unavailable, a new leader is elected and hot-swapped
    in.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者或主选举是分布式系统中常见的模式。您经常有多个相同的组件协作和共享负载，但其中一个组件被选为领导者，并且某些操作通过领导者进行序列化。您可以将具有领导者选举的分布式系统视为冗余和热插拔的组合。这些组件都是冗余的，当当前领导者失败或不可用时，将选举出一个新的领导者并进行热插拔。
- en: Smart load balancing
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能负载平衡
- en: Load balancing is about distributing the workload across multiple components
    that service incoming requests. When some components fail the load balancer must
    first stop sending requests to failed or unreachable components. The second step
    is to provision new components to restore capacity and update the load balancer.
    Kubernetes provides great facilities to support this through services, endpoints,
    and labels.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 负载平衡是指在多个组件之间分配服务传入请求的工作负载。当一些组件失败时，负载平衡器必须首先停止向失败或不可达的组件发送请求。第二步是提供新的组件来恢复容量并更新负载平衡器。Kubernetes通过服务、端点和标签提供了支持这一点的很好的设施。
- en: Idempotency
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幂等性
- en: Many types of failure can be temporary. This is most common with networking
    issues or with too-stringent timeouts. A component that doesn't respond to a health
    check will be considered unreachable, and another component will take its place.
    Work that was scheduled to the presumably failed component may be sent to another
    component, but the original component may still be working and complete the same
    work. The end result is that the same work may be performed twice. It is very
    difficult to avoid this situation. To support exactly once semantics, you need
    to pay a heavy price in overhead, performance, latency, and complexity. Thus,
    most systems opt to support at least once semantics, which means it is OK for
    the same work to be performed multiple times without violating the system's data
    integrity. This property is named idempotency. Idempotent systems maintain their
    state if an operation is performed multiple times.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多类型的故障都可能是暂时的。这在网络问题或过于严格的超时情况下最常见。不响应健康检查的组件将被视为不可达，另一个组件将取而代之。原本计划发送到可能失败的组件的工作可能被发送到另一个组件，但原始组件可能仍在工作并完成相同的工作。最终结果是相同的工作可能会执行两次。很难避免这种情况。为了支持精确一次语义，您需要付出沉重的代价，包括开销、性能、延迟和复杂性。因此，大多数系统选择支持至少一次语义，这意味着可以多次执行相同的工作而不违反系统的数据完整性。这种属性被称为幂等性。幂等系统在多次执行操作时保持其状态。
- en: Self-healing
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自愈
- en: When component failures occur in dynamic systems, you usually want the system
    to be able to heal itself. Kubernetes replication controllers and replica sets
    are great examples of self-healing systems, but failure can extend well beyond
    pods. In the previous chapter, we discussed resource monitoring and node problem
    detection. The remedy controller is a great example of the concept of self-healing.
    Self-healing starts with automated detection of problems followed by automated
    resolution. Quotas and limits help create checks and balances to ensure an automated
    self-healing doesn't run amok due to unpredictable circumstances such as DDOS
    attacks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当动态系统中发生组件故障时，通常希望系统能够自我修复。Kubernetes复制控制器和副本集是自愈系统的很好例子，但故障可能远不止于Pod。在上一章中，我们讨论了资源监控和节点问题检测。补救控制器是自愈概念的一个很好的例子。自愈始于自动检测问题，然后是自动解决。配额和限制有助于创建检查和平衡，以确保自动自愈不会因不可预测的情况（如DDOS攻击）而失控。
- en: In this section, we considered various concepts involved in creating reliable
    and highly available systems. In the next section, we will apply them and demonstrate
    best practices for systems deployed on Kubernetes clusters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们考虑了创建可靠和高可用系统涉及的各种概念。在下一节中，我们将应用它们，并展示部署在Kubernetes集群上的系统的最佳实践。
- en: High-availability best practices
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性最佳实践
- en: Building reliable and highly available distributed systems is an important endeavor.
    In this section, we will check some of the best practices that enable a Kubernetes-based
    system to function reliably and be available in the face of various failure categories.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 构建可靠和高可用的分布式系统是一项重要的工作。在本节中，我们将检查一些最佳实践，使基于Kubernetes的系统能够可靠地运行，并在面对各种故障类别时可用。
- en: Creating highly available clusters
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建高可用集群
- en: 'To create a highly available Kubernetes cluster, the master components must
    be redundant. This means that `etcd` must be deployed as a cluster (typically
    across three or five nodes) and the Kubernetes API server must be redundant. Auxiliary
    cluster management services, such as Heapster''s storage, may be deployed redundantly
    too, if necessary. The following diagram depicts a typical reliable and highly
    available Kubernetes cluster. There are several load-balanced master nodes, each
    one containing whole master components as well as an `etcd` component:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个高可用的Kubernetes集群，主要组件必须是冗余的。这意味着`etcd`必须部署为一个集群（通常跨三个或五个节点），Kubernetes
    API服务器必须是冗余的。辅助集群管理服务，如Heapster的存储，如果必要的话也可以部署为冗余。以下图表描述了一个典型的可靠和高可用的Kubernetes集群。有几个负载均衡的主节点，每个节点都包含整个主要组件以及一个`etcd`组件：
- en: '![](Images/9a4726b1-7f50-4611-bed4-91d7663833b8.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/9a4726b1-7f50-4611-bed4-91d7663833b8.png)
- en: This is not the only way to configure highly available clusters. You may prefer,
    for example, to deploy a standalone `etcd` cluster to optimize the machines to
    their workload or if you require more redundancy for your `etcd` cluster than
    the rest of the master nodes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是配置高可用集群的唯一方式。例如，您可能更喜欢部署一个独立的`etcd`集群，以优化机器的工作负载，或者如果您的`etcd`集群需要比其他主节点更多的冗余。
- en: Self-hosted Kubernetes where control plane components are deployed as pods and
    stateful sets in the cluster is a great approach to simplify the robustness, disaster
    recovery, and self-healing of the control plane components by applying Kubernetes
    to Kubernetes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自托管的Kubernetes，其中控制平面组件部署为集群中的pod和有状态集，是简化控制平面组件的健壮性、灾难恢复和自愈的一个很好的方法，通过将Kubernetes应用于Kubernetes。
- en: Making your nodes reliable
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使您的节点可靠
- en: Nodes will fail, or some components will fail, but many failures are transient.
    The basic guarantee is to make sure that the Docker daemon (or whatever the CRI
    implementation is) and the Kubelet restart automatically in case of a failure.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 节点会失败，或者一些组件会失败，但许多故障是暂时的。基本的保证是确保Docker守护程序（或任何CRI实现）和Kubelet在发生故障时能够自动重启。
- en: 'If you run CoreOS, a modern Debian-based OS (including Ubuntu >= 16.04), or
    any other OS that uses `systemd` as its `init` mechanism, then it''s easy to deploy
    `Docker` and the `kubelet` as self-starting daemons:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行CoreOS，一个现代的基于Debian的操作系统（包括Ubuntu >= 16.04），或者任何其他使用`systemd`作为其`init`机制的操作系统，那么很容易将`Docker`和`kubelet`部署为自启动守护程序：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For other operating systems, the Kubernetes project selected monit for their
    high-availability example, but you can use any process monitor you prefer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他操作系统，Kubernetes项目选择了monit作为高可用示例，但您可以使用任何您喜欢的进程监视器。
- en: Protecting your cluster state
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护您的集群状态
- en: The Kubernetes cluster state is stored in `etcd`. The `etcd` cluster was designed
    to be super reliable and distributed across multiple nodes. It's important to
    take advantage of these capabilities for a reliable and highly available Kubernetes
    cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群状态存储在`etcd`中。`etcd`集群被设计为超级可靠，并分布在多个节点上。利用这些功能对于一个可靠和高可用的Kubernetes集群非常重要。
- en: Clustering etcd
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群化etcd
- en: You should have at least three nodes in your etcd cluster. If you need more
    reliability and redundancy, you can go to five, seven, or any other odd number
    of nodes. The number of nodes must be odd to have a clear majority in case of
    a network split.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您的etcd集群中应该至少有三个节点。如果您需要更可靠和冗余性，可以增加到五个、七个或任何其他奇数节点。节点数量必须是奇数，以便在网络分裂的情况下有明确的多数。
- en: 'In order to create a cluster, the `etcd` nodes should be able to discover each
    other. There are several methods to accomplish that. I recommend using the excellent
    `etcd-operator` from CoreOS:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个集群，`etcd`节点应该能够发现彼此。有几种方法可以实现这一点。我建议使用CoreOS的优秀的`etcd-operator`：
- en: '![](Images/9f1020a1-9e4e-4d18-af97-d17c4af9f0dd.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/9f1020a1-9e4e-4d18-af97-d17c4af9f0dd.png)'
- en: 'The operator takes care of many complicated aspects of `etcd` operation, such
    as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 操作员负责处理etcd操作的许多复杂方面，例如：
- en: Create and destroy
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和销毁
- en: Resize
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整大小
- en: Failover
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障转移
- en: Rolling upgrade
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动升级
- en: Backup and restore
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和恢复
- en: Installing the etcd operator
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装etcd操作员
- en: The easiest way to install the `etcd-operator` is using Helm-the Kubernetes
    package manager. If you don't have Helm installed yet, follow the instructions
    given at [https://github.com/kubernetes/helm#install](https://github.com/kubernetes/helm#install).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`etcd-operator`的最简单方法是使用Helm-Kubernetes包管理器。如果您尚未安装Helm，请按照[https://github.com/kubernetes/helm#install](https://github.com/kubernetes/helm#install)中给出的说明进行操作。
- en: 'Then, initialize `helm`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始化`helm`：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will dive deep into Helm in [Chapter 13](74a06d16-4c3d-40b5-b87f-1f74e8c8ec25.xhtml),
    *Handling the Kubernetes Package Manager*. For now, we''ll just use it to install
    the `etcd` operator. On Minikube 0.24.1, which supports Kubernetes 1.8 (although
    Kubernetes 1.10 is already out), there are some permission issues out of the box.
    To overcome these issues, we need to create some roles and role bindings. Here
    is the `rbac.yaml` file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第13章](74a06d16-4c3d-40b5-b87f-1f74e8c8ec25.xhtml)中深入探讨Helm，*处理Kubernetes包管理器*。目前，我们只是用它来安装`etcd`操作员。在支持Kubernetes
    1.8的Minikube 0.24.1上（尽管Kubernetes 1.10已经发布），默认情况下存在一些权限问题。为了克服这些问题，我们需要创建一些角色和角色绑定。以下是`rbac.yaml`文件：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can apply it like any other Kubernetes manifest:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像应用其他Kubernetes清单一样应用它：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can finally install the `etcd-operator`. I use `x` as a short release
    name to make the output less verbose. You may want to use more meaningful names:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以安装`etcd-operator`了。我使用`x`作为一个简短的发布名称，以使输出更简洁。您可能希望使用更有意义的名称：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Creating the etcd cluster
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建etcd集群
- en: 'Save the following to `etcd-cluster.yaml`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容保存到`etcd-cluster.yaml`中：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To create the cluster type:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建集群类型：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Verifying the etcd cluster
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证etcd集群
- en: Once the `etcd` cluster is up and running, you can access the `etcdctl` tool
    to check on the cluster status and health. Kubernetes lets you execute commands
    directly inside pods or container through the `exec` command (similar to Docker
    exec).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`etcd`集群正常运行，您可以访问`etcdctl`工具来检查集群状态和健康状况。Kubernetes允许您通过`exec`命令（类似于Docker
    exec）直接在pod或容器内执行命令。
- en: 'Here is how to check whether the cluster is healthy:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何检查集群是否健康：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is to how to set and get key-value pairs:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何设置和获取键值对：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Yeah, it works!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，它有效！
- en: Protecting your data
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护您的数据
- en: Protecting the cluster state and configuration is great, but even more important
    is protecting your own data. If somehow the cluster state gets corrupted, you
    can always rebuild the cluster from scratch (although the cluster will not be
    available during the rebuild). But if your own data is corrupted or lost, you're
    in deep trouble. The same rules apply; redundancy is king. However, while the
    Kubernetes cluster state is very dynamic, much of your data may be less dynamic.
    For example, a lot of historic data is often important and can be backed up and
    restored. Live data might be lost, but the overall system may be restored to an
    earlier snapshot and suffer only temporary damage.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 保护集群状态和配置非常重要，但更重要的是保护您自己的数据。如果一些方式集群状态被损坏，您可以始终从头开始重建集群（尽管在重建期间集群将不可用）。但如果您自己的数据被损坏或丢失，您将陷入深深的麻烦。相同的规则适用；冗余是王道。然而，尽管Kubernetes集群状态非常动态，但您的许多数据可能不太动态。例如，许多历史数据通常很重要，可以进行备份和恢复。实时数据可能会丢失，但整个系统可以恢复到较早的快照，并且只会遭受暂时的损害。
- en: Running redundant API servers
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行冗余的API服务器
- en: The API servers are stateless, fetching all the necessary data on the fly from
    the `etcd` cluster. This means that you can easily run multiple API servers without
    needing to coordinate between them. Once you have multiple API servers running,
    you can put a load balancer in front of them to make it transparent to clients.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器是无状态的，可以从`etcd`集群中动态获取所有必要的数据。这意味着您可以轻松地运行多个API服务器，而无需在它们之间进行协调。一旦有多个API服务器运行，您可以在它们前面放置一个负载均衡器，使客户端对此毫无察觉。
- en: Running leader election with Kubernetes
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中运行领导者选举
- en: Some master components, such as the scheduler and the controller manager, can't
    have multiple instances active at the same time. This will be chaos, as multiple
    schedulers try to schedule the same pod into multiple nodes or multiple times
    into the same node. The correct way to have a highly-scalable Kubernetes cluster
    is to have these components run in leader election mode. This means that multiple
    instances are running, but only one is active at a time, and if it fails, another
    one is elected as leader and takes its place.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一些主要组件，如调度程序和控制器管理器，不能同时处于活动状态。这将是一片混乱，因为多个调度程序尝试将相同的pod调度到多个节点或多次调度到同一节点。拥有高度可扩展的Kubernetes集群的正确方法是使这些组件以领导者选举模式运行。这意味着多个实例正在运行，但一次只有一个实例处于活动状态，如果它失败，另一个实例将被选为领导者并接替其位置。
- en: Kubernetes supports this mode through the `leader-elect` flag. The scheduler
    and the controller manager can be deployed as pods by copying their respective
    manifests to `/etc/kubernetes/manifests`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过`leader-elect`标志支持这种模式。调度程序和控制器管理器可以通过将它们各自的清单复制到`/etc/kubernetes/manifests`来部署为pod。
- en: 'Here is a snippet from a scheduler manifest that shows the use of the flag:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是调度程序清单中显示标志使用的片段：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is a snippet from a controller manager manifest that shows the use of
    the flag:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是控制器管理器清单中显示标志使用的片段：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that it is not possible to have these components restarted automatically
    by Kubernetes like other pods because these are exactly the Kubernetes components
    responsible for restarting failed pods, so they can't restart themselves if they
    fail. There must be a ready-to-go replacement already running.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些组件无法像其他pod一样由Kubernetes自动重新启动，因为它们正是负责重新启动失败的pod的Kubernetes组件，因此如果它们失败，它们无法重新启动自己。必须已经有一个准备就绪的替代品正在运行。
- en: Leader election for your application
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序的领导者选举
- en: Leader election can be very useful for your application too, but it is notoriously
    difficult to implement. Luckily, Kubernetes comes to the rescue. There is a documented
    procedure for supporting leader election for your application through the `leader-elector`
    container from Google. The basic concept is to use the Kubernetes endpoints combined
    with `ResourceVersion` and `Annotations`. When you couple this container as a
    sidecar in your application pod, you get leader-election capabilities in a very
    streamlined fashion.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者选举对你的应用也可能非常有用，但实现起来非常困难。幸运的是，Kubernetes来拯救了。有一个经过记录的程序，可以通过Google的`leader-elector`容器来支持你的应用进行领导者选举。基本概念是使用Kubernetes端点结合`ResourceVersion`和`Annotations`。当你将这个容器作为你的应用pod的sidecar时，你可以以非常简化的方式获得领导者选举的能力。
- en: 'Let''s run the `leader-elector` container with three pods and an election called
    election:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用三个pod运行`leader-elector`容器，并进行名为election的选举：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After a while, you''ll see three new pods in your cluster, named `leader-elector-xxx`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 过一段时间，你会在你的集群中看到三个名为`leader-elector-xxx`的新pod。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'OK. But who is the master? Let''s query the election endpoints:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 好了。但是谁是主人？让我们查询选举端点：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you look really hard, you can see it buried in the `metadata.annotations`.
    To make it easy to detect, I recommend the fantastic `jq` program for slicing
    and dicing JSON ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/)).
    It is very useful to parse the output of the Kubernetes API or `kubectl`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看，你可以在`metadata.annotations`中找到它。为了方便检测，我推荐使用神奇的`jq`程序来切割和解析JSON（[https://stedolan.github.io/jq/](https://stedolan.github.io/jq/)）。它非常有用，可以解析Kubernetes
    API或`kubectl`的输出：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To prove that leader election works, let''s kill the leader and see if a new
    leader is elected:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明领导者选举有效，让我们杀死领导者，看看是否选举出了新的领导者：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And we have a new leader:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了一个新的领导者：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can also find the leader through HTTP, because each `leader-elector` container
    exposes the leader through a local web server (running on port `4040`) though
    a proxy:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过HTTP找到领导者，因为每个`leader-elector`容器都通过一个本地web服务器（运行在端口`4040`上）来暴露领导者，尽管一个代理：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The local web server allows the leader-elector container to function as a sidecar
    container to your main application container within the same pod. Your application
    container shares the same local network as the `leader-elector` container, so
    it can access `http://localhost:4040` and get the name of the current leader.
    Only the application container that shares the pod with the elected leader will
    run the application; the other application containers in the other pods will be
    dormant. If they receive requests, they'll forward them to the leader, or some
    clever load-balancing tricks can be done to automatically send all requests to
    the current leader.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本地web服务器允许leader-elector容器作为同一个pod中主应用容器的sidecar容器运行。你的应用容器与`leader-elector`容器共享同一个本地网络，因此它可以访问`http://localhost:4040`并获取当前领导者的名称。只有与当选领导者共享pod的应用容器才会运行应用程序；其他pod中的应用容器将处于休眠状态。如果它们收到请求，它们将把请求转发给领导者，或者可以通过一些巧妙的负载均衡技巧自动将所有请求发送到当前的领导者。
- en: Making your staging environment highly available
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使你的暂存环境高度可用
- en: High availability is important to set up. If you go to the trouble of setting
    up high availability, it means that there is a business case for a highly available
    system. It follows that you want to test your reliable and highly available cluster
    before you deploy it to production (unless you're Netflix, where you test in production).
    Also, any change to the cluster may, in theory, break your high availability without
    disrupting other cluster functions. The essential point is that, just like anything
    else, if you don't test it, assume it doesn't work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性的设置很重要。如果您费心设置高可用性，这意味着存在高可用性系统的业务案例。因此，您希望在部署到生产环境之前测试可靠且高可用的集群（除非您是Netflix，在那里您在生产环境中进行测试）。此外，理论上，对集群的任何更改都可能破坏高可用性，而不会影响其他集群功能。关键点是，就像其他任何事物一样，如果您不进行测试，就假设它不起作用。
- en: 'We''ve established that you need to test reliability and high availability.
    The best way to do it is to create a staging environment that replicates your
    production environment as closely as possible. This can get expensive. There are
    several ways to manage the cost:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定您需要测试可靠性和高可用性。最好的方法是创建一个尽可能接近生产环境的分阶段环境。这可能会很昂贵。有几种方法可以管理成本：
- en: '**Ad hoc HA staging environment**: Create a large HA cluster only for the duration
    of HA testing'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**临时高可用性（HA）分阶段环境**：仅在HA测试期间创建一个大型HA集群'
- en: '**Compress time**: Create interesting event streams and scenarios ahead of
    time, feed the input, and simulate the situations in rapid succession'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩时间**：提前创建有趣的事件流和场景，输入并快速模拟情况'
- en: '**Combine HA testing with performance and stress testing**: At the end of your
    performance and stress tests, overload the system and see how the reliability
    and high-availability configuration handles the load'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将HA测试与性能和压力测试相结合**：在性能和压力测试结束时，超载系统，看可靠性和高可用性配置如何处理负载'
- en: Testing high availability
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试高可用性
- en: Testing high availability takes planning and a deep understanding of your system.
    The goal of every test is to reveal flaws in the system's design and/or implementation,
    and to provide good enough coverage that, if the tests pass, you'll be confident
    that the system behaves as expected.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 测试高可用性需要计划和对系统的深入了解。每项测试的目标是揭示系统设计和/或实施中的缺陷，并提供足够的覆盖范围，如果测试通过，您将对系统的行为感到满意。
- en: In the realm of reliability and high availability, it means that you need to
    figure out ways to break the system and watch it put itself back together.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在可靠性和高可用性领域，这意味着您需要找出破坏系统并观察其自我修复的方法。
- en: 'This requires several pieces, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要几个部分，如下：
- en: A comprehensive list of possible failures (including reasonable combinations)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能故障的全面列表（包括合理的组合）
- en: For each possible failure, it should be clear how the system should respond
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每种可能的故障，系统应该如何做出清晰的响应
- en: A way to induce the failure
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诱发故障的方法
- en: A way to observe how the system reacts
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察系统反应的方法
- en: None of the pieces are trivial. The best approach in my experience is to do
    it incrementally and try to come up with a relatively small number of generic
    failure categories and generic responses, rather than an exhaustive, ever-changing
    list of low-level failures.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分都不是微不足道的。根据我的经验，最好的方法是逐步进行，并尝试提出相对较少的通用故障类别和通用响应，而不是详尽且不断变化的低级故障列表。
- en: For example, a generic failure category is node-unresponsive; the generic response
    could be rebooting the node. The way to induce the failure can be stopping the
    VM of the node (if it's a VM), and the observation should be that, while the node
    is down, the system still functions properly based on standard acceptance tests.
    The node is eventually up, and the system gets back to normal. There may be many
    other things you want to test, such as whether the problem was logged, relevant
    alerts went out to the right people, and various stats and reports were updated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个通用的故障类别是节点无响应；通用的响应可能是重启节点。诱发故障的方法可以是停止节点的虚拟机（如果是虚拟机），观察应该是，尽管节点宕机，系统仍然根据标准验收测试正常运行。节点最终恢复正常，系统恢复正常。您可能还想测试许多其他事情，比如问题是否已记录，相关警报是否已发送给正确的人，以及各种统计数据和报告是否已更新。
- en: Note that, sometimes, a failure can't be resolved in a single response. For
    example, in our unresponsive node case, if it's a hardware failure, then reboot
    will not help. In this case, a second line of response comes into play and maybe
    a new VM is started, configured, and hooked up to the node. In this case, you
    can't be too generic, and you may need to create tests for specific types of pod/role
    that were on the node (etcd, master, worker, database, and monitoring).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有时故障无法在单一响应中解决。例如，在我们的节点无响应案例中，如果是硬件故障，那么重启是无济于事的。在这种情况下，第二种响应方式就会发挥作用，也许会启动、配置并连接到节点的新虚拟机。在这种情况下，您不能太通用，可能需要为节点上的特定类型的pod/角色创建测试（etcd、master、worker、数据库和监控）。
- en: If you have high-quality requirements, be prepared to spend much more time setting
    up the proper testing environments and the tests than even the production environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有高质量的要求，那么准备花费比生产环境更多的时间来设置适当的测试环境和测试。
- en: One last, important point is to try to be as nonintrusive as possible. This
    means that, ideally, your production system will not have testing features that
    allow shutting down parts of it or cause it to be configured to run in reduced
    capacity for testing. The reason is that it increases the attack surface of your
    system, and it can be triggered by accident by mistakes in configuration. Ideally,
    you can control your testing environment without resorting to modifying the code
    or configuration that will be deployed in production. With Kubernetes, it is usually
    easy to inject pods and containers with custom test functionality that can interact
    with system components in the staging environment, but will never be deployed
    in production.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个重要的观点是尽量不要侵入。这意味着，理想情况下，您的生产系统不会具有允许关闭部分系统或导致其配置为以减少容量运行进行测试的测试功能。原因是这会增加系统的攻击面，并且可能会因配置错误而意外触发。理想情况下，您可以在不修改将部署在生产环境中的代码或配置的情况下控制测试环境。使用Kubernetes，通常很容易向暂存环境中的pod和容器注入自定义测试功能，这些功能可以与系统组件交互，但永远不会部署在生产环境中。
- en: In this section, we looked at what it takes to actually have a reliable and
    highly available cluster, including etcd, the API server, the scheduler, and the
    controller manager. We considered best practices for protecting the cluster itself
    as well as your data, and paid special attention to the issue of starting environments
    and testing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看了一下实际上拥有可靠和高可用的集群所需的条件，包括etcd、API服务器、调度器和控制器管理器。我们考虑了保护集群本身以及您的数据的最佳实践，并特别关注了启动环境和测试的问题。
- en: Live cluster upgrades
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时集群升级
- en: One of the most complicated and risky tasks involved in running a Kubernetes
    cluster is a live upgrade. The interactions between different parts of the system
    of different versions are often difficult to predict, but in many situations,
    it is required. Large clusters with many users can't afford to be offline for
    maintenance. The best way to attack complexity is to divide and conquer. Microservice
    architecture helps a lot here. You never upgrade your entire system. You just
    constantly upgrade several sets of related microservices, and if APIs have changed,
    then you upgrade their clients too. A properly-designed upgrade will preserve
    backward compatibility at least until all clients have been upgraded, and then
    deprecate old APIs across several releases.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行Kubernetes集群中涉及的最复杂和风险最高的任务之一是实时升级。不同版本系统的不同部分之间的交互通常很难预测，但在许多情况下是必需的。具有许多用户的大型集群无法承担维护期间的离线状态。攻击复杂性的最佳方法是分而治之。微服务架构在这里非常有帮助。您永远不会升级整个系统。您只是不断升级几组相关的微服务，如果API已更改，那么也会升级它们的客户端。一个经过良好设计的升级将至少保留向后兼容性，直到所有客户端都已升级，然后在几个版本中弃用旧的API。
- en: In this section, we will discuss how to go about upgrading your cluster using
    various strategies, such as rolling upgrades and blue-green upgrades. We will
    also discuss when it's appropriate to introduce breaking upgrades versus backward-compatible
    upgrades. Then, we will get into the critical topic of schema and data migration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用各种策略升级您的集群，例如滚动升级和蓝绿升级。我们还将讨论何时适合引入破坏性升级与向后兼容升级。然后，我们将进入关键的模式和数据迁移主题。
- en: Rolling upgrades
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动升级
- en: 'Rolling upgrades are upgrades where you gradually upgrade components from the
    current version to the next. This means that your cluster will run current and
    new components at the same time. There are two cases to consider here, where:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动升级是逐渐将组件从当前版本升级到下一个版本的升级。这意味着您的集群将同时运行当前版本和新版本的组件。这里有两种情况需要考虑：
- en: New components are backward compatible
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新组件向后兼容
- en: New components are not backward compatible
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新组件不向后兼容
- en: 'If the new components are backward compatible, then the upgrade should be very
    easy. In earlier versions of Kubernetes, you had to manage rolling upgrades very
    carefully with labels and change the number of replicas gradually for both the
    old and new version (although `kubectl` rolling-update is a convenient shortcut
    for replication controllers). But, the deployment resource introduced in Kubernetes
    1.2 makes it much easier and supports replica sets. It has the following capabilities
    built-in:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新组件向后兼容，那么升级应该非常容易。在Kubernetes的早期版本中，您必须非常小心地使用标签管理滚动升级，并逐渐改变旧版本和新版本的副本数量（尽管`kubectl`滚动更新是复制控制器的便捷快捷方式）。但是，在Kubernetes
    1.2中引入的部署资源使其变得更加容易，并支持副本集。它具有以下内置功能：
- en: Running server-side (it keeps going if your machine disconnects)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行服务器端（如果您的机器断开连接，它将继续运行）
- en: Versioning
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制
- en: Multiple concurrent rollouts
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个并发部署
- en: Updating deployments
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新部署
- en: Aggregating status across all pods
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汇总所有pod的状态
- en: Rollbacks
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚
- en: Canary deployments
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: Multiple upgrade strategies (rolling upgrade is the default)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多种升级策略（滚动升级是默认值）
- en: 'Here is a sample manifest for a deployment that deploys three NGINX pods:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个部署三个NGINX pod的部署示例清单：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resource kind is deployment, and it''s got the name `nginx-deployment`,
    which you can use to refer to this deployment later (for example, for updates
    or rollbacks). The most important part is, of course, the spec, which contains
    a pod template. The replicas determine how many pods will be in the cluster, and
    the template spec has the configuration for each container: in this case, just
    a single container.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 资源种类是部署，它的名称是`nginx-deployment`，您可以在以后引用此部署（例如，用于更新或回滚）。最重要的部分当然是规范，其中包含一个pod模板。副本确定集群中将有多少个pod，并且模板规范包含每个容器的配置：在这种情况下，只有一个容器。
- en: 'To start the rolling update, you create the deployment resource:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始滚动更新，您需要创建部署资源：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can view the status of the deployment later, using:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以稍后使用以下命令查看部署的状态：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Complex deployments
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂的部署
- en: The deployment resource is great when you just want to upgrade one pod, but
    you may often need to upgrade multiple pods, and those pods sometimes have version
    interdependencies. In those situations, you sometimes must forego a rolling update
    or introduce a temporary compatibility layer. For example, suppose service A depends
    on service B. Service B now has a breaking change. The v1 pods of service A can't
    interoperate with the pods from service B v2\. It is also undesirable, from a
    reliability and change-management point of view, to make the v2 pods of service
    B support the old and new APIs. In this case, the solution may be to introduce
    an adapter service that implements the v1 API of the B service. This service will
    sit between A and B, and will translate requests and responses across versions.
    This adds complexity to the deployment process and requires several steps, but
    the benefit is that A and B services themselves are simple. You can do rolling
    updates across incompatible versions, and all indirection will go away once everybody
    upgrades to v2 (all A pods and all B pods).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当您只想升级一个pod时，部署资源非常有用，但您经常需要升级多个pod，并且这些pod有时存在版本相互依赖关系。在这种情况下，有时您必须放弃滚动更新或引入临时兼容层。例如，假设服务A依赖于服务B。服务B现在有一个破坏性的变化。服务A的v1
    pod无法与服务B的v2 pod进行交互操作。从可靠性和变更管理的角度来看，让服务B的v2 pod支持旧的和新的API是不可取的。在这种情况下，解决方案可能是引入一个适配器服务，该服务实现了B服务的v1
    API。该服务将位于A和B之间，并将跨版本转换请求和响应。这增加了部署过程的复杂性，并需要多个步骤，但好处是A和B服务本身很简单。您可以在不兼容版本之间进行滚动更新，一旦所有人升级到v2（所有A
    pod和所有B pod），所有间接性都将消失。
- en: Blue-green upgrades
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿升级
- en: Rolling updates are great for availability, but, sometimes, the complexity involved
    in managing a proper rolling update is considered too high, or it adds a significant
    amount of work that pushes back more important projects. In these cases, blue-green
    upgrades provide a great alternative. With a blue-green release, you prepare a
    full copy of your production environment with the new version. Now you have two
    copies, old (blue) and new (green). It doesn't matter which one is blue and which
    one is green. The important thing is that you have two fully-independent production
    environments. Currently, blue is active and services all requests. You can run
    all your tests on green. Once you're happy, you flip the switch and green becomes
    active. If something goes wrong, rolling back is just as easy; just switch back
    from green to blue. I elegantly ignored the storage and in-memory state here.
    This immediate switch assumes that blue and green are composed of stateless components
    only and share a common persistence layer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新对可用性来说非常好，但有时管理正确的滚动更新涉及的复杂性被认为太高，或者它增加了大量工作，推迟了更重要的项目。在这些情况下，蓝绿升级提供了一个很好的替代方案。使用蓝绿发布，您准备了一个完整的生产环境的副本，其中包含新版本。现在你有两个副本，旧的（蓝色）和新的（绿色）。蓝色和绿色哪个是哪个并不重要。重要的是你有两个完全独立的生产环境。目前，蓝色是活动的并处理所有请求。您可以在绿色上运行所有测试。一旦满意，您可以切换绿色变为活动状态。如果出现问题，回滚同样容易；只需从绿色切换回蓝色。我在这里优雅地忽略了存储和内存状态。这种立即切换假设蓝色和绿色只由无状态组件组成，并共享一个公共持久层。
- en: If there were storage changes or breaking changes to the API accessible to external
    clients, then additional steps need to be taken. For example, if blue and green
    have their own storage, then all incoming requests may need to be sent to both
    blue and green, and green may need to ingest historical data from blue to get
    in sync before switching.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存储发生了变化或对外部客户端可访问的API发生了破坏性变化，则需要采取额外的步骤。例如，如果蓝色和绿色有自己的存储，那么所有传入的请求可能需要同时发送到蓝色和绿色，并且绿色可能需要从蓝色那里摄取历史数据以在切换之前同步。
- en: Managing data-contract changes
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理数据合同变更
- en: Data contracts describe how the data is organized. It's an umbrella term for
    structure metadata. A database schema is the most typical example. The most common
    example is a relational database schema. Other examples include network payloads,
    file formats, and even the content of string arguments or responses. If you have
    a configuration file, then this configuration file has both a file format (JSON,
    YAML, TOML, XML, INI, and custom format) and some internal structure that describes
    what kind of hierarchy, keys, values, and data types are valid. Sometimes, the
    data contract is explicit, and sometimes it's implicit. Either way, you need to
    manage it carefully, or else you'll get runtime errors when code that's reading,
    parsing, or validating encounters data with an unfamiliar structure.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合同描述数据的组织方式。这是结构元数据的一个总称。数据库模式是最典型的例子。最常见的例子是关系数据库模式。其他例子包括网络负载、文件格式，甚至字符串参数或响应的内容。如果有配置文件，那么这个配置文件既有文件格式（JSON、YAML、TOML、XML、INI和自定义格式），也有一些内部结构，描述了什么样的层次结构、键、值和数据类型是有效的。有时，数据合同是显式的，有时是隐式的。无论哪种方式，您都需要小心管理它，否则当读取、解析或验证数据时遇到不熟悉的结构时，就会出现运行时错误。
- en: Migrating data
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移数据
- en: Data migration is a big deal. Many systems these days manage measured terabytes,
    petabytes, or more. The amount of collected and managed data will continue to
    increase for the foreseeable future. The pace of data collection exceeds the pace
    of hardware innovation. The essential point is that if you have a lot of data
    and you need to migrate it, it can take a while. In a previous company, I oversaw
    a project to migrate close to 100 terabytes of data from one Cassandra cluster
    of a legacy system to another Cassandra cluster.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据迁移是一件大事。如今，许多系统管理着以TB、PB或更多为单位的数据。在可预见的未来，收集和管理的数据量将继续增加。数据收集的速度超过了硬件创新的速度。关键点是，如果您有大量数据需要迁移，这可能需要一段时间。在以前的一家公司，我负责一个项目，将近100TB的数据从一个传统系统的Cassandra集群迁移到另一个Cassandra集群。
- en: The second Cassandra cluster had different schema and was accessed by a Kubernetes
    cluster 24/7\. The project was very complicated, and thus, it kept getting pushed
    back when urgent issues popped up. The legacy system was still in place side by
    side with the next-gen system long after the original estimate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个Cassandra集群具有不同的架构，并且由Kubernetes集群全天候访问。该项目非常复杂，因此在紧急问题出现时，它一直被推迟。传统系统仍然与新一代系统并存，远远超出了最初的估计时间。
- en: There were a lot of mechanisms in place to split the data and send it to both
    clusters, but then we ran into scalability issues with the new system and we had
    to address those before we could continue. The historical data was important,
    but it didn't have to be accessed with the same service level as recent hot data.
    So, we embarked on yet another project to send historical data to cheaper storage.
    This meant, of course, that client libraries or frontend services had to know
    how to query both stores and merge the results. When you deal with a lot of data,
    you can't take anything for granted. You run into scalability issues with your
    tools, your infrastructure, your third-party dependencies, and your processes.
    Large scale is not just quantity change, it is often qualitative change as well.
    Don't expect it to go smoothly. It is much more than copying some files from A
    to B.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多机制可以将数据分割并发送到两个集群，但是我们遇到了新系统的可扩展性问题，我们必须在继续之前解决这些问题。历史数据很重要，但不必以与最近的热数据相同的服务水平访问。因此，我们着手进行了另一个项目，将历史数据发送到更便宜的存储中。当然，这意味着客户库或前端服务必须知道如何查询两个存储并合并结果。当您处理大量数据时，您不能认为任何事情都是理所当然的。您会遇到工具、基础设施、第三方依赖和流程的可扩展性问题。大规模不仅仅是数量的变化，通常也是质量的变化。不要指望一切都会顺利进行。这远不止是从A复制一些文件到B。
- en: Deprecating APIs
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弃用API
- en: 'API deprecation comes in two flavors: internal and external. Internal APIs
    are APIs used by components that are fully controlled by you and your team or
    organization. You can be sure that all API users will upgrade to the new API within
    a short time. External APIs are used by users or services outside your direct
    sphere of influence. There are a few gray-area situations where you work for a
    huge organization (think Google), and even internal APIs may need to be treated
    as external APIs. If you''re lucky, all your external APIs are used by self-updating
    applications or through a web interface which you control. In those cases, the
    API is practically hidden, and you don''t even need to publish it.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: API的弃用有两种情况：内部和外部。内部API是由您和您的团队或组织完全控制的组件使用的API。您可以确保所有API用户将在短时间内升级到新的API。外部API是由您直接影响范围之外的用户或服务使用的。有一些灰色地带的情况，您在一个庞大的组织（比如谷歌）工作，甚至内部API可能需要被视为外部API。如果您很幸运，所有外部API都由自更新的应用程序或通过您控制的Web界面使用。在这些情况下，API实际上是隐藏的，您甚至不需要发布它。
- en: If you have a lot of users (or a few very important users) using your API, you
    should consider deprecation very carefully. Deprecating an API means that you
    force your users to change their application to work with you or stay locked to
    an earlier version.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有很多用户（或者一些非常重要的用户）使用您的API，您应该非常谨慎地考虑弃用。弃用API意味着您强迫用户更改其应用程序以与您合作，或者保持与早期版本的锁定。
- en: 'There are a few ways you can mitigate the pain:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以减轻痛苦：
- en: Don't deprecate. Extend the existing API or keep the previous API active. It
    is sometimes pretty simple although it adds testing burden.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要弃用。扩展现有API或保持以前的API活动。尽管这有时很简单，但会增加测试负担。
- en: Provide client libraries in all relevant programming languages to your target
    audience. This is always a good practice. It allows you to make many changes to
    the underlying API without disrupting users (as long as you keep the programming
    language interface stable).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的目标受众提供所有相关编程语言的客户端库。这总是一个很好的做法。它允许您对底层API进行许多更改，而不会干扰用户（只要您保持编程语言接口稳定）。
- en: If you have to deprecate, explain why, allow ample time for users to upgrade
    and provide as much support as possible (for example, an upgrade guide with examples).
    Your users will appreciate it.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果必须弃用，请解释原因，为用户提供充足的升级时间，并尽可能提供支持（例如，带有示例的升级指南）。您的用户会感激的。
- en: Large-cluster performance, cost, and design trade-offs
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型集群的性能、成本和设计权衡
- en: In the previous section, we looked at live cluster upgrades. We explored various
    techniques and how Kubernetes supports them. We also discussed difficult problems,
    such as breaking changes, data contract changes, data migration, and API deprecation.
    In this section, we will consider the various options and configurations of large
    clusters with different reliability and high-availability properties. When you
    design your cluster, you need to understand your options and choose wisely based
    on the needs of your organization.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看了现场集群升级。我们探讨了各种技术以及Kubernetes如何支持它们。我们还讨论了一些困难的问题，比如破坏性变化、数据合同变化、数据迁移和API弃用。在本节中，我们将考虑大型集群的各种选项和配置，具有不同的可靠性和高可用性属性。当您设计您的集群时，您需要了解您的选项，并根据您组织的需求进行明智的选择。
- en: In this section, we will cover various availability requirements, from best
    effort all the way to the Holy Grail of zero downtime, and for each category of
    availability, we will consider what it means from the perspectives of performance
    and cost.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖各种可用性要求，从尽力而为一直到零停机的圣杯，对于每个可用性类别，我们将考虑从性能和成本的角度来看它意味着什么。
- en: Availability requirements
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用性要求
- en: Different systems have very different requirements for reliability and availability.
    Moreover, different subsystems have very different requirements. For example,
    billing systems are always a high priority because if the billing system is down,
    you can't make money. However, even within the billing system, if the ability
    to dispute charges is sometimes unavailable, it may be OK from the business point
    of view.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的系统对可靠性和可用性有非常不同的要求。此外，不同的子系统有非常不同的要求。例如，计费系统总是高优先级，因为如果计费系统停机，您就无法赚钱。然而，即使在计费系统内部，如果有时无法争议费用，从业务角度来看可能也可以接受。
- en: Quick recovery
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速恢复
- en: Quick recovery is another important aspect of highly available clusters. Something
    will go wrong at some point. Your unavailability clock starts running. How quickly
    can you get back to normal?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 快速恢复是高可用集群的另一个重要方面。某些时候会出现问题。您的不可用时钟开始运行。您能多快恢复正常？
- en: Sometimes, it's not up to you. For example, if your cloud provider has an outage
    (and you didn't implement a federated cluster, as we will discuss later, then
    you just have to sit and wait until they sort it out. But the most likely culprit
    is a problem with a recent deployment. There are, of course, time-related issues,
    and even calendar-related issues. Do you remember the leap-year bug that took
    down Microsoft Azure on February 29, 2012?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The poster boy of quick recovery is, of course, the blue-green deployment-if
    you keep the previous version running when the problem is discovered.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, rolling updates mean that if the problem is discovered early,
    then most of your pods still run the previous version.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Data-related problems can take a long time to reverse, even if your backups
    are up to date and your restore procedure actually works (definitely test this
    regularly).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as Heptio Ark can help in some scenarios by creating snapshot backup
    of your cluster that you can just restore too, in case something goes wrong and
    you're not sure how to fix it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Best effort
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Best effort means, counter intuitively, no guarantee whatsoever. If it works,
    great! If it doesn't work-oh well. What are you going to do? This level of reliability
    and availability may be appropriate for internal components that change often
    and the effort to make them robust is not worth it. It may also be appropriate
    for services released into the wild as beta.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Best effort is great for developers. Developers can move fast and break things.
    They are not worried about the consequences, and they don't have to go through
    a gauntlet of rigorous tests and approvals. The performance of best effort services
    may be better than more robust services because it can often skip expensive steps,
    such as verifying requests, persisting intermediate results, and replicating data.
    However, on the other hand, more robust services are often heavily optimized and
    their supporting hardware is fine-tuned to their workload. The cost of best-effort
    services is usually lower because they don't need to employ redundancy, unless
    the operators neglect to do basic capacity planning and just over-provision needlessly.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: In the context of Kubernetes, the big question is whether all the services provided
    by the cluster are best effort. If this is the case, then the cluster itself doesn't
    have to be highly available. You can probably have a single master node with a
    single instance of `etcd`, and Heapster or another monitoring solution may not
    need to be deployed.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的背景下，一个重要问题是集群提供的所有服务是否都是尽力而为的。如果是这种情况，那么集群本身就不需要高可用性。你可能只需要一个单一的主节点和一个单一的`etcd`实例，而Heapster或其他监控解决方案可能不需要部署。
- en: Maintenance windows
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维护窗口
- en: In a system with maintenance windows, special times are dedicated for performing
    various maintenance activities, such as applying security patches, upgrading software,
    pruning log files, and database cleanups. With a maintenance window, the system
    (or a subsystem) becomes unavailable. This is planned off-time, and users are
    often notified. The benefit of maintenance windows is that you don't have to worry
    how your maintenance actions are going to interact with live requests coming into
    the system. It can drastically simplify operations. System administrators love
    maintenance windows just as much as developers love best-effort systems.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在有维护窗口的系统中，专门的时间用于执行各种维护活动，如应用安全补丁、升级软件、修剪日志文件和数据库清理。在维护窗口期间，系统（或子系统）将不可用。这是计划中的非工作时间，并且通常会通知用户。维护窗口的好处是你不必担心维护操作会如何与系统中的实时请求交互。这可以极大地简化操作。系统管理员和开发人员一样喜欢维护窗口和尽力而为的系统。
- en: The downside, of course, is that the system is down during maintenance. It may
    only be acceptable for systems where user activity is limited to certain times
    (US office hours or week days only).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，缺点是系统在维护期间会停机。这可能只适用于用户活动受限于特定时间（美国办公时间或仅工作日）的系统。
- en: With Kubernetes, you can do maintenance windows by redirecting all incoming
    requests through the load balancer to a web page (or JSON response) that notifies
    users about the maintenance window.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes，你可以通过将所有传入的请求重定向到负载均衡器上的网页（或JSON响应）来进行维护窗口。
- en: But in most cases, the flexibility of Kubernetes should allow you to do live
    maintenance. In extreme cases, such as upgrading the Kubernetes version, or the
    switch from etcd v2 to etcd v3, you may want to resort to a maintenance window.
    Blue-green deployment is another alternative. But the larger the cluster, the
    more expansive the blue-green alternative because you must duplicate your entire
    production cluster, which is both costly and can cause you to run into insufficient
    quota issues.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 但在大多数情况下，Kubernetes的灵活性应该允许你进行在线维护。在极端情况下，比如升级Kubernetes版本，或从etcd v2切换到etcd
    v3，你可能需要使用维护窗口。蓝绿部署是另一种选择。但集群越大，蓝绿部署的成本就越高，因为你必须复制整个生产集群，这既昂贵又可能导致你遇到配额不足的问题。
- en: Zero-downtime
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机
- en: Finally, we arrive at the zero-downtime system. There is no such thing as a
    zero-downtime system. All systems fail and all software systems definitely fail.
    Sometimes, the failure is serious enough that the system or some of its services
    will be down. Think about zero-downtime as a best-effort distributed system design.
    You design for zero-downtime in the sense that you provide a lot of redundancy
    and mechanisms to address expected failures without bringing the system down.
    As always, remember that even if there is a business case for zero-downtime, it
    doesn't mean that every component must be.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The plan for zero-downtime is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '**Redundancy at every level**: This is a required condition. You can''t have
    a single point of failure in your design, because when it fails, your system is
    down.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated hot swapping of failed components**: Redundancy is only as good
    as the ability of the redundant components to kick into action as soon as the
    original component has failed. Some components can share the load (for example,
    stateless web servers), so there is no need for explicit action. In other cases,
    such as the Kubernetes scheduler and controller manager, you need leader election
    in place to make sure that the cluster keeps humming along.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tons of monitoring and alerts to detect problems early**: Even with a careful
    design, you may miss something or some implicit assumption might invalidate your
    design. Often such subtle issues creep up on you, and, with enough attention,
    you may discover it before it becomes an all-out system failure. For example,
    suppose that there is a mechanism in place to clean up old log files when disk
    space is over 90% full, but, for some reason, it doesn''t work. If you set an
    alert for when disk space is over 95% full, then you''ll catch it and be able
    to prevent the system failure.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tenacious testing before deployment to production**: Comprehensive tests
    have proven themselves as a reliable way to improve quality. It is hard work to
    have comprehensive tests for something as complicated as a large Kubernetes cluster
    running a massive distributed system, but you need it. What should you test? Everything.
    That''s right, for zero-downtime, you need to test both the application and the
    infrastructure together. Your 100% passing unit tests are a good start, but they
    don''t provide much confidence that when you deploy your application on your production
    Kubernetes cluster, it will still run as expected. The best tests are, of course,
    on your production cluster after a blue-green deployment or identical cluster.
    In lieu of a fully-fledged identical cluster, consider a staging environment with
    as much fidelity as possible to your production environment. Here is a list of
    tests you should run. Each of these tests should be comprehensive because if you
    leave something untested it might be broken:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests
  id: totrans-187
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance tests
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stress tests
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rollback tests
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data restore tests
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Penetration tests
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does that sound crazy? Good. Zero-downtime large-scale systems are hard. There
    is a reason why Microsoft, Google, Amazon, Facebook, and other big companies have
    tens of thousands of software engineers (combined) just working on infrastructure,
    operations, and making sure that things are up and running.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Keep the raw data**: For many systems, the data is the most critical asset.
    If you keep the raw data, you can recover from any data corruption and processed
    data loss that happens later. This will not really help you with zero-downtime
    because it can take a while to reprocess the raw data, but it will help with zero-data
    loss, which is often more important. The downside to this approach is that the
    raw data is often huge compared with the processed data. A good option may be
    to store the raw data in cheaper storage than the processed data.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perceived uptime as a last resort**: OK, some part of the system is down.
    You may still be able to maintain some level of service. In many situations, you
    may have access to a slightly stale version of the data or can let the user access
    some other part of the system. It is not a great user experience, but technically
    the system is still available.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance and data consistency
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you develop or operate distributed systems, the CAP theorem should always
    be in the back of your mind. CAP stands for consistency, availability, and partition
    tolerance. The theorem says that you can have, at most, two out of the three.
    Since any distributed system can suffer from network partition in practice, you
    can choose between CP or AP. CP means that in order to remain consistent, the
    system will not be available in the event of a network partition. AP means that
    the system will always be available but might not be consistent. For example,
    reads from different partitions might return different results because one of
    the partitions didn't receive a write. In this section, we will focus on highly
    available systems, that is, AP. To achieve high availability, we must sacrifice
    consistency, but it doesn't mean that our system will have corrupt or arbitrary
    data. The keyword is eventual consistency. Our system may be a little bit behind
    and provide access to somewhat stale data, but, eventually, you'll get what you
    expect. When you start thinking in terms of eventual consistency, it opens the
    door to potentially significant performance improvements.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: For example, if some important value is updated frequently (for example, every
    second), but you send its value only every minute, you have reduced your network
    traffic by a factor of 60 and you're on, average, only 30 seconds behind real-time
    updates. This is very significant. This is huge. You have just scaled your system
    to handle 60 times more users or requests.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at reliable and highly available large-scale Kubernetes
    clusters. This is arguably the sweet spot for Kubernetes. Although it is useful
    to be able to orchestrate a small cluster running a few containers, it is not
    necessary, but, at scale, you must have an orchestration solution in place which
    you can trust to scale with your system and provide the tools and the best practices
    to do that.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: You now have a solid understanding of the concepts of reliability and high-availability
    in distributed systems. You have delved into the best practices for running reliable
    and highly available Kubernetes clusters. You have explored the nuances of live
    Kubernetes cluster upgrades, and you can make wise design choices regarding levels
    of reliability and availability, as well as their performance and cost.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will address the important topic of security in Kubernetes.
    We will also discuss the challenges of securing Kubernetes and the risks involved.
    You will learn all about namespaces, service accounts, admission control, authentication,
    authorization, and encryption.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论Kubernetes中重要的安全主题。我们还将讨论保护Kubernetes的挑战和涉及的风险。您将学习有关命名空间、服务账户、准入控制、身份验证、授权和加密的所有内容。

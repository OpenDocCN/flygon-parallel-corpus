- en: Running Stateful Applications with Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kubernetes运行有状态应用
- en: In this chapter, we will look into what it takes to run stateful applications
    on Kubernetes. Kubernetes takes a lot of work out of our hands by automatically
    starting and restarting pods across the cluster nodes as needed, based on complex
    requirements and configurations such as namespaces, limits, and quotas. But when
    pods run storage-aware software, such as databases and queues, relocating a pod
    can cause the system to break. First, we'll understand the essence of stateful
    pods and why they are much more complicated to manage in Kubernetes. We will look
    at a few ways to manage the complexity, such as shared environment variables and
    DNS records. In some situations, a redundant in-memory state, a DaemonSet, or
    persistent storage claims can do the trick. The main solution that Kubernetes
    promotes for state-aware pods is the StatefulSet (previously called PetSet) resource,
    which allows us to manage an indexed collection of pods with stable properties.
    Finally, we will dive deep into a full-fledged example of running a Cassandra
    cluster on top of Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探讨在Kubernetes上运行有状态应用所需的条件。Kubernetes通过根据复杂的要求和配置（如命名空间、限制和配额）自动在集群节点上启动和重新启动pod，从而减少了我们的工作量。但是，当pod运行存储感知软件（如数据库和队列）时，重新定位一个pod可能会导致系统崩溃。首先，我们将了解有状态pod的本质，以及它们在Kubernetes中管理起来更加复杂的原因。我们将探讨一些管理复杂性的方法，比如共享环境变量和DNS记录。在某些情况下，冗余的内存状态、DaemonSet或持久存储声明可以解决问题。Kubernetes为有状态pod推广的主要解决方案是StatefulSet（以前称为PetSet）资源，它允许我们管理具有稳定属性的索引集合的pod。最后，我们将深入探讨在Kubernetes上运行Cassandra集群的一个完整示例。
- en: Stateful versus stateless applications in Kubernetes
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中有状态与无状态应用
- en: A stateless Kubernetes application is an application that doesn't manage its
    state in the Kubernetes cluster. All of the state is stored outside the cluster
    and the cluster containers access it in some manner. In this section, we'll understand
    why state management is critical to the design of a distributed system and the
    benefits of managing state within the Kubernetes cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，无状态应用是指不在Kubernetes集群中管理其状态的应用。所有状态都存储在集群外，集群容器以某种方式访问它。在本节中，我们将了解为什么状态管理对于分布式系统的设计至关重要，以及在Kubernetes集群内管理状态的好处。
- en: Understanding the nature of distributed data-intensive apps
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布式数据密集型应用的性质
- en: Let's start from the basics here. Distributed applications are a collection
    of processes that run on multiple machines, process inputs, manipulate data, expose
    APIs, and possibly have other side effects. Each process is a combination of its
    program, its runtime environment, and its inputs and outputs. The programs you
    write at school get their input as command-line arguments, maybe they read a file
    or access a database, and then write their results to the screen or a file or
    a database. Some programs keep state in-memory and can serve requests over the
    network. Simple programs run on a single machine, can hold all their state in
    memory or read from a file. Their runtime environment is their operating system.
    If they crash, the user has to restart them manually. They are tied to their machine.
    A distributed application is a different animal. A single machine is not enough
    to process all the data or serve all the requests fast enough. A single machine
    can't hold all the data. The data that needs to be processed is so large that
    it can't be downloaded cost-effectively into each processing machine. Machines
    can fail and need to be replaced. Upgrades need to be performed over all the processing
    machines. Users may be distributed across the globe.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础知识开始。分布式应用程序是在多台计算机上运行的一组进程，处理输入，操作数据，公开API，并可能具有其他副作用。每个进程是其程序、运行时环境和输入输出的组合。你在学校写的程序会作为命令行参数获取输入，也许它们会读取文件或访问数据库，然后将结果写入屏幕、文件或数据库。一些程序在内存中保持状态，并可以通过网络提供请求。简单的程序在单台计算机上运行，可以将所有状态保存在内存中或从文件中读取。它们的运行时环境是它们的操作系统。如果它们崩溃，用户必须手动重新启动它们。它们与它们的计算机绑定在一起。分布式应用程序是一个不同的动物。单台计算机不足以处理所有数据或足够快地提供所有请求。单台计算机无法容纳所有数据。需要处理的数据如此之大，以至于无法以成本效益的方式下载到每个处理机器中。机器可能会出现故障，需要被替换。需要在所有处理机器上执行升级。用户可能分布在全球各地。
- en: Taking all these issues into account, it becomes clear that the traditional
    approach doesn't work. The limiting factor becomes the data. Users/client must
    receive only summary or processed data. All massive data processing must be done
    close to the data itself because transferring data is prohibitively slow and expensive.
    Instead, the bulk of processing code must run in the same data center and network
    environment of the data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑所有这些问题后，很明显传统方法行不通。限制因素变成了数据。用户/客户端必须只接收摘要或处理过的数据。所有大规模数据处理必须在数据附近进行，因为传输数据的速度慢且昂贵。相反，大部分处理代码必须在相同的数据中心和网络环境中运行。
- en: Shared environment variables versus DNS records for discovery
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享环境变量与DNS记录用于发现
- en: 'Kubernetes provides several mechanisms for global discovery across the cluster.
    If your storage cluster is not managed by Kubernetes, you still need to tell Kubernetes
    pods how to find it and access it. There are two main methods:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为集群中的全局发现提供了几种机制。如果您的存储集群不是由Kubernetes管理，您仍然需要告诉Kubernetes pod如何找到它并访问它。主要有两种方法：
- en: DNS
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS
- en: Environment variables
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量
- en: In some cases, you may want to use both where environment variables can override
    DNS.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望同时使用环境变量和DNS，其中环境变量可以覆盖DNS。
- en: Why manage state in Kubernetes?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要在Kubernetes中管理状态？
- en: The main reason to manage state in Kubernetes itself as opposed to a separate
    cluster is that a lot of the infrastructure needed to monitor, scale, allocate,
    secure and operate a storage cluster is already provided by Kubernetes. Running
    a parallel storage cluster will lead to a lot of duplicated effort.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中管理状态的主要原因是，与在单独的集群中管理相比，Kubernetes已经提供了许多监视、扩展、分配、安全和操作存储集群所需的基础设施。运行并行存储集群将导致大量重复的工作。
- en: Why manage state outside of Kubernetes?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要在Kubernetes之外管理状态？
- en: Let's not rule out the other option. It may be better in some situations to
    manage state in a separate non-Kubernetes cluster, as long as it shares the same
    internal network (data proximity trumps everything).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们不排除其他选择。在某些情况下，将状态管理在一个单独的非Kubernetes集群中可能更好，只要它与相同的内部网络共享（数据接近性胜过一切）。
- en: 'Some valid reasons are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有效的原因如下：
- en: You already have a separate storage cluster and you don't want to rock the boat
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经有一个单独的存储集群，不想引起麻烦
- en: Your storage cluster is used by other non-Kubernetes applications
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的存储集群被其他非Kubernetes应用程序使用
- en: Kubernetes support for your storage cluster is not stable or mature enough
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes对您的存储集群的支持还不够稳定或成熟
- en: You may want to approach stateful applications in Kubernetes incrementally,
    starting with a separate storage cluster and integrating more tightly with Kubernetes
    later.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望逐步在Kubernetes中处理有状态的应用程序，首先从一个单独的存储集群开始，然后再与Kubernetes更紧密地集成。
- en: Accessing external data stores via DNS
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过DNS访问外部数据存储
- en: The DNS approach is simple and straightforward. Assuming your external storage
    cluster is load balanced and can provide a stable endpoint, then pods can just
    hit that endpoint directly and connect to the external cluster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DNS方法简单直接。假设您的外部存储集群是负载均衡的，并且可以提供稳定的端点，那么pod可以直接命中该端点并连接到外部集群。
- en: Accessing external data stores via environment variables
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过环境变量访问外部数据存储
- en: Another simple approach is to use environment variables to pass connection information
    to an external storage cluster. Kubernetes offers the `ConfigMap` resource as
    a way to keep configuration separate from the container image. The configuration
    is a set of key-value pairs. The configuration information can be exposed as an
    environment variable inside the container as well as volumes. You may prefer to
    use secrets for sensitive connection information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简单的方法是使用环境变量传递连接信息到外部存储集群。Kubernetes提供`ConfigMap`资源作为一种将配置与容器镜像分开的方式。配置是一组键值对。配置信息可以作为环境变量暴露在容器内部以及卷中。您可能更喜欢使用秘密来存储敏感的连接信息。
- en: Creating a ConfigMap
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建ConfigMap
- en: 'The following configuration file will create a configuration file that keeps
    a list of addresses:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配置文件将创建一个保留地址列表的配置文件：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `data` section contains all the key-value pairs, in this case, just a single
    pair with a key name of `db-ip-addresses`. It will be important later when consuming
    the `configmap` in a pod. You can check out the content to make sure it''s OK:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`data`部分包含所有的键值对，这种情况下，只有一个键名为`db-ip-addresses`的键值对。在后面消耗`configmap`时将会很重要。您可以检查内容以确保它是正确的：'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are other ways to create `ConfigMap`. You can directly create them using
    the `--from-value` or `--from-file` command-line arguments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他创建`ConfigMap`的方法。您可以直接使用`--from-value`或`--from-file`命令行参数来创建它们。
- en: Consuming a ConfigMap as an environment variable
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将ConfigMap作为环境变量消耗
- en: 'When you are creating a pod, you can specify a `ConfigMap` and consume its
    values in several ways. Here is how to consume our configuration map as an environment
    variable:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个pod时，可以指定一个`ConfigMap`并以多种方式使用其值。以下是如何将我们的配置映射为环境变量：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This pod runs the `busybox` minimal container and executes an `env bash` command
    and immediately exists. The `db-ip-addresses` key from the `db-config` map is
    mapped to the `DB_IP_ADDRESSES` environment variable, and is reflected in the
    output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个pod运行`busybox`最小容器，并执行`env bash`命令，然后立即退出。`db-config`映射中的`db-ip-addresses`键被映射到`DB_IP_ADDRESSES`环境变量，并反映在输出中：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using a redundant in-memory state
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用冗余的内存状态
- en: In some cases, you may want to keep a transient state in-memory. Distributed
    caching is a common case. Time-sensitive information is another one. For these
    use cases, there is no need for persistent storage, and multiple pods accessed
    through a service may be just the right solution. We can use standard Kubernetes
    techniques, such as labeling, to identify pods that belong to the store redundant
    copies of the same state and expose it through a service. If a pod dies, Kubernetes
    will create a new one and, until it catches up, the other pods will serve the
    state. We can even use the pod's anti-affinity alpha feature to ensure that pods
    that maintain redundant copies of the same state are not scheduled to the same
    node.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望在内存中保留瞬态状态。分布式缓存是一个常见情况。时间敏感的信息是另一个情况。对于这些用例，不需要持久存储，通过服务访问多个Pod可能是正确的解决方案。我们可以使用标签等标准Kubernetes技术来识别属于存储冗余副本的Pod，并通过服务公开它。如果一个Pod死掉，Kubernetes将创建一个新的Pod，并且在它赶上之前，其他Pod将服务于该状态。我们甚至可以使用Pod的反亲和性alpha功能来确保维护相同状态的冗余副本的Pod不被调度到同一节点。
- en: Using DaemonSet for redundant persistent storage
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DaemonSet进行冗余持久存储
- en: Some stateful applications, such as distributed databases or queues, manage
    their state redundantly and sync their nodes automatically (we'll take a very
    deep look into Cassandra later). In these cases, it is important that pods are
    scheduled to separate nodes. It is also important that pods are scheduled to nodes
    with a particular hardware configuration or are even dedicated to the stateful
    application. The DaemonSet feature is perfect for this use case. We can label
    a set of nodes and make sure that the stateful pods are scheduled on a one-by-one
    basis to the selected group of nodes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有状态的应用程序，如分布式数据库或队列，会冗余地管理它们的状态并自动同步它们的节点（我们稍后将深入研究Cassandra）。在这些情况下，重要的是将Pod调度到单独的节点。同样重要的是，Pod应该被调度到具有特定硬件配置的节点，甚至专门用于有状态应用程序。DaemonSet功能非常适合这种用例。我们可以为一组节点打上标签，并确保有状态的Pod被逐个地调度到所选的节点组。
- en: Applying persistent volume claims
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用持久卷索赔
- en: If the stateful application can use effectively shared persistent storage, then
    using a persistent volume claim in each pod is the way to go, as we demonstrated
    in [Chapter 7](2651f39a-2cf8-4562-9729-bc8927b07e66.xhtml), *Handling Kubernetes
    Storage*. The stateful application will be presented with a mounted volume that
    looks just like a local filesystem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有状态的应用程序可以有效地使用共享的持久存储，那么在每个Pod中使用持久卷索赔是正确的方法，就像我们在[第7章](2651f39a-2cf8-4562-9729-bc8927b07e66.xhtml)中演示的那样，*处理Kubernetes存储*。有状态的应用程序将被呈现为一个看起来就像本地文件系统的挂载卷。
- en: Utilizing StatefulSet
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用StatefulSet
- en: The StatefulSet controller is a relatively new addition to Kubernetes (introduced
    as PetSets in Kubernetes 1.3 and renamed StatefulSet in Kubernetes 1.5). It is
    especially designed to support distributed stateful applications where the identities
    of the members are important, and if a pod is restarted it must retain its identity
    in the set. It provides ordered deployment and scaling. Unlike regular pods, the
    pods of a stateful set are associated with persistent storage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet控制器是Kubernetes的一个相对较新的添加（在Kubernetes 1.3中作为PetSets引入，然后在Kubernetes
    1.5中更名为StatefulSet）。它专门设计用于支持分布式有状态应用程序，其中成员的身份很重要，如果一个Pod被重新启动，它必须保留在集合中的身份。它提供有序的部署和扩展。与常规Pod不同，StatefulSet的Pod与持久存储相关联。
- en: When to use StatefulSet
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用StatefulSet
- en: 'StatefulSet is great for applications that require one or more of the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet非常适合需要以下一项或多项功能的应用程序：
- en: Stable, unique network identifiers
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定、独特的网络标识符
- en: Stable, persistent storage
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定的持久存储
- en: Ordered, graceful deployment, and scaling
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序、优雅的部署和扩展
- en: Ordered, graceful deletion, and termination
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序、优雅的删除和终止
- en: The components of StatefulSet
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StatefulSet的组件
- en: 'There are several pieces that need to be configured correctly in order to have
    a working StatefulSet:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个部分需要正确配置，才能使StatefulSet正常工作：
- en: A headless service responsible for managing the network identity of the StatefulSet
    pods
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个负责管理StatefulSet pod的网络标识的无头服务
- en: The StatefulSet itself with a number of replicas
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有多个副本的StatefulSet本身
- en: Persistent storage provision dynamically or by an administrator
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态或由管理员持久存储提供
- en: 'Here is an example of a service called `nginx` that will be used for a StatefulSet:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个名为`nginx`的服务的示例，将用于StatefulSet：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, the `StatefulSet` configuration file will reference the service:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`StatefulSet`配置文件将引用该服务：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next part is the pod template that includes a mounted volume named `www`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是包含名为`www`的挂载卷的pod模板：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Last but not least, `volumeClaimTemplates` use a claim named `www` matching
    the mounted volume. The claim requests `1Gib` of `storage` with `ReadWriteOnce`
    access:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`volumeClaimTemplates`使用名为`www`的声明匹配挂载的卷。声明请求`1Gib`的`存储`，具有`ReadWriteOnce`访问权限：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Running a Cassandra cluster in Kubernetes
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中运行Cassandra集群
- en: 'In this section, we will explore in detail a very large example of configuring
    a Cassandra cluster to run on a Kubernetes cluster. The full example can be accessed
    here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细探讨配置Cassandra集群在Kubernetes集群上运行的一个非常大的示例。完整的示例可以在这里访问：
- en: '[https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra](https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra](https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra)'
- en: First, we'll learn a little bit about Cassandra and its idiosyncrasies, and
    then follow a step-by-step procedure to get it running using several of the techniques
    and strategies we've covered in the previous section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将学习一些关于Cassandra及其特殊性的知识，然后按照逐步的步骤来使其运行，使用我们在前一节中介绍的几种技术和策略。
- en: Quick introduction to Cassandra
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra的简要介绍
- en: Cassandra is a distributed columnar data store. It was designed from the get-go
    for big data. Cassandra is fast, robust (no single point of failure), highly available,
    and linearly scalable. It also has multi-data center support. It achieves all
    this by having a laser focus and carefully crafting the features it supports—and
    just as importantly—the features it doesn't support. In a previous company, I
    ran a Kubernetes cluster that used Cassandra as the main data store for sensor
    data (about 100 TB). Cassandra allocates the data to a set of nodes (node ring)
    based on a **distributed hash table** (**DHT**) algorithm. The cluster nodes talk
    to each other via a gossip protocol and learn quickly about the overall state
    of the cluster (what nodes joined and what nodes left or are unavailable). Cassandra
    constantly compacts the data and balances the cluster. The data is typically replicated
    multiple times for redundancy, robustness, and high-availability. From a developer's
    point of view, Cassandra is very good for time-series data and provides a flexible
    model where you can specify the consistency level in each query. It is also idempotent
    (a very important feature for a distributed database), which means repeated inserts
    or updates are allowed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra是一个分布式列式数据存储。它从一开始就为大数据而设计。Cassandra快速、健壮（没有单点故障）、高可用性和线性可扩展。它还支持多数据中心。它通过专注于并精心打造支持的功能，以及同样重要的是不支持的功能，来实现所有这些。在以前的公司中，我运行了一个使用Cassandra作为传感器数据主要数据存储的Kubernetes集群（约100
    TB）。Cassandra根据**分布式哈希表**（**DHT**）算法将数据分配给一组节点（节点环）。集群节点通过八卦协议相互通信，并迅速了解集群的整体状态（哪些节点加入，哪些节点离开或不可用）。Cassandra不断压缩数据并平衡集群。数据通常被复制多次以实现冗余、健壮性和高可用性。从开发者的角度来看，Cassandra非常适合时间序列数据，并提供了一个灵活的模型，可以在每个查询中指定一致性级别。它还是幂等的（对于分布式数据库来说非常重要的特性），这意味着允许重复插入或更新。
- en: 'Here is a diagram that shows how a Cassandra cluster is organized and how a
    client can access any node and how the request will be forwarded automatically
    to the nodes that have the requested data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个图表，显示了Cassandra集群的组织方式，以及客户端如何访问任何节点，请求将如何自动转发到具有所请求数据的节点：
- en: '![](Images/3176a1dd-f524-4deb-8638-1b04872ec14d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/3176a1dd-f524-4deb-8638-1b04872ec14d.png)'
- en: The Cassandra Docker image
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra Docker镜像
- en: 'Deploying Cassandra on Kubernetes as opposed to a standalone Cassandra cluster
    deployment requires a special Docker image. This is an important step because
    it means we can use Kubernetes to keep track of our Cassandra pods. The image
    is available here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署Cassandra与独立的Cassandra集群部署相反，需要一个特殊的Docker镜像。这是一个重要的步骤，因为这意味着我们可以使用Kubernetes来跟踪我们的Cassandra
    pod。该镜像在这里可用：
- en: '[https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra/image](https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra/image)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra/image](https://github.com/kubernetes/kubernetes/tree/master/examples/storage/cassandra/image)'
- en: 'Here are the essential parts of the Docker file. The image is based on Ubuntu
    Slim:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Docker文件的基本部分。该镜像基于Ubuntu Slim：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add and copy the necessary files (`Cassandra.jar`, various configuration files,
    run script, and read-probe script), create a `data` directory for Cassandra to
    store its SSTables, and mount it:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 添加和复制必要的文件（`Cassandra.jar`，各种配置文件，运行脚本和读取探测脚本），创建一个`data`目录供Cassandra存储其SSTable，并挂载它：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Expose important ports for accessing Cassandra and to let Cassandra nodes gossip
    with each other:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 暴露访问Cassandra的重要端口，并让Cassandra节点相互通信：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, the command, which uses `dumb-init`, a simple container `init` system
    from yelp, eventually runs the `run.sh` script:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`dumb-init`命令运行`run.sh`脚本，这是一个来自yelp的简单容器`init`系统：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Exploring the run.sh script
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索`run.sh`脚本
- en: The `run.sh` script requires some shell skills, but it's worth the effort. Since
    Docker allows running only one command, it is very common with non-trivial applications
    to have a launcher script that sets up the environment and prepares for the actual
    application. In this case, the image supports several deployment options (stateful
    set, replication controller, DaemonSet) that we'll cover later, and the run script
    accommodates it all by being very configurable via environment variables.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`run.sh`脚本需要一些shell技能，但这是值得的。由于Docker只允许运行一个命令，对于非平凡的应用程序来说，有一个设置环境并为实际应用程序做准备的启动脚本是非常常见的。在这种情况下，镜像支持几种部署选项（有状态集、复制控制器、DaemonSet），我们稍后会介绍，而运行脚本通过环境变量非常可配置。'
- en: 'First, some local variables are set for the Cassandra configuration file at
    `/etc/cassandra/cassandra.yaml`. The `CASSANDRA_CFG` variable will be used in
    the rest of the script:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为`/etc/cassandra/cassandra.yaml`中的Cassandra配置文件设置了一些本地变量。`CASSANDRA_CFG`变量将在脚本的其余部分中使用：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If no `CASSANDRA_SEEDS` were specified, then set the `HOSTNAME`, which is used
    in the StatefulSet solution:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有指定`CASSANDRA_SEEDS`，那么设置`HOSTNAME`，它在StatefulSet解决方案中使用：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then comes a long list of environment variables with defaults. The syntax, `${VAR_NAME:-<default}`,
    uses the `VAR_NAME` environment variable, if it's defined, or the default value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是一长串带有默认值的环境变量。语法`${VAR_NAME:-<default>}`使用`VAR_NAME`环境变量，如果定义了的话，或者使用默认值。
- en: A similar syntax, `${VAR_NAME:=<default}`, does the same thing, but also assigns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的语法`${VAR_NAME:=<default}`也可以做同样的事情，但同时也赋值
- en: the default value to the environment variable if it's not defined.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未定义环境变量，则将默认值分配给它。
- en: 'Both variations are used here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里都用到了两种变体：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then comes a section where all the variables are printed to the screen. Let''s
    skip most of it:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是一个部分，其中所有变量都打印到屏幕上。让我们跳过大部分内容：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The next section is very important. By default, Cassandra uses a simple snitch,
    which is unaware of racks and data centers. This is not optimal when the cluster
    spans multiple data centers and racks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分非常重要。默认情况下，Cassandra使用简单的snitch，不知道机架和数据中心。当集群跨多个数据中心和机架时，这并不是最佳选择。
- en: 'Cassandra is rack- and data center-aware and can optimize both for redundancy
    and high availability while limiting communication across data centers appropriately:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra是机架和数据中心感知的，可以优化冗余性和高可用性，同时适当地限制跨数据中心的通信：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Memory management is important, and you can control the maximum heap size to
    ensure Cassandra doesn''t start thrashing and swapping to disk:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理很重要，您可以控制最大堆大小，以确保Cassandra不会开始抖动并开始与磁盘交换：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The rack and data center information is stored in a simple Java `properties`
    file:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 机架和数据中心信息存储在一个简单的Java `properties`文件中：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next section loops over all the variables defined earlier, finds the corresponding
    key in the `Cassandra.yaml` configuration files, and overwrites them. That ensures
    that each configuration file is customized on the fly just before it launches
    Cassandra itself:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分循环遍历之前定义的所有变量，在`Cassandra.yaml`配置文件中找到相应的键，并进行覆盖。这确保了每个配置文件在启动Cassandra本身之前都是动态定制的：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next section is all about setting the seeds or seed provider depending
    on the deployment solution (StatefulSet or not). There is a little trick for the
    first pod to bootstrap as its own seed:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分都是关于根据部署解决方案（StatefulSet或其他）设置种子或种子提供程序。对于第一个pod来说，有一个小技巧可以作为自己的种子引导：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following section sets up various options for remote management and JMX
    monitoring. It''s critical in complicated distributed systems to have proper administration
    tools. Cassandra has deep support for the ubiquitous **Java Management Extensions**
    (**JMX**) standard:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分设置了远程管理和JMX监控的各种选项。在复杂的分布式系统中，拥有适当的管理工具至关重要。Cassandra对普遍的**Java管理扩展**（**JMX**）标准有深入的支持：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, the `CLASSPATH` is set to the `Cassandra` JAR file, and it launches
    Cassandra in the foreground (not daemonized) as the Cassandra user:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`CLASSPATH`设置为`Cassandra` JAR文件，并将Cassandra作为Cassandra用户在前台（非守护进程）启动：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Hooking up Kubernetes and Cassandra
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接Kubernetes和Cassandra
- en: Connecting Kubernetes and Cassandra takes some work because Cassandra was designed
    to be very self-sufficient, but we want to let it hook Kubernetes at the right
    time to provide capabilities, such as automatically restarting failed nodes, monitoring,
    allocating Cassandra pods, and providing a unified view of the Cassandra pods
    side by side with other pods. Cassandra is a complicated beast and has many knobs
    to control it. It comes with a `Cassandra.yaml` configuration file, and you can
    override all the options with environment variables.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 连接Kubernetes和Cassandra需要一些工作，因为Cassandra被设计为非常自给自足，但我们希望让它在适当的时候连接Kubernetes以提供功能，例如自动重新启动失败的节点、监视、分配Cassandra
    pods，并在其他pods旁边提供Cassandra pods的统一视图。Cassandra是一个复杂的系统，有许多控制选项。它带有一个`Cassandra.yaml`配置文件，您可以使用环境变量覆盖所有选项。
- en: Digging into the Cassandra configuration
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Cassandra配置
- en: 'There are two settings that are particularly relevant: the seed provider and
    the snitch. The seed provider is responsible for publishing a list of IP addresses
    (seeds) of nodes in the cluster. Every node that starts running connects to the
    seeds (there are usually at least three) and if it successfully reaches one of
    them they immediately exchange information about all the nodes in the cluster.
    This information is updated constantly for each node as the nodes gossip with
    each other.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个特别相关的设置：seed提供程序和snitch。seed提供程序负责发布集群中节点的IP地址（seeds）列表。每个启动的节点都连接到seeds（通常至少有三个），如果成功到达其中一个，它们立即交换有关集群中所有节点的信息。随着节点之间的gossip，这些信息会不断更新每个节点。
- en: 'The default seed provider configured in `Cassandra.yaml` is just a static list
    of IP addresses, in this case just the loopback interface:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cassandra.yaml`中配置的默认seed提供程序只是一个静态的IP地址列表，在这种情况下只有环回接口：'
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The other important setting is the snitch. It has two roles:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的设置是snitch。它有两个角色：
- en: It teaches Cassandra enough about your network topology to route requests efficiently.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它教会Cassandra足够了解您的网络拓扑以有效地路由请求。
- en: It allows Cassandra to spread replicas around your cluster to avoid correlated
    failures. It does this by grouping machines into data centers and racks. Cassandra
    will do its best not to have more than one replica on the same rack (which may
    not actually be a physical location).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许Cassandra在集群中分散副本以避免相关故障。它通过将机器分组到数据中心和机架来实现这一点。Cassandra会尽量避免在同一机架上拥有多个副本（这实际上可能不是一个物理位置）。
- en: 'Cassandra comes pre-loaded with several snitch classes, but none of them are
    Kubernetes-aware. The default is `SimpleSnitch`, but it can be overridden:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra预装了几个snitch类，但它们都不了解Kubernetes。默认是`SimpleSnitch`，但可以被覆盖。
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The custom seed provider
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义seed提供程序
- en: When running Cassandra nodes as pods in Kubernetes, Kubernetes may move pods
    around, including seeds. To accommodate that, a Cassandra seed provider needs
    to interact with the Kubernetes API server.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中将Cassandra节点作为pod运行时，Kubernetes可能会移动pod，包括seeds。为了适应这一点，Cassandra
    seed提供程序需要与Kubernetes API服务器进行交互。
- en: 'Here is a short snippet from the custom `KubernetesSeedProvider` Java class
    that implements the Cassandra `SeedProvider` API:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自定义的`KubernetesSeedProvider` Java类的一个简短片段，它实现了Cassandra的`SeedProvider` API：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Creating a Cassandra headless service
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个Cassandra无头服务
- en: The role of the headless service is to allow clients in the Kubernetes cluster
    to connect to the Cassandra cluster through a standard Kubernetes service instead
    of keeping track of the network identities of the nodes or putting a dedicated
    load balancer in front of all the nodes. Kubernetes provides all that out of the
    box through its services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务的作用是允许Kubernetes集群中的客户端通过标准的Kubernetes服务连接到Cassandra集群，而不是跟踪节点的网络标识或在所有节点前面放置专用的负载均衡器。Kubernetes通过其服务提供了所有这些功能。
- en: 'Here is the configuration file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置文件：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `app: Cassandra` label will group all the pods to participate in the service.
    Kubernetes will create endpoint records and the DNS will return a record for discovery.
    The `clusterIP` is `None`, which means the service is headless and Kubernetes
    will not do any load balancing or proxying. This is important because Cassandra
    nodes do their own communication directly.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`app: Cassandra`标签将把所有参与服务的pod分组。Kubernetes将创建端点记录，DNS将返回一个用于发现的记录。`clusterIP`是`None`，这意味着服务是无头的，Kubernetes不会进行任何负载平衡或代理。这很重要，因为Cassandra节点直接进行通信。'
- en: The `9042` port is used by Cassandra to serve CQL requests. Those can be queries,
    inserts/updates (it's always an upsert with Cassandra), or deletes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`9042`端口被Cassandra用于提供CQL请求。这些可以是查询、插入/更新（Cassandra总是使用upsert），或者删除。'
- en: Using StatefulSet to create the Cassandra cluster
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StatefulSet创建Cassandra集群
- en: 'Declaring a StatefulSet is not trivial. It is arguably the most complex Kubernetes
    resource. It has a lot of moving parts: standard metadata, the stateful set spec,
    the pod template (which is often pretty complex itself), and volume claim templates.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 声明StatefulSet并不是一件简单的事情。可以说它是最复杂的Kubernetes资源。它有很多组成部分：标准元数据，StatefulSet规范，Pod模板（通常本身就相当复杂），以及卷索赔模板。
- en: Dissecting the stateful set configuration file
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析StatefulSet配置文件
- en: Let's go methodically over this example stateful set configuration file that
    declares a three-node Cassandra cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按部就班地查看声明一个三节点Cassandra集群的示例StatefulSet配置文件。
- en: 'Here is the basic metadata. Note the `apiVersion` string is `apps/v1` (StatefulSet
    became generally available from Kubernetes 1.9):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基本的元数据。请注意，`apiVersion`字符串是`apps/v1`（StatefulSet从Kubernetes 1.9开始普遍可用）：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The stateful set `spec` defines the headless service name, how many pods there
    are in the stateful set, and the pod template (explained later). The `replicas`
    field specifies how many pods are in the stateful set:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet的`spec`定义了无头服务的名称，StatefulSet中有多少个pod，以及pod模板（稍后解释）。`replicas`字段指定了StatefulSet中有多少个pod：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The term `replicas` for the pods is an unfortunate choice because the pods
    are not replicas of each other. They share the same pod template, but they have
    a unique identity and they are responsible for different subsets of the state
    in general. This is even more confusing in the case of Cassandra, which uses the
    same term, `replicas`, to refer to groups of nodes that redundantly duplicate
    some subset of the state (but are not identical, because each can manage additional
    state too). I opened a GitHub issue with the Kubernetes project to change the
    term from `replicas` to `members`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于pod来说，术语`replicas`是一个不幸的选择，因为这些pod并不是彼此的副本。它们共享相同的pod模板，但它们有独特的身份，它们负责一般状态的不同子集。在Cassandra的情况下，这更加令人困惑，因为它使用相同的术语`replicas`来指代冗余复制一些状态的节点组（但它们并不相同，因为每个节点也可以管理额外的状态）。我向Kubernetes项目提出了一个GitHub问题，要求将术语从`replicas`更改为`members`：
- en: '[https://github.com/kubernetes/kubernetes.github.io/issues/2103](https://github.com/kubernetes/kubernetes.github.io/issues/2103)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The pod template contains a single container based on the custom Cassandra
    image. Here is the pod template with the `app: cassandra` label:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The container spec has multiple important parts. It starts with a `name` and
    the `image` we looked at earlier:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, it defines multiple container ports needed for external and internal
    communication by Cassandra nodes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The resources section specifies the CPU and memory needed by the container.
    This is critical because the storage management layer should never be a performance
    bottleneck due to `cpu` or `memory`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Cassandra needs access to `IPC`, which the container requests through the security
    content''s capabilities:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `env` section specifies environment variables that will be available inside
    the container. The following is a partial list of the necessary variables. The
    `CASSANDRA_SEEDS` variable is set to the headless service, so a Cassandra node
    can talk to seeds on startup and discover the whole cluster. Note that in this
    configuration we don''t use the special Kubernetes seed provider. `POD_IP` is
    interesting because it utilizes the Downward API to populate its value via the
    field reference to `status.podIP`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The container has a readiness probe, too, to ensure the Cassandra node doesn''t
    receive requests before it''s fully online:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Cassandra needs to read and write the data, of course. The `cassandra-data`
    volume mount is where it''s at:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: That's it for the container spec. The last part is the volume claim template.
    In this case, dynamic provisioning is used. It's highly recommended to use SSD
    drives for Cassandra storage, and especially its journal. The requested storage
    in this example is `1 Gi`. I discovered through experimentation that 1-2 TB is
    ideal for a single Cassandra node. The reason is that Cassandra does a lot of
    data shuffling under the covers, compacting and rebalancing the data. If a node
    leaves the cluster or a new one joins the cluster, you have to wait until the
    data is properly rebalanced before the data from the node that left is properly
    re-distributed or a new node is populated. Note that Cassandra needs a lot of
    disk space to do all this shuffling. It is recommended to have 50% free disk space.
    When you consider that you also need replication (typically 3x), then the required
    storage space can be 6x your data size. You can get by with 30% free space if
    you're adventurous and maybe use just 2x replication depending on your use case.
    But don't get below 10% free disk space, even on a single node. I learned the
    hard way that Cassandra will simply get stuck and will be unable to compact and
    rebalance such nodes without extreme measures.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 容器规范就是这样。最后一部分是卷索赔模板。在这种情况下，使用了动态配置。强烈建议为Cassandra存储使用SSD驱动器，特别是其日志。在这个例子中，请求的存储空间是`1
    Gi`。通过实验，我发现单个Cassandra节点的理想存储空间是1-2 TB。原因是Cassandra在后台进行大量的数据重排、压缩和数据再平衡。如果一个节点离开集群或一个新节点加入集群，你必须等到数据被正确再平衡，然后才能重新分布来自离开节点的数据或者填充新节点。请注意，Cassandra需要大量的磁盘空间来进行所有这些操作。建议保留50%的空闲磁盘空间。当考虑到你还需要复制（通常是3倍）时，所需的存储空间可能是你的数据大小的6倍。如果你愿意冒险，也许根据你的用例，你可以用30%的空闲空间，甚至只使用2倍的复制。但是，即使是在单个节点上，也不要低于10%的空闲磁盘空间。我以艰难的方式得知，Cassandra会简单地卡住，无法在没有极端措施的情况下进行压缩和再平衡这样的节点。
- en: 'The access mode is, of course, `ReadWriteOnce`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 访问模式当然是`ReadWriteOnce`：
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: When deploying a stateful set, Kubernetes creates the pod in order per its index
    number. When scaling up or down, it also does it in order. For Cassandra, this
    is not important because it can handle nodes joining or leaving the cluster in
    any order. When a Cassandra pod is destroyed, the persistent volume remains. If
    a pod with the same index is created later, the original persistent volume will
    be mounted into it. This stable connection between a particular pod and its storage
    enables Cassandra to manage the state properly.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署有状态集时，Kubernetes根据索引号按顺序创建pod。当扩展或缩减规模时，也是按顺序进行的。对于Cassandra来说，这并不重要，因为它可以处理节点以任何顺序加入或离开集群。当销毁一个Cassandra
    pod时，持久卷仍然存在。如果以后创建了具有相同索引的pod，原始的持久卷将被挂载到其中。这种稳定的连接使得Cassandra能够正确管理状态。
- en: Using a replication controller to distribute Cassandra
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用复制控制器来分发Cassandra
- en: A StatefulSet is great, but, as mentioned earlier, Cassandra is already a sophisticated
    distributed database. It has a lot of mechanisms for automatically distributing,
    balancing, and replicating the data around the cluster. These mechanisms are not
    optimized for working with network persistent storage. Cassandra was designed
    to work with the data stored directly on the nodes. When a node dies, Cassandra
    can recover having redundant data stored on other nodes. Let's look at a different
    way to deploy Cassandra on a Kubernetes cluster, which is more aligned with Cassandra's
    semantics. Another benefit of this approach is that if you have an existing Kubernetes
    cluster; you don't have to upgrade it to the latest and greatest just to use a
    stateful set.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet非常好，但是如前所述，Cassandra已经是一个复杂的分布式数据库。它有很多机制可以自动分发、平衡和复制集群中的数据。这些机制并不是为了与网络持久存储一起工作而进行优化的。Cassandra被设计为与直接存储在节点上的数据一起工作。当一个节点死机时，Cassandra可以通过在其他节点上存储冗余数据来进行恢复。让我们来看看在Kubernetes集群上部署Cassandra的另一种方式，这种方式更符合Cassandra的语义。这种方法的另一个好处是，如果您已经有一个现有的Kubernetes集群，您不必将其升级到最新版本，只是为了使用一个有状态的集。
- en: 'We will still use the headless service, but instead of a stateful set we''ll
    use a regular replication controller. There are some important differences:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍将使用无头服务，但是我们将使用常规的复制控制器，而不是有状态集。有一些重要的区别：
- en: Replication controller instead of a stateful set
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制控制器而不是有状态集
- en: Storage on the node the pod is scheduled to run on
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点上安排运行的pod的存储
- en: The custom Kubernetes seed provider class is used
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用了自定义的Kubernetes种子提供程序类
- en: Dissecting the replication controller configuration file
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解剖复制控制器配置文件
- en: 'The metadata is pretty minimal, with just a name (labels are not required):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据非常简单，只有一个名称（标签不是必需的）：
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `spec` specifies the number of `replicas`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec`指定了`replicas`的数量：'
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The pod template''s metadata is where the `app: Cassandra` label is specified.
    The replication controller will keep track and make sure that there are exactly
    three pods with that label:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'pod模板的元数据是指定`app: Cassandra`标签的地方。复制控制器将跟踪并确保具有该标签的pod恰好有三个：'
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The pod template''s `spec` describes the list of containers. In this case,
    there is just one container. It uses the same Cassandra Docker image named `cassandra`
    and runs the `run.sh` script:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: pod模板的`spec`描述了容器的列表。在这种情况下，只有一个容器。它使用相同的名为`cassandra`的Cassandra Docker镜像，并运行`run.sh`脚本：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The resources section just requires `0.5` units of CPU in this example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，资源部分只需要`0.5`个CPU单位：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The environment section is a little different. The `CASSANDRA_SEED_PROVDIER`
    specifies the custom Kubernetes seed provider class we examined earlier. Another
    new addition here is `POD_NAMESPACE`, which uses the Downward API again to fetch
    the value from the metadata:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 环境部分有点不同。`CASSANDRA_SEED_PROVDIER`指定了我们之前检查过的自定义Kubernetes种子提供程序类。这里的另一个新添加是`POD_NAMESPACE`，它再次使用Downward
    API从元数据中获取值：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `ports` section is identical, exposing the intra-node communication ports
    (`7000` and `7001`), the `7199` JMX port used by external tools, such as Cassandra
    OpsCenter, to communicate with the Cassandra cluster, and of course the `9042`
    CQL port, through which clients communicate with the cluster:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`ports`部分是相同的，暴露节点内通信端口（`7000`和`7001`），`7199` JMX端口用于外部工具（如Cassandra OpsCenter）与Cassandra集群通信，当然还有`9042`
    CQL端口，通过它客户端与集群通信：'
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Once again, the volume is mounted into `/cassandra_data`. This is important
    because the same Cassandra image configured properly just expects its `data` directory
    to be at a certain path. Cassandra doesn''t care about the backing storage (although
    you should care, as the cluster administrator). Cassandra will just read and write
    using filesystem calls:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一次又一次，卷被挂载到`/cassandra_data`中。这很重要，因为同样配置正确的Cassandra镜像只期望其`data`目录位于特定路径。Cassandra不关心后备存储（尽管作为集群管理员，你应该关心）。Cassandra只会使用文件系统调用进行读写。
- en: '[PRE45]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The volumes section is the biggest difference from the stateful set solution.
    A stateful set uses persistent storage claims to connect a particular pod with
    a stable identity to a particular persistent volume. The replication controller
    solution just uses an `emptyDir` on the hosting node:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 卷部分是与有状态集解决方案最大的不同之处。有状态集使用持久存储索赔将特定的pod与特定的持久卷连接起来，以便具有稳定身份。复制控制器解决方案只是在托管节点上使用`emptyDir`。
- en: '[PRE46]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This has many ramifications. You have to provision enough storage on each node.
    If a Cassandra pod dies, its storage goes away. Even if the pod is restarted on
    the same physical (or virtual) machine, the data on disk will be lost because
    `emptyDir` is deleted once its pod is removed. Note that container restarts are
    OK because `emptyDir` survives container crashes. So, what happens when the pod
    dies? The replication controller will start a new pod with empty data. Cassandra
    will detect that a new node was added to the cluster, assign it some portion of
    the data, and start rebalancing automatically by moving data from other nodes.
    This is where Cassandra shines. It constantly compacts, rebalances, and distributes
    the data evenly across the cluster. It will just figure out what to do on your
    behalf.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这有许多影响。你必须为每个节点提供足够的存储空间。如果Cassandra pod死掉，它的存储空间也会消失。即使pod在同一台物理（或虚拟）机器上重新启动，磁盘上的数据也会丢失，因为`emptyDir`一旦其pod被删除就会被删除。请注意，容器重新启动是可以的，因为`emptyDir`可以在容器崩溃时幸存下来。那么，当pod死掉时会发生什么呢？复制控制器将启动一个带有空数据的新pod。Cassandra将检测到集群中添加了一个新节点，为其分配一些数据，并通过从其他节点移动数据来自动开始重新平衡。这就是Cassandra的亮点所在。它不断地压缩、重新平衡和均匀地分布数据到整个集群中。它会自动弄清楚该为你做什么。
- en: Assigning pods to nodes
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为节点分配pod
- en: The main problem with the replication controller approach is that multiple pods
    can get scheduled on the same Kubernetes node. What if you have a replication
    factor of three and all three pods that are responsible for some range of the
    keyspace are all scheduled to the same Kubernetes node? First, all requests for
    read or writes of that range of keys will go to the same node, creating more pressure.
    But, even worse, we just lost our redundancy. We have a **single point of failure**
    (**SPOF**). If that node dies, the replication controller will happily start three
    new pods on some other Kubernetes node, but none of them will have data, and no
    other Cassandra node in the cluster (the other pods) will have data to copy from.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 复制控制器方法的主要问题是多个pod可以被调度到同一Kubernetes节点上。如果你的复制因子是三，负责某个键空间范围的所有三个pod都被调度到同一个Kubernetes节点上会怎么样？首先，所有对该键范围的读取或写入请求都将发送到同一个节点，增加了更多的压力。但更糟糕的是，我们刚刚失去了冗余性。我们有一个**单点故障**（**SPOF**）。如果该节点死掉，复制控制器将愉快地在其他Kubernetes节点上启动三个新的pod，但它们都不会有数据，而且集群中的其他Cassandra节点（其他pod）也没有数据可供复制。
- en: 'This can be solved using a Kubernetes scheduling concept called anti-affinity.
    When assigning pods to nodes, a pod can be annotated so that the scheduler will
    not schedule it to a node that already has a pod with a particular set of labels.
    Add this to the pod `spec` to ensure that at most a single Cassandra pod will
    be assigned to a node:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过使用Kubernetes调度概念中的反亲和性来解决。在将pod分配给节点时，可以对pod进行注释，以便调度程序不会将其调度到已经具有特定标签集的节点上。将此添加到pod的`spec`中，以确保最多只有一个Cassandra
    pod被分配给一个节点：
- en: '[PRE47]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Using DaemonSet to distribute Cassandra
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DaemonSet来分发Cassandra
- en: 'A better solution to the problem of assigning Cassandra pods to different nodes
    is to use a DaemonSet. A DaemonSet has a pod template like a replication controller.
    But a DaemonSet has a node selector that determines on which nodes to schedule
    its pods. It doesn''t have a certain number of replicas, it just schedules a pod
    on each node that matches its selector. The simplest case is to schedule a pod
    on each node in the Kubernetes cluster. But the node selector can also use match
    expressions against labels to deploy to a particular subset of nodes. Let''s create
    a DaemonSet for deploying our Cassandra cluster onto the Kubernetes cluster:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 解决将Cassandra pod分配给不同节点的问题的更好方法是使用DaemonSet。DaemonSet具有类似于复制控制器的pod模板。但是DaemonSet有一个节点选择器，用于确定在哪些节点上调度其pod。它没有特定数量的副本，它只是在与其选择器匹配的每个节点上调度一个pod。最简单的情况是在Kubernetes集群中的每个节点上调度一个pod。但是节点选择器也可以使用标签的匹配表达式来部署到特定的节点子集。让我们为在Kubernetes集群上部署我们的Cassandra集群创建一个DaemonSet：
- en: '[PRE48]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `spec` of the DaemonSet contains a regular pod template. The `nodeSelector`
    section is where the magic happens, and it ensures that one and exactly one pod
    will always be scheduled to each node with a label of `app: Cassandra`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 'DaemonSet的`spec`包含一个常规的pod模板。`nodeSelector`部分是魔术发生的地方，它确保每个带有`app: Cassandra`标签的节点上始终会被调度一个且仅有一个pod：'
- en: '[PRE49]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The rest is identical to the replication controller. Note that `nodeSelector`
    is expected to be deprecated in favor of affinity. When that will happen, it's
    not clear.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分与复制控制器相同。请注意，预计`nodeSelector`将被弃用，而亲和性将被取代。这将在何时发生，目前尚不清楚。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the topic of stateful applications and how to integrate
    them with Kubernetes. We discovered that stateful applications are complicated
    and considered several mechanisms for discovery, such as DNS and environment variables.
    We also discussed several state management solutions, such as in-memory redundant
    storage and persistent storage. The bulk of the chapter revolved around deploying
    a Cassandra cluster inside a Kubernetes cluster using several options, such as
    a stateful set, a replication controller, and a DaemonSet. Each approach has its
    own pros and cons. At this point, you should have a thorough understanding of
    stateful applications and how to apply them in your Kubernetes-based system. You
    are armed with multiple methods for various use cases, and maybe you've even learned
    a little bit about Cassandra.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了有关有状态应用程序以及如何将其与Kubernetes集成的主题。我们发现有状态应用程序很复杂，并考虑了几种发现机制，例如DNS和环境变量。我们还讨论了几种状态管理解决方案，例如内存冗余存储和持久存储。本章的大部分内容围绕在Kubernetes集群内部部署Cassandra集群，使用了几种选项，例如有状态集、复制控制器和DaemonSet。每种方法都有其优缺点。在这一点上，您应该对有状态应用程序有深入的了解，以及如何在基于Kubernetes的系统中应用它们。您已经掌握了多种用例的多种方法，也许甚至学到了一些关于Cassandra的知识。
- en: In the next chapter, we will continue our journey and explore the important
    topic of scalability, in particular auto-scalability, and how to deploy and do
    live upgrades and updates as the cluster dynamically grows. These issues are very
    intricate, especially when the cluster has stateful apps running on it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续我们的旅程，探讨可扩展性的重要主题，特别是自动扩展性，以及在集群动态增长时如何部署和进行实时升级和更新。这些问题非常复杂，特别是当集群上运行有状态应用程序时。

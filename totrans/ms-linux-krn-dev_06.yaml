- en: Interprocess Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A complex application-programming model might include a number of processes,
    each implemented to handle a specific job, which contribute to the end functionality
    of the application as a whole. Depending on the objective, design, and environment
    in which such applications are hosted, processes involved might be related (parent-child,
    siblings) or unrelated. Often, such processes need various resources to communicate,
    share data, and synchronize their execution to achieve desired results. These
    are provided by the operating system's kernel as services called **interprocess
    communication** (**IPC**). We have already discussed the usage of signals as an
    IPC mechanism; in this chapter, we shall begin to explore various other resources
    available for process communication and data sharing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pipes and FIFOs as messaging resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SysV IPC resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POSX IPC mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipes and FIFOs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pipes form a basic unidirectional, self-synchronous means of communication
    between processes. As the name suggests, they have two ends: one where a process
    writes and the opposite end from where another process reads the data. Presumably
    what goes in first will be read out first in this kind of a setup. Pipes innately
    result in communication synchronization due to their limited capacity: if the
    writing process writes much faster than the reading process reads, the pipe’s
    capacity will fail to hold excess data and invariably block the writing process
    until the reader reads and frees up data. Similarly, if the reader reads data
    faster than the writer, it will be left with no data to read, thus being blocked
    until data becomes available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipes can be used as a messaging resource for both cases of communication:
    between related processes and between unrelated processes. When applied between
    related processes, pipes are referred to as **unnamed pipes**, since they are
    not enumerated as files under the `rootfs` tree. An unnamed pipe can be allocated
    through the `pipe()` API.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: API invokes a corresponding system call, which allocates appropriate data structures
    and sets up pipe buffers. It maps a pair of file descriptors, one for reading
    on the pipe buffer and another for writing on the pipe buffer. These descriptors
    are returned to the caller. The caller process normally forks the child process,
    which inherits the pipe file descriptors that can be used for messaging.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code excerpt shows the pipe system call implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Communication between unrelated processes requires the pipe file to be enumerated
    into **rootfs***.* Such pipes are often called **named pipes***,* and can be created
    either from the command line (`mkfifo`) or from a process using the `mkfifo` API*.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A named pipe is created with the name specified and with appropriate permissions
    as specified by the mode argument. The `mknod` system call is invoked for creating
    a FIFO, which internally invokes VFS routines to set up the named pipe. Processes
    with access permissions can initiate operations on FIFOs through common VFS file
    APIs `open`, `read`, `write`, and `close`.
  prefs: []
  type: TYPE_NORMAL
- en: pipefs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pipes and FIFOs are created and managed by a special filesystem called `pipefs`.
    It registers with VFS as a special filesystem. The following is a code excerpt
    from `fs/pipe.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It integrates pipe files into VFS by enumerating an `inode` instance representing
    each pipe; this allows applications to engage common file APIs `read` and `write`.
    The `inode` structure contains a union of pointers that are relevant for special
    files such as pipes and device files. For pipe file `inodes`, one of the pointers,
    `i_pipe`, is initialized to `pipefs`, defined as an instance of type `pipe_inode_info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`struct pipe_inode_info` contains all pipe-related metadata as defined by `pipefs`,
    which includes information of the pipe buffer and other important management data.
    This structure is defined in `<linux/pipe_fs_i.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `bufs` pointer refers to the pipe buffer; each pipe is by default assigned
    a total buffer of 65,535 bytes (64k) arranged as a circular array of 16 pages.
    User processes can alter the total size of the pipe buffer via a `fcntl()` operation
    on the pipe descriptor. The default maximum limit for the pipe buffer is 1,048,576
    bytes, which can be changed by a privileged process via the `/proc/sys/fs/pipe-max-size`
    file interface. Following is a summarized table that describes the rest of the
    important elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `mutex` | Exclusion lock protecting the pipe |'
  prefs: []
  type: TYPE_TB
- en: '| `wait` | Wait queue for readers and writers |'
  prefs: []
  type: TYPE_TB
- en: '| `nrbufs` | Count of non-empty pipe buffers for this pipe |'
  prefs: []
  type: TYPE_TB
- en: '| `curbuf` | Current pipe buffer |'
  prefs: []
  type: TYPE_TB
- en: '| `buffers` | Total number of buffers |'
  prefs: []
  type: TYPE_TB
- en: '| `readers` | Number of current readers |'
  prefs: []
  type: TYPE_TB
- en: '| `writers` | Number of current writers |'
  prefs: []
  type: TYPE_TB
- en: '| `files` | Number of struct file instances currently referring to this pipe
    |'
  prefs: []
  type: TYPE_TB
- en: '| `waiting_writers` | Number of writers currently blocked on the pipe |'
  prefs: []
  type: TYPE_TB
- en: '| `r_coutner` | Reader counter (relevant for FIFO) |'
  prefs: []
  type: TYPE_TB
- en: '| `w_counter` | Writer counter (relevant for FIFO) |'
  prefs: []
  type: TYPE_TB
- en: '| `*fasync_readers` | Reader side fasync |'
  prefs: []
  type: TYPE_TB
- en: '| `*fasync_writers` | Writer side fasync |'
  prefs: []
  type: TYPE_TB
- en: '| `*bufs` | Pointer to circular array of pipe buffers |'
  prefs: []
  type: TYPE_TB
- en: '| `*user` | Pointer to the `user_struct` instance that represents the user
    who created this pipe |'
  prefs: []
  type: TYPE_TB
- en: 'Reference to each page of the pipe buffer is wrapped into a circular array
    of instances of *type* `struct pipe_buffer`. This structure is defined in `<linux/pipe_fs_i.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`*page` is a pointer to the page descriptor of the page buffer, and the `offset`
    and `len` fields contain the offset to the data contained in the page buffer and
    its length. `*ops` is a pointer to a structure of type `pipe_buf_operations`,
    which encapsulates pipe buffer operations implemented by `pipefs`. It also implements
    file operations that are bound to pipe and FIFO inodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Message queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Message queues** are lists of message buffers through which an arbitrary
    number of processes can communicate. Unlike pipes, the writer does not have to
    wait for the reader to open the pipe and listen for data. Similar to a mailbox,
    writers can drop a fixed-length message wrapped in a buffer into the queue, which
    the reader can pick whenever it is ready. The message queue does not retain the
    message packet after it is picked by the reader, which means that each message
    packet is assured to be process persistent. Linux supports two distinct implementations
    of message queues: classic Unix SYSV message queues and contemporary POSIX message
    queues.'
  prefs: []
  type: TYPE_NORMAL
- en: System V message queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the classic AT&T message queue implementation suitable for messaging
    between an arbitrary number of unrelated processes. Sender processes wrap each
    message into a packet containing message data and a message number. The message
    queue implementation does not define the meaning of the message number, and it
    is left to the application designers to define appropriate meanings for message
    numbers and program readers and writers to interpret the same. This mechanism
    provides flexibility for programmers to use message numbers as message IDs or
    receiver IDs. It enables reader processes to selectively read messages that match
    specific IDs. However, messages with the same ID are always read in FIFO order
    (first in, first out).
  prefs: []
  type: TYPE_NORMAL
- en: 'Processes can create and open a SysV message queue with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `key` parameter is a unique constant that serves as a magic number to identify
    the message queue. All programs that are required to access this message queue
    will need to use the same magic number; this number is usually hard-coded into
    relevant processes at compile time. However, applications need to ensure that
    the key value is unique for each message queue, and there are alternate library
    functions available through which unique keys can be dynamically generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unique key and `msgflag` parameter values, if set to `IPC_CREATE`, will
    cause a new message queue to be set up. Valid processes that have access to the
    queue can read or write messages into the queue using `msgsnd` and `msgrcv` routines
    (we will not discuss them in detail here; refer to Linux system programming manuals):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each message queue is created by enumerating a set of data structures by the
    underlying SysV IPC subsystem. `struct msg_queue` is the core data structure,
    and an instance of this is enumerated for each message queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `q_messages` field represents the head node of a double-linked circular
    list that contains all messages currently in the queue. Each message begins with
    a header followed by message data; each message can consume one of more pages
    depending on length of message data. The message header is always at the start
    of the first page and is represented by an instance of `struct msg_msg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `m_list` field contains pointers to previous and next messages in the queue.
    The `*next` pointer refers to an instance of type `struct msg_msgseg`, which contains
    the address of the next page of message data. This pointer is relevant only when
    message data exceeds the first page. The second page frame starts with a descriptor
    `msg_msgseg`, which further contains a pointer to a subsequent page, and this
    order continues until the last page of the message data is reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: POSIX message queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POSIX message queues implement priority-ordered messages. Each message written
    by a sender process is associated with an integer number which is interpreted
    as message priority; messages with a higher number are considered higher in priority.
    The message queue orders current messages as per priority and delivers them to
    the reader process in descending order (highest priority first). This implementation
    also supports a wider API interface with facilities of bounded wait send and receive
    operations and asynchronous message arrival notifications for receivers through
    signals or threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'This implementation provides a distinct API interface to `create`, `open`,
    `read`, `write`, and `destroy` message queues. Following is a summarized description
    of APIs (we will not discuss usage semantics here, refer to system programming
    manuals for more details):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **API interface** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_open()` | Create or open a POSIX message queue |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_send()` | Write a message to the queue |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_timedsend()` | Similar to `mq_send`, but with a timeout parameter for
    bounded operations |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_receive()` | Fetch a message from the queue; this operation is possible
    on unbounded blocking calls |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_timedreceive()` | Similar to `mq_receive()` but with a timeout parameter
    that limits possible blocking for bounded time |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_close()` | Close a message queue |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_unlink()` | Destroy message queue |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_notify()` | Customize and set up message arrival notifications |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_getattr()` | Get attributes associated with a message queue |'
  prefs: []
  type: TYPE_TB
- en: '| `mq_setattr()` | Set attributes specified on a message queue |'
  prefs: []
  type: TYPE_TB
- en: 'POSIX message queues are managed by a special filesystem called `mqueue`. Each
    message queue is identified by a filename. Metadata for each queue is described
    by an instance of struct `mqueue_inode_info`, which symbolizes the inode object
    associated with the message queue file in the `mqueue` filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `*node_cache` pointer refers to the `posix_msg_tree_node` descriptor that
    contains the header to a linked list of message nodes, in which each message is
    represented by a descriptor of type `msg_msg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike message queues, which offer a process-persistent messaging infrastructure,
    the shared memory service of IPC provides kernel-persistent memory that can be
    attached by an arbitrary number of processes that share common data. A shared
    memory infrastructure provides operation interfaces to allocate, attach, detach,
    and destroy shared memory regions. A process that needs access to shared data
    will *attach* or *map* a shared memory region into its address space; it can then
    access data in shared memory through the address returned by the mapping routine.
    This makes shared memory one of the fastest means of IPC since from a process's
    perspective it is akin to accessing local memory, which does not involve switch
    into kernel mode.
  prefs: []
  type: TYPE_NORMAL
- en: System V shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux supports legacy SysV shared memory implementation under the IPC subsystem.
    Similar to SysV message queues, each shared memory region is identified by a unique
    IPC identifier.
  prefs: []
  type: TYPE_NORMAL
- en: Operation interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The kernel provides distinct system call interfaces for initiating shared memory
    operations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocating shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`shmget()` system call is invoked by a process to get an IPC identifier for
    a shared memory region; if the region does not exists, it creates one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function returns the identifier of the shared memory segment corresponding
    to the value contained in the *key* parameter. If other processes intend to use
    an existing segment, they can use the segment's *key* value when looking for its
    identifier. A new segment is however created if the *key* parameter is unique
    or has the value `IPC_PRIVATE`.`size` indicates the number of bytes that needs
    to be allocated, as segments are allocated as memory pages. The number of pages
    to be allocated is obtained by rounding off the *size* value to the nearest multiple
    of a page size.\
  prefs: []
  type: TYPE_NORMAL
- en: 'The `shmflg` flag specifies how the segment needs to be created. It can contain
    two values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IPC_CREATE`: This indicates creating a new segment. If this flag is unused,
    the segment associated with the key value is found, and if the user has the access
    permissions, the segment''s identifier is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPC_EXCL`: This flag is always used with `IPC_CREAT`, to ensure that the call
    fails if the *key* value exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaching a shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The shared memory region must be attached to its address space for a process
    to access it. `shmat()` is invoked to attach the shared memory to the address
    space of the calling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The segment indicated by `shmid` is attached by this function. `shmaddr` specifies
    a pointer indicating the location in the process''s address space where the segment
    is to be mapped. The third argument `shmflg` is a flag, which can be one of the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SHM_RND`: This is specified when `shmaddr` isn''t a NULL value, indicating
    the function to attach the segment at the address, computed by rounding off the
    `shmaddr` value to the nearest multiple of page size; otherwise, the user must
    take care that `shmaddr` be page-aligned so that the segment gets attached correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHM_RDONLY`: This is to specify that the segment will only be read if the
    user has the necessary read permissions. Otherwise, both read and write access
    for the segment is given (the process must have the respective permissions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHM_REMAP`: This is a Linux-specific flag that indicates that any existing
    mapping at the address specified by `shmaddr` be replaced with the new mapping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detaching shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Likewise, to detach the shared memory from the process address space, `shmdt()`
    is invoked. As IPC shared memory regions are persistent in the kernel, they continue
    to exist even after the processes detach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The segment at the address specified by `shmaddr` is detached from the address
    space of the calling process.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these interface operations invoke relevant system calls implemented
    in the `<ipc/shm.c>` source file.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each shared memory segment is represented by a `struct shmid_kernel` descriptor.
    This structure contains all metadata relevant to the management of SysV shared
    memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For reliability and ease of management, the kernel''s IPC subsystem manages
    shared memory segments through a special file system called `shmfs`*.* This filesystem
    is not mounted on to the rootfs tree; its operations are only accessible through
    SysV shared memory system calls. The `*shm_file` pointer refers to the `struct
    file` object of `shmfs` that represents a shared memory block. When a process
    initiates an attach operation, the underlying system call invokes `do_mmap()`
    to create relevant mapping into the caller''s address space (through `struct vm_area_struct`)
    and steps into the `*shmfs-*`defined `shm_mmap()` operation to map corresponding
    shared memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: POSIX shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Linux kernel supports POSIX shared memory through a special filesystem called
    `tmpfs`*,* which is mounted on to `/dev/shm` of the `rootfs`*.* This implementation
    offers a distinct API which is consistent with the Unix file model, resulting
    in each shared memory allocation to be represented by a unique filename and inode.
    This interface is considered more flexible by application programmers since it
    allows standard POSIX file-mapping routines `mmap()` and `unmap()` for attaching
    and detaching memory segments into the caller process address space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a summarized description of interface routines:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **API** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `shm_open()` | Create and open a shared memory segment identified by a filename
    |'
  prefs: []
  type: TYPE_TB
- en: '| `mmap()` | POSIX standard file mapping interface for attaching shared memory
    to caller''s address space |'
  prefs: []
  type: TYPE_TB
- en: '| `sh_unlink()` | Destroy specified shared memory block |'
  prefs: []
  type: TYPE_TB
- en: '| `unmap()` | Detach specified shared memory map from caller address space
    |'
  prefs: []
  type: TYPE_TB
- en: The underlying implementation is similar to that of SysV shared memory with
    the difference that the mapping implementation is handled by the `tmpfs` filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Although shared memory is the easiest way of sharing common data or resources,
    it dumps the burden of implementing synchronization on the processes, as a shared
    memory infrastructure does not provide any synchronization or protection mechanism
    for the data or resources in the shared memory region. An application designer
    must consider synchronization of shared memory access between contending processes
    to ensure reliability and validity of shared data, for instance, preventing a
    possible write by two processes on the same region at a time, restricting a reading
    process to wait until a write is completed by another process, and so on. Often,
    to synchronize such race conditions another IPC resource called semaphores is
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Semaphores** are synchronization primitives provided by the IPC subsystem.
    They deliver a protective mechanism for shared data structures or resources against
    concurrent access by processes in a multithreaded environment. At its core, each
    semaphore is composed of an integer counter that can be atomically accessed by
    a caller process. Semaphore implementations provide two operations, one for waiting
    on a semaphore variable and another to signal the semaphore variable. In other
    words, waiting on the semaphore decreases the counter by 1 and signaling the semaphore
    increases the counter by 1\. Typically, when a process wants to access a shared
    resource, it tries to decrease the semaphore counter. This attempt is however
    handled by the kernel as it blocks the attempting process until the counter yields
    a positive value. Similarly, when a process relinquishes the resource, it increases
    the semaphore counter, which wakes up any process that is waiting for the resource.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semaphore versions**'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally all `*nix` systems implement the System V semaphore mechanism;
    however, POSIX has its own implementation of semaphores aiming at portability
    and leveling a few clumsy issues which the System V version carries. Let’s begin
    by looking at System V semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: System V semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Semaphores in System V are not just a single counter as you might think, but
    rather a set of counters. This implies that a semaphore set can contain single
    or multiple counters (0 to n) with an identical semaphore ID. Each counter in
    the set can protect a shared resource, and a single semaphore set can protect
    multiple resources. The system call that helps create this kind of semaphore is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`key` is used to identify the semaphore. If the key value is `IPC_PRIVATE`,
    a new set of semaphores is created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nsems` indicates the semaphore set with the number of counters needed in the
    set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`semflg` dictates how the semaphore should be created. It can contain two values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPC_CREATE:` If the key does not exist, it creates a new semaphore'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPC_EXCL`: If the key exists, it throws an error and fails'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On success, the call returns the semaphore set identifier (a positive value).
  prefs: []
  type: TYPE_NORMAL
- en: 'A semaphore thus created contains uninitialized values and requires the initialization
    to be carried out using the `semctl()` function. After initialization, the semaphore
    set can be used by the processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `Semop()` function lets the process initiate operations on the semaphore
    set. This function offers a facility unique to the SysV semaphore implementation
    called **undoable operations** through a special flag called `SEM_UNDO`. When
    this flag is set, the kernel allows a semaphore to be restored to a consistent
    state if a process aborts before completing the relevant shared data access operation.
    For instance, consider a case where one of the processes locks the semaphore and
    begins its access operations on shared data; during this time if the process aborts
    before completion of shared data access, the semaphore will be left in an inconsistent
    state, making it unavailable for other contending processes. However, if the process
    had acquired a lock on the semaphore by setting the `SEM_UNDO` flag with `semop()`,
    its termination would allow the kernel to revert the semaphore to a consistent
    state (unlocked state) making it available for other contending processes in wait.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each SysV semaphore set is represented in the kernel by a descriptor of type
    `struct sem_array`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Each semaphore in the array is enumerated as an instance of `struct sem` defined
    in `<ipc/sem.c>`; the `*sem_base` pointer refers to the first semaphore object
    in the set. ;Each semaphore set contains a list of pending queue per process waiting;
    `pending_alter` is the head node for this pending queue of type `struct sem_queue`.
    Each semaphore set also contains per-semaphore undoable operations. `list_id`
    is a head node to a list of `struct sem_undo` instances; there is one instance
    in the list for each semaphore in the set. The following diagram sums up the semaphore
    set data structure and its lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: POSIX semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POSIX semaphore semantics are rather simple when compared to System V. Each
    semaphore is a simple counter that can never be less than zero. The implementation
    provides function interfaces for initialization, increment, and decrement operations.
    They can be used for synchronizing threads by allocating the semaphore instance
    in memory accessible to all the threads. They can also be used for synchronizing
    processes by placing the semaphore in shared memory. Linux implementation of POSIX
    semaphores is optimized to deliver better performance for non-contending synchronization
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'POSIX semaphores are available in two variants: named semaphores and unnamed
    semaphores. A named semaphore is identified by a filename and is suitable for
    use between unrelated processes. An unnamed semaphore is just a global instance
    of type `sem_t`; this form is generally preferred for use between threads. POSIX
    semaphore interface operations are part of the POSIX threads library implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function interfaces** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_open()` | Opens an existing named semaphore file or creates a new named
    semaphore and returns its descriptor |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_init()` | Initializer routine for an unnamed semaphore |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_post()` | Operation to increment semaphore |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_wait()` | Operation to decrement semaphore, blocks if invoked when semaphore
    value is zero |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_timedwait()` | Extends `sem_wait()` with a timeout parameter for bounded
    wait |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_getvalue()` | Returns the current value of the semaphore counter |'
  prefs: []
  type: TYPE_TB
- en: '| `sem_unlink()` | Removes a named semaphore identified with a file |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we touched on various IPC mechanisms offered by the kernel.
    We explored the layout and relationship between various data structures for each
    mechanism, and also looked at both SysV and POSIX IPC mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take this discussion further into locking and kernel-synchronization
    mechanisms.
  prefs: []
  type: TYPE_NORMAL

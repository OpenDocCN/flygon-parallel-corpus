- en: Kernel Synchronization and Locking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kernel address space is shared by all user-mode processes, which enables concurrent
    access to kernel services and data structures. For reliable functioning of the
    system, it is imperative that kernel services be implemented to be re-entrant.
    Kernel code paths accessing global data structures need to be synchronized to
    ensure consistency and validity of shared data. In this chapter, we will get into
    details of various resources at the disposal of kernel programmers for synchronization
    of kernel code paths and protection of shared data from concurrent access.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinlocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard mutexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait/wound mutex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semaphores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atomic operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A computation operation is considered to be **atomic** if it appears to the
    rest of the system to occur instantaneously. Atomicity guarantees indivisible
    and uninterruptible execution of the operation initiated. Most CPU instruction
    set architectures define instruction opcodes that can perform atomic read-modify-write
    operations on a memory location. These operations have a succeed-or-fail definition,
    that is, they either successfully change the state of the memory location or fail
    with no apparent effect. These operations are handy for manipulation of shared
    data atomically in a multi-threaded scenario. They also serve as foundational
    building blocks for implementation of exclusion locks, which are engaged to protect
    shared memory locations from concurrent access by parallel code paths.
  prefs: []
  type: TYPE_NORMAL
- en: Linux kernel code uses atomic operations for various use cases, such as reference
    counters in shared data structures *(*which are used to track concurrent access
    to various kernel data structures), wait-notify flags, and for enabling exclusive
    ownership of data structures to a specific code path. To ensure portability of
    kernel services that directly deal with atomic operations, the kernel provides
    a rich library of architecture-neutral interface macros and inline functions that
    serve as abstractions to processor-dependent atomic instructions. Relevant CPU-specific
    atomic instructions under these neutral interfaces are implemented by the architecture
    branch of the kernel code.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic integer operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generic atomic operation interfaces include support for integer and bitwise
    operations. Integer operations are implemented to operate on special kernel-defined
    types called `atomic_t` (32-bit integer) and `atomic64_t` (64-bit integer). Definitions
    for these types can be found in the generic kernel header `<linux/types.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation provides two groups of integer operations; one set applicable
    on 32 bit and the other group for 64 bit atomic variables. These interface operations
    are implemented as a set of macros and inline functions. Following is a summarized
    list of operations applicable on `atomic_t` type variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Interface macro/Inline function** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `ATOMIC_INIT(i)` | Macro to initialize an atomic counter |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_read(v)` | Read value of the atomic counter `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_set(v, i)` | Atomically set counter `v` to value specified in `i`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_add(int i, atomic_t *v)` | Atomically add `i` to counter `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_sub(int i, atomic_t *v)` | Atomically subtract `i` from counter `v`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_inc(atomic_t *v)` | Atomically increment counter `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_dec(atomic_t *v)` | Atomically decrement counter `v` |'
  prefs: []
  type: TYPE_TB
- en: 'Following is a list of functions that perform relevant **read-modify-write**
    (**RMW**) operations and return the result (that is, they return the value that
    was written to the memory address after the modification):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `bool atomic_sub_and_test(int i, atomic_t *v)` | Atomically subtracts `i`
    from `v` and returns `true` if the result is zero, or `false` otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| `bool atomic_dec_and_test(atomic_t *v)` | Atomically decrements `v` by 1
    and returns `true` if the result is 0, or `false` for all other cases |'
  prefs: []
  type: TYPE_TB
- en: '| `bool atomic_inc_and_test(atomic_t *v)` | Atomically adds `i` to `v` and
    returns `true` if the result is 0, or `false` for all other cases |'
  prefs: []
  type: TYPE_TB
- en: '| `bool atomic_add_negative(int i, atomic_t *v)` | Atomically adds `i` to `v`
    and returns `true` if the result is negative, or `false` when result is greater
    than or equal to zero |'
  prefs: []
  type: TYPE_TB
- en: '| `int atomic_add_return(int i, atomic_t *v)` | Atomically adds `i` to `v`
    and returns the result |'
  prefs: []
  type: TYPE_TB
- en: '| `int atomic_sub_return(int i, atomic_t *v)` | Atomically subtracts `i` from
    `v` and returns the result |'
  prefs: []
  type: TYPE_TB
- en: '| `int atomic_fetch_add(int i, atomic_t *v)` | Atomically adds `i` to `v` and
    return pre-addition value at `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `int atomic_fetch_sub(int i, atomic_t *v)` | Atomically subtracts `i` from
    `v`, and return pre-subtract value at `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `int atomic_cmpxchg(atomic_t *v, int old,` int new) | Reads the value at
    location `v`, and checks if it is equal to `old`*;* if `true`, swaps value at
    `v` with `*new*`, and always returns value read at `v` |'
  prefs: []
  type: TYPE_TB
- en: '| `int atomic_xchg(atomic_t *v, int new)` | Swaps the old value stored at location
    `v` with `new`, and returns old value `v` |'
  prefs: []
  type: TYPE_TB
- en: For all of these operations, 64-bit variants exist for use with `atomic64_t`;
    these functions have the naming convention `atomic64_*()`.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic bitwise operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kernel-provided generic atomic operation interfaces also include bitwise operations.
    Unlike integer operations, which are implemented to operate on the `atomic(64)_t`
    type, these bit operations can be applied on any memory location. The arguments
    to these operations are the position of the bit or bit number, and a pointer with
    a valid address. The bit range is 0-31 for 32-bit machines and 0-63 for 64-bit
    machines. Following is a summarized list of bitwise operations available:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation interface** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `set_bit(int nr, volatile unsigned long *addr)` | Atomically set the bit
    `nr` in location starting from `addr` |'
  prefs: []
  type: TYPE_TB
- en: '| `clear_bit(int nr, volatile unsigned long *addr)` | Atomically clear the
    bit `nr` in location starting from `addr` |'
  prefs: []
  type: TYPE_TB
- en: '| `change_bit(int nr, volatile unsigned long *addr)` | Atomically flip the
    bit `nr` in the location starting from `addr` |'
  prefs: []
  type: TYPE_TB
- en: '| `int test_and_set_bit(int nr, volatile unsigned long *addr)` | Atomically
    set the bit `nr` in the location starting from `addr`, and return old value at
    the `nr^(th)` bit |'
  prefs: []
  type: TYPE_TB
- en: '| `int test_and_clear_bit(int nr, volatile unsigned long *addr)` | Atomically
    clear the bit `nr` in the location starting from `addr`, and return old value
    at the `nr``^(th)` bit |'
  prefs: []
  type: TYPE_TB
- en: '| `int test_and_change_bit(int nr, volatile unsigned long *addr)` | Atomically
    flip the bit `nr` in the location starting from `addr`, and return old value at
    the `nr^(th)` bit |'
  prefs: []
  type: TYPE_TB
- en: For all the operations with a return type, the value returned is the old state
    of the bit that was read out of the memory address before the specified modification
    happened. Non-atomic versions of these operations also exist; they are efficient
    and useful for cases that might need bit manipulations, initiated from code statements
    in a mutually exclusive critical block. These are declared in the kernel header
    `<linux/bitops/non-atomic.h>`.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing exclusion locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hardware-specific atomic instructions can operate only on CPU word- and doubleword-size
    data; they cannot be directly applied on shared data structures of custom size.
    For most multi-threaded scenarios, often it can be observed that shared data is
    of custom sizes, for example, a structure with *n* elements of various types.
    Concurrent code paths accessing such data usually comprise a bunch of instructions
    that are programmed to access and manipulate shared data; such access operations
    must be executed *atomically* to prevent races. To ensure atomicity of such code
    blocks, mutual exclusion locks are used. All multi-threading environments provide
    implementation of exclusion locks that are based on exclusion protocols. These
    locking implementations are built on top of hardware-specific atomic instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux kernel implements operation interfaces for standard exclusion mechanisms
    such as mutual and reader-writer exclusions. It also contains support for various
    other contemporary lightweight and lock-free synchronization mechanisms. Most
    kernel data structures and other shared data elements such as shared buffers and
    device registers are protected from concurrent access through appropriate exclusion-locking
    interfaces offered by the kernel. In this section we will explore available exclusions
    and their implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: Spinlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Spinlocks** are one of the simplest and lightweight mutual exclusion mechanisms
    widely implemented by most concurrent programming environments. A spinlock implementation
    defines a lock structure and operations that manipulate the lock structure. The
    lock structure primarily hosts an atomic lock counter among other elements, and
    operations interfaces include:'
  prefs: []
  type: TYPE_NORMAL
- en: An **initializer routine**, that initializes a spinlock instance to the default
    (unlock) state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **lock routine**, that attempts to acquire spinlock by altering the state
    of the lock counter atomically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **unlock routine**, that releases the spinlock by altering counter into unlock
    state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a caller context attempts to acquire spinlock while it is locked (or held
    by another context), the lock function iteratively polls or spins for the lock
    until available, causing the caller context to hog the CPU until lock is acquired.
    It is due to this fact that this exclusion mechanism is aptly named spinlock.
    It is therefore advised to ensure that code within critical sections is atomic
    or non-blocking, so that lock can be held for a short, deterministic duration,
    as it is apparent that holding a spinlock for a long duration could prove disastrous.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, spinlocks are built around processor-specific atomic operations;
    the architecture branch of the kernel implements core spinlock operations (assembly
    programmed). The kernel wraps the architecture-specific implementation through
    a generic platform-neutral interface that is directly usable by kernel service;
    this enables portability of the service code which engages spinlocks for protection
    of shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generic spinlock interfaces can be found in the kernel header `<linux/spinlock.h>`
    while architecture-specific definitions are part of `<asm/spinlock.h>`. The generic
    interface provides a bunch of `lock()` and `unlock()` operations, each implemented
    for a specific use case. We will discuss each of these interfaces in the sections
    to follow; for now, let''s begin our discussion with the standard and most basic
    variants of `lock()` and `unlock()` operations offered by the interface. The following
    code sample shows the usage of a basic spinlock interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine the implementation of these functions under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Kernel code implements two variants of spinlock operations; one suitable for
    SMP platforms and the other for uniprocessor platforms. Spinlock data structure
    and operations related to the architecture and type of build (SMP and UP) are
    defined in various headers of the kernel source tree. Let''s familiarize ourselves
    with the role and importance of these headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<include/linux/spinlock.h>` contains generic spinlock/rwlock declarations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following headers are related to SMP platform builds:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<asm/spinlock_types.h>` contains `arch_spinlock_t/arch_rwlock_t` and initializers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock_types.h>` defines the generic type and initializers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<asm/spinlock.h>` contains the `arch_spin_*()` and similar low-level operation
    implementations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock_api_smp.h>` contains the prototypes for the `_spin_*()` APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock.h>` builds the final `spin_*()` APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following headers are related to uniprocessor (UP) platform builds:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<linux/spinlock_type_up.h>` contains the generic, simplified UP spinlock type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock_types.h>` defines the generic type and initializers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock_up.h>` contains the `arch_spin_*()` and similar version of
    UP builds (which are NOPs on non-debug, non-preempt builds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock_api_up.h>` builds the `_spin_*()` APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<linux/spinlock.h>` builds the final `spin_*()` APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generic kernel header `<linux/spinlock.h>` contains a conditional directive
    to decide on the appropriate (SMP or UP) API to pull.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `raw_spin_lock()` and `raw_spin_unlock()` macros dynamically expand to
    the appropriate version of spinlock operations based on the type of platform (SMP
    or UP) chosen in the build configuration. For SMP platforms, `raw_spin_lock()`
    expands to the `__raw_spin_lock()` operation implemented in the kernel source
    file `kernel/locking/spinlock.c`. Following is the locking operation code defined
    with a macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This routine is composed of nested loop constructs, an outer `for` loop construct,
    and an inner `while` loop that spins until the specified condition is satisfied.
    The first block of code in the outer loop attempts to acquire lock atomically
    by invoking the architecture-specific `##_trylock()` routine. Notice that this
    function is invoked with kernel preemption disabled on the local processor. If
    lock is acquired successfully, it breaks out of the loop construct and the call
    returns with preemption turned off. This ensures that the caller context holding
    the lock is not preemptable during execution of a critical section. This approach
    also ensures that no other context can contend for the same lock on the local
    CPU until the current owner releases it.
  prefs: []
  type: TYPE_NORMAL
- en: However, if it fails to acquire lock, preemption is enabled through the `preempt_enable()`
    call, and the caller context enters the inner loop. This loop is implemented through
    a conditional `while` that spins until lock is found to be available. Each iteration
    of the loop checks for lock, and when it detects that the lock is not available
    yet, it invokes an architecture-specific relax routine (which executes a CPU-specific
    nop instruction) before spinning again to check for lock. Recall that during this
    time preemption is enabled; this ensures that the caller context is preemptable
    and does not hog CPU for long duration, which can happen especially when lock
    is highly contended. It also allows the possibility of two or more threads scheduled
    on the same CPU to contend for the same lock, possibly by preempting each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a spinning context detects that lock is available through `raw_spin_can_lock()`,
    it breaks out of the `while` loop, causing the caller to iterate back to the beginning
    of the outer loop (`for` loop) where it again attempts to grab lock through `##_trylock()`
    by disabling preemption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the SMP variant, spinlock implementation for UP platforms is quite simple;
    in fact, the lock routine just disables kernel preemption and puts the caller
    into a critical section. This works since there is no possibility of another context
    to contend for the lock with preemption suspended.
  prefs: []
  type: TYPE_NORMAL
- en: Alternate spinlock APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard spinlock operations that we discussed so far are suitable for the protection
    of shared resources that are accessed only from the process context kernel path.
    However, there might be scenarios where a specific shared resource or data might
    be accessed from both the process and interrupt context code of a kernel service.
    For instance, think of a device driver service that might contain both process
    context and interrupt context routines, both programmed to access the shared driver
    buffer for execution of appropriate I/O operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s presume that a spinlock was engaged to protect the driver''s shared
    resource from concurrent access, and all routines of the driver service (both
    process and interrupt context) seeking access to the shared resource are programmed
    with appropriate critical sections using standard `spin_lock()` and `spin_unlock()`
    operations. This strategy would ensure protection of the shared resource by enforcing
    exclusion, but can cause a *hard lock condition* on the CPU at random times, due
    to *lock* contention by the interrupt path code on the same CPU where the *lock*
    was held by a process context path. To further understand this, let''s assume
    the following events occur in the same order:'
  prefs: []
  type: TYPE_NORMAL
- en: Process context routine of the driver acquires *lock (*using the standard `spin_lock()`
    call).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the critical section is in execution, an interrupt occurs and is driven
    to the local CPU, causing the process context routine to preempt and give away
    the CPU for interrupt handlers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interrupt context path of the driver (ISR) starts and tries to acquire *lock
    (*using the standard `spin_lock()` call*),* which then starts to spin for *lock*
    to be available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the duration of the ISR, the process context is preempted and can never
    resume execution, resulting in a *lock* that can never be released, and the CPU
    is hard locked with a spinning interrupt handler that never yields.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent such occurrences, the process context code needs to disable interrupts
    on the current processor while it takes the *lock.* This will ensure that an interrupt
    can never preempt the current context until the completion of the critical section
    and lock release*.* Note that interrupts can still occur but are routed to other
    available CPUs, on which the interrupt handler can spin until *lock* becomes available.
    The spinlock interface provides an alternate locking routine `spin_lock_irqsave()`,
    which disables interrupts on the current processor along with kernel preemption.
    The following snippet shows the routine''s underlying code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`local_irq_save()` is invoked to disable hard interrupts for the current processor;
    notice how on failure to acquire the lock, interrupts are enabled by calling `local_irq_restore()`.
    Note that a `lock` taken by the caller using `spin_lock_irqsave()` needs to be
    unlocked using `spin_lock_irqrestore()`, which enables both kernel preemption
    and interrupts for the current processor before releasing lock.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to hard interrupt handlers, it is also possible for soft interrupt context
    routines such as *softirqs, tasklets,* and other such *bottom halves* to contend
    for a *lock* held by the process context code on the same processor. This can
    be prevented by disabling the execution of *bottom halves* while acquiring *lock*
    in the process context. `spin_lock_bh()` is another variant of the locking routine
    that takes care of suspending the execution of interrupt context bottom halves
    on the local CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`local_bh_disable()` suspends bottom half execution for the local CPU. To release
    a *lock* acquired by `spin_lock_bh()`, the caller context will need to invoke
    `spin_unlock_bh()`, which releases spinlock and BH lock for the local CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a summarized list of the kernel spinlock API interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_lock_init()` | Initialize spinlock |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_lock()` | Acquire lock, spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_trylock()` | Attempt to acquire lock, returns error on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_lock_bh()` | Acquire lock by suspending BH routines on the local processor,
    spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_lock_irqsave()` | Acquire lock by suspending interrupts on the local
    processor by saving current interrupt state, spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_lock_irq()` | Acquire lock by suspending interrupts on the local processor,
    spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_unlock()` | Release the lock |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_unlock_bh()` | Release lock and enable bottom half for the local processor
    |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_unlock_irqrestore()` | Release lock and restore local interrupts to
    previous state |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_unlock_irq()` | Release lock and restore interrupts for the local processor
    |'
  prefs: []
  type: TYPE_TB
- en: '| `spin_is_locked()` | Return state of the lock, nonzero if lock is held or
    zero if lock is available |'
  prefs: []
  type: TYPE_TB
- en: Reader-writer spinlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spinlock implementation discussed until now protects shared data by enforcing
    standard mutual exclusion between concurrent code paths racing for shared data
    access. This form of exclusion is not suitable for the protection of shared data
    which is often read by concurrent code paths, with infrequent writers or updates.
    Reader-writer locks enforce exclusion between reader and writer paths; this allows
    concurrent readers to share lock and a reader task will need to wait for the lock
    while a writer owns the lock. Rw-locks enforce standard exclusion between concurrent
    writers, which is desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rw-locks are represented by `struct rwlock_t` declared in kernel header `<linux/rwlock_types.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: rwlocks can be initialized statically through the macro `DEFINE_RWLOCK(v_rwlock)`
    or dynamically at runtime through `rwlock_init(v_rwlock)`.
  prefs: []
  type: TYPE_NORMAL
- en: Reader code paths will need to invoke the `read_lock` routine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Writer code paths use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Both read and write lock routines spin when lock is contended. The interface
    also offers non-spinning versions of lock functions called `read_trylock()` and
    `write_trylock()`. It also offers interrupt-disabling versions of the locking
    calls, which are handy when either the read or write path happens to execute in
    interrupt or bottom-half context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a summarized list of interface operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `read_lock()` | Standard read lock interface, spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `read_trylock()` | Attempts to acquire lock, returns error if lock is unavailable
    |'
  prefs: []
  type: TYPE_TB
- en: '| `read_lock_bh()` | Attempts to acquire lock by suspending BH execution for
    the local CPU, spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `read_lock_irqsave()` | Attempts to acquire lock by suspending interrupts
    for the current CPU by saving current state of local interrupts, spins on contention
    |'
  prefs: []
  type: TYPE_TB
- en: '| `read_unlock()` | Releases read lock |'
  prefs: []
  type: TYPE_TB
- en: '| `read_unlock_irqrestore()` | Releases lock held and restores local interrupts
    to the previous state |'
  prefs: []
  type: TYPE_TB
- en: '| `read_unlock_bh()` | Releases read lock and enables BH on the local processor
    |'
  prefs: []
  type: TYPE_TB
- en: '| `write_lock()` | Standard write lock interface, spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `write_trylock()` | Attempts to acquire lock, returns error on contention
    |'
  prefs: []
  type: TYPE_TB
- en: '| `write_lock_bh()` | Attempts to acquire write lock by suspending bottom halves
    for the local CPU, spins on contention |'
  prefs: []
  type: TYPE_TB
- en: '| `wrtie_lock_irqsave()` | Attempts to acquire write lock by suspending interrupts
    for the local CPU by saving current state of local interrupts,. spins on contention
    |'
  prefs: []
  type: TYPE_TB
- en: '| `write_unlock()` | Releases write lock |'
  prefs: []
  type: TYPE_TB
- en: '| `write_unlock_irqrestore()` | Releases lock and restores local interrupts
    to the previous state |'
  prefs: []
  type: TYPE_TB
- en: '| `write_unlock_bh()` | Releases write lock and enables BH on the local processor
    |'
  prefs: []
  type: TYPE_TB
- en: Underlying calls for all of these operations are similar to that of spinlock
    implementations and can be found in headers specified in the aforementioned spinlock
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spinlocks by design are better suited for scenarios where *lock* is held for
    short, fixed intervals of time, since *busy-waiting* for an indefinite duration
    would have a dire impact on performance of the system. However, there are ample
    situations where a *lock* is held for longer, non-deterministic durations; **sleeping
    locks** are precisely designed to be engaged for such situations. Kernel mutexes
    are an implementation of sleeping locks: when a caller task attempts to acquire
    a mutex that is unavailable (already owned by another context), it is put into
    sleep and moved out into a wait queue, forcing a context switch allowing the CPU
    to run other productive tasks. When the mutex becomes available, the task in the
    wait queue is woken up and moved by the unlock path of the mutex, which can then
    attempt to *lock* the mutex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mutexes are represented by `struct mutex`, defined in `include/linux/mutex.h`
    and corresponding operations implemented in the source file `kernel/locking/mutex.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In its basic form, each mutex contains a 64-bit `atomic_long_t` counter (`owner`),
    which is used both for holding lock state, and to store a reference to the task
    structure of the current task owning the lock. Each mutex contains a wait-queue
    (`wait_list`), and a spin lock(`wait_lock`) that serializes access to `wait_list`.
  prefs: []
  type: TYPE_NORMAL
- en: The mutex API interface provides a set of macros and functions for initialization,
    lock, unlock, and to access the status of the mutex. These operation interfaces
    are defined in `<include/linux/mutex.h>`.
  prefs: []
  type: TYPE_NORMAL
- en: A mutex can be declared and initialized with the macro `DEFINE_MUTEX(name)`.
  prefs: []
  type: TYPE_NORMAL
- en: There is also an option of initializing a valid mutex dynamically through `mutex_init(mutex)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed earlier, on contention, lock operations put the caller thread
    into sleep, which requires the caller thread to be put into `TASK_INTERRUPTIBLE`,
    `TASK_UNINTERRUPTIBLE`, or `TASK_KILLABLE` states, before moving it into the mutex
    wait list. To support this, the mutex implementation offers two variants of lock
    operations, one for **uninterruptible** and other for **interruptible** sleep.
    Following is a list of standard mutex operations with a short description for
    each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Despite being possible blocking calls, mutex locking functions have been greatly
    optimized for performance. They are programmed to engage fast and slow path approaches
    while attempting lock acquisition. Let''s explore the code under the hood of the
    locking calls to better understand fast path and slow path. The following code
    excerpt is of the `mutex_lock()` routine from `<kernel/locking/mutex.c>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Lock acquisition is first attempted by invoking a non-blocking fast path call
    `__mutex_trylock_fast()`. If it fails to acquire lock through due to contention,
    it enters slow path by invoking `__mutex_lock_slowpath()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This function is programmed to acquire lock atomically if available. It invokes
    the `atomic_long_cmpxchg_acquire()` macro, which attempts to assign the current
    thread as the owner of the mutex; this operation will succeed if the mutex is
    available, in which case the function returns `true`. Should some other thread
    own the mutex, this function will fail and return `false`. On failure, the caller
    thread will enter the slow path routine.
  prefs: []
  type: TYPE_NORMAL
- en: Conventionally, the concept of slow path has always been to put the caller task
    into sleep while waiting for the lock to become available. However, with the advent
    of many-core CPUs, there is a growing need for scalability and improved performance,
    so with an objective to achieve scalability, the mutex slow path implementation
    has been reworked with an optimization called **optimistic spinning**, a.k.a.
    **midpath**, which can improve performance considerably*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The core idea of optimistic spinning is to push contending tasks into poll
    or spin instead of sleep when the mutex owner is found to be running. Once the
    mutex becomes available (which is expected to be sooner, since the owner is found
    to be running) it is assumed that a spinning task could always acquire it quicker
    as compared to a suspended or sleeping task in the mutex wait list. However, such
    spinning is only a possibility when there are no other higher-priority tasks in
    ready state. With this feature, spinning tasks are more likely to be cache-hot,
    resulting in deterministic execution that yields noticeable performance improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `__mutex_lock_common()` function contains a slow path implementation with
    optimistic spinning; this routine is invoked by all sleep variants of mutex locking
    functions with appropriate flags as argument. This function first attempts to
    acquire mutex through optimistic spinning implemented through cancellable mcs
    spinlocks (`osq` field in mutex structure) associated with the mutex. When the
    caller task fails to acquire mutex with optimistic spinning, as a last resort
    this function switches to conventional slow path, resulting in the caller task
    to be put into sleep and queued into the mutex `wait_list` until woken up by the
    unlock path.
  prefs: []
  type: TYPE_NORMAL
- en: Debug checks and validations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Incorrect use of mutex operations can cause deadlocks, failure of exclusion,
    and so on. To detect and prevent such possible occurrences, the mutex subsystem
    is equipped with appropriate checks or validations instrumented into mutex operations.
    These checks are by default disabled, and can be enabled by choosing the configuration
    option `CONFIG_DEBUG_MUTEXES=y` during kernel build.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a list of checks enforced by instrumented debug code:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutex can be owned by one task at a given point in time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutex can be released (unlocked) only by the valid owner, and an attempt to
    release mutex by a context that does not own the lock will fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursive locking or unlocking attempts will fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mutex can only be initialized via the initializer call, and any attempt to
    *memset* mutex will never succeed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A caller task may not exit with a mutex lock held
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic memory areas where held locks reside must not be freed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mutex can be initialized once, and any attempt to re-initialize an already
    initialized mutex will fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutexes may not be used in hard/soft interrupt context routines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deadlocks can trigger due to many reasons, such as the execution pattern of
    the kernel code and careless usage of locking calls. For instance, let's consider
    a situation where concurrent code paths need to take ownership of *L[1]* and *L[2]*
    locks by nesting the locking functions. It must be ensured that all the kernel
    functions that require these locks are programmed to acquire them in the same
    order. When such ordering is not strictly imposed, there is always a possibility
    of two different functions trying to lock *L1* and *L2* in opposite order, which
    could trigger lock inversion deadlock, when these functions execute concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel lock validator infrastructure has been implemented to check and
    prove that none of the locking patterns observed during kernel runtime could ever
    cause deadlock. This infrastructure prints data pertaining to locking pattern
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Point-of-acquire tracking, symbolic lookup of function names, and list of all
    locks held in the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Owner tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection of self-recursing locks and printing out all relevant info
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection of lock inversion deadlocks and printing out all affected locks and
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lock validator can be enabled by choosing `CONFIG_PROVE_LOCKING=y` during
    kernel build.
  prefs: []
  type: TYPE_NORMAL
- en: Wait/wound mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the earlier section, unordered nested locking in the kernel
    functions could pose a risk of lock-inversion deadlocks, and kernel developers
    avoid this by defining rules for nested lock ordering and perform runtime checks
    through the lock validator infrastructure. Yet, there are situations where lock
    ordering is dynamic, and nested locking calls cannot be hardcoded or imposed as
    per preconceived rules.
  prefs: []
  type: TYPE_NORMAL
- en: One such use case is to do with GPU buffers; these buffers are to be owned and
    accessed by various system entities such as GPU hardware, GPU driver, user-mode
    applications, and other video-related drivers. User mode contexts can submit the
    dma buffers for processing in an arbitrary order, and the GPU hardware may process
    them at arbitrary times. If locking is used to control the ownership of the buffers,
    and if multiple buffers must be manipulated at the same time, deadlocks cannot
    be avoided. Wait/wound mutexes are designed to facilitate dynamic ordering of
    nested locks, without causing lock-inversion deadlocks. This is achieved by forcing
    the context in contention to *wound*, meaning forcing it to release the holding
    lock.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s presume two buffers, each protected with a lock, and further
    consider two threads, say `T[1]` and `T`[`2`,] seek ownership of the buffers by
    attempting locks in opposite order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Execution of `T[1]` and `T[2]` concurrently might result in each thread waiting
    for the lock held by the other, causing deadlock. Wait/wound mutex prevents this
    by letting the *thread that grabbed the lock first* to remain in sleep, waiting
    for nested lock to be available. The other thread is *wound*, causing it to release
    its holding lock and start over again. Suppose `T[1]` got to lock on `bufA` before
    `T[2]` could acquire lock on `bufB`. `T[1]` would be considered as the thread
    that *got there first* and is put to sleep for lock on `bufB`, and `T[2]` would
    be wound, causing it to release lock on `bufB` and start all over. This avoids
    deadlock and `T[2]` would start all over when `T[1]` releases locks held.
  prefs: []
  type: TYPE_NORMAL
- en: 'Operation interfaces:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wait/wound mutexes are represented through `struct ww_mutex` defined in the
    header `<linux/ww_mutex.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first step to use wait/wound mutex is to define a *class,* which is a mechanism
    to represent a group of locks. When concurrent tasks contend for the same locks,
    they must do so by specifying this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'A class can be defined using a macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Each class declared is an instance of type `struct ww_class` and contains an
    atomic counter `stamp`, which is used to hold a sequence number that records which
    one of the contending tasks *got there first*. Other fields are used by the kernel's
    lock validator to verify correct usage of the wait/wound mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Each contending thread must invoke `ww_acquire_init()` before attempting nested
    locking calls. This sets up the context by assigning a sequence number to track
    locks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the context is set up and initialized, tasks can begin acquiring locks
    with either `ww_mutex_lock()` or `ww_mutex_lock_interruptible()` calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When a task grabs all nested locks (using any of these locking routines) associated
    with a class, it needs to notify acquisition of ownership using the function `ww_acquire_done()`.
    This call marks the end of the acquisition phase, and the task can proceed to
    process shared data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When a task completes its processing of shared data, it can begin releasing
    all of the locks held, with calls to the `ww_mutex_unlock(`) routine. Once all
    of the locks are released, the *context* must be released with a call to `ww_acquire_fini()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until early versions of 2.6 kernel releases, semaphores were the primary form
    of sleep locks. A typical semaphore implementation comprises a counter, wait queue,
    and set of operations that can increment/decrement the counter atomically.
  prefs: []
  type: TYPE_NORMAL
- en: When a semaphore is used to protect a shared resource, its counter is initialized
    to a number greater than zero, which is considered to be unlocked state. A task
    seeking access to a shared resource begins by invoking the decrement operation
    on the semaphore. This call checks the semaphore counter; if it is found to be
    greater than zero, the counter is decremented and the function returns success.
    However, if the counter is found to be zero, the decrement operation puts the
    caller task to sleep until the counter is found to have increased to a number
    greater than zero.
  prefs: []
  type: TYPE_NORMAL
- en: This simple design offers great flexibility, which allows adaptability and application
    of semaphores for different situations. For instance, for cases where a resource
    needs to be accessible to a specific number of tasks at any point in time, the
    semaphore count can be initialized to the number of tasks that require access,
    say 10, which allows a maximum of 10 tasks access to shared resource at any time.
    For yet other cases, such as a number of tasks that require mutually exclusive
    access to a shared resource, the semaphore count can be initialized to 1, resulting
    in a maximum of one task to access the resource at any given point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semaphore structure and its interface operations are declared in the kernel
    header `<include/linux/semaphore.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Spinlock (the `lock` field) serves as a protection for `count`, that is, semaphore
    operations (inc/dec) are programmed to acquire `lock` before manipulating `count`.
    `wait_list` is used to queue tasks to sleep while they wait for the semaphore
    count to increase beyond zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semaphores can be declared and initialized to 1 through a macro: `DEFINE_SEMAPHORE(s)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A semaphore can also be initialized dynamically to any positive number through
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is a list of operation interfaces with a brief description of each.
    Routines with naming convention `down_xxx()` attempt to decrement the semaphore,
    and are possible blocking calls (except `down_trylock()`), while routine `up()`
    increments the semaphore and always succeeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Unlike mutex implementation, semaphore operations do not support debug checks
    or validations; this constraint is due to their inherent generic design which
    allows them to be used as exclusion locks, event notification counters, and so
    on. Ever since mutexes made their way into the kernel (2.6.16), semaphores are
    no longer the preferred choice for exclusion, and the use of semaphores as locks
    has considerably reduced, and for other purposes, the kernel has alternate interfaces.
    Most of the kernel code using semaphores has be converted into mutexes with a
    few minor exceptions. Yet semaphores still exist and are likely to remain at least
    until all of the kernel code using them is converted to mutex or other suitable
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Reader-writer semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This interface is an implementation of sleeping reader-writer exclusion, which
    serves as an alternative for spinning ones. Reader-writer semaphores are represented
    by `struct rw_semaphore`, declared in the kernel header `<linux/rwsem.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This structure is identical to that of a mutex, and is designed to support optimistic
    spinning with `osq`; it also includes debug support through the kernel's *lockdep*.
    `Count` serves as an exclusion counter, which is set to 1, allowing a maximum
    of one writer to own the lock at a point in time. This works since mutual exclusion
    is only enforced between contending writers, and any number of readers can concurrently
    share the read lock. `wait_lock` is a spinlock which protects the semaphore `wait_list`.
  prefs: []
  type: TYPE_NORMAL
- en: An `rw_semaphore` can be instantiated and initialized statically through `DECLARE_RWSEM(name)`,
    and alternatively, it can be dynamically initialized through `init_rwsem(sem)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the case of rw-spinlocks, this interface too offers distinct routines
    for lock acquisition in reader and writer paths. Following is a list of interface
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: These operations are implemented in the source file `<kernel/locking/rwsem.c>`;
    the code is quite self explanatory and we will not discuss it any further.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conventional reader-writer locks are designed with reader priority, and they
    might cause a writer task to wait for a non-deterministic duration, which might
    not be suitable on shared data with time-sensitive updates. This is where sequential
    lock comes in handy, as it aims at providing a quick and lock-free access to shared
    resources. Sequential locks are best when the resource that needs to be protected
    is small and simple, with write access being quick and non-frequent, as internally
    sequential locks fall back on the spinlock primitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential locks introduce a special counter that is incremented every time
    a writer acquires a sequential lock along with a spinlock. After the writer completes,
    it releases the spinlock and increments the counter again and opens the access
    for other writers. For read, there are two types of readers: sequence readers
    and locking readers. The **sequence reader** checks for the counter before it
    enters the critical section and then checks again at the end of it without blocking
    any writer. If the counter remains the same, it implies that no writer had accessed
    the section during read, but if there is an increment of the counter at the end
    of the section, it is an indication that a writer had accessed, which calls for
    the reader to re-read the critical section for updated data. A **locking reader**,
    as the name implies, will get a lock and block other readers and writers when
    it is in progress; it will also wait when another locking reader or writer is
    in progress.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequence lock is represented by the following type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can initialize a sequence lock statically using the following macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Actual initialization is done using the `__SEQLOCK_UNLOCKED(x)`, which is defined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To dynamically initialize sequence lock, we need to use the `seqlock_init`
    macro, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linux provides many APIs for using sequence locks, which are defined in `</linux/seqlock.h>`.
    Some of the important ones are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following two functions are used for reading by starting and finalizing
    a read section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Completion locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Completion locks** are an efficient way to achieve code synchronization if
    you need one or multiple threads of execution to wait for completion of some event,
    such as waiting for another process to reach a point or state. Completion locks
    may be preferred over a semaphore for a couple of reasons: multiple threads of
    execution can wait for a completion, and using `complete_all()`, they can all
    be released at once. This is way better than a semaphore waking up to multiple
    threads. Secondly, semaphores can lead to race conditions if a waiting thread
    deallocates the synchronization object; this problem doesnt exist when using
    completion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Completion can be used by including `<linux/completion.h>` and by creating
    a variable of type `struct completion`, which is an opaque structure for maintaining
    the state of completion. It uses a FIFO to queue the threads waiting for the completion
    event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Completion basically consists of initializing the completion structure, waiting
    through any of the variants of `wait_for_completion()` call, and finally signalling
    the completion through `complete()` or the `complete_all()` call. There are also
    functions to check the state of completions during its lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following macro can be used for static declaration and initialization of
    a completion structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following inline function will initialize a dynamically created completion
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following inline function will be used to reinitialize a completion structure
    if you need to reuse it. This can be used after `complete_all()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Waiting for completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If any thread needs to wait for a task to complete, it will call `wait_for_completion()`
    on the initialized completion structure. If the `wait_for_completion` operation
    happens after the call to `complete()` or `complete_all()`, the thread simply
    continues, as the reason it wanted to wait for has been satisfied; else, it waits
    till `complete()` is signalled. There are variants available for the `wait_for_completion()`
    calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Signalling completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The execution thread that wants to signal the completion of the intended task
    calls `complete()` to a waiting thread so that it can continue. Threads will be
    awakened in the same order in which they were queued. In the case of multiple
    waiters, it calls `complete_all()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we not only understood the various protection and synchronization
    mechanisms provided by the kernel, but also made an underlying attempt at appreciating
    the effectiveness of these options, with their varied functionalities and shortcomings.
    Our takeaway from this chapter has to be the tenacity with which the kernel addresses
    these varying complexities for providing protection and synchronization of data.
    Another notable fact remains in the way the kernel maintains ease of coding along
    with design panache when tackling these issues.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will look at another crucial aspect of how interrupts
    are handled by the kernel.
  prefs: []
  type: TYPE_NORMAL

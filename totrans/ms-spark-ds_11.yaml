- en: Chapter 11. Anomaly Detection on Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we look back at the year 2016, we will surely remember it as a time of
    many significant geo-political events ranging from Brexit, Great Britain''s vote
    to leave the European Union, to the untimely passing of many beloved celebrities,
    including the sudden death of the singer David Bowie (covered in [Chapter 6](ch06.xhtml
    "Chapter 6. Scraping Link-Based External Data"), *Scraping Link-Based External
    Data* and [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*). However, perhaps the most notable occurrence of the year was the
    tense US presidential election and its eventual outcome, the election of President
    Donald Trump. A campaign that will long be remembered, not least for its unprecedented
    use of social media, and the stirring up of passion among its users, most of whom
    made their feelings known through the use of hashtags: either positive ones, such
    as *#MakeAmericaGreatAgain* or *#StrongerTogether*, or conversely negative ones,
    such as *#DumpTrump* or *#LockHerUp*. Since this chapter is about sentiment analysis,
    the election presents the ideal use case. However, instead of trying to predict
    the outcome itself, we will aim to detect abnormal tweets during the US election
    using a real-time Twitter feed. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring Twitter data in real-time and batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting sentiment using Stanford NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing sentiment time series in *Timely*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving features from only 140 characters using *Word2Vec*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the concepts of graph *ergodicity* and *shortest paths*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a KMeans model to detect potential anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing models with *Embedding Projector* from *TensorFlow*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the US elections on Twitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On November 8, 2016, American citizens went in millions to polling stations
    to cast their votes for the next President of the United States. Counting began
    almost immediately and, although not officially confirmed until sometime later,
    the forecasted result was well known by the next morning. Let's start our investigation
    a couple of days before the major event itself, on November 6, 2016, so that we
    can preserve some context in the run-up. Although we do not exactly know what
    we will find in advance, we know that *Twitter* will play an oversized role in
    the political commentary given its influence in the build-up, and it makes sense
    to start collecting data as soon as possible. In fact, data scientists may sometimes
    experience this as a *gut feeling* - a strange and often exciting notion that
    compels us to commence working on something without a clear plan or absolute justification,
    just a sense that it will pay off. And actually, this approach can be vital since,
    given the normal time required to formulate and realize such a plan and the transient
    nature of events, a major news event may occur (refer to [Chapter 10](ch10.xhtml
    "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication and Mutation*),
    a new product may have been released, or the stock market may be trending differently
    (see [Chapter 12](ch12.xhtml "Chapter 12. TrendCalculus"), *TrendCalculus*); by
    this time, the original dataset may no longer be available
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring data in stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first action is to start acquiring Twitter data. As we plan to download
    more than 48 hours worth of tweets, the code should be robust enough to not fail
    somewhere in the middle of the process; there is nothing more frustrating than
    a fatal `NullPointerException` occurring after many hours of intense processing.
    We know we will be working on sentiment analysis at some point down the line,
    but for now we do not wish to over-complicate our code with large dependencies
    as this can decrease stability and lead to more unchecked exceptions. Instead,
    we will start by collecting and storing the data and subsequent processing will
    be done offline on the collected data, rather than applying this logic to the
    live stream.
  prefs: []
  type: TYPE_NORMAL
- en: We create a new Streaming context reading from Twitter 1% firehose using the
    utility methods created in [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary
    and Real-Time Tagging System") *, News Dictionary and Real-Time Tagging System*.
    We also use the excellent GSON library to serialize Java class `Status` (Java
    class embedding Twitter4J records) to JSON objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We read Twitter data every 5 minutes and have a choice to optionally supply
    Twitter filters as command line arguments. Filters can be keywords such as ***Trump***
    , ***Clinton* **or ***#MAGA*** , ***#StrongerTogether*** . However, we must bear
    in mind that by doing this we may not capture all relevant tweets as we can never
    be fully up to date with the latest hashtag trends (such as ***#DumpTrump*** ,
    ***#DrainTheSwamp*** , ***#LockHerUp*** , or *#LoveTrumpsHate*) and many tweets
    will be overlooked with an inadequate filter, so we will use an empty filter list
    to ensure that we catch everything.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We serialize our `Status` class using the GSON library and persist our JSON
    objects in HDFS. Note that the serialization occurs within a `Try` clause to ensure
    that unwanted exceptions are not thrown. Instead, we return JSON as an optional
    `String`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we run our Spark Streaming context and keep it alive until a new president
    has been elected, no matter what happens!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Acquiring data in batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only 1% of tweets are retrieved through the Spark Streaming API, meaning that
    99% of records will be discarded. Although able to download around 10 million
    tweets, we can potentially download more data, but this time only for a selected
    hashtag and within a small period of time. For example, we can download all tweets
    related to the ***#LockHerUp*** or ***#BuildTheWall*** hashtags.
  prefs: []
  type: TYPE_NORMAL
- en: The search API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For that purpose, we consume Twitter historical data through the `twitter4j`
    Java API. This library comes as a transitive dependency of `spark-streaming-twitter_2.11`.
    To use it outside of a Spark project, the following maven dependency should be
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a Twitter4J client as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we consume the `/search/tweets` service through the `Query` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we get a list of `Status` objects that can easily be serialized using
    the GSON library introduced earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Twitter is a fantastic resource for data science, but it is far from a non-profit
    organization, and as such, they know how to value and price data. Without any
    special agreement, the search API is limited to a few days retrospective, a maximum
    of 180 queries per 15 minute window and 450 records per query. This limit can
    be confirmed on both the Twitter DEV website ([https://dev.twitter.com/rest/public/rate-limits](https://dev.twitter.com/rest/public/rate-limits))
    and from the API itself using the `RateLimitStatus` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Unsurprisingly, any queries on popular terms, such as ***#MAGA*** on November
    9, 2016, hit this threshold. To avoid a rate limit exception, we have to page
    and throttle our download requests by keeping track of the maximum number of tweet
    IDs processed and monitor our status limit after each search request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With around half a billion tweets a day, it will be optimistic, if not Naive,
    to gather all US-related data. Instead, the simple ingest process detailed earlier
    should be used to intercept tweets matching specific queries only. Packaged as
    main class in an assembly jar, it can be executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `twitter.properties` file contains your Twitter API keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Analysing sentiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After 4 days of intense processing, we extracted around 10 million tweets; representing
    approximately 30 GB worth of JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: Massaging Twitter data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key reasons Twitter became so popular is that any message has to
    fit into a maximum of 140 characters. The drawback is also that every message
    has to fit into a maximum of 140 characters! Hence, the result is massive increase
    in the use of abbreviations, acronyms, slang words, emoticons, and hashtags. In
    this case, the main emotion may no longer come from the text itself, but rather
    from the emoticons used ([http://dl.acm.org/citation.cfm?id=1628969](http://dl.acm.org/citation.cfm?id=1628969)),
    though some studies showed that the emoticons may sometimes lead to inadequate
    predictions in sentiment ([https://arxiv.org/pdf/1511.02556.pdf](https://arxiv.org/pdf/1511.02556.pdf)).
    Emojis are even broader than emoticons as they include pictures of animals, transportation,
    business icons, and so on. Also, while emoticons can easily be retrieved through
    simple regular expressions, emojis are usually encoded in Unicode and are more
    difficult to extract without a dedicated library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `Emoji4J` library is easy to use (although computationally expensive) and
    given some text with emojis/emoticons, we can either `codify` - replace Unicode
    values with actual code names - or `clean` - simply remove any emojis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Massaging Twitter data](img/B05261_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Emoji parsing'
  prefs: []
  type: TYPE_NORMAL
- en: 'So firstly, let''s clean our text from any junk (special characters, emojis,
    accents, URLs, and so on) to access plain English content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also codify and extract all emojis and emoticons and keep them aside
    as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Writing these methods inside an *implicit class* means that they can be applied
    directly a String through a simple import statement.
  prefs: []
  type: TYPE_NORMAL
- en: '![Massaging Twitter data](img/image_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Twitter parsing'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Stanford NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next step is to pass our cleaned text through a *Sentiment Annotator*.
    We use the Stanford NLP library for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a Stanford `annotator` that tokenizes content into sentences (`tokenize`),
    splits sentences (`ssplit`), tags elements (`pos`), and lemmatizes each word (`lemma`)
    before analyzing the overall sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Any word is replaced by its most basic form, that is, *you're* is replaced with
    *you be* and *aren't you doing* replaced with *be not you do*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A sentiment spans from *Very Negative* (0.0) to *Very Positive* (4.0) and is
    averaged per sentence. As we do not get more than 1 or 2 sentences per tweet,
    we expect a very small variance; most of the tweets should be *Neutral* (around
    2.0), with only extremes to be scored (below ~1.5 or above ~2.5).
  prefs: []
  type: TYPE_NORMAL
- en: Building the Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each of our Twitter records (stored as JSON objects), we do the following
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Parse the JSON object using `json4s` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the location and map it to a US state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract emojis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatize text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze sentiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then wrap all these values into the following `Tweet` case class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in previous chapters, creating a new NLP instance wouldn''t scale
    for each record out of our dataset of 10 million records. Instead, we create only
    one `annotator` per `Iterator` (which means one per partition):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Using Timely as a time series database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are able to transform raw information into a clean series of Twitter
    sentiment with parameters such as hashtags, emojis, or US states, such a time
    series should be stored reliably and made available for fast query lookups.
  prefs: []
  type: TYPE_NORMAL
- en: In the Hadoop ecosystem, *OpenTSDB* ([http://opentsdb.net/](http://opentsdb.net/))
    is the default database for storing millions of chronological data points. However,
    instead of using the obvious candidate, we will introduce one you may not have
    come across before, called *Timely* ([https://nationalsecurityagency.github.io/timely/](https://nationalsecurityagency.github.io/timely/)).
    Timely is a recently open sourced project started by the **National Security Agency**
    (**NSA**), as a clone of OpenTSDB, which uses Accumulo instead of HBase for its
    underlying storage. As you may recall, Accumulo supports cell-level security,
    and we will see this later on.
  prefs: []
  type: TYPE_NORMAL
- en: Storing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each record is composed of a metric name (for example, hashtag), timestamp,
    metric value (for example, sentiment), an associated set of tags (for example,
    state), and a cell visibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For this exercise, we will filter out data for tweets only mentioning Trump
    or Clinton:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build a `Metric` object with names `io.gzet.state.clinton` and `io.gzet.state.trump`
    and an associated visibility. For the purpose of this exercise, we will assume
    that a junior analyst without the `SECRET` permission will not be granted access
    to highly negative tweets. This allows us to demonstrate Accumulo''s excellent
    cell-level security:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, we will also need to handle *duplicate records*. In the event
    where multiple tweets are received at the exact same time (with potentially different
    sentiments), they will override an existing cell on Accumulo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We insert data either from a `POST` request or simply by piping data through
    an opened socket back to the Timely server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Our data is now securely stored in Accumulo and available to anyone with the
    correct access permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created a series of input formats to retrieve Timely data back into
    a Spark job. This will not be covered here but can be found in our GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, Timely is still under active development and, as such,
    does not yet have a clean input/output format that can be used from Spark/MapReduce.
    The only ways to send data are via HTTP or Telnet.
  prefs: []
  type: TYPE_NORMAL
- en: Using Grafana to visualize sentiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Timely does not come with a visualization tool as such. However, it does integrate
    well, and securely, with *Grafana* ([https://grafana.net/](https://grafana.net/))
    using the timely-grafana plugin. More information can be found on the Timely website.
  prefs: []
  type: TYPE_NORMAL
- en: Number of processed tweets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a first simple visualization, we display the number of tweets for both the
    candidates on November 8 and 9, 2016 (UTC):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Number of processed tweets](img/B05261_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Timely-processed tweets'
  prefs: []
  type: TYPE_NORMAL
- en: We observe more and more tweets related to Trump as the results of the election
    are published. On average, we observe around 6 times more Trump-related tweets
    than Clinton-related tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Give me my Twitter account back
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A quick study of the sentiment shows that it's relatively negative (1.3 on an
    average) and there's no significant difference between the tweets of both the
    candidates that would have helped predict the outcome of the US election.
  prefs: []
  type: TYPE_NORMAL
- en: '![Give me my Twitter account back](img/B05261_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Timely-timeseries'
  prefs: []
  type: TYPE_NORMAL
- en: However, on closer inspection, we find a truly interesting phenomenon. On November
    8, 2016, around 1pm GMT (8am EST, that is, when the first polling stations opened
    in New York), we observe a massive drop-off in the *sentiment variance*. An oddity,
    seen in the preceding figure, which can't be completely explained. We can speculate
    that either the first vote cast officially marked the end of the turbulent presidential
    campaign and was the starting point of a retrospective period after the election
    - perhaps, a more *fact-based* dialog than before - or maybe Trump's advisors
    taking away his Twitter account really was their greatest idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we give an example of the versatility of Accumulo security by logging into
    Grafana as another user, this time with no `SECRET` authorization granted. As
    expected, in the proceeding image , the sentiment looks much more positive (as
    extremely negative sentiment is hidden), hence confirming the visibility settings
    on Timely; the elegance of Accumulo speaks for itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Give me my Twitter account back](img/image_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Timely-timeseries for non-SECRET'
  prefs: []
  type: TYPE_NORMAL
- en: An example of how to create an Accumulo user can be found in [Chapter 7](ch07.xhtml
    "Chapter 7. Building Communities"), *Building Communities*.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the swing states
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last interesting feature we will leverage from Timely and Grafana is tree
    map aggregations. As all the US states'' names are stored as part of the metric
    attributes, we will create a simple tree map for both the candidates. The size
    of each box corresponds to the number of observations, while the color is relative
    to the observed sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying the swing states](img/image_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Timely - Tree map of the US states for Hillary Clinton'
  prefs: []
  type: TYPE_NORMAL
- en: When we used the 2-day sentiment average previously, we couldn't differentiate
    between the republican and democrat states as the sentiment was statistically
    flat and relatively bad (1.3 on average). However, if we consider only the day
    prior to the election, then it seems much more interesting because we observed
    much more variance in our sentiment data. In the preceding image, we see Florida,
    North Carolina, and Pennsylvania - 3 of the 12 swing states-showing unexpectedly
    bad sentiment for Hillary Clinton. Could this pattern be an early indicator of
    the election outcome?
  prefs: []
  type: TYPE_NORMAL
- en: Twitter and the Godwin point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our text content properly cleaned up, we can feed a *Word2Vec* algorithm
    and attempt to understand the words in their actual *context*.
  prefs: []
  type: TYPE_NORMAL
- en: Learning context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As it says on the tin, the *Word2Ve*c algorithm transforms a word into a vector.
    The idea is that similar words will be embedded into similar vector spaces and,
    as such, will look close to one another contextually.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about `Word2Vec` algorithm can be found at [https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Well integrated into Spark, a `Word2Vec` model can be trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here we extract each tweet as a sequence of words, only keeping records with
    at least `4` distinct words. Note that the list of all words needs to fit in memory
    as it is collected back to the driver as a map of word and vector (as an array
    of float). The vector size and learning rate can be tuned through the `setVectorSize`
    and `setLearningRate` methods respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use a Zeppelin notebook to interact with our model, sending different
    words and asking the model to obtain the closest synonyms. The results are quite
    impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'While hashtags generally pass through standard NLP unnoticed, they do have
    a major contribution to make to tone and emotion. A tweet marked as neutral can
    be, in fact, much worse than it sounds using hashtags like *#HillaryForPrison*
    or ***#LockHerUp*** . So, let''s attempt to take this into account using an interesting
    feature called *word-vector association*. A common example of this association
    given by the original *Word2Vec* algorithm is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be translated as the following vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The nearest point should therefore be `[WOMEN]`. Technically speaking, this
    can be translated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Saving/retrieving this model can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing our model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our vectors are 100 dimensions wide, they are difficult to represent in a
    graph using traditional methods. However, you may have come across the *Tensor
    Flow* project and its recently open sourced *Embedding Projector* ([http://projector.tensorflow.org/](http://projector.tensorflow.org/)).
    This project offers a nice way to visualize our models due to its ability to quickly
    render high-dimensional data. It's easy to use as well - we simply export our
    vectors as tab-separated data points, load them into a web browser, and voila!
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing our model](img/image_11_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Embedding project, neighbours of Computer'
  prefs: []
  type: TYPE_NORMAL
- en: '*Embedding Projector* projects high-dimensional vector onto 3D space, where
    each dimension represents one of the first three **principal components** (**PCA**).
    We can also build our own projection where we basically stretch our vectors toward
    four specific directions. In the following representation, we stretch our vectors
    left, right, up, and down to [`Trump`], [`Clinton`], [`Love`], and [`Hate`]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing our model](img/B05261_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Embedding project, custom projection'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a greatly simplified vector space, we can more easily understand
    each word and how it relates to its neighbors (`democrat` versus `republican`
    and `love` versus `hate`). For example, with the French election coming up next
    year, we see that France is closer to Trump than it is to Clinton. Could this
    be seen as an early indicator of the upcoming election?
  prefs: []
  type: TYPE_NORMAL
- en: Word2Graph and Godwin point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You don''t have to play around with the Twitter *Word2Vec* model for very long
    before you come across sensitive terms and references to World War II. In fact,
    this is an occurrence that was originally asserted by Mike Godwin in 1990 as Godwin''s
    Law ([https://www.wired.com/1994/10/godwin-if-2/](https://www.wired.com/1994/10/godwin-if-2/)),
    which states as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*As an online discussion grows longer, the probability of a comparison involving
    Nazis or Hitler approaches 1*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As of 2012, it is even part of the Oxford English Dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Word2Graph and Godwin point](img/image_11_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The Godwin law'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Word2Graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although more of a rhetorical device than an actual mathematical law, Godwin''s
    Law remains a fascinating anomaly and seems to be relevant to the US election.
    Naturally, we will decide to explore the idea further using the graph theory.
    The first step is to broadcast our model back to the executors and parallelize
    our list of words. For each word, we output the top five synonyms and build an
    `Edge` object with word similarity as edge weight. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To prove Godwin''s law, we will have to prove that no matter the input node,
    we can always find a path from that node to the *Godwin point*. In mathematical
    terms, this assumes the graph to be *ergodic*. With more than one connected component,
    our graph cannot be ergodic as some nodes will never lead to the Godwin point.
    Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As we only have one connected component, the next step is to compute the shortest
    path for each node to that Godwin point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The shortest path algorithm is quite simple and can be easily implemented with
    *Pregel* using the same techniques described in [Chapter 7](ch07.xhtml "Chapter 7. Building
    Communities"), *Building Communities*. The basic approach is to start Pregel on
    the target node (our Godwin point) and send a message back to its incoming edges,
    incrementing a counter at each hop. Each node will always keep the smallest possible
    counter and propagate this value downstream to its incoming edges. The algorithm
    stops when no further edge is found.
  prefs: []
  type: TYPE_NORMAL
- en: 'We normalize this distance using a Godwin depth of 16, calculated as the maximum
    of each of the shortest paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows a depth of 4 - we normalize the scores of 0, 1,
    2, 3, and 4 to **0.0**, **0.25**, **0.5**, **0.75**, and **1.0** respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a Word2Graph](img/image_11_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The normalized Godwin distance'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we collect each vertex with its associated distance as a map. We can
    easily sort this collection from the most to the least-sensitive word, but we
    will not report our findings here (for obvious reasons!).
  prefs: []
  type: TYPE_NORMAL
- en: On November 7 and 8, 2016, this map contained all the words from our Twitter
    dictionary, implying a full ergodicity. According to Godwin's Law, any word, given
    enough time, can lead to the Godwin point. We will use this map later in the chapter
    when we build features from Twitter text content.
  prefs: []
  type: TYPE_NORMAL
- en: Random walks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One way to simulate random walks through the *Word2Vec* algorithm is to treat
    the graph as a series of **Markov chains**. Assuming *N* random walks and a transition
    matrix *T*, we compute the transition matrix *T^N*. Given a state, *S[1]* (meaning
    a word *w[1]*), we extract the probability distribution to jump from *S[1]* to
    an *S[N]* state in *N* given transitions. In practice, given a dictionary of ~100k
    words, a dense representation of such a transition matrix will require around
    50 GB to fit in memory. We can easily build a sparse representation of *T* using
    the `IndexedRowMatrix` class from MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, there is no built-in method in Spark to perform matrix multiplication
    with sparse support. Therefore, the m2 matrix needs to be dense and must fit in
    memory. A solution can be to decompose this matrix (using SVD) and play with the
    symmetric property of the word2vec matrix (if word *w[1]* is a synonym to *w[2]*,
    then *w[2]* is a synonym to *w[1]*) in order to simplify this process. Using simple
    matrix algebra, one can prove that given a matrix *M*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random walks](img/B05261_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and *M* symmetric, then
  prefs: []
  type: TYPE_NORMAL
- en: '![Random walks](img/B05261_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for even and odd value of *n* respectively. In theory, we only need to compute
    the multiplication of *S* that is a diagonal matrix. In practice, this requires
    lot of effort and is computationally expensive for no real value (all we want
    is to generate random word association). Instead, we generate random walks using
    our Word2Vec graph, the Pregel API, and a Monte Carlo simulation. This will generate
    word associations starting from a seed `love`. The algorithm stops after 100 iterations
    or when a path reaches our Godwin point. The detail of this algorithm can be found
    in our code repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is also worth mentioning that a Matrix, *M*, is said to be ergodic (hence
    also proving the Godwin Law) if there exists an integer, *n*, such that M^n> 0.
  prefs: []
  type: TYPE_NORMAL
- en: A Small Step into sarcasm detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting sarcasm is an active area of research ([http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf](http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf)).
    In fact, detecting sarcasm is often not easy for humans, so how can it be easy
    for computers? If I say "*We will make America great again*"; without knowing
    me, observing me, or hearing the tone I'm using, how could you know if I really
    meant what I said? Now, if you were to read a tweet from me that says "*We will
    make America great again :(:(:(*", does it help in a sense?
  prefs: []
  type: TYPE_NORMAL
- en: Building features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We believe that sarcasm cannot be detected using plain English text only, especially
    not when the plain text fits into less than 140 characters. However, we showed
    in this chapter that emojis can play a major role in the definition of emotion.
    A naive assumption is that a tweet with both positive sentiment and negative emojis
    can potentially lead to sarcasm. In addition to the tone, we also found that some
    words were closer to some ideas/ideologies that can be classified as fairly negative.
  prefs: []
  type: TYPE_NORMAL
- en: '#LoveTrumpsHates'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have demonstrated that any word can be represented in a highly dimensional
    space between words such as [`clinton`], [`trump`], [`love`], and [`hate`]. Therefore,
    for our first extractor, we build features using the average cosine similarity
    between these words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We expose this method as a user-defined function so that each tweet can be
    scored against each of these four dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Scoring Emojis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can extract all emojis and run a basic word count to retrieve only the most
    used emojis. We can then categorize them into five different groups: `love`, `joy`,
    `joke`, `sad`, and `cry`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Again, we expose this method as a UDF that can be applied to a DataFrame. An
    emoji score of 1.0 will be extremely positive, and 0.0 will be highly negative.
  prefs: []
  type: TYPE_NORMAL
- en: Training a KMeans model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the UDFs set, we get our initial Twitter DataFrame and build the feature
    vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We normalize our vectors using the `Normalizer` class and feed a KMeans algorithm
    with only five clusters. Compared to [Chapter 10](ch10.xhtml "Chapter 10. Story
    De-duplication and Mutation"), *Story De-duplication and Mutation*, the KMeans
    optimization (in terms of *k*) does not really matter here as we are not interested
    in grouping tweets into categories, but rather detecting outliers (tweets that
    are far away from any cluster center):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We recommend the use of the ML package instead of MLlib. There have been huge
    improvements to this package over the past few versions of Spark in terms of dataset
    adoption and catalyst optimization. Unfortunately, there is a major limitation:
    all ML classes are defined as private and cannot be extended. As we want to extract
    the distance alongside the predicted cluster, we will have to build our own Euclidean
    measure as a UDF function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we predict our clusters and Euclidean distances from our *featured
    tweets* DataFrame and register this DataFrame as a persistent Hive table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Detecting anomalies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider a tweet as abnormal if its feature vector is too far from any known
    cluster center (in terms of Euclidean distance). Since we stored our predictions
    as a Hive table, we can sort all points through a simple SQL statement and only
    take the first few records.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is reported, as follows, when querying Hive from our Zeppelin notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting anomalies](img/B05261_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Zeppelin notebook for detecting anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without getting into too much detail (abnormal tweets can be sensitive), a
    few examples extracted from Hive queries are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'good luck today america #vote #imwithher [grimacing]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is so great we be america great again [cry, scream]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we love you sir thank you for you constant love [cry]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'i can not describe how incredibly happy i am right now #maga [cry, rage]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, however, that the outliers we found were not all sarcastic tweets. We
    have only just begun our study of sarcasm, and lots of refining (including manual
    work) and probably more advanced models (such as *neural networks*) will be needed
    in order to write a comprehensive detector.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of this chapter was to cover different topics around time series,
    word embedding, sentiment analysis, graph theory, and anomaly detection. It''s
    worth noting that the tweets used to illustrate the examples in no way reflect
    the authors'' own opinions: "Whether or not America will be great again is out
    of scope here":(:( - sarcasm or not?'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover an innovative approach to detect trends out
    of Time Series data using the *TrendCalculus* method. This will be used against
    market data, but can easily be applied in different use cases, including the *Sentiment
    Time Series* we built here.
  prefs: []
  type: TYPE_NORMAL

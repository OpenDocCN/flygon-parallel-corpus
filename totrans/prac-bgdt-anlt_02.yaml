- en: Big Data Mining for the Masses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a big data mining platform in an enterprise environment that serves
    specific business requirements is non-trivial. While it is relatively simple to
    build a big data platform, the novel nature of the tools present a challenge in
    terms of adoption by business-facing users used to traditional methods of data
    mining. This, ultimately, is a measure of how successful the platform becomes
    within an organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces some of the salient characteristics of big data analytics
    relevant for both practitioners and end users of analytics tools. This will include
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is big data mining?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Big data mining in the enterprise:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stakeholders of the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key technologies in big data mining:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selecting the hardware stack:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single/multinode architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selecting the software stack:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop, Spark, and NoSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is big data mining?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data mining forms the first of two broad categories of big data analytics,
    the other being Predictive Analytics, which we will cover in later chapters. In
    simple terms, big data mining refers to the entire life cycle of processing large-scale
    datasets, from procurement to implementation of the respective tools to analyze
    them.
  prefs: []
  type: TYPE_NORMAL
- en: The next few chapters will illustrate some of the high-level characteristics
    of any big data project that is undertaken in an organization.
  prefs: []
  type: TYPE_NORMAL
- en: Big data mining in the enterprise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a big data solution in a medium to large size enterprise can be
    a challenging task due to the extremely dynamic and diverse range of considerations,
    not the least of which is determining what specific business objectives the solution
    will address.
  prefs: []
  type: TYPE_NORMAL
- en: Building the case for a Big Data strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the most important aspect of big data mining is determining the appropriate
    use cases and needs that the platform would address. The success of any big data
    platform depends largely on finding relevant problems in business units that will
    deliver measurable value for the department or organization. The hardware and
    software stack for a solution that collects large volumes of sensor or streaming
    data will be materially different from one that is used to analyze large volumes
    of internal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some suggested steps that, in my experience, have been found
    to be particularly effective in building and implementing a corporate big data
    strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who needs big data mining**: Determining which business groups will benefit
    most significantly from a big data mining solution is the first step in this process.
    This would typically entail groups that are already working with large datasets,
    are important to the business, and have a direct revenue impact, and optimizing
    their processes in terms of data access or time to analyze information would have
    an impact on the daily work processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, in a pharmaceutical organization, this could include Commercial
    Research, Epidemiology, Health Economics, and Outcomes. At a financial services
    organization, this could include Algorithmic Trading Desks, Quantitative Research,
    and even Back Office.
  prefs: []
  type: TYPE_NORMAL
- en: '**Determining the use cases**: The departments identified in the preceding
    step might already have a platform that delivers the needs of the group satisfactorily.
    Prioritizing among multiple use cases and departments (or a collection of them)
    requires personal familiarity with the work being done by the respective business
    groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most organizations follow a hierarchical structure where the interaction among
    business colleagues is likely to be mainly along **rank lines**. Determining impactful
    analytics use cases requires a close collaboration between both the practitioner
    as well as the stakeholder; namely, both the management who has oversight of a
    department as well as the staff members who perform the hands-on analysis. The
    business stakeholder can shed light on which aspects of his or her business will
    benefit the most from more efficient data mining and analytics environment. The
    practitioners provide insight on the challenges that exist at the hands-on operational
    level. Incremental improvements that consolidate both the operational as well
    as the managerial aspects to determine an optimal outcome are bound to deliver
    faster and better results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stakeholders'' buy-in**: The buy-in of the stakeholders—in other words, a
    consensus among decision-makers and those who can make independent budget decisions—should
    be established prior to commencing work on the use case(s). In general, multiple
    buy-ins should be secured for redundancy such that there is a pool of primary
    and secondary sources that can provide appropriate support and funding for an
    extension of any early-win into a broader goal. The buy-in process does not have
    to be deterministic and this may not be possible in most circumstances. Rather,
    a general agreement on the value that a certain use case will bring is helpful
    in establishing a baseline that can be leveraged on the successful execution of
    the use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early-wins and the effort-to-reward ratio**: Once the appropriate use cases
    have been identified, finding the ones that have an optimal effort-to-reward ratio
    is critical. A relatively small use case that can be implemented in a short time
    within a smaller budget to optimize a specific business-critical function helps
    in showcasing early-wins, thus adding credibility to the big data solution in
    question. We cannot precisely quantify these intangible properties, but we can
    hypothesize:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a4c6d708-32ca-4b4a-8832-43f32e9567ab.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, *effort* is the time and work required to implement the use case.
    This includes aspects such as how long it would take to procure the relevant hardware
    and/or software that is part of the solution, the resources or equivalent *man-hours*
    it will take to implement the solution, and the overall operational overhead.
    An open source tool might have a lower barrier to entry relative to implementing
    a commercial solution that may involve lengthy procurement and risk analysis by
    the organization. Similarly, a project that spans across departments and would
    require time from multiple resources who are already engaged in other projects
    is likely to have a longer duration than one that can be executed by the staff
    of a single department. If the net effort is low enough, one can also run more
    than one exercise in parallel as long as it doesn’t compromise the quality of
    the projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Leveraging the early-wins**: The successful implementation of one or more
    of the projects in the early-wins phase often lays the groundwork to develop a
    bigger strategy for the big data analytics platform that goes far beyond the needs
    of just a single department and has a broader organizational-level impact. As
    such, the early-win serves as a first, but crucial, step in establishing the value
    of big data to an audience, who may or may not be skeptical of its viability and
    relevance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As outlined earlier, the implementation process can span multiple steps. These
    steps are often iterative in nature and require a trial-and-error approach. This
    will require a fair amount of perseverance and persistence as most undertakings
    will be characterized by varying degrees of successes and failures.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a Big Data strategy will include multiple stakeholders and a collaborative
    approach often yields the best results. Business sponsors, business support and
    IT &amp; Analytics are three broad categories of stakeholders that together create
    a proper unified solution, catering to the needs of the business to the extent
    that budget and IT capabilities will permit.
  prefs: []
  type: TYPE_NORMAL
- en: Stakeholders of the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exact nature of the stakeholders of a big data solution is subjective and
    would vary depending on the use case and problem domain. In general, the following
    can be considered a high-level representation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business sponsor**: The individual or department that provides the support
    and/or funding for the project. In most cases, this entity would also be the beneficiary
    of the solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation group**: The team that implements the solution from a hands-on
    perspective. This is usually the IT or Analytics department of most companies
    that is responsible for the design and deployment of the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IT procurement**: The procurement department in most organizations is responsible
    for vetting a solution to evaluate its competitive pricing and viability from
    an organizational perspective. Compliance with internal IT policies and assessment
    of other aspects such as licensing costs are some of services provided by procurement,
    especially for commercial products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal**: All products, unless developed in-house, will most certainly have
    associated terms and conditions of use. Open source products can have a wide range
    of properties that defines the permissibility and restrictiveness of use. Open
    source software licenses such as Apache 2.0, MIT, and BSD are generally more permissible
    relative to GNU **GPL** (**General Purpose License**). For commercial solutions,
    the process is more involved as it requires the analysis of vendor-specific agreements
    and can take a long time to evaluate and get approved depending on the nature
    of the licensing terms and conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final implementation of the solution is the culmination of the collaboration
    between the implementation group, business beneficiaries, and auxiliary departments.
    The time to undertake projects from start to end can vary anywhere from 3-6 months
    for most small-sized projects as explained in the section on early-wins. Larger
    endeavors can take several months to years to accomplish and are marked by an
    agile framework of product management where capabilities are added incrementally
    during the implementation and deployment period.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot gives us a good understanding of the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20ad7f91-d721-447f-8465-010285dd4f34.png)'
  prefs: []
  type: TYPE_IMG
- en: High level image showing the workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'The images and icons have been taken from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://creativecommons.org/licenses/by/3.0/us/](https://creativecommons.org/licenses/by/3.0/us/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Icons made by Freepik ([http://www.freepik.com](http://www.freepik.com)) from
    www.flaticon.com is licensed by CC 3.0 BY
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Icons made by Vectors Market ([http://www.flaticon.com/authors/vectors-market](http://www.flaticon.com/authors/vectors-market))
    from [www.flaticon.com](http://www.flaticon.com) is licensed by CC 3.0 BY
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Icons made by Prosymbols ([http://www.flaticon.com/authors/prosymbols](http://www.flaticon.com/authors/prosymbols))
    from [www.flaticon.com](http://www.flaticon.com) is licensed by CC 3.0 BY
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors by Vecteezy ([https://www.vecteezy.com](https://www.vecteezy.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical elements of the big data platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our discussion, so far, has been focused on the high-level characteristics of
    design and deployment of big data solutions in an enterprise environment. We will
    now shift attention to the technical aspects of such undertakings. From time to
    time, we’ll incorporate high-level messages where appropriate in addition to the
    technical underpinnings of the topics in discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the technical level, there are primarily two main considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: Selection of the hardware stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selection of the software and **BI** (**business intelligence**) platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the recent 2-3 years, it has become increasingly common for corporations
    to move their processes to cloud-based environments as a complementary solution
    for in-house infrastructures. As such, cloud-based deployments have become exceedingly
    common and hence, an additional section on on-premises versus cloud-based has
    been added. Note that the term *On-premises* can be used interchangeably with
    **In-house**, **On-site**, and other similar terminologies.
  prefs: []
  type: TYPE_NORMAL
- en: You’d often hear the term **On-premise** being used as an alternative for *On-premises.*
    The correct term is **On-premises**. The term **premise** is defined by the Chambers
    Dictionary as *premise noun 1 (also premises) something assumed to be true as
    a basis for stating something further.* **Premises,** on the other hand, is a
    term used to denote buildings (among others) and arguably makes a whole lot more
    sense.
  prefs: []
  type: TYPE_NORMAL
- en: Selection of the hardware stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The choice of hardware often depends on the type of solution that is chosen
    and where the hardware would be located. The proper choice depends on several
    key metrics such as the type of data (structured, unstructured, or semi-structured),
    the size of data (gigabytes versus terabytes versus petabytes), and, to an extent,
    the frequency with which the data will be updated. The optimal choice requires
    a formal assessment of these variables and will be discussed later on in the book.
    At a high-level, we can surmise three broad models of hardware architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multinode architecture**: This would typically entail multiple nodes (or
    servers) that are interconnected and work on the principle of multinode or distributed
    computing. A classic example of a multinode architecture is Hadoop, where multiple
    servers maintain bi-directional communication to coordinate a job. Other technologies
    such as a NoSQL database like Cassandra and search and analytics platform like
    Elasticsearch also run on the principle of multinode computing architecture. Most
    of them leverage *commodity servers*, another name for relatively low-end machines
    by enterprise standards that work in tandem to provide large-scale data mining
    and analytics capabilities. Multinode architectures are suitable for hosting data
    that is in the range of terabytes and above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single-node architecture**: Single-node refers to computation done on a single
    server. This is relatively uncommon with the advent of multinode computing tools,
    but still retains a huge advantage over distributed computing architectures. The
    *Fallacy of Distributed Computing* outlines a set of assertions, or assumptions,
    related to the implementation of distributed systems such as the reliability of
    the network, cost of latency, bandwidth, and other considerations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the dataset is structured, contains primarily textual data, and is in the
    order of 1-5 TB, in today’s computing environment, it is entirely possible to
    host such datasets on single-node machines using specific technologies as has
    been demonstrated in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud-based architecture**: Over the past few years, numerous cloud-based
    solutions have appeared in the industry. These solutions have greatly reduced
    the barrier to entry in big data analytics by providing a platform that makes
    it incredibly easy to provision hardware resources on demand based on the needs
    of the task at hand. This materially reduces the significant overhead in procuring,
    managing, and maintaining physical hardware and hosting them at in-house data
    center facilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud platforms such as Amazon Web Services, Azure from Microsoft, and the Google
    Compute Environment permit enterprises to provision 10s to 1000s of nodes at costs
    starting as low as 1 cent per hour per instance.
  prefs: []
  type: TYPE_NORMAL
- en: In the wake of the growing dominance of cloud vendors over traditional brick-and-mortar
    hosting facilities, several complementary services to manage client cloud environments
    have come into existence.
  prefs: []
  type: TYPE_NORMAL
- en: Examples include cloud management companies, such as Altiscale that provides
    big data as a service solutions and IBM Cloud Brokerage that facilitates selection
    and management of multiple cloud-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**The exponential decrease in the cost of hardware**: The cost of hardware
    has gone down exponentially over the past few years. As a case in point, per Statistic
    Brain’s research, the cost of hard drive storage in 2013 was approximately 4 cents
    per GB. Compare that with $7 per GB as recent as 2000 and over $100,000 per GB
    in the early 80’s. Given the high cost of licensing commercial software, which
    can often exceed the cost of the hardware, it makes sense to allocate enough budget
    toward procuring capable hardware solutions. Software needs appropriate hardware
    to provide optimal performance and providing level importance toward hardware
    selection is just as important as selecting the appropriate software.'
  prefs: []
  type: TYPE_NORMAL
- en: Selection of the software stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The selection of the software stack for data mining varies based on individual
    circumstances. The most popular options specific to data mining are shown along
    with a couple of alternatives which, although not as well-known, are just as capable
    of managing large-scale datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Hadoop ecosystem**: The big data terms arguably got their start in the
    popular domain with the advent of Hadoop. The Hadoop ecosystem consists of multiple
    projects run under the auspices of the Apache Software Foundation. Hadoop supports
    nearly all the various types of datasets—such as structured, unstructured, and
    semi-structured—well-known in the big data space. Its thriving ecosystem of auxiliary
    tools that add new functionalities as well as a rapidly evolving marketplace where
    companies are vying to demonstrate the next-big-thing-in-Big-Data means that Hadoop
    will be here for the foreseeable future. There are four primary components of
    Hadoop, apart from the projects present in the large ecosystem. They are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Common**: The common utilities that support the other Hadoop modules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System (HDFS™)**: A distributed filesystem that provides
    high-throughput access to application data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN**: A framework for job scheduling and cluster resource management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop MapReduce**: A YARN-based system for parallel processing of large
    datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Spark™**: Apache Spark was a project for a multinode computing framework
    first conceived at University of California at Berkeley’s AMPLab as a platform
    that provided a seamless interface to run parallel computations and overcome limitations
    in the Hadoop MapReduce framework. In particular, Spark internally leverages a
    concept known as **DAG**—**directed acyclic graphs**—which indicates a functionality
    that optimizes a set of operations into a smaller, or more computationally efficient,
    set of operations. In addition, Spark exposes several **APIs**—**application programming
    interfaces**—to commonly used languages such as Python (PySpark) and Scala (natively
    available interface). This removes one of the barriers of entry into the Hadoop
    space where a knowledge of Java is essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, Spark introduces a data structure called **Resilient Distributed Datasets**
    (**RDD**), which provides a mechanism to store data in-memory, thus improving
    data retrieval and subsequently processing times dramatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster manager**: The nodes constituting a Spark cluster communicate using
    cluster managers, which manage the overall coordination among the nodes that are
    part of the cluster. As of writing this, the cluster manager can be the standalone
    Spark cluster manager, Apache Mesos, or YARN. There is also an additional facility
    of running Spark on AWS EC2 instances using spark-ec2 that automatically sets
    up an environment to run Spark programs.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed** **storage**: Spark can access data from a range of underlying
    distributed storage systems such as HDFS, S3 (AWS Storage), Cassandra, HBase,
    Hive, Tachyon, and any Hadoop data source. It should be noted that Spark can be
    used as a standalone product and does *not* require Hadoop for operations. Newcomers
    to Spark are often under the impression that Hadoop, or more concretely an HDFS
    filesystem, is needed for Spark operations. This is not true. Spark can support
    multiple types of cluster managers as well as backend storage systems, as shown
    in this section.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NoSQL and traditional databases**: A third consideration in terms of selecting
    the software stack are NoSQL databases. The term NoSQL came into existence recently
    and is meant to distinguish databases that do not follow the traditional relational-database
    models. There are both open source and commercial variations of NoSQL databases
    and indeed even cloud-based options that have become increasingly common. There
    are various broad classifications of NoSQL databases and some of the more common
    paradigms are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key-value**: These NoSQL databases store data on a principle of hashing—a
    unique key identifies a set of properties about the key. An example of a key in
    this parlance could be the national ID number of an individual (such as the Social
    Security Number or SSN in the US and Aadhaar in India). This could be associated
    with various aspects relating to the individual such as name, address, phone number,
    and other variables. The end user of the database would query by the ID number
    to directly access information about the individual. Open source Key-Value databases
    such as Redis and commercial ones such as Riak are very popular.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-memory**: While databases that have used in-memory facilities, such as
    storing caches in the memory to provide faster access relative to storing on disk,
    have always existed, they were adopted more broadly with the advent of big data.
    Accessing data in-memory is orders of magnitude faster (~ 100 nanoseconds) than
    accessing the same information from disk (1-10 milliseconds or 100,000 times slower).
    Several NoSQL databases, such as Redis and KDB+, leverage temporary in-memory
    **storage** in order to provide faster access to frequently used data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Columnar**: These databases append multiple columns of data as opposed to
    rows to create a table. The primary advantage of columnar storage over row-based
    storage is that a columnar layout provides the means to access data faster with
    reduced I/O overhead and is particularly well-suited for analytics use cases.
    By segregating data into individual columns, the database query can retrieve data
    by scanning the appropriate columns instead of scanning a table on a row-by-row
    basis and can leverage parallel processing facilities extremely well. Well-known
    columnar databases include Cassandra, Google BigTable, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document-oriented**: In many ways considered a step up from pure key-value
    stores, document-oriented databases store data that do not conform to any specific
    schema such as unstructured text like news articles. These databases provide ways
    to encapsulate the information in multiple key-value pairs that do not have to
    be necessarily consistent in structure across all other entries. As a consequence,
    document databases such as MongoDB are used widely in media-related organizations
    such as NY Times and Forbes in addition to other mainstream companies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud-based solutions:** Finally, cloud-based solutions for large-scale data
    mining such as AWS Redshift, Azure SQL Data Warehouse, and Google Bigquery permit
    users to query datasets directly on the cloud-vendor’s platform without having
    to create their own architecture. Although the end user can choose to have their
    own in-house specialists such as Redshift System Administrators, the management
    of the infrastructure, maintenance, and day-to-day routine tasks are mostly carried
    out by the vendor, thus reducing the operational overhead on the client side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a high-level overview of Big Data and some of the components
    of implementing a Big Data solution in the Enterprise. Big Data requires selection
    of an optimal software and hardware stack, an effort that is non-trivial, not
    least because of the hundreds of solutions in the industry. Although the topic
    of a Big Data strategy may be deemed as a subject best left for management rather
    than a technical audience, it is essential to understand the nuances.
  prefs: []
  type: TYPE_NORMAL
- en: Note that without a proper, well-defined strategy and corresponding high level
    support, IT departments will remain limited in the extent to which they can provide
    successful solutions. Further, the solution, including the hardware-software stack
    should be such that it can be adequately managed and supported by existing IT
    resources. Most companies will find that it would be essential to recruit new
    hires for the Big Data implementation. Since such implementations require evaluation
    of various elements - business needs, budget, resources and other variables, a
    lead time, often of a few months to an year and more would be needed depending
    on the scale and scope.
  prefs: []
  type: TYPE_NORMAL
- en: These topics will be discussed in depth in later chapters and this section serves
    as a preliminary introduction to the subject.
  prefs: []
  type: TYPE_NORMAL

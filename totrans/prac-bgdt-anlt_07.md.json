["```scala\n> scores <- c(45,66,66,55,55,52,61,64,65,68) \n> scale(scores) \n            [,1] \n [1,] -1.9412062 \n [2,]  0.8319455 \n [3,]  0.8319455 \n [4,] -0.6206578 \n [5,] -0.6206578 \n [6,] -1.0168223 \n [7,]  0.1716713 \n [8,]  0.5678358 \n [9,]  0.6998907 \n[10,]  1.0960552 \nattr(,\"scaled:center\") \n[1] 59.7 \nattr(,\"scaled:scale\") \n[1] 7.572611 \n```", "```scala\n> library(caret) \nLoading required package: lattice \nLoading required package: ggplot2 \nNeed help getting started? Try the cookbook for R: http://www.cookbook-r.com/Graphs/ \n\n> repeated <- c(rep(100,9999),10) # 9999 values are 100 and the last value is 10 \n\n>random<- sample(100,10000,T) # 10,000 random values from 1 - 100 \n\n>data<- data.frame(random = random, repeated = repeated) \n\n>nearZeroVar(data) \n[1] 2 \n\n> names(data)[nearZeroVar(data)] \n[1] \"repeated\" \n```", "```scala\npregnant Number of times pregnant \nglucose  Plasma glucose concentration (glucose tolerance test) \npressure Diastolic blood pressure (mm Hg) \ntriceps  Triceps skin fold thickness (mm) \ninsulin  2-Hour serum insulin (mu U/ml) \nmass     Body mass index (weight in kg/(height in m)\\^2) \npedigree Diabetes pedigree function \nage            Age (years) \ndiabetes Class variable (test for diabetes) \n```", "```scala\ninstall.packages(\"mlbench\") \ninstall.packages(\"corrplot\") \n\nlibrary(corrplot) \nlibrary(mlbench) \ndata (PimaIndiansDiabetes)\ndiab <- PimaIndiansDiabetes # To produce a correlogram \ncorrplot(cor(diab[,-ncol(diab)]), method=\"color\", type=\"upper\") # To get the actual numbers \ncorrplot(cor(diab[,-ncol(diab)]), method=\"number\", type=\"upper\")\n```", "```scala\ncorrelated_columns<- findCorrelation(cor(diab[,-ncol(diab)]), cutoff = 0.5) \ncorrelated_columns \n```", "```scala\nMethod \n\na character vector specifying the type of processing. \n\nPossible values are \"BoxCox\", \"YeoJohnson\", \"expoTrans\", \"center\", \"scale\", \"range\", \"knnImpute\", \"bagImpute\", \"medianImpute\", \"pca\", \"ica\", \"spatialSign\", \"corr\", \"zv\", \"nzv\", and \"conditionalX\" (see Details below) \n```", "```scala\n(990/1000) * 100 = 99% \n```", "```scala\nlibrary(mlbench) \nlibrary(caret) \ndiab<- PimaIndiansDiabetes \n\ndiabsim<- diab \ndiabrows<- nrow(diabsim) \nnegrows<- floor(.95 * diabrows) \nposrows<- (diabrows - negrows) \n\nnegrows \n[1] 729 \n\nposrows \n[1] 39 \n\ndiabsim$diabetes[1:729]     <- as.factor(\"neg\")\ndiabsim$diabetes[-c(1:729)] <- as.factor(\"pos\")\ntable(diabsim$diabetes) \n\nneg. pos \n729  39 \n\n# We observe that in this simulated dataset, we have 729 occurrences of positive outcome and 39 occurrences of negative outcome\n\n# Method 1: Upsampling, i.e., increasing the number of observations marked as 'pos' (i.e., positive) \n\nupsampled_simdata<- upSample(diabsim[,-ncol(diabsim)], diabsim$diabetes) \ntable(upsampled_simdata$Class) \n\nnegpos \n729 729 \n\n# NOTE THAT THE OUTCOME IS CALLED AS 'Class' and not 'diabetes' \n# This is because of the use of the variable separately \n# We can always rename the column to revert to the original name \n\n# Method 2: Downsampling, i.e., reducing the number of observations marked as 'pos' (i.e., positive) \n\ndownsampled_simdata<- downSample(diabsim[,-ncol(diabsim)], diabsim$diabetes) \ntable(downsampled_simdata$Class) \n\nneg pos \n39  39 \n```", "```scala\n# Method 3: SMOTE \n# The function SMOTE is available in the R Package DMwR \n# In order to use it, we first need to install DmWR as follows \n\ninstall.packages (\"DMwR\") \n\n# Once the package has been installed, we will create a synthetic \n# Dataset in which we will increase the number of 'neg' records \n# Let us check once again the distribution of neg/pos in the dataset \n\ntable(diabsim$diabetes) \n\nnegpos \n729  39 \n\n# Using SMOTE we can create synthetic cases of 'pos' as follows \n\ndiabsyn<- SMOTE(diabetes ~ ., diabsim, perc.over = 500, perc.under = 150) \n\n# perc.over = 500 means, increase the occurrence of the minority \n# class by 500%, i.e., 39 + 5*39 = 39 + 195 = 234 \n\n# perc.under = 150 means, that for each new record generated for the \n# Minority class, we will generate 1.5 cases of the majority class \n# In this case, we created 195 new records (500% of 39) and hence \n# we will generate 150% of 195 records = 195 * 150% = 195 * 1.5 \n# = 292.5, or 292 (rounded down) new records \n\n# We can verify this by running the table command against the newly \n# Created synthetic dataset, diabsyn \n\ntable(diabsyn$diabetes) \n\nnegpos \n292 234\n```", "```scala\ninstall.packages(\"ROSE\") \nlibrary(ROSE) \n\n# Loaded ROSE 0.0-3 \nset.seed(1) \n\ndiabsyn2 <- ROSE(diabetes ~ ., data=diabsim) \n\ntable(diabsyn2$data$diabetes) \n\n# negpos \n# 395 373 \n```", "```scala\nlibrary(DMwR) \nlibrary(caret) \n\ndiab<- PimaIndiansDiabetes \n\n# In the dataset, the column mass represents the body mass index \n# Of the individuals represented in the corresponding row \n\n# mass: Body mass index (weight in kg/(height in m)\\^2) \n\n# Creating a backup of the diabetes dataframe \ndiabmiss_orig<- diab \n\n# Creating a separate dataframe which we will modify \ndiabmiss<- diabmiss_orig \n\n# Saving the original values for body mass \nactual <- diabmiss_orig$mass \n\n# Change 91 values of mass to NA in the dataset \ndiabmiss$mass[10:100] <- NA \n\n# Number of missing values in mass \nsum(is.na(diabmiss$mass)) \n\n# 91 \n\n# View the missing values \ndiabmiss[5:15,] \n```", "```scala\n# Test with using the mean, we will set all the missing values \n# To the mean value for the column \n\ndiabmiss$mass[is.na(diabmiss$mass)] <- mean(diabmiss$mass,na.rm = TRUE) \n\n# Check the values that have been imputed \ndata.frame(actual=actual[10:100], impute_with_mean=diabmiss$mass[10:100]) \n```", "```scala\n# Check the Root-Mean-Squared-Error for the entire column \n# Root Mean Squared Error provides an estimate for the \n# Difference between the actual and the predicted values \n# On 'average' \n\ndiabmissdf<- data.frame(actual=actual, impute_with_mean=diabmiss$mass) \nrmse1 <- RMSE(diabmissdf$impute_with_mean,actual) \nrmse1 \n\n# [1] 3.417476 \n\n# We will re-run the exercise using knnImputation (from package DMwR) \n\n# Change the value of the records back to NA \ndiabmiss<- diabmiss_orig \ndiabmiss$mass[10:100] <- NA \n\n# Perform knnImputation \ndiabknn<- knnImputation(diabmiss,k=25) \n\n# Check the RMSE value for the knnImputation method \nrmse2 <- RMSE(diabknn$mass,actual) \nrmse2 \n\n# [1] 3.093827 \n\n# Improvement using the knnImputation methods in percentage terms \n\n100 * (rmse1-rmse2)/rmse1 \n\n[1] 22.20689 \n```", "```scala\ndiab<- PimaIndiansDiabetes \n\n# We will use the createDataPartition function from caret to split \n# The data. The function produces a set of indices using which we \n# will create the corresponding training and test sets \n\ntraining_index<- createDataPartition(diab$diabetes, p = 0.80, list = FALSE, times = 1) \n\n# Creating the training set \ndiab_train<- diab[training_index,] \n\n# Create the test set \ndiab_test<- diab[-training_index,] \n\n# Create the trainControl parameters for the model \ndiab_control<- trainControl(\"repeatedcv\", number = 3, repeats = 2, classProbs = TRUE, summaryFunction = twoClassSummary) \n\n# Build the model \nrf_model<- train(diabetes ~ ., data = diab_train, method = \"rf\", preProc = c(\"center\", \"scale\"), tuneLength = 5, trControl = diab_control, metric = \"ROC\") \n\n# Find the Variable Importance \nvarImp(rf_model) \nrf variable importance \n\n         Overall \nglucose  100.000 \nmass      52.669 \nage       39.230 \npedigree  24.885 \npressure  12.619 \npregnant   6.919 \ninsulin    2.294 \ntriceps    0.000 \n\n# This indicates that glucose levels, body mass index and age are the top 3 predictors of diabetes. \n\n# caret also includes several useful plot functions. We can visualize the variable importance using the command: \n\nplot(varImp(rf_model)) \n```", "```scala\ntraining_index<- createDataPartition(diab$diabetes, p = 0.80, list = FALSE, times = 1) \n\nlength(training_index) # Number of items that we will select for the train set [1] 615 \n\nnrow(diab) # The total number of rows in the dataset [1] 768 \n\n# Creating the training set, this is the data we will use to build our model \ndiab_train<- diab[training_index,] \n\n# Create the test set, this is the data against which we will test the performance of our model \ndiab_test<- diab[-training_index,] \n```", "```scala\n# Create a set of random indices representing 80% of the data \ntraining_index2 <- sample(nrow(diab),floor(0.80*nrow(diab))) \n\n# Check the size of the indices just created \nlength(training_index2) [1] 614 \n\n# Create the training set \ndiab_train2 <- diab[training_index2,] \n\n# Create the test set \ndiab_test2 <- diab[-training_index2] \n```", "```scala\n# Create the trainControl parameters for the model \n# The parameters indicate that a 3-Fold CV would be created \n# and that the process would be repeated 2 times (repeats) \n# The class probabilities in each run will be stored \n# And we'll use the twoClassSummary* function to measure the model \n# Performance \ndiab_control<- trainControl(\"repeatedcv\", number = 3, repeats = 2, classProbs = TRUE, summaryFunction = twoClassSummary) \n\n# Build the model \n# We used the train function of caret to build the model \n# As part of the training process, we specified a tunelength** of 5 \n# This parameter lets caret select a set of default model parameters \n# trControl = diab_control indicates that the model will be built \n# Using the cross-validation method specified in diab_control \n# Finally preProc = c(\"center\", \"scale\") indicate that the data \n# Would be centered and scaled at each pass of the model iteration \n\nrf_model<- train(diabetes ~ ., data = diab_train, method = \"rf\", preProc = c(\"center\", \"scale\"), tuneLength = 5, trControl = diab_control, metric = \"ROC\") \n```", "```scala\n# Install the R Package e1071, if you haven't already \n# By running install.packages(\"e1071\") \n\n# Use the predict function and the rf_model that was previously built \n# To get the predictions on the test dataset \n# Note that we are not including the column diabetes in the test \n# dataset by using diab_test[,-ncol(diab_test)] \n\npredictions<- predict(rf_model, diab_test[,-ncol(diab_test)]) \n\n# First few records predicted \nhead(predictions)\n[1] negnegpospospospos \nLevels: negpos \n\n# The confusion matrix allows us to see the number of true positives \n# False positives, True negatives and False negatives \n\ncf<- confusionMatrix(predictions, diab_test$diabetes) \ncf \n\n# Confusion Matrix and Statistics \n#  \n#        Reference \n# Prediction negpos \n#        neg  89  21 \n#        pos  11  32 \n#  \n# Accuracy : 0.7908           \n# 95% CI : (0.7178, 0.8523) \n# No Information Rate : 0.6536           \n# P-Value [Acc> NIR] : 0.0001499        \n#  \n# Kappa : 0.5167           \n# Mcnemar's Test P-Value : 0.1116118        \n#  \n# Sensitivity : 0.8900           \n# Specificity : 0.6038           \n# PosPredValue : 0.8091           \n# NegPredValue : 0.7442           \n# Prevalence : 0.6536           \n# Detection Rate : 0.5817           \n# Detection Prevalence : 0.7190           \n# Balanced Accuracy : 0.7469           \n#  \n# 'Positive' Class :neg \n```", "```scala\n# This indicates that of the records that were marked negative (neg) \n# We predicted 89 of them as negative and 11 as positive (i.e., they \n# were negative but we incorrectly classified them as a positive \n\n# We correctly identified 32 positives but incorrectly classified \n# 21 positives as negative \n\n# \n#           Reference \n# Prediction neg  pos \n#        neg  89  21 \n#        pos  11  32 \n\n# The overall accuracy was 79% \n# This can be improved (significantly) by using more  \n# Accuracy : 0.7908           \n\n# We can plot the model using plot(rf_model) as follows \nplot(rf_model) \n```", "```scala\n# And finally we can also visualize our confusion matrix using the \n# inbuilt fourfoldplot function in R \n\nfourfoldplot(cf$table) \n```", "```scala\nInstall.packages(\"doMC\")  # Install package for multicore processing \nInstall.packages(\"nnet\") # Install package for neural networks in R \n```", "```scala\n# Load the library doMC \nlibrary(doMC) \n\n# Register all cores \nregisterDoMC(cores = 8) \n\n# Set seed to create a reproducible example \nset.seed(100) \n\n# Load the PimaIndiansDiabetes2 dataset \ndata(\"PimaIndiansDiabetes2\",package = 'mlbench') \ndiab<- PimaIndiansDiabetes2 \n\n# This dataset, unlike PimaIndiansDiabetes has 652 missing values! \n> sum(is.na(diab)) [1] 652 \n\n# We will use knnImputation to fill in the missing values \ndiab<- knnImputation(diab) \n\n# Create the train-test set split \ntraining_index<- createDataPartition(diab$diabetes, p = .8, list = FALSE, times = 1) \n\n# Create the training and test dataset \ndiab_train<- diab[training_index,] \ndiab_test<- diab[-training_index,] \n\n# We will use 10-Fold Cross Validations \ndiab_control<- trainControl(\"repeatedcv\", number = 10, repeats = 3, search = \"random\", classProbs = TRUE) \n\n# Create the model using methodnnet (a Neural Network package in R) \n# Note that we have changed the metric here to \"Accuracy\" instead of # ROC \nnn_model<- train(diabetes ~ ., data = diab_train, method = \"nnet\",   preProc = c(\"center\", \"scale\"), trControl = diab_control, tuneLength = 10, metric = \"Accuracy\") \n\npredictions<- predict(nn_model, diab_test[,-ncol(diab_test)]) \ncf<- confusionMatrix(predictions, diab_test$diabetes) \ncf \n\n# >cf \n# Confusion Matrix and Statistics \n#  \n#        Reference \n# Prediction negpos \n#        neg  89  19 \n#        pos  11  34 \n#  \n# Accuracy : 0.8039           \n# 95% CI : (0.7321, 0.8636) \n# No Information Rate : 0.6536           \n# P-Value [Acc> NIR] : 3.3e-05          \n#  \n```", "```scala\n plot(nn_model) \n```", "```scala\nfourfoldplot(cf$table) \n```"]
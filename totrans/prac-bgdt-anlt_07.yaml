- en: An Introduction to Machine Learning Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning has become a commonplace topic in our day-to-day lives. The
    advancement in the field has been so dramatic that today, even cell phones incorporate
    advanced machine learning and artificial intelligence-related facilities, capable
    of responding and taking actions based on human instructions.
  prefs: []
  type: TYPE_NORMAL
- en: A subject that was once limited to university classrooms has transformed into
    a full-fledged industry, pervading our daily lives in ways we could not have envisioned
    even just a few years ago.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to introduce the reader to the underpinnings of machine
    learning and explain the concepts in simple, lucid terms that will help readers
    become familiar with the core ideas in the subject. We'll start off with a high-level
    overview of machine learning, and explain the different categories and how to
    distinguish them. We'll explain some of the salient concepts in machine learning,
    such as data pre-processing, feature engineering, and variable importance. The
    next chapter will go into more detail regarding individual algorithms and theoretical
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: We'll conclude with exercises that leverage real-world datasets to perform machine
    learning operations using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popular emergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning, statistics, and artificial intelligence (AI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categories of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core concepts in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning tutorial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** is not a new subject; it has existed in academia for well
    over 70 years as a formal discipline, but known by different names: statistics,
    and more generally mathematics, then **artificial intelligence** (**AI**), and
    today as machine learning. While the other related subject areas of statistics
    and AI are just as prevalent, machine learning has carved out a separate niche
    and become an independent discipline in and of itself.'
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, machine learning involves predicting future events based on
    historical data. We see it manifested in our day-to-day lives and indeed we employ,
    knowingly or otherwise, principles of machine learning on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: When we casually comment on whether a movie will succeed at the box office using
    our understanding of the popularity of the individuals in the lead roles, we are
    applying machine learning, albeit subconsciously. Our understanding of the characters
    in the lead roles has been shaped over years of watching movies where they appeared.
    And, when we make a determination of the success of a future movie featuring the
    same person, we are using historical information to make an assessment.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, if we had data on temperature, humidity, and precipitation
    (rain) over a period of say, 12 months, can we use that information to predict
    whether it will rain today, given information on temperature and humidity?
  prefs: []
  type: TYPE_NORMAL
- en: This is akin to common regression problems found in statistics. But, machine
    learning involves applying a much higher level of rigor to the exercise to reach
    a conclusive decision based not only on theoretical calculations, but verification
    of the calculations hundreds or thousands of times using iterative methods before
    reaching a conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted and clarified here that the term *machine learning* relates
    to algorithms or programs that are executed typically on a computing device whose
    objective it is to predict outcomes. The algorithms build mathematical models
    that can then be used to make predictions. It is a common misconception that machine
    learning quite literally refers to a *machine* that is *learning*. The actual
    implication, as just explained, is much less dramatic.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The timeline of machine learning, as available on Wikipedia ([https://en.wikipedia.org/wiki/Timeline_of_machine_learning](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)),
    provides a succinct and insightful overview of the evolution of the field. The
    roots can be traced back to as early as the mid-1700s, when Thomas Bayes presented
    his paper on *inverse probability* at the Royal Society of London. Inverse probability,
    more commonly known today as probability distribution, deals with the problem
    of determining the state of a system given a prior set of events. For example,
    if a box contained milk chocolate and white chocolate, you took out a few at random,
    and received two milk and three white chocolates, can we infer how many of each
    chocolate there are in the box?
  prefs: []
  type: TYPE_NORMAL
- en: In other words, what can we infer about the unknown given a few points of data
    with which we can postulate a formal theory? Bayes' work was developed further
    into Bayes' Theorem by Pierre-Simon Laplace in his text, *Théorie Analytique des
    Probabilités*.
  prefs: []
  type: TYPE_NORMAL
- en: In the early 1900s, Andrey Markov's analysis of Pushkin's Poem, Eugeny Onegin,
    to determine the alliteration of consonants and vowels in Russian literature,
    led to the development of a technique known as Markov Chains, used today to model
    complex situations involving random events. Google's PageRank algorithm implements
    a form of Markov Chains.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first formal application of machine learning, or more generally, AI, and
    its eventual emergence as a discipline, should be attributed to Alan Turing. He
    developed the Turing Test - a way to determine whether a machine is intelligent
    enough to mimic human behavior. Turing presented this in his paper, *Computing
    Machinery and Intelligence*, which starts out with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: I propose to consider the question, "Can machines think?" This should begin
    with definitions of the meaning of the terms "machine" and "think." The definitions
    might be framed so as to reflect so far as possible the normal use of the words,
    but this attitude is dangerous, If the meaning of the words "machine" and "think"
    are to be found by examining how they are commonly used it is difficult to escape
    the conclusion that the meaning and the answer to the question, "Can machines
    think?" is to be sought in a statistical survey such as a Gallup poll. But this
    is absurd. Instead of attempting such a definition I shall replace the question
    by another, which is closely related to it and is expressed in relatively unambiguous
    words.[...]
  prefs: []
  type: TYPE_NORMAL
- en: 'Later in the paper, Turing writes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The original question, "Can machines think?" I believe to be too meaningless
    to deserve discussion. Nevertheless I believe that at the end of the century the
    use of words and general educated opinion will have altered so much that one will
    be able to speak of machines thinking without expecting to be contradicted. I
    believe further that no useful purpose is served by concealing these beliefs.*'
  prefs: []
  type: TYPE_NORMAL
- en: Turing's work on AI was followed by a series of seminal events in machine learning
    and AI. The first neural network was developed by Marvin Misky in 1951, Arthur
    Samuel began his work on the first machine learning programs that played checkers
    in 1952, and Rosenblatt invented the perceptron, a fundamental unit of neural
    networks, in 1957\. Pioneers such as Leo Breiman, Jerome Friedman, Vladimir Vapnik
    and Alexey Chervonenkis, Geoff Hinton, and YannLeCun made significant contributions
    through the late 1990s to bring machine learning into the limelight. We are greatly
    indebted to their work and contributions, which have made machine learning stand
    out as a distinct area of research today.
  prefs: []
  type: TYPE_NORMAL
- en: In 1997, IBM's Deep Blue beat Kasparov and it immediately became a worldwide
    sensation. The ability of a machine to beat the world's top chess champion was
    no ordinary achievement. The event gave some much-needed credibility to machine
    learning as a formidable contender for the intelligent machines that Turing envisaged.
  prefs: []
  type: TYPE_NORMAL
- en: Factors that led to the success of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given machine learning, as a subject, has existed for many decades, it begs
    the question: why hadn''t it become as popular as it is today much sooner? Indeed,
    the theories of complex machine learning algorithms such as neural networks were
    well known by the late 1990s, and the foundation had been established well before
    that in the theoretical realm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few factors that can be attributed to the success of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Internet**: The web played a critical role in democratizing information
    and connecting people in an unprecedented way. It made the exchange of information
    simple in a way that could not have been achieved through the pre-existing methods
    of print media communication. Not only did the web transform and revolutionize
    the dissemination of information, it also opened up new opportunities. Google''s
    PageRank, as mentioned earlier, was one of the first large-scale and highly visible
    successes in the application of statistical models to develop a highly successful
    web enterprise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social media**: While the web provided a platform for communication, it lacked
    a level of flexibility akin to how people interacted with one another in the real
    world. There was a noticeable, but understated, and arguably unexplored gap. Tools
    such as IRC and Usenet were the precursors to social network websites such as
    Myspace, which was one of the first web-based platforms intended to create personal
    networks. By early-mid 2000, Facebook had emerged as the leader in social networking.
    These platforms provided a unique opportunity to leverage the Internet to collect
    data at an individual level. Each user left a trail of messages, ripe for collection
    and analysis using Natural Language Processing and other techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computing hardware**: Hardware used for computers developed at an exponential
    rate. Machine learning algorithms are inherently compute and resource intensive,
    that is, they require powerful CPUs, fast disks, and high memory depending on
    the size of data. The invention of new ways to store data on **solid state drives**
    (**SSDs**) was a leap from the erstwhile process of storing on spinning hard drives.
    Faster access meant that data could be delivered to the CPU at a much faster rate
    and reduce the I/O bottleneck that has traditionally been a weak area in computing.
    Faster CPUs meant it was possible to perform hundreds and thousands of iterations
    demanded by machine learning algorithms in a timely manner. Finally, the demand
    led to the reduction in prices for computing resources, allowing more people to
    be able to afford buying computing hardware that was prohibitively expensive.
    Algorithms existed, but the resources were finally able to execute them in a reasonable
    time and cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Programming languages and packages**: Communities such as R and Python developers
    seized the opportunity, and individuals started releasing packages that exposed
    their work to a broader community of programmers. In particular, packages that
    provided machine learning algorithms became immediately popular and inspired other
    practitioners to release their individual code repositories, making platforms
    such as R a truly global collaborative effort. Today there are over 10,000 packages
    in R, up from 2000 in 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning, statistics, and AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a term that has various synonyms - names that are the result
    of either marketing activities by corporates or just terms that have been used
    interchangeably. Although some may argue that they have different implications,
    they all ultimately refer to machine learning as a subject that facilitates the
    prediction of future events using historical information.
  prefs: []
  type: TYPE_NORMAL
- en: The commonly heard terms for machine learning include predictive analysis, predictive
    analytics, predictive modeling, and many others. As such, unless the entity that
    publishes material explaining their interpretation of the term and more specifically,
    how it is different, it is generally safe to assume that they are referring to
    machine learning. This is often a source of confusion among those new to the subject,
    largely due to the misuse and overuse of technical verbiage.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics, on the other hand, is a distinct subject area that has been well
    known for over 200 years. The word is derived from the new Latin, *statisticum
    collegium* (council of state, in English) and the Italian word *statista*, meaning
    statesman or politician. You can visit [https://en.wikipedia.org/wiki/History_of_statistics#Etymology](https://en.wikipedia.org/wiki/History_of_statistics#Etymology)
    for more details on this topic. Machine learning implements various statistical
    models, which due to the rigor of computation involved, is distinct from the branch
    of classical statistics.
  prefs: []
  type: TYPE_NORMAL
- en: AI is also closely related to machine learning, but is a much broader subject.
    It can be loosely defined as systems (software/hardware) that, in the presence
    of uncertainties, can arrive at a concrete decision in (usually) a responsible
    and socially aware manner to attain a target end objective. In other words, AI
    aims to produce actions by systematically processing a situation that involves
    both known and unknown (latent) factors.
  prefs: []
  type: TYPE_NORMAL
- en: AI conjures up images of smart and sometimes rebellious robots in sci-fi movies,
    just as much as it reminds us of intelligent systems, such as IBM Watson, that
    can parse complex questions and process ambiguous statements to find concrete
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning shares some of the same traits - the step-wise development
    of a model using training data, and measuring accuracy using test data. However,
    AI has existed for many decades and has been a familiar household term. Institutions
    in the US, such as Carnegie Mellon University, have led the way in establishing
    key principles and guidelines of AI.
  prefs: []
  type: TYPE_NORMAL
- en: The online resources/articles on AI versus machine learning do not seem to provide
    any conclusive answers on how they differ. However, the syllabus of AI courses
    at universities makes the differences very obvious. You can learn more about AI
    at [https://cs.brown.edu/courses/csci1410/lectures.html](https://cs.brown.edu/courses/csci1410/lectures.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'AI refers to a vast array of study areas that involve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constrained optimization**: Reach best possible results given a set of constraints
    or limitations in a given situation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Game theory**: For instance, zero-sum games, equilibrium, and others - taking
    a measured decision based on how the decision can affect future decisions and
    impact desired end goals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uncertainty/Bayes'' rule**: Given prior information, what is the likelihood
    of this happening given something else has already happened'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Planning**: Formulating a plan of action = a set of paths (graph) to tackle
    a situation/reach an end goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning**: The implementation (realization) of the preceding goals
    by using algorithms that are designed to handle uncertainties and imitate human
    reasoning. The machine learning algorithms generally used for AI include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks/deep learning (find hidden factors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing (NLP) (understand context using tenor, linguistics,
    and such)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual object recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic models (for example, Bayes' classifiers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov decision processes (decisions for random events, for example, gambling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various other machine learning Algorithms (clustering, SVM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sociology**: A study of how machine learning decisions affect society and
    take remedial steps to correct issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categories of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Arthur Samuel coined the term **machine learning** in 1959 while at IBM. A popular
    definition of machine learning is due to Arthur, who, it is believed, called machine
    learning *a field of computer science that gives computers the ability to learn
    without being explicitly programmed*.
  prefs: []
  type: TYPE_NORMAL
- en: Tom Mitchell, in 1998, added a more specific definition to machine learning
    and called it a, study of algorithms that improve their performance P at some
    task T with experience E.
  prefs: []
  type: TYPE_NORMAL
- en: A simple explanation would help to illustrate this concept. By now, most of
    us are familiar with the concept of spam in emails. Most email accounts also contain
    a separate folder known as **Junk**, **Spam**, or a related term. A cursory check
    of the folders will usually indicate the presence of several emails, many of which
    were presumably unsolicited and contain meaningless information.
  prefs: []
  type: TYPE_NORMAL
- en: The mere task of categorizing emails as spam and moving them to a folder involves
    the application of machine learning. Andrew Ng highlighted this elegantly in his
    popular MOOC course on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Mitchell''s terms, the spam classification process involves:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task T**: Classifying emails as spam/not spam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance P**: Number of emails accurately identified as spam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experience E**: The model is provided emails that have been marked as spam/not
    spam and uses that information to determine whether a new email is spam or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Broadly speaking, there are two distinct types of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We shall discuss them in turn here.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised and unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us start with supervised machine learning first.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Supervised machine learning** refers to machine learning exercises that involve
    predicting outcomes with labelled data. Labelled data simply refers to the fact
    that the dataset we are using to make the predictions (as well as the outcome
    we will predict) has a definite value (irrespective of what it is). For instance,
    classifying emails as spam or not spam, predicting temperature, and identifying
    faces from images are all examples of supervised machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Vehicle Mileage, Number Recognition and other examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a dataset containing information on miles per gallon, number of cylinders,
    and such of various cars, can we predict what the value for miles per gallon would
    be if we only had the other values available?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, our outcome is `mpg` and we are using the other variables of
    `cyl` (Cylinders), `hp` (Horsepower), `gear` (number of gears), and others to
    build a model that can then be applied against a dataset where the values for
    mpg are marked as `MISSING`. The model reads the information in these columns
    in the first five rows of the data and, based on that information, predicts what
    the value for `mpg` would be in the other rows, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c1b160c-2b40-4223-9d7c-b9fb43c86b0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The reason this is considered supervised is that in the course of building
    our machine learning model, we provided the model with information on what the
    outcome was. Other examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recognizing letters and numbers**: In such cases, the input to the model
    are the images, say of letters and numbers, and the outcome is the alpha-numeric
    value shown on the image. Once the model is built, it can then be used against
    pictures to recognize and predict what numbers are shown in the picture. A simple
    example, but very powerful. Imagine if you were given 100,000 images of houses
    with house numbers. The manual way of identifying the house numbers would be to
    go through each image individually and write down the numbers. A machine learning
    model allows us to completely automate the entire operation. Instead of having
    to manually go through individual images, you could simply run the model against
    the images and get the results in a very short amount of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-driving autonomous cars**: The input to the algorithms are images where
    the objects in the image have been identified, for example, person, street sign,
    car, trees, shops, and other elements. The algorithm *learns* to recognize and
    differentiate among different elements once a sufficient number of images have
    been shown and thereafter given an unlabeled image, that is, an image where the
    objects have not been identified is able to recognize them individually. To be
    fair, this is a highly simplified explanation of a very complex topic, but the
    overall principle is the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MNIST Dataset used for number recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18749693-441f-4359-8c23-45af6249b0c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Unsupervised machine learning** involves datasets that do not have labeled
    outcomes. Taking the example of predicting mpg values for cars, in an unsupervised
    exercise, our dataset would have looked as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22338df9-3af6-4217-a4b9-be7b4556b7db.png)'
  prefs: []
  type: TYPE_IMG
- en: If all the outcomes are *missing*, it would be impossible to know what the values
    might have been. Recall that the primary premise of machine learning is to use
    historical information to make predictions on datasets whose outcome is not known.
    But, if the historical information itself does not have any identified outcomes,
    then it would not be possible to build a model. Without knowing any other information,
    the values of mpg in the table could be all 0 or all 100; it is not possible to
    tell, as we do not have any data point that will help lead us to the value.
  prefs: []
  type: TYPE_NORMAL
- en: This is where *unsupervised* machine learning gets applied. In this type of
    machine learning, we are not trying to predict outcomes. Rather, we are trying
    to determine which items are most similar to one another.
  prefs: []
  type: TYPE_NORMAL
- en: A common name for such an exercise is *clustering*, that is, we are attempting
    to find *clusters* or groups of records that are most similar to one another.
    Where can we use this information and what are some examples of unsupervised learning?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various news aggregators on the web - sites that do not themselves
    publish information, but collect information from other news sources. One such
    aggregator is Google News. If, say, we had to search for information on the last
    images taken by the satellite Cassini of Saturn, we could do a simple search for
    the phrase on Google News [https://news.google.com/news/?gl=US&amp;ned=us&amp;hl=en](https://news.google.com/news/?gl=US&ned=us&hl=en).
    An example is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ad124ec-c378-451b-a50d-c4da5aab8847.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that there is a link for View all at the bottom of the news articles.
    Clicking the link will take you to a page with all the other related news articles.
    Surely, Google didn't manually classify the articles as belonging to the specific
    search term. In fact, Google doesn't know in advance what the user will search
    for. The search term could have well been *images of Saturn rings from space*.
  prefs: []
  type: TYPE_NORMAL
- en: So, how does Google know which articles belong to a specific search term? The
    answer lies in the application of clustering or principles of unsupervised learning.
    Unsupervised learning examines the attributes of a specific dataset in order to
    determine which articles are most similar to one another. To do this, the algorithm
    doesn't even need to know the contextual background.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you were given two sets of books with no covers, a set of books on gardening
    and a set of books on computer programming. Although you may not know the title
    of the book, it would be fairly easy to distinguish books on computers from books
    on gardening. One set of books would have an overwhelming number of terms related
    to computing, while the other would have an overwhelming number of terms related
    to plants. To make the distinction that there were two distinct categories of
    books would not be difficult just by virtue of the images in the books, even for
    a reader who, let's assume, is not aware of either computers or gardening.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples of unsupervised machine learning include detection of malignant
    and non-malignant tumors, and gene sequencing.
  prefs: []
  type: TYPE_NORMAL
- en: Subdividing supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised machine learning can be further subdivided into exercises that involve
    either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts are quite straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Classification involves a machine learning task that has a discrete outcome
    - a **categorical** outcome. All **nouns** are categorical variables, such as
    fruits, trees, color, and true/false.
  prefs: []
  type: TYPE_NORMAL
- en: The outcome variables in classification exercises are also known as **discrete
    or categorical variables**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the fruit given size, weight, and shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying numbers given a set of images of numbers (as shown in the earlier
    chapter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying objects on the streets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying playing cards as diamonds, spades, hearts and clubs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the class rank of a student based on the student's grade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last one might not seem obvious, but a rank, that is, 1^(st), 2^(nd), 3^(rd)
    denotes a fixed category. A student could rank, say, 1^(st) or 2^(nd), but not
    have a rank of 1.5!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Images of some atypical classification examples are shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/06771d1b-3810-4a9b-a9d2-5f5f6c48be27.png)Classification of different
    types of fruits | ![](img/02f51c3d-2e4f-46dc-b332-3cbac84c3bfe.png)Classification
    of playing cards: diamonds, spades, hearts, and clubs |'
  prefs: []
  type: TYPE_TB
- en: '**Regression**, on the other hand, involves calculating numeric outcomes. Any
    outcome on which you can perform numeric operations, such as addition, subtraction,
    multiplication, and division, would constitute a regression problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of regression include:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting daily temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating stock prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the sales price of residential properties and others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images of some atypical regression examples are shown below. In both the cases,
    we are dealing with quantitative numeric data that is continuous. Hence, the outcome
    variables of regression are also known as **quantitative or continuous variables**.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/10902bef-e6e1-49ef-879d-212ac46fbea9.png)Calculating house prices
    | ![](img/40718ef7-1629-42bc-868a-c3e136387bf9.png)Calculating stock prices using
    other market data |'
  prefs: []
  type: TYPE_TB
- en: Note that the concepts of classification or regression do not as such apply
    to unsupervised learning. Since there are no labels in unsupervised learning,
    there is no discrete classification or regression in the strict sense. That said,
    since unsupervised learning categories data into clusters, objects in a cluster
    are often said to belong to the same class (as other objects in the same cluster).
    This is akin to classification, except that it is created after-the-fact and no
    classes existed prior to the objects being classified into individual clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Common terminologies in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, you'll often hear the terms features, predictors, and dependent
    variables. They are all one and the same. They all refer to the variables that
    are used to predict an outcome. In our previous example of cars, the variables
    **cyl** (Cylinder), **hp** (Horsepower), **wt** (Weight), and **gear** (Gear)
    are the predictors and **mpg** (Miles Per Gallon) is the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simpler terms, taking the example of a spreadsheet, the names of the columns
    are, in essence, known as features, predictors, and dependent variables. As an
    example, if we were given a dataset of toll booth charges and were tasked with
    predicting the amount charged based on the time of day and other factors, a hypothetical
    example could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fbda53b-25e6-41f7-996f-4c992c3bbee5.png)'
  prefs: []
  type: TYPE_IMG
- en: In this spreadsheet, the columns **date**, **time**, **agency**, **type**, **prepaid**,
    and **rate** are the features or predictors, whereas, the column **amount** is
    our outcome or dependent variable (what we are predicting).
  prefs: []
  type: TYPE_NORMAL
- en: The value of amount *depends* on the value of the other variables (which are
    thus known as *independent variables*).
  prefs: []
  type: TYPE_NORMAL
- en: Simple equations also reflect the obvious distinction, for example, in an equation,
    *y = a + b + c*, the **left hand side** (**LHS**) is the dependent/outcome variable
    and *a*, *b* and *c* are the features/predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09eb5136-5bcd-42a3-960d-e0cefa6602b0.png)'
  prefs: []
  type: TYPE_IMG
- en: The core concepts in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many important concepts in machine learning; we'll go over some of
    the more common topics. Machine learning involves a multi-step process that starts
    with data acquisition, data mining, and eventually leads to building the predictive
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key aspects of the model-building process involve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pre-processing**: Pre-processing and feature selection (for example,
    centering and scaling, class imbalances, and variable importance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Train, test splits and cross-validation**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the training set (say, 80 percent of the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the test set (~ 20 percent of the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create model, get predictions**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which algorithms should you try?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What accuracy measures are you trying to optimize?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What tuning parameters should you use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data management steps in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pre-processing, or more generally processing the data, is an integral part of
    most machine learning exercises. A dataset that you start out with is seldom going
    to be in the exact format against which you'll be building your machine learning
    models; it will invariably require a fair amount of cleansing in the majority
    of cases. In fact, data cleansing is often the most time-consuming part of the
    entire process. In this section, we will briefly highlight a few of the top data
    processing steps that you may encounter in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing and feature selection techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data pre-processing**, as the name implies, involves curating the data to
    make it suitable for machine learning exercises. There are various methods for
    pre-processing and a few of the more common ones have been illustrated here.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that data pre-processing should be performed as part of the cross-validation
    step, that is, pre-processing should not be done *before the fact*, but rather
    during the model-building process. This will be explained in more detail afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Centering and scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying center and scale function on numeric columns is often done in order
    to standardize data and remove the effect of large variations in the magnitude
    or differences of numbers. You may have encountered this in college or university
    courses where students would be graded on a standardized basis, or a curve.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, say an exam paper was unusually difficult and half of all the
    students in a class of 10 students received scores below 60 - the passing rate
    set for the course. The professor can either a) make a determination that 50%
    of the students should re-take the course, or b) standardize the scores to find
    how students performed relative to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say the class scores were:'
  prefs: []
  type: TYPE_NORMAL
- en: 45,66,66,55,55,52,61,64,65,49
  prefs: []
  type: TYPE_NORMAL
- en: With the passing score set at 60, this implies that the students who scored
    45, 55, 55, 52 and 49 will not successfully complete the course.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this might not be a truly accurate representation of their relative
    merits. The professor may alternatively choose to instead use a center-and-scale
    method, commonly known as standardization, which involves:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the mean of all the scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subtracting the mean from the scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing the result by the standard deviation of all the scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operation is illustrated below for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean of the scores is 57.8\. Hence, subtracting 57.8 from each of the numbers
    produce the numbers shown in the second row. But, we are not done yet. We need
    to divide the numbers by the *standard deviation* of the scores to get the final
    standardized values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b4b2a16-5582-41f7-84df-f219eec1c0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Dividing by the **SD** (**standard deviation**) shows that there were only two
    students whose scores were below one standard deviation across the range of all
    the test scores. Hence, instead of five students who do not complete the course
    successfully based on the raw numbers, we can narrow it down to only two students.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a truly simple operation, it is not hard to see that it is
    very effective in smoothing out large variations in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Centering and scaling can be performed very easily in R using the scale command
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The near-zero variance function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The near-zero variance, available in the `nearZeroVar` function in the `R package,
    caret`, is used to identify variables that have little or no variance. Consider
    a set of 10,000 numbers with only three distinct values. Such a variable may add
    very little value to an algorithm. In order to use the `nearZeroVar` function,
    first install the R package, caret, in RStudio (which we had set up [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml), *The
    Analytics Toolkit*. The exact code to replicate the effect of using `nearZeroVar`
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As the example shows, the function was able to correctly detect the variable
    that met the criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Removing correlated variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correlated variables can produce results that over-emphasize the contribution
    of the variables. In regression exercises, this has the effect of increasing the
    value of R^2, and does not accurately represent the actual performance of the
    model. Although many classes of machine learning algorithms are resistant to the
    effects of correlated variables, it deserves some mention as it is a common topic
    in the discipline.
  prefs: []
  type: TYPE_NORMAL
- en: The premise of removing such variables is related to the fact that redundant
    variables do not add incremental value to a model. For instance, if a dataset
    contained height in inches and height in meters, these variables would have a
    near exact correlation of 1, and using one of them is just as good as using the
    other. Practical exercises that involve variables that we cannot judge intuitively,
    using methods of removing correlated variables, can greatly help in simplifying
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: The following example illustrates the process of removing correlated variables.
    The dataset, **Pima Indians Diabetes**, contains vital statistics about the diet
    of Pima Indians and an outcome variable called `diabetes`.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the course of the examples in successive chapters, we will refer to
    this dataset often. A high level overview of the meaning of the different columns
    in the dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are interested in finding out if any of the variables, apart from diabetes
    (which is our outcome variable) are correlated. If so, it may be useful to remove
    the redundant variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the packages `mlbench` and `corrplot` in RStudio and execute the commands
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The command will produce a plot as shown here using the `corrplot` package
    from [http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/edccd408-6cf1-49de-b18d-a77282d496a8.png) | >![](img/81bbdc2b-2af2-4adb-93c0-b35330a18bb5.png)
    |'
  prefs: []
  type: TYPE_TB
- en: The darker the shade, the higher the correlation. In this case, it shows that
    age and pregnancy have a relatively high correlation. We can find the exact values
    by using `method="number"` as shown. You can also view the plot at [http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use functions such as the following to directly find the correlated
    variables without plotting the correlograms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Other common data transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several other data transformations are available and applicable to different
    situations. A summary of these transformations can be found at the documentation
    site for the `caret` package under **Pre-Processing** at [https://topepo.github.io/caret/pre-processing.html](https://topepo.github.io/caret/pre-processing.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The options available in the pre-process function of caret can be found from
    its help section, by running the command `?preProcess` in RStudio. The code for
    it is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Data sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may encounter datasets that have a high level of imbalanced outcome classes.
    For instance, if you were working with a dataset on a rare disease, with your
    outcome variable being true or false, due to the rarity of the occurrence, you
    may find that the number of observations marked as false (that is, the person
    did not have the rare disease) is much higher than the number of observations
    marked as true (that is, the person did have the rare disease).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms attempt to maximize performance, which in many cases
    could be the accuracy of the predictions. Say, in a sample of 1000 records, only
    10 are marked as true and the rest of the `990` observations are false.
  prefs: []
  type: TYPE_NORMAL
- en: 'If someone were to randomly assign *all* observations as false, the accuracy
    rate would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: But, the objective of the exercise was to find the *individuals who had the
    rare disease*. We are already well aware that due to the nature of the disease,
    most individuals will not belong to the category.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling, in essence, is the process of *maximizing machine learning metrics
    such as specificity, sensitivity, precision, recall, and kappa*. These will be
    discussed at a later stage, but for the purposes of this section, we'll show some
    ways by which you can *sample* the data so as to produce a more evenly balanced
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The R package, `caret`, includes several helpful functions to create a balanced
    distribution of the classes from an imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, we need to re-sample the data to get a better distribution of
    the classes in order to build a more effective model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the general methods include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Up-sample**: Increase instances of the class with lesser examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Down-sample**: Reduce the instances of the class with higher examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create synthetic examples** (for example, **SMOTE** (**Synthetic Minority
    Oversampling TechniquE**))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random oversampling (for example, (**ROSE**) **Randomly OverSampling Examples**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will create a simulated dataset using the same data from the prior example
    where 95% of the rows will be marked as negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The **SMOTE** (**Synthetic Minority Over-sampling TechniquE**) is a third method
    that, instead of plain vanilla up-/down-sampling, creates synthetic records from
    the nearest neighbors of the minority class. In our simulated dataset, it is obvious
    that `neg` is the minority class, that is, the class with the lowest number of
    occurrences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The help file on the SMOTE function explains the concept succinctly:'
  prefs: []
  type: TYPE_NORMAL
- en: Unbalanced classification problems cause problems to many learning algorithms.
    These problems are characterized by the uneven proportion of cases that are available
    for each class of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'SMOTE (Chawla et al., 2002) is a well-known algorithm to fight this problem.
    The general idea of this method is to artificially generate new examples of the
    minority class using the nearest neighbors of these cases. Furthermore, the majority
    class examples are also under-sampled, leading to a more balanced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**ROSE (Randomly OverSampling Examples)**, the final method in this section,
    is available via the ROSE package in R. Like SMOTE, it is a method for generating
    synthetic samples. The help file for ROSE states the high-level use of the function
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation of synthetic data by Randomly Over Sampling Examples creates a sample
    of synthetic data by enlarging the features space of minority and majority class
    examples. Operationally, the new examples are drawn from a conditional kernel
    density estimate of the two classes, as described in Menardi and Torelli (2013).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, your data may have missing values. This could be due to errors in
    the data collection process, genuinely missing data, or any other reason, with
    the net result being that the information is not available. Real world examples
    of missing data can be found in surveys where the respondent did not answer a
    specific question on the survey.
  prefs: []
  type: TYPE_NORMAL
- en: You may have a dataset of, say, 1,000 records and 20 columns of which a certain
    column has 100 missing values. You may choose to discard this column altogether,
    but that also means discarding 90 percent of the information. You still have 19
    other columns that have complete data. Another option is to simply exclude the
    column, but that means you cannot leverage the benefit afforded by the data that
    is available in the respective column.
  prefs: []
  type: TYPE_NORMAL
- en: Several methods exist for data imputation, that is, the process of filling in
    missing data. We do not know what the exact values are, but by looking at the
    other entries in the table, we may be able to make an educated and systematic
    assessment of what the values might be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the common methods in data imputation involve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean, median, mode imputation**: Substituting the missing values using the
    mean, median, or mode value for the column. This, however, has the disadvantage
    of increasing the correlation among the variables that are imputed, which might
    not be desirable for multivariate analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-nearest neighbors imputation**: kNN imputation is a process of using a
    machine learning approach (nearest-neighbors) in order to impute missing values.
    It works by finding k records that are most similar to the one that has missing
    values and calculates the weighted average using Euclidean distance relative to
    k records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imputation using regression models**: Regression methods use standard regression
    methods in R to predict the value of the missing variables. However, as noted
    in the respective section on Regression-based imputation on Wikipedia [https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression](https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression),
    the problem (with regression imputation) is that the imputed data do not have
    an error term included in their estimation. Thus, the estimates fit perfectly
    along the regression line without any residual variance. This causes relationships
    to be over identified and suggests greater precision in the imputed values than
    is warranted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hot-deck imputation**: Another technique for filling missing values with
    observations from the dataset itself. This method, although very prevalent, does
    have a limitation in that, by assigning say, a single value, to a large range
    of missing values, it could add a significant bias in the observations and can
    produce misleading results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A short example has been provided here to demonstrate how imputation can be
    done using kNN Imputation. We simulate missing data by changing a large number
    of values to NA in the `PimaIndiansDiabetes` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We make use of the following factors for the process:'
  prefs: []
  type: TYPE_NORMAL
- en: We use mean to fill in the NA values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use kNN imputation to fill in the missing values. We then compare how the
    two methods performed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dec61a7-716d-4f85-9804-2290ca5419ea.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ad62a66-066c-4ce9-b69b-258304df178c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: While it may not represent a dramatic change, it's still better than using a
    naïve approach such as using simply a mean or constant value.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several packages in R for data imputation. A few prominent ones are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amelia II**: Missing information in time-series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://gking.harvard.edu/amelia](https://gking.harvard.edu/amelia)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hot-deck imputation with R package**: HotDeckImputation and hot.deck'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cran.r-project.org/web/packages/HotDeckImputation/](https://cran.r-project.org/web/packages/HotDeckImputation/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cran.r-project.org/web/packages/hot.deck/](https://cran.r-project.org/web/packages/hot.deck/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multivariate imputation (by Chained Equations)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cran.r-project.org/web/packages/mice/index.html](https://cran.r-project.org/web/packages/mice/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imputing values in a Bayesian framework with R package**: mi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cran.r-project.org/web/packages/mi/index.html](https://cran.r-project.org/web/packages/mi/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During model-building exercises, datasets may have tens of variables. Not all
    of them may add value to the predictive model. It is not uncommon to reduce the
    dataset to include a subset of the variables and allow the machine learning programmer
    to devote more time toward fine-tuning the chosen variables and the model-building
    process. There is also a technical justification for reducing the number of variables
    in the dataset. Performing machine learning modeling on very large, that is, high
    dimensional datasets can be very compute-intensive, that is, it may require a
    significant amount of time, CPU, and RAM to perform the numerical operations.
    This not only makes the application of certain algorithms impractical, it also
    has the effect of causing unwarranted delays. Hence, the methodical selection
    of variables helps both in terms of analysis time as well as computational requirements
    for algorithmic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variable selection** is also known as feature selection/attribute selection.
    Algorithms such as random forests and lasso regression implement variable selection
    as part of their algorithmic operations. But, variable selection can be done as
    a separate exercise.'
  prefs: []
  type: TYPE_NORMAL
- en: The R package, `caret`, provides a very simple-to-use and intuitive interface
    for variable selection. As we haven't yet discussed the modeling process, we will
    learn how to find the important variables, and in the next chapter delve deeper
    into the subject.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use a common, well-known algorithm called `RandomForest` that is used
    for building decision trees. The algorithm will be described in more detail in
    the next chapter, but the purpose of using it here is merely to show how variable
    selection can be performed. The example is illustrative of what the general process
    is.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll re-use the dataset we have been working with, that is, the `PimaIndiansDiabetes`
    data from the `mlbench` package. We haven''t discussed the model training process
    yet, but it has been used here in order to derive the values for variable importance.
    The outcome variable in this case is diabetes and the other variables are used
    as the independent variables. In other words, can we predict if the person has
    diabetes using the data available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding code is as shown below. It indicates that glucose,
    mass and age were the variables that contributed the most towards creating the
    model (to predict diabetes)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6f247da-1244-4e39-b6e8-fd56c9607d9f.png)'
  prefs: []
  type: TYPE_IMG
- en: The train, test splits, and cross-validation concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The train, test splits, and cross-validation sets are a fundamental concept
    in machine learning. This is one of the areas where a pure statistical approach
    differs materially from the machine learning approach. Whereas in a statistical
    modeling task, one may perform regressions, parametric/non-parametric tests, and
    apply other methods, in machine learning, the algorithmic approach is supplemented
    with an element of iterative assessment of the results being produced and subsequent
    improvisation of the model with each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into train and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every machine learning modeling exercise begins with the process of data cleansing,
    as discussed earlier. The next step is to split the data into a train and test
    set. This is usually done by randomly selecting rows from the data that will be
    used to create the model. The rows that weren't selected would then be used to
    test the final model.
  prefs: []
  type: TYPE_NORMAL
- en: The usual split varies between 70-80 percent (training data versus test data).
    In an 80-20 split, 80% of the data would be used in order to create the model.
    The remaining 20% would be used to test the final model produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We applied this in the earlier section, but we can revisit the code once again.
    The `createDataPartition` function was used with the parameter `p = 0.80` in order
    to split the data. The `training_index` variable holds the training indices (of
    the `dataset`, `diab`) that we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We do not have to necessarily use the `createDataPartition` function and instead,
    a random sample created using simple R commands as shown here will suffice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The cross-validation parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation takes the train-test split concept to the next stage. The aim
    of the machine learning exercise is, in essence, to find what set of model parameters
    will provide the best performance. A model parameter indicates the arguments that
    the function (the model) takes. For example, for a decision tree model, parameters
    may include the number of levels deep the model should be built, number of splits,
    and so on. If, say, there are *n* different parameters, each having *k* different
    values, the total number of parameters would be *k*^*n*. We generally select a
    fixed set of combinations for each of the parameters and could easily end with
    100-1000+ combinations. We will test the performance of the model (for example,
    accuracy in predicting the outcome correctly) for each of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: With a simple train-test split, say, if there were 500 combinations of parameters
    we had selected, we just need to run them against the training dataset and determine
    which one shows the optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: With cross-validation, we further split the training set into smaller subsets,
    for example, three- or five-fold is commonly used. If there are three folds, that
    is, we split the training set into three subsets, we keep aside one fold, say,
    Fold 2, and create a model using a set of parameters using Folds 1 and 3\. We
    then test its accuracy against Fold 2\. This step is repeated several times, with
    each iteration representing a unique set of folds on which the training-test process
    is being executed and accuracy measures are collected. Eventually, we would arrive
    at an optimal combination by selecting the parameters that showed the best overall
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard approach can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an 80-20 train-test split
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute your model(s) using different combinations of model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the model parameters that show the best overall performance and create
    the final model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the final model on the test set to see the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cross-validation approach mandates that we should further split the training
    dataset into smaller subsets. These subsets are generally known as **folds** and
    collectively they are known as the **k-folds**, where *k* represents the number
    of splits:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an 80-20 train-test split
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the training set into k-folds, say, three folds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set aside Fold 1 and build a model using Fold 2 and Fold 3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test your model performance on Fold 1 (for example, the percentage of accurate
    results)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set aside Fold 2 and build a model using Fold 1 and Fold 3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test your model performance on Fold 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set aside Fold 3 and build a model using Fold 1 and Fold 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test your model performance on Fold 3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the average performance of the model across all three folds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat Step 1 for *each set of model parameters*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the model parameters that show the best overall performance and create
    the final model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the final model on the test set to see the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c300d7bd-53ec-426a-9b9d-71455234a676.png)'
  prefs: []
  type: TYPE_IMG
- en: This image illustrates the difference between using an approach without cross-validation
    and one with cross-validation. The cross-validation method is arguably more robust
    and involves a rigorous evaluation of the model. That said, it is often useful
    to attempt creating a model initially without cross-validation to get a sense
    of the kind of performance that may be expected. For example, if a model built
    with say 2-3 training-test splits shows a performance of say, 30% accuracy, it
    is unlikely that any other approach, including cross-validation would somehow
    make that 90%. In other words the standard approach helps to get a sense of the
    kind of performance that may be expected. As cross-validations can be quite compute-intensive
    and time consuming getting an initial feedback on performance helps in a preliminary
    analysis of the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: The caret package in R provides a very user-friendly approach to building models
    using cross-validation. Recall that data pre-processing must be passed or made
    an integral part of the cross-validation process. So, say, we had to center and
    scale the dataset and perform a five-fold cross-validation, all we would have
    to do is define the type of sampling we'd like to use in caret's `trainControl`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Caret's webpage on `trainControl` provides a detailed overview of the functions
    along with worked-out examples at [https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning](https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have used this approach in our earlier exercise where we built a model using
    `RandomForest` on the `PimaIndiansDiabetes` dataset. It is shown again here to
    indicate where the technique was used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can get a more detailed explanation of `summaryFunction` from [https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `summaryFunction` argument is used to pass in a function that takes the
    observed and predicted values and estimates some measure of performance. Two such
    functions are already included in the package: `defaultSummary` and `twoClassSummary`.
    The latter will compute measures specific to two-class problems, such as the area
    under the ROC curve, the sensitivity and specificity. Since the ROC curve is based
    on the predicted class probabilities (which are not computed automatically), another
    option is required. The `classProbs = TRUE` option is used to include these calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an explanation of `tuneLength` from the help file for the train function
    of `caret`.
  prefs: []
  type: TYPE_NORMAL
- en: '`tuneLength` is an integer denoting the amount of granularity in the tuning
    parameter grid. By default, this argument is the number of levels for each tuning
    parameter that should be generated by train. If `trainControl` has the option
    `search = random`, this is the maximum number of tuning parameter combinations
    that will be generated by the random search.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that if this argument is given it must be named.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final step after creating the model is to use the model against the test
    dataset to get the predictions. This is generally done using the `predict` function
    in R, with the first argument being the model that was created and the second
    argument being the dataset against which you'd like to get the predictions for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking our example of the `PimaIndiansDiabetes` dataset, after the model has
    been built, we can get the predictions on the test dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check what the confusion matrix tells us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d0a354a-6134-40f5-adc9-2a7a7932b0f5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08ce8ffe-fa37-435e-a852-bd95dd78a78d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Per the documentation of *`fourfoldplot`* [Source: [https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html)],
    an association (odds ratio different from 1) between the binary row and column
    variables is indicated by the tendency of diagonally opposite cells in one direction
    to differ in size from those in the other direction; color is used to show this
    direction. Confidence rings for the odds ratio allow a visual test of the null
    of no association; the rings for adjacent quadrants overlap if and only if the
    observed counts are consistent with the null hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging multicore processing in the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exercise in the previous section is repeated here using the PimaIndianDiabetes2
    dataset instead. This dataset contains several missing values. As a result, we
    will first impute the missing values and then run the machine learning example.
  prefs: []
  type: TYPE_NORMAL
- en: The exercise has been repeated with some additional nuances, such as using multicore/parallel
    processing in order to make the cross-validations run faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To leverage multicore processing, install the package `doMC` using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will run the program as shown in the code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Even with 650+ missing values, our model was able to achieve an accuracy of
    80%+.
  prefs: []
  type: TYPE_NORMAL
- en: It can certainly be improved, but as a baseline, it shows the kind of performance
    that can be expected of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a case of a dichotomous outcome variable, a random guess would have had
    a 50% chance of being accurate. An accuracy of 80% is significantly higher than
    the accuracy we could have achieved using just guess-work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3f98e5b-e43e-4a7e-bad9-3242aed7141b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is depicted in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2a4c384-7f1d-4f36-ac67-f15fd3df0f0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt about the basic fundamentals of Machine Learning,
    the different types such as Supervised and Unsupervised and major concepts such
    as data pre-processing, data imputation, managing imbalanced classes and other
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: We also learnt about the key distinctions between terms that are being used
    interchangeably today, in particular the terms AI and Machine Learning. We learned
    that artificial intelligence deals with a vast array of topics, such as game theory,
    sociology, constrained optimizations, and machine learning; AI is much broader
    in scope relative to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning facilitates AI; namely, machine learning algorithms are used
    to create systems that are *artificially intelligent*, but they differ in scope.
    A regression problem (finding the line of best fit given a set of points) can
    be considered a machine learning *algorithm*, but it is much less likely to be
    seen as an AI algorithm (conceptually, although it technically could be).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some of the other concepts in Machine Learning
    such as Bias, Variance and Regularization. We will also read about a few important
    algorithms and learn how to apply them using machine learning packages in R.
  prefs: []
  type: TYPE_NORMAL

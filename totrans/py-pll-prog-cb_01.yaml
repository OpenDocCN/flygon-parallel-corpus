- en: Getting Started with Parallel Computing and Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *parallel* and *distributed computing* models are based on the simultaneous
    use of different processing units for program execution. Although the distinction
    between parallel and distributed computing is very thin, one of the possible definitions
    associates the parallel calculation model with the shared memory calculation model,
    and the distributed calculation model with the message passing model.
  prefs: []
  type: TYPE_NORMAL
- en: From this point onward, we will use the term *parallel computing* to refer to
    both parallel and distributed calculation models.
  prefs: []
  type: TYPE_NORMAL
- en: The next sections provide an overview of parallel programming architectures
    and programming models. These concepts are useful for inexperienced programmers
    who are approaching parallel programming techniques for the first time. Moreover,
    it can be a basic reference for experienced programmers. The dual characterization
    of parallel systems is also presented. The first characterization is based on
    the system architecture, while the second characterization is based on parallel
    programming paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter ends with a brief introduction to the Pythonprogramming language.
    The characteristics of the language, ease of use and learning, and the extensibility
    and richness of software libraries and applications make Python a valuable tool
    for any application, and also for parallel computing. The concepts of threads
    and processes are introduced in relation to their use in the language.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need parallel computing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flynn's taxonomy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel programming models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python and parallel programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing processes and threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we need parallel computing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The growth in computing power made available by modern computers has resulted
    in us facing computational problems of increasing complexity in relatively short
    time frames. Until the early 2000s, complexity was dealt with by increasing the
    number of transistors as well as the clock frequency of single-processor systems,
    which reached peaks of 3.5-4 GHz. However, the increase in the number of transistors
    causes the exponential increase of the power dissipated by the processors themselves.
    In essence, there is, therefore, a physical limitation that prevents further improvement
    in the performance of single-processor systems.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, in recent years, microprocessor manufacturers have focused
    their attention on *multi-core* systems. These are based on a core of several
    physical processors that share the same memory, thus bypassing the problem of
    dissipated power described earlier. In recent years, *quad-core* and *octa-core*
    systems have also become standard on normal desktop and laptop configurations.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, such a significant change in hardware has also resulted in
    an evolution of software structure, which has always been designed to be executed
    sequentially on a single processor. To take advantage of the greater computational
    resources made available by increasing the number of processors, the existing
    software must be redesigned in a form appropriate to the parallel structure of
    the CPU, so as to obtain greater efficiency through the simultaneous execution
    of the single units of several parts of the same program.
  prefs: []
  type: TYPE_NORMAL
- en: Flynn's taxonomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flynn''s taxonomy is a system for classifying computer architectures. It is
    based on two main concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instruction flow**: A system with *n* CPU has *n* program counters and, therefore, *n *instructions
    flows. This corresponds to a program counter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data flow**: A program that calculates a function on a list of data has a
    data flow. The program that calculates the same function on several different
    lists of data has more data flows. This is made up of a set of operands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As the instruction and data flows are independent, there are four categories
    of parallel machines: **Single Instruction Single Data** (**SISD**), **Single
    Instruction Multiple Data** (**SIMD**), **Multiple Instruction Single Data** (**MISD**),
    and **Multiple Instruction Multiple Data** (**MIMD**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/315c390f-4c31-4a69-811e-5696dff064d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Flynn's taxonomy
  prefs: []
  type: TYPE_NORMAL
- en: Single Instruction Single Data (SISD)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SISD computing system is like the von Neumann machine, which is a uniprocessor
    machine. As you can see in *Flynn's taxonomy* diagram, it executes a single instruction
    that operates on a single data stream. In SISD, machine instructions are processed
    sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a clock cycle, the CPU executes the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fetch**: The CPU fetches the data and instructions from a memory area, which
    is called a *register*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decode**: The CPU decodes the instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execute**: The instruction is carried out on the data. The result of the
    operation is stored in another register.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the execution stage is complete, the CPU sets itself to begin another
    CPU cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8d131bb3-cd52-4f51-969c-8c836a394f89.png)'
  prefs: []
  type: TYPE_IMG
- en: The fetch, decode, and execute cycle
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms that run on this type of computer are sequential (or serial)
    since they do not contain any parallelism. An example of a SISD computer is a
    hardware system with a single CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main elements of these architectures (namely, von Neumann architectures)
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Central memory unit**: This is used to store both instructions and program
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: This is used to get the instruction and/or data from the memory unit,
    which decodes the instructions and sequentially implements them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The I/O system**: This refers to the input and output data of the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conventional single-processor computers are classified as SISD systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1a6b6929-c4c4-41bb-96ae-2ec6b2aea55d.png)'
  prefs: []
  type: TYPE_IMG
- en: The SISD architecture schema
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram specifically shows which areas of a CPU are used in the
    stages of fetch, decode, and execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9ceec645-0aa1-4c90-97a7-cba9f8eb5031.png)'
  prefs: []
  type: TYPE_IMG
- en: CPU components in the fetch-decode-execute phase
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Instruction Single Data (MISD)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this model, *n* processors, each with their own control unit, share a single
    memory unit. In each clock cycle, the data received from the memory is processed
    by all processors simultaneously, each in accordance with the instructions received
    from its control unit.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the parallelism (instruction-level parallelism) is obtained by
    performing several operations on the same piece of data. The types of problems
    that can be solved efficiently in these architectures are rather special, such
    as data encryption. For this reason, the MISD computer has not foundspace in the
    commercial sector. MISD computers are more of an intellectual exercise than a
    practical configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Single Instruction Multiple Data (SIMD)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A SIMD computer consists of *n* identical processors, each with their own local
    memory, where it is possible to store data. All processors work under the control
    of a single instruction stream. In addition to this, there are *n* data streams,
    one for each processor. The processors work simultaneously on each step and execute
    the same instructions, but on different data elements. This is an example of data-level
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: The SIMD architectures are much more versatile than MISD architectures. Numerous
    problems covering a wide range of applications can be solved by parallel algorithms
    on SIMD computers. Another interesting feature is that the algorithms for these
    computers are relatively easy to design, analyze, and implement. The limitation
    is that only the problems that can be divided into a number of subproblems (which
    are all identical, each of which will then be solved simultaneously through the
    same set of instructions) can be addressed with the SIMD computer.
  prefs: []
  type: TYPE_NORMAL
- en: With the supercomputer developed according to this paradigm, we must mention
    the *Connection Machine* (Thinking Machine,1985) and *MPP* (NASA, 1983).
  prefs: []
  type: TYPE_NORMAL
- en: As we will see in [Chapter 6](1ea5f8e3-bc1e-4d48-8ffe-d96ed8d56259.xhtml), *Distributed
    Python*, and [Chapter 7](c043f263-c2f1-40ce-a390-c0999635225c.xhtml), *Cloud Computing*,
    the advent of modern graphics cards (GPUs), built with many SIMD-embedded units,
    has led to the more widespread use of this computational paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Instruction Multiple Data (MIMD)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This class of parallel computers is the most general and most powerful class,
    according to Flynn's classification. This contains *n* processors, *n* instruction
    streams, and *n* data streams. Each processor has its own control unit and local
    memory, which makes MIMD architectures more computationally powerful than SIMD
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Each processor operates under the control of a flow of instructions issued by
    its own control unit. Therefore, the processors can potentially run different
    programs with different data, which allows them to solve subproblems that are
    different and can be a part of a single larger problem. In MIMD, the architecture
    is achieved with the help of the parallelism level with threads and/or processes.
    This also means that the processors usually operate asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, this architecture is applied to many PCs, supercomputers, and computer
    networks. However, there is a counter that you need to consider: asynchronous
    algorithms are difficult to design, analyze, and implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f3fa5a98-1a89-4d76-a8af-c25a376c1ac8.png)'
  prefs: []
  type: TYPE_IMG
- en: The SIMD architecture (A) and the MIMD architecture (B)
  prefs: []
  type: TYPE_NORMAL
- en: 'Flynn''s taxonomy can be extended by considering that SIMD machines can be
    divided into two subgroups:'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical supercomputers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorial machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, MIMD can be divided into machines that have a shared memoryand
    those that have a distributed memory.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed the next section focuses on this last aspect of the organization of the
    memory of MIMD machines.
  prefs: []
  type: TYPE_NORMAL
- en: Memory organization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another aspect that we need to consider in order to evaluate parallel architectures
    is memory organization, or rather, the way in which data is accessed. No matter
    how fast the processing unit is, if memory cannot maintain and provide instructions
    and data at a sufficient speed, then there will be no improvement in performance.
  prefs: []
  type: TYPE_NORMAL
- en: The main problem that we need to overcome to make the response time of memory
    compatible with the speed of the processor is the memory cycle time, which is
    defined as the time that has elapsed between two successive operations. The cycle
    time of the processor is typically much shorter than the cycle time of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a processor initiates a transfer to or from memory, the processor''s resources
    will remain occupied for the entire duration of the memory cycle; furthermore,
    during this period, no other device (for example, I/O controller, processor, or
    even the processor that made the request) will be able to use the memory due to
    the transfer in progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d56ae362-cff1-4d47-97aa-78419437b2b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Memory organization in the MIMD architecture
  prefs: []
  type: TYPE_NORMAL
- en: Solutions to the problem of memory access have resulted in a dichotomy of MIMD
    architectures. The first type of system, known as the *shared memory* system,
    has high virtual memory and all processors have equal access to data and instructions
    in this memory. The other type of system is the ***distributed memory*** model,
    wherein each processor has local memory that is not accessible to other processors.
  prefs: []
  type: TYPE_NORMAL
- en: What distinguishes memory shared by distributed memory is the management of
    memory access, which is performed by the processing unit; this distinction is
    very important for programmers because it determines how different parts of a
    parallel program must communicate.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, a distributed memory machine must make copies of shared data
    in each local memory. These copies are created by sending a message containing
    the data to be shared from one processor to another. A drawback of this memory
    organization is that, sometimes, these messages can be very large and take a relatively
    long time to transfer, while in a shared memory system, there is no exchange of
    messages, and the main problem lies in synchronizing access to shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The schema of a shared memory multiprocessor system is shown in the following
    diagram. The physical connections here are quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0f53e868-04c0-4493-9b33-f9be28089ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: Shared memory architecture schema
  prefs: []
  type: TYPE_NORMAL
- en: Here, the bus structure allows an arbitrary number of devices (**CPU** + **C****ache**
    in the preceding diagram) that share the same channel (**Main Memory**, as shown
    in the preceding diagram). The bus protocols were originally designed to allow
    a single processor and one or more disks or tape controllers to communicate through
    the shared memory here.
  prefs: []
  type: TYPE_NORMAL
- en: Each processor has been associated with cache memory, as it is assumed that
    the probability that a processor needs to have data or instructions present in
    the local memory is very high.
  prefs: []
  type: TYPE_NORMAL
- en: The problem occurs when a processor modifies data stored in the memory system
    that is simultaneously used by other processors. The new value will pass from
    the processor cache that has been changed to the shared memory. Later, however,
    it must also be passed to all the other processors, so that they do not work with
    the obsolete value. This problem is known as the problem of *cache coherency*—a
    special case of the problem of memory consistency, which requires hardware implementations
    that can handle concurrency issues and synchronization, similar to that of thread
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main features of shared memory systems are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The memory is the same for all processors. For example, all the processors associated
    with the same data structure will work with the same logical memory addresses,
    thus accessing the same memory locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synchronization is obtained by reading the tasks of various processors and
    allowing the shared memory. In fact, the processors can only access one memory
    at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A shared memory location must not be changed from a task while another task
    accesses it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing data between tasks is fast. The time required to communicate is the
    time that one of them takes to read a single location (depending on the speed
    of memory access).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The memory access in shared memory systems is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform Memory Access** (**UMA**): The fundamental characteristic of this
    system is the access time to the memory that is constant for each processor and
    for any area of memory. For this reason, these systems are also called **Symmetric
    Multiprocessors** (**SMPs**). They are relatively simple to implement, but not
    very scalable. The coder is responsible for the management of the synchronization
    by inserting appropriate controls, semaphores, locks, and more in the program
    that manages resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-Uniform Memory Access** (**NUMA**): These architectures divide the memory
    into high-speed access area that is assigned to each processor, and also, a common
    area for the data exchange, with slower access. These systems are also called
    **Distributed Shared Memory** (**DSM**) systems. They are very scalable, but complex
    to develop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Remote Memory Access** (**NoRMA**): The memory is physically distributed
    among the processors (local memory). All local memories are private and can only
    access the local processor. The communication between the processors is through
    a communication protocol used for exchanging messages, which is known as the *message-passing
    protocol*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache-Only Memory Architecture** (**COMA**): These systems are equipped with
    only cached memories. While analyzing NUMA architectures, it was noticed that
    this architecture kept the local copies of the data in the cache and that this
    data was stored as duplicates in the main memory. This architecture removes duplicates
    and keeps only the cached memories; the memory is physically distributed among
    the processors (local memory). All local memories are private and can only access
    the local processor. The communication between the processors is also through
    the message-passing protocol.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a system with distributed memory, the memory is associated with each processor
    and a processor is only able to address its own memory. Some authors refer to
    this type of system as a multicomputer, reflecting the fact that the elements
    of the system are, themselves, small and complete systems of a processor and memory,
    as you can see in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/edbcb72a-7807-4dba-97da-a1e69af3c29d.png)'
  prefs: []
  type: TYPE_IMG
- en: The distributed memory architecture schema
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of organization has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: There are no conflicts at the level of the communication bus or switch. Each
    processor can use the full bandwidth of their own local memory without any interference
    from other processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lack of a common bus means that there is no intrinsic limit to the number
    of processors. The size of the system is only limited by the network used to connect
    the processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no problems with cache coherency. Each processor is responsible for
    its own data and does not have to worry about upgrading any copies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main disadvantage is that communication between processors is more difficult
    to implement. If a processor requires data in the memory of another processor,
    then the two processors should not necessarily exchange messages via the message-passing
    protocol. This introduces two sources of slowdown: to build and send a message
    from one processor to another takes time, and also, any processor should be stopped
    in order to manage the messages received from other processors. A program designed
    to work on a distributed memory machine must be organized as a set of independent
    tasks that communicate via messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6174b7b7-c606-48e1-a585-3412e6d542c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic message passing
  prefs: []
  type: TYPE_NORMAL
- en: 'The main features of distributed memory systems are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory is physically distributed between processors; each local memory is directly
    accessible only by its processor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization is achieved by moving data (even if it's just the message itself)
    between processors (communication).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subdivision of data in the local memories affects the performance of the
    machine—it is essential to make subdivisions accurate, so as to minimize the communication
    between the CPUs. In addition to this, the processor that coordinates these operations
    of decomposition and composition must effectively communicate with the processors
    that operate on the individual parts of data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The message-passing protocol is used so that the CPUs can communicate with each
    other through the exchange of data packets. The messages are discrete units of
    information, in the sense that they have a well-defined identity, so it is always
    possible to distinguish them from each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massively Parallel Processing (MPP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPP machines are composed of hundreds of processors (which can be as large as
    hundreds of thousands of processors in some machines) that are connected by a
    communication network. The fastest computers in the world are based on these architectures;
    some examples of these architecture systems are Earth Simulator, Blue Gene, ASCI
    White, ASCI Red, and ASCI Purple and Red Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters of workstations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These processing systems are based on classical computers that are connected
    by communication networks. Computational clusters fall into this classification.
  prefs: []
  type: TYPE_NORMAL
- en: In a cluster architecture, we define a node as a single computing unit that
    takes part in the cluster. For the user, the cluster is fully transparent—all
    the hardware and software complexity is masked and data and applications are made
    accessible as if they were all from a single node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we''ve identified three types of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fail-over cluster**: In this, the node''s activity is continuously monitored,
    and when one stops working, another machine takes over the charge of those activities.
    The aim is to ensure a continuous service due to the redundancy of the architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing cluster**: In this system, a job request is sent to the node
    that has less activity. This ensures that less time is taken to process the job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-performance computing cluster**: In this, each node is configured to
    provide extremely high performance. The process is also divided into multiple
    jobs on multiple nodes. The jobs are parallelized and will be distributed to different
    machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneous architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The introduction of GPU accelerators in the homogeneous world of supercomputing
    has changed the nature of how supercomputers are both used and programmed now.
    Despite the high performance offered by GPUs, they cannot be considered as an
    autonomous processing unit as they should always be accompanied by a combination
    of CPUs. The programming paradigm, therefore, is very simple: the CPU takes control
    and computes in a serial manner, assigning tasks to the graphics accelerator that
    are, computationally, very expensive and have a high degree of parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: The communication between a CPU and a GPU can take place, not only through the
    use of a high-speed bus but also through the sharing of a single area of memory
    for both physical or virtual memory. In fact, in the case where both the devices
    are not equipped with their own memory areas, it is possible to refer to a common
    memory area using the software libraries provided by the various programming models,
    such as *CUDA* and *OpenCL*.
  prefs: []
  type: TYPE_NORMAL
- en: These architectures are called *heterogeneous architectures*, wherein applications
    can create data structures in a single address space and send a job to the device
    hardware, which is appropriate for the resolution of the task. Several processing
    tasks can operate safely in the same regions to avoid data consistency problems,
    thanks to the atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, despite the fact that the CPU and GPU do not seem to work efficiently together,
    with the use of this new architecture, we can optimize their interaction with,
    and the performance of, parallel applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/46dac58e-d70e-4177-bc35-1018279093a9.png)'
  prefs: []
  type: TYPE_IMG
- en: The heterogeneous architecture schema
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we introduce the main parallel programming models.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel programming models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel programming models exist as an abstraction of hardware and memory architectures.
    In fact, these models are not specific and do not refer to any particular types
    of machines or memory architectures. They can be implemented (at least theoretically)
    on any kind of machines. Compared to the previous subdivisions, these programming
    models are made at a higher level and represent the way in which the software
    must be implemented to perform parallel computation. Each model has its own way
    of sharing information with other processors in order to access memory and divide
    the work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In absolute terms, no one model is better than the other. Therefore, the best
    solution to be applied will depend very much on the problem that a programmer
    should address and resolve. The most widely used models for parallel programming
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithread model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed memory/message passing model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data-parallel model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will give you an overview of these models.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this model, tasks share a single memory area in which we can read and write
    asynchronously. There are mechanisms that allow the coder to control the access
    to the shared memory; for example, locks or semaphores. This model offers the
    advantage that the coder does not have to clarify the communication between tasks.
    An important disadvantage, in terms of performance, is that it becomes more difficult
    to understand and manage data locality. This refers to keeping data local to the
    processor that works on conserving memory access, cache refreshes, and bus traffic
    that occurs when multiple processors use the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Multithread model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this model, a process can have multiple flows of execution. For example,
    a sequential part is created and, subsequently, a series of tasks are created
    that can be executed in parallel. Usually, this type of model is used on shared
    memory architectures. So, it will be very important for us to manage the synchronization
    between threads, as they operate on shared memory, and the programmer must prevent
    multiple threads from updating the same locations at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The current-generation CPUs are multithreaded in software and hardware. **POSIX**
    (short for **Portable Operating System Interface**) threads are classic examples
    of the implementation of multithreading on software. Intel's Hyper-Threading technology
    implements multithreading on hardware by switching between two threads when one
    is stalled or waiting on I/O. Parallelism can be achieved from this model, even
    if the data alignment is nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: Message passing model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The message passing model is usually applied in cases where each processor has
    its own memory (distributed memory system). More tasks can reside on the same
    physical machine or on an arbitrary number of machines. The coder is responsible
    for determining the parallelism and data exchange that occurs through the messages,
    and it is necessary to request and call a library of functions within the code.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the examples have been around since the 1980s, but only in the mid-1990s
    was a standardized model created, leading to a de facto standard called a **Message
    Passing Interface **(**MPI**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MPI model is clearly designed with distributed memory, but being models
    of parallel programming, a multiplatform model can also be used with a shared
    memory machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bd5deb5a-ea45-42d8-ba4e-b7852b6e0fcd.png)'
  prefs: []
  type: TYPE_IMG
- en: Message passing paradigm model
  prefs: []
  type: TYPE_NORMAL
- en: Data-parallel model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this model, we have more tasks that operate on the same data structure, but
    each task operates on a different portion of data. In the shared memory architecture,
    all tasks have access to data through shared memory and distributed memory architectures,
    where the data structure is divided and resides in the local memory of each task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this model, a coder must develop a program that specifies the
    distribution and alignment of data; for example, the current-generation GPUs are
    highly operational only if data (**Task** **1**, **Task** **2**, **Task** **3**)
    is aligned, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/93cf041f-f65b-46e5-b36d-59d4ed537910.png)'
  prefs: []
  type: TYPE_IMG
- en: The data-parallel paradigm model
  prefs: []
  type: TYPE_NORMAL
- en: Designing a parallel program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The design of algorithms that exploit parallelism is based on a series of operations,
    which must be carried out for the program to perform the job correctly without
    producing partial or erroneous results. The macro operations that must be carried
    out for a correct parallelization of an algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Task decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task assignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agglomeration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this first phase, the software program is split into tasks or a set of instructions
    that can then be executed on different processors to implement parallelism. To
    perform this subdivision, two methods are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain decomposition**: Here, the data of the problems is decomposed. The
    application is common to all the processors that work on different portions of
    data. This methodology is used when we have a large amount of data that must be
    processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional decomposition**: In this case, the problem is split into tasks,
    where each task will perform a particular operation on all the available data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task assignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, the mechanism by which the tasks will be distributed among the
    various processes is specified. This phase is very important because it establishes
    the distribution of workload among the various processors. Load balancing is crucial
    here; in fact, all processors must work with continuity, avoiding being in an
    idle state for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: To perform this, the coder takes into account the possible heterogeneity of
    the system that tries to assign more tasks to better-performing processors. Finally,
    for greater efficiency of parallelization, it is necessary to limit communication
    as much as possible between processors, as they are often the source of slowdowns
    and consumption of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Agglomeration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agglomeration is the process of combining smaller tasks with larger ones in
    order to improve performance. If the previous two stages of the design process
    partitioned the problem into a number of tasks that greatly exceed the number
    of processors available, and if the computer is not specifically designed to handle
    a huge number of small tasks (some architectures, such as GPUs, handle this fine
    and indeed benefit from running millions, or even billions, of tasks), then the
    design can turn out to be highly inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly, this is because tasks have to be communicated to the processor or
    thread so that they compute the said task. Most communications have costs that
    are disproportionate to the amount of data transferred, but also incur a fixed
    cost for every communication operation (such as the latency, which is inherent
    in setting up a TCP connection). If the tasks are too small, then this fixed cost
    can easily make the design inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the mapping stage of the parallel algorithm design process, we specify where
    each task is to be executed. The goal is to minimize the total execution time.
    Here, you must often make trade-offs, as the two main strategies often conflict
    with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: The tasks that communicate frequently should be placed in the same processor
    to increase locality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tasks that can be executed concurrently should be placed in different processors
    to enhance concurrency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is known as the *mapping problem*, and it is known to be **NP-complete**.
    As such, no polynomial-time solutions to the problem in the general case exist.
    For tasks of equal size and tasks with easily identified communication patterns,
    the mapping is straightforward (we can also perform agglomeration here to combine
    tasks that map to the same processor). However, if the tasks have communication
    patterns that are hard to predict or the amount of work varies per task, then
    it is hard to design an efficient mapping and agglomeration scheme.
  prefs: []
  type: TYPE_NORMAL
- en: For these types of problems, load balancing algorithms can be used to identify
    agglomeration and mapping strategies during runtime. The hardest problems are
    those in which the amount of communication or the number of tasks changes during
    the execution of the program. For these kinds of problems, dynamic load balancing
    algorithms can be used, which run periodically during the execution.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Numerous load balancing algorithms exist for a variety of problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global algorithms**: These require global knowledge of the computation being
    performed, which often adds a lot of overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local algorithms**: These rely only on information that is local to the task
    in question, which reduces overhead compared to global algorithms, but they are
    usually worse at finding optimal agglomeration and mapping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the reduced overhead may reduce the execution time, even though the
    mapping is worse by itself. If the tasks rarely communicate other than at the
    start and end of the execution, then a task-scheduling algorithm is often used,
    which simply maps tasks to processors as they become idle. In a task-scheduling
    algorithm, a task pool is maintained. Tasks are placed in this pool and are taken
    from it by workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three common approaches in this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manager/worker: **This is the basic dynamic mapping scheme in which all the
    workers connect to a centralized manager. The manager repeatedly sends tasks to
    the workers and collects the results. This strategy is probably the best for a
    relatively small number of processors. The basic strategy can be improved by fetching
    tasks in advance so that communication and computation overlap each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical manager/worker**: This is the variant of a manager/worker that
    has a semi-distributed layout. Workers are split into groups, each with their
    own manager. These group managers communicate with the central manager (and possibly
    among themselves as well), while workers request tasks from the group managers.
    This spreads the load among several managers and can, as such, handle a larger
    number of processors if all workers request tasks from the same manager.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decentralize**: In this scheme, everything is decentralized. Each processor
    maintains its own task pool and communicates with the other processors in order
    to request tasks. How the processors choose other processors to request tasks
    varies and is determined on the basis of the problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of a parallel program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The development of parallel programming created the need for performance metrics
    in order to decide whether its use is convenient or not. Indeed, the focus of
    parallel computing is to solve large problems in a relatively short period of
    time. The factors contributing to this objective are, for example, the type of
    hardware used, the degree of parallelism of the problem, and the parallel programming
    model adopted. To facilitate this, the analysis of basic concepts was introduced,
    which compares the parallel algorithm obtained from the original sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance is achieved by analyzing and quantifying the number of threads
    and/or the number of processes used. To analyze this, let''s introduce a few performance
    indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speedup**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of parallel computation are introduced by **Amdahl**'s law.
    To evaluate the *degree of efficiency* of the parallelization of a sequential
    algorithm, we have**Gustafson**'s law.
  prefs: []
  type: TYPE_NORMAL
- en: Speedup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **speedup** is the measure that displays the benefit of solving a problem
    in parallel. It is defined as the ratio of the time taken to solve a problem on
    a single processing element (*Ts*) to the time required to solve the same problem
    on *p* identical processing elements (*Tp*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We denote speedup as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d06f79cc-0130-4c88-9669-be45342198b8.png)'
  prefs: []
  type: TYPE_IMG
- en: We have a linear speedup, where if *S=p*, then it means that the speed of execution
    increases with the number of processors. Of course, this is an ideal case. While
    the speedup is absolute when *Ts* is the execution time of the best sequential
    algorithm, the speedup is relative when *Ts* is the execution time of the parallel
    algorithm for a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recap these conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S = p* is a linear or ideal speedup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S < p* is a real speedup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S > p* is a superlinear speedup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an ideal world, a parallel system with *p* processing elements can give us
    a speedup that is equal to *p*. However, this is very rarely achieved. Usually,
    some time is wasted in either idling or communicating. Efficiency is a measure
    of how much of the execution time a processing element puts toward doing useful
    work, given as a fraction of the time spent.
  prefs: []
  type: TYPE_NORMAL
- en: 'We denote it by *E* and can define it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a51cb8a0-063e-4177-a8e3-caa123753d53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithms with linear speedup have a value of *E = 1*. In other cases,
    they have the value of *E* is less than *1*. The three cases are identified as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When *E = 1*, it is a linear case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When *E < 1*, it is a real case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When *E << 1*, it is a problem that is parallelizable with low efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling is defined as the ability to be efficient on a parallel machine. It
    identifies the computing power (speed of execution) in proportion to the number
    of processors. By increasing the size of the problem and, at the same time, the
    number of processors, there will be no loss in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: The scalable system, depending on the increments of the different factors, may
    maintain the same efficiency or improve it.
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amdahl''s law is a widely used law that is used to design processors and parallel
    algorithms. It states that the maximum speedup that can be achieved is limited
    by the serial component of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0ef21cb-ef7a-401d-990a-5a3b45325d05.png)'
  prefs: []
  type: TYPE_IMG
- en: '*1 – P* denotes the serial component (not parallelized) of a program.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that, for example, if a program in which 90% of the code can be made
    parallel, but 10% must remain serial, then the maximum achievable speedup is 9,
    even for an infinite number of processors.
  prefs: []
  type: TYPE_NORMAL
- en: Gustafson's law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gustafson''s law states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5b33302b-8561-4073-a6df-09072923e8f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, as we indicated in the equation the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P* is the *number of processors.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S* is the *speedup* factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α* is the *non-parallelizable fraction* of any parallel process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gustafson's law is in cont*rast* to Amdahl's law, which, as we described, assumes
    that the overall workload of a program does not change with respect to the number
    of processors.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, Gustafson's law suggests that programmers first set the *time* allowed
    for solving a problem in parallel and then based on that (that is time) *to size*
    the problem. Therefore, the *faster*the parallel system is, the *greater* the
    problems that can be solved over the same period of time.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of Gustafson's law was to direct the objectives of computer research
    towards the selection or reformulation of problems in such a way that the solution
    of a larger problem would still be possible in the same amount of time. Furthermore,
    this law redefines the concept of *efficiency* as a need *to reduce at least the
    sequential part* of a program, despite the *increase in workload*.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python is a powerful, dynamic, and interpreted programming language that is
    used in a wide variety of applications. Some of its features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A clear and readable syntax.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A very extensive standard library, where, through additional software modules,
    we can add data types, functions, and objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy-to-learn rapid development and debugging. Developing Python code in Python
    can be up to 10 times faster than in C/C++ code. The code can also work as a prototype
    and then translated into C/C ++.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exception-based error handling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A strong introspection functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The richness of documentation and a software community.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python can be seen as a glue language. Using Python, better applications can
    be developed because different kinds of coders can work together on a project.
    For example, when building a scientific application, C/C++ programmers can implement
    efficient numerical algorithms, while scientists on the same project can write
    Python programs that test and use those algorithms. Scientists don't have to learn
    a low-level programming language and C/C++ programmers don't need to understand
    the science involved.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about this from [https://www.python.org/doc/essays/omg-darpa-mcc-position](https://www.python.org/doc/essays/omg-darpa-mcc-position).
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at some examples of very basic code to get an idea of the
    features of Python.
  prefs: []
  type: TYPE_NORMAL
- en: The following section can be a refresher for most of you. We will use these
    techniques practically in [Chapter 2](c95be391-9558-4d2d-867e-96f61fbc5bbf.xhtml), *Thread-Based
    Parallelism*, and [Chapter 3](5d4a1d39-061e-4c7c-937c-4ce3c9c6ea93.xhtml), *Process-Based
    Parallelism*.
  prefs: []
  type: TYPE_NORMAL
- en: Help functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python interpreter already provides a valid help system. If you want to
    know how to use an object, then just type `help(object)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see, for example, how to use the `help` function on integer `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The description of the `int` object is followed by a list of methods that are
    applicable to it. The first five methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Also useful is `dir(object)`, which lists the methods available for an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the relevant documentation for an object is provided by the `.__doc__`
    function, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python doesn''t adopt statement terminators, and code blocks are specified
    through indentation. Statements that expect an indentation level must end in a
    colon (`:`). This leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Python code is clearer and more readable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The program structure always coincides with that of the indentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The style of indentation is uniform in any listing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad indentation can lead to errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to use the `if` construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following statements: `print("first print")`, `if condition:`, `print("third
    print")` have the same indentation level and are always executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the `if` statement, there is a block of code with a higher indentation
    level, which includes the `print ("second print")` statement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the condition of `if` is true, then the `print ("second print")` statement
    is executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the condition of `if` is false, then the `print ("second print")` statement
    is not executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is, therefore, very important to pay attention to indentation because it
    is always evaluated in the program parsing process.
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Comments start with the hash sign (`#`) and are on a single line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Multi-line strings are used for multi-line comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Assignments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assignments are made with the equals symbol (`=`). For equality tests, the same
    amount (`==`) is used. You can increase and decrease a value using the `+=` and
    `-=` operators, followed by an addendum. This works with many types of data, including
    strings. You can assign and use multiple variables on the same line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most significant structures in Python are *lists*, *tuples*,and *dictionaries*.
    Sets have been integrated into Python since version 2.5 (the previous versions
    are available in the `sets` library):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lists**: These are similar to one-dimensional arrays, but you can create
    lists that contain other lists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dictionaries**: These are arrays that contain key pairs and values (hash
    tables).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tuples**: These are immutable mono-dimensional objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arrays can be of any type, so you can mix variables such as integers and strings
    into your lists, dictionaries and tuples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The index of the first object in any type of array is always zero. Negative
    indexes are allowed and count from the end of the array; `-1` indicates the last
    element of the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get an array range using the colon (`:`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Strings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python strings are indicated using either the single (`''`) or double (`"`)
    quotation mark and they are allowed to use one notation within a string delimited
    by the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'On multiple lines, they are enclosed in triple (or three single) quotation
    marks (`''''''` multi-line string `''''''`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Python also supports Unicode; just use the `u "This is a unicode string"` syntax
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To enter values in a string, type the `%` operator and a tuple. Then, each `%`
    operator is replaced by a tuple element, from left to right*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Flow control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flow control instructions are `if`, `for`, and `while`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we check whether the number is positive, negative, or
    zero and display the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block finds the sum of all the numbers stored in a list,
    using a `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute the `while` loop to iterate the code until the condition result
    is true. We will use this loop over the `for`loop since we are unaware of the
    number of iterations that will result in the code. In this example, we use `while`
    to add natural numbers up to *sum = 1+2+3+...+n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs for the preceding three examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python functions are declared with the `def` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a function, use the function name, followed by parentheses, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters must be specified after the function name, inside the parentheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiple parameters must be separated with a comma:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the equals sign to define a default parameter. If you call the function
    without the parameter, then the default value will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of a function can be of any type of data (such as string, number,
    list, and dictionary). Here, the following list, `lcities`, is used as a parameter
    for `my_function`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `return` statement to return a value from a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Python supports an interesting syntax that allows you to define small, single-line
    functions on the fly. Derived from the Lisp programming language, these lambda
    functions can be used wherever a function is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a lambda function, `functionvar`, is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python supports multiple inheritances of classes. Conventionally (not a language
    rule), private variables and methods are declared by being preceded with two underscores
    (`__`). We can assign arbitrary attributes (properties) to the instances of a
    class, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Exceptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Exceptions in Python are managed with `try-except` blocks (`exception_name`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Importing libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'External libraries are imported with `import [library name]`. Alternatively,
    you can use the `from [library name] import [function name]` syntax to import
    a specific function. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Managing files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To allow us to interact with the filesystem, Python provides us with the built-in `open` function.This
    function can be invoked to open a file and return an object file. The latter allows
    us to perform various operations on the file, such as reading and writing. When
    we have finished interacting with the file, we must finally remember to close
    it by using the `file.close` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: List comprehensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'List comprehensions are a powerful tool for creating and manipulating lists.
    They consist of an expression that is followed by a `for` clause and then followed
    by zero, or more,`if` clauses. The syntax for list comprehensions is simply the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Running Python scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To execute a Python script, simply invoke the Python interpreter followed by
    the script name, in this case, `my_pythonscript.py`. Or, if we are in a different
    working directory, then use its full address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: From now on, for every invocation of a Python script, we will use the preceding
    notation; that is, `python`, followed by `script_name.py`, assuming that the directory
    from which the Python interpreter is launched is the one where the script to be
    execu*ted* resides.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python packages using pip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip` is a tool that allows us to search, download, and install Python packages
    found on the Python Package Index, which is a repository that contains tens of
    thousands of packages written in Python. This also allows us to manage the packages
    we have already downloaded, allowing us to update or remove them.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing pip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip` is already included in Python versions ≥ 3.4 and ≥ 2.7.9\. To check whether
    this tool is already installed, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If `pip` is already installed, then this command will show us the installed
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Updating pip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is also recommended to check that the `pip` version you are using is always
    up to date. To update it, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Using pip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip` supports a series of commands that allow us,among other things, to *search,
    download, install, update,* and *remove* packages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `PACKAGE`, just run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Introducing Python parallel programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python provides many libraries and frameworks that facilitate high-performance
    computations. However, doing parallel programming with Python can be quite insidious
    due to the **Global Interpreter Lock** (**GIL**).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the most widespread and widely used Python interpreter, **CPython**,
    is developed in the C programming language. The CPython interpreter needs GIL
    for thread-safe operations. The use of GIL implies that you will encounter a global
    lock when you attempt to access any Python objects contained within threads. And
    only one thread at a time can acquire the lock for a Python object or C API.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, things are not so serious, because, outside the realm of GIL, we
    can freely use parallelism. This category includes all the topics that we will
    discuss in the next chapters, including multiprocessing, distributed computing,
    and GPU computing.
  prefs: []
  type: TYPE_NORMAL
- en: So, Python is not really multithreaded. But what is a thread? What is a process?
    In the following sections, we will introduce these two fundamental concepts and
    how they are addressed by the Python programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Processes and threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Threads* can be compared to light processes, in the sense that they offer
    advantages similar to those of processes, without, however, requiring the typical
    communication techniques of processes. Threads allow you to divide the main control
    flow of a program into multiple concurrently running control streams. Processes,
    by contrast, have their *own* *addressing space* and their own resources*.*It
    follows that communication between parts of code running on different processes
    can only take place through appropriate management mechanisms, including pipes,
    code FIFO, mailboxes, shared memory areas, and message passing. Threads, on the
    other hand, allow the creation of concurrent parts of the program, in which each
    part can access the same address space, variables, and constants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the main differences between threads and processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Threads** | **Processes** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Share memory. | Do not share memory. |'
  prefs: []
  type: TYPE_TB
- en: '| Start/change are computationally less expensive. | Start/change are computationally
    expensive. |'
  prefs: []
  type: TYPE_TB
- en: '| Require fewer resources (light processes). | Require more computational resources.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Need synchronization mechanisms to handle data correctly. | No memory synchronization
    is required. |'
  prefs: []
  type: TYPE_TB
- en: After this brief introduction, we can finally show how processes and threads
    operate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we want to compare the serial, multithread, and multiprocess
    execution times of the following function, `do_something`, which performs some
    basic calculations, including building a list of integers selected randomly (a `do_something.py` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, there is the serial (`serial_test.py`) implementation. Let''s start with
    the relevant imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the importing of the module time, which will be used to evaluate the execution
    time, in this instance, and the serial implementation of the `do_something` function. `size`
    of the list to build is equal to `10000000`, while the `do_something` function
    will be executed `10` times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Next, we have the multithreaded implementation (`multithreading_test.py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note the import of the `threading` module in order to operate with the multithreading
    capabilities of Python.
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is the multithreading execution of the `do_something` function.
    We will not comment in-depth on the instructions in the following code, as they
    will be discussed in more detail in [Chapter 2](c95be391-9558-4d2d-867e-96f61fbc5bbf.xhtml), *Thread-Based
    Parallelism*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it should be noted in this case, too, that the length of the list
    is obviously the same as in the serial case, `size = 10000000`, while the number
    of threads defined is 10, `threads = 10`, which is also the number of times the
    `do_something` function must be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Note also the construction of the single thread, through the `threading.Thread`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The sequence of cycles in which we start executing threads and then stop them
    immediately afterwards is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Finally, there is the multiprocessing implementation (`multiprocessing_test.py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the necessary modules and, in particular, the `multiprocessing`
    library, whose features will be explained in-depth in [Chapter 3](5d4a1d39-061e-4c7c-937c-4ce3c9c6ea93.xhtml),
    *Process-Based Parallelism*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous cases, the length of the list to build, the size, and the
    execution number of the `do_something` function remain the same (`procs = 10`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the implementation of a single process through the `multiprocessing.Process`
    method call is affected as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the sequence of cycles in which we start executing processes and then
    stop them immediately afterwards is executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Then, we open the command shell and run the three functions described previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the folder where the functions have been copied and then type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The result, obtained on a machine with the following features—CPU Intel i7/8
    GB of RAM, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of the `multithreading` implementation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, there is the **multiprocessing** implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Its result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, the results of the serial implementation (that is, using `serial_test.py`)
    are similar to those obtained with the implementation of multithreading (using
    `multithreading_test.py`) where the threads are essentially launched one after
    the other, giving precedence to the one over the other until the end, while we
    have benefits in terms of execution times using the Python multiprocessing capability
    (using `multiprocessing_test.py`).
  prefs: []
  type: TYPE_NORMAL

["```py\nimport pycuda.driver as drv\n```", "```py\ndrv.init()\n```", "```py\nprint (\"%d device(s) found.\" % drv.Device.count())\n```", "```py\nfor ordinal i n range(drv.Device.count()): \n       dev = drv.Device(ordinal) \n       print (\"Device #%d: %s\" % (ordinal, dev.name()) \n       print (\"Compute Capability: %d.%d\"% dev.compute_capability()) \n       print (\"Total Memory: %s KB\" % (dev.total_memory()//(1024))) \n```", "```py\nimport pycuda.driver as drv  \ndrv.init() \n```", "```py\nprint (\"Device #%d: %s\" % (ordinal, dev.name()))  \nprint (\"Compute Capability: %d.%d\" % dev.compute_capability()) \nprint (\"Total Memory: %s KB\" % (dev.total_memory()//(1024))) \n```", "```py\nC:\\>python dealingWithPycuda.py\n```", "```py\n1 device(s) found.\nDevice #0: GeForce GT 240\nCompute Capability: 1.2\nTotal Memory: 1048576 KB\n```", "```py\nimport PyCUDA.driver as CUDA \nimport PyCUDA.autoinit \nfrom PyCUDA.compiler import SourceModule \nimport numpy \n```", "```py\na = numpy.random.randn(5,5) \na = a.astype(numpy.float32) \n```", "```py\na_gpu = cuda.mem_alloc(a.nbytes) \n```", "```py\ncuda.memcpy_htod(a_gpu, a) \n```", "```py\nmod = SourceModule(\"\"\" \n  __global__ void doubles_matrix(float *a){ \n    int idx = threadIdx.x + threadIdx.y*4; \n    a[idx] *= 2;} \n  \"\"\")\n```", "```py\nfunc = mod.get_function(\"doubles_matrix\") \n```", "```py\nfunc(a_gpu, block=(5,5,1)) \n```", "```py\na_doubled = numpy.empty_like(a) \n```", "```py\ncuda.memcpy_dtoh(a_doubled, a_gpu) \n```", "```py\nprint (\"ORIGINAL MATRIX\") \nprint (a) \nprint (\"DOUBLED MATRIX AFTER PyCUDA EXECUTION\") \nprint (a_doubled) \n```", "```py\nimport PyCUDA.driver as CUDA \nimport PyCUDA.autoinit \nfrom PyCUDA.compiler import SourceModule \n```", "```py\nimport numpy \na = numpy.random.randn(5,5) \n```", "```py\na = a.astype(numpy.float32) \n```", "```py\na_gpu = CUDA.mem_alloc(a.nbytes) \nCUDA.memcpy_htod(a_gpu, a) \n```", "```py\nmod = SourceModule(\"\"\" \n  __global__ void doubleMatrix(float *a) \n```", "```py\n int idx = threadIdx.x + threadIdx.y*4; \n    a[idx] *= 2; \n```", "```py\nfunc = mod.get_function(\"doubleMatrix \") \n```", "```py\nfunc(a_gpu, block = (5, 5, 1)) \n```", "```py\na_doubled = numpy.empty_like(a) \nCUDA.memcpy_dtoh(a_doubled, a_gpu) \n```", "```py\nC:\\>python heterogenousPycuda.py\n```", "```py\nORIGINAL MATRIX\n[[-0.59975582 1.93627465 0.65337795 0.13205571 -0.46468592]\n[ 0.01441949 1.40946579 0.5343408 -0.46614054 -0.31727529]\n[-0.06868593 1.21149373 -0.6035406 -1.29117763 0.47762445]\n[ 0.36176383 -1.443097 1.21592784 -1.04906416 -1.18935871]\n[-0.06960868 -1.44647694 -1.22041082 1.17092752 0.3686313 ]] \nDOUBLED MATRIX AFTER PyCUDA EXECUTION\n[[-1.19951165 3.8725493 1.3067559 0.26411143 -0.92937183]\n[ 0.02883899 2.81893158 1.0686816 -0.93228108 -0.63455057]\n[-0.13737187 2.42298746 -1.2070812 -2.58235526 0.95524889]\n[ 0.72352767 -2.886194 2.43185568 -2.09812832 -2.37871742]\n[-0.13921736 -2.89295388 -2.44082164 2.34185504 0.73726263 ]]\n```", "```py\nvoid SequentialMatrixMultiplication(float*M,float *N,float *P, int width){ \n  for (int i=0; i< width; ++i) \n      for(int j=0;j < width; ++j) { \n          float sum = 0; \n          for (int k = 0 ; k < width; ++k) { \n              float a = M[I * width + k]; \n              float b = N[k * width + j]; \n              sum += a * b; \n                     } \n         P[I * width + j] = sum; \n    } \n} \nP[I * width + j] = sum; \n```", "```py\nimport numpy as np \nfrom pycuda import driver, compiler, gpuarray, tools \n```", "```py\nimport pycuda.autoinit \n```", "```py\nkernel_code_template = \"\"\" \n__global__ void MatrixMulKernel(float *a, float *b, float *c) \n{ \n    int tx = threadIdx.x; \n    int ty = threadIdx.y; \n    float Pvalue = 0; \n    for (int k = 0; k < %(MATRIX_SIZE)s; ++k) { \n        float Aelement = a[ty * %(MATRIX_SIZE)s + k]; \n        float Belement = b[k * %(MATRIX_SIZE)s + tx]; \n        Pvalue += Aelement * Belement; \n    } \n    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue; \n}\"\"\" \n```", "```py\nMATRIX_SIZE = 5\n```", "```py\na_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) \nb_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n```", "```py\nc_cpu = np.dot(a_cpu, b_cpu) \n```", "```py\na_gpu = gpuarray.to_gpu(a_cpu)  \nb_gpu = gpuarray.to_gpu(b_cpu) \n```", "```py\nc_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) \n```", "```py\nkernel_code = kernel_code_template % { \n    'MATRIX_SIZE': MATRIX_SIZE} \n```", "```py\nmod = compiler.SourceModule(kernel_code) \n```", "```py\nmatrixmul = mod.get_function(\"MatrixMulKernel\")\n```", "```py\nmatrixmul( \n    a_gpu, b_gpu,  \n    c_gpu,  \n    block = (MATRIX_SIZE, MATRIX_SIZE, 1))\n```", "```py\nprint (\"-\" * 80) \nprint (\"Matrix A (GPU):\") \nprint (a_gpu.get()) \nprint (\"-\" * 80) \nprint (\"Matrix B (GPU):\") \nprint (b_gpu.get()) \nprint (\"-\" * 80) \nprint (\"Matrix C (GPU):\") \nprint (c_gpu.get()) \n```", "```py\nnp.allclose(c_cpu, c_gpu.get()) \n```", "```py\nMATRIX_SIZE = 5 \na_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) \nb_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) \nc_cpu = np.dot(a_cpu, b_cpu) \n```", "```py\na_gpu = gpuarray.to_gpu(a_cpu)  \nb_gpu = gpuarray.to_gpu(b_cpu) \nc_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) \n```", "```py\n__global__ void MatrixMulKernel(float *a, float *b, float *c){\n int tx = threadIdx.x;\n int ty = threadIdx.y;\n float Pvalue = 0;\n for (int k = 0; k < %(MATRIX_SIZE)s; ++k) {\n float Aelement = a[ty * %(MATRIX_SIZE)s + k];\n float Belement = b[k * %(MATRIX_SIZE)s + tx];\n Pvalue += Aelement * Belement;}\n c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;\n}\n```", "```py\nnp.allclose(c_cpu, c_gpu.get())\n```", "```py\nC:\\>python memManagementPycuda.py\n\n---------------------------------------------------------------------\nMatrix A (GPU):\n[[ 0.90780383 -0.4782407 0.23222363 -0.63184392 1.05509627]\n [-1.27266967 -1.02834761 -0.15528528 -0.09468858 1.037099 ]\n [-0.18135822 -0.69884419 0.29881889 -1.15969539 1.21021318]\n [ 0.20939326 -0.27155793 -0.57454145 0.1466181 1.84723163]\n [ 1.33780348 -0.42343542 -0.50257754 -0.73388749 -1.883829 ]]\n---------------------------------------------------------------------\nMatrix B (GPU):\n[[ 0.04523897 0.99969769 -1.04473436 1.28909719 1.10332143]\n [-0.08900332 -1.3893919 0.06948703 -0.25977209 -0.49602833]\n [-0.6463753 -1.4424541 -0.81715286 0.67685211 -0.94934392]\n [ 0.4485206 -0.77086055 -0.16582981 0.08478995 1.26223004]\n [-0.79841441 -0.16199949 -0.35969591 -0.46809086 0.20455229]]\n---------------------------------------------------------------------\nMatrix C (GPU):\n[[-1.19226956 1.55315971 -1.44614291 0.90420711 0.43665022]\n [-0.73617989 0.28546685 1.02769876 -1.97204924 -0.65403283]\n [-1.62555301 1.05654192 -0.34626681 -0.51481217 -1.35338223]\n [-1.0040834 1.00310731 -0.4568972 -0.90064859 1.47408712]\n [ 1.59797418 3.52156591 -0.21708387 2.31396151 0.85150564]]\n---------------------------------------------------------------------\n\nTRUE\n```", "```py\n**(base) C:\\> pip install <directory>\\pyopencl-2019.1+cl12-cp37-cp37m-win_amd64.whl** \n```", "```py\n**(base) C:\\>**\n```", "```py\nimport pyopencl as cl\n```", "```py\ndef print_device_info() :\n print('\\n' + '=' * 60 + '\\nOpenCL Platforms and Devices')\n for platform in cl.get_platforms():\n print('=' * 60)\n print('Platform - Name: ' + platform.name)\n print('Platform - Vendor: ' + platform.vendor)\n print('Platform - Version: ' + platform.version)\n print('Platform - Profile: ' + platform.profile)\n\n for device in platform.get_devices():\n print(' ' + '-' * 56)\n print(' Device - Name: ' \\\n + device.name)\n print(' Device - Type: ' \\\n + cl.device_type.to_string(device.type))\n print(' Device - Max Clock Speed: {0} Mhz'\\\n .format(device.max_clock_frequency))\n print(' Device - Compute Units: {0}'\\\n .format(device.max_compute_units))\n print(' Device - Local Memory: {0:.0f} KB'\\\n .format(device.local_mem_size/1024.0))\n print(' Device - Constant Memory: {0:.0f} KB'\\\n .format(device.max_constant_buffer_size/1024.0))\n print(' Device - Global Memory: {0:.0f} GB'\\\n .format(device.global_mem_size/1073741824.0))\n print(' Device - Max Buffer/Image Size: {0:.0f} MB'\\\n .format(device.max_mem_alloc_size/1048576.0))\n print(' Device - Max Work Group Size: {0:.0f}'\\\n .format(device.max_work_group_size))\n print('\\n')\n```", "```py\nif __name__ == \"__main__\":\n print_device_info()\n```", "```py\nimport pyopencl as cl\n```", "```py\nfor platform in cl.get_platforms():\n```", "```py\n(base) C:\\>python deviceInfoPyopencl.py\n\n=============================================================\nOpenCL Platforms and Devices\n============================================================\nPlatform - Name: NVIDIA CUDA\nPlatform - Vendor: NVIDIA Corporation\nPlatform - Version: OpenCL 1.2 CUDA 10.1.152\nPlatform - Profile: FULL_PROFILE\n --------------------------------------------------------\n Device - Name: GeForce 840M\n Device - Type: GPU\n Device - Max Clock Speed: 1124 Mhz\n Device - Compute Units: 3\n Device - Local Memory: 48 KB\n Device - Constant Memory: 64 KB\n Device - Global Memory: 2 GB\n Device - Max Buffer/Image Size: 512 MB\n Device - Max Work Group Size: 1024\n============================================================\nPlatform - Name: Intel(R) OpenCL\nPlatform - Vendor: Intel(R) Corporation\nPlatform - Version: OpenCL 2.0\nPlatform - Profile: FULL_PROFILE\n --------------------------------------------------------\n Device - Name: Intel(R) HD Graphics 5500\n Device - Type: GPU\n Device - Max Clock Speed: 950 Mhz\n Device - Compute Units: 24\n Device - Local Memory: 64 KB\n Device - Constant Memory: 64 KB\n Device - Global Memory: 3 GB\n Device - Max Buffer/Image Size: 808 MB\n Device - Max Work Group Size: 256\n --------------------------------------------------------\n Device - Name: Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz\n Device - Type: CPU\n Device - Max Clock Speed: 2400 Mhz\n Device - Compute Units: 4\n Device - Local Memory: 32 KB\n Device - Constant Memory: 128 KB\n Device - Global Memory: 8 GB\n Device - Max Buffer/Image Size: 2026 MB\n Device - Max Work Group Size: 8192\n```", "```py\nimport numpy as np \nimport pyopencl as cl \nimport numpy.linalg as la \n```", "```py\nvector_dimension = 100 \n```", "```py\nvector_a = np.random.randint(vector_dimension,size=vector_dimension) \nvector_b = np.random.randint(vector_dimension,size=vector_dimension) \n```", "```py\nplatform = cl.get_platforms()[1] \ndevice = platform.get_devices()[0] \ncontext = cl.Context([device]) \nqueue = cl.CommandQueue(context) \n```", "```py\nmf = cl.mem_flags \na_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\\ hostbuf=vector_a) \nb_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\\ hostbuf=vector_b) \n```", "```py\nprogram = cl.Program(context, \"\"\" \n__kernel void vectorSum(__global const int *a_g, __global const int *b_g, __global int *res_g) { \n  int gid = get_global_id(0); \n  res_g[gid] = a_g[gid] + b_g[gid]; \n} \n\"\"\").build()\n```", "```py\nres_g = cl.Buffer(context, mf.WRITE_ONLY, vector_a.nbytes) \n```", "```py\nprogram.vectorSum(queue, vector_a.shape, None, a_g, b_g, res_g) \n```", "```py\nres_np = np.empty_like(vector_a) \n```", "```py\ncl._enqueue_copy(queue, res_np, res_g) \n```", "```py\nprint (\"PyOPENCL SUM OF TWO VECTORS\") \nprint (\"Platform Selected = %s\" %platform.name ) \nprint (\"Device Selected = %s\" %device.name) \nprint (\"VECTOR LENGTH = %s\" %vector_dimension) \nprint (\"INPUT VECTOR A\") \nprint (vector_a) \nprint (\"INPUT VECTOR B\") \nprint (vector_b) \nprint (\"OUTPUT VECTOR RESULT A + B \") \nprint (res_np) \n```", "```py\nassert(la.norm(res_np - (vector_a + vector_b))) < 1e-5 \n```", "```py\nvector_dimension = 100 \nvector_a = np.random.randint(vector_dimension, size= vector_dimension) \nvector_b = np.random.randint(vector_dimension, size= vector_dimension) \n```", "```py\nnp.random.randint(max integer , size of the vector) \n```", "```py\nplatform = cl.get_platforms()[1] \n```", "```py\ndevice = platform.get_devices()[0]\n```", "```py\ncontext = cl.Context([device]) \nqueue = cl.CommandQueue(context) \n```", "```py\nmf = cl.mem_flags \na_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\\\nhostbuf=vector_a) \nb_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\\\n hostbuf=vector_b) \n```", "```py\nres_g = cl.Buffer(context, mf.WRITE_ONLY, vector_a.nbytes) \n```", "```py\nprogram = cl.Program(context, \"\"\" \n__kernel void vectorSum(__global const int *a_g, __global const int *b_g, __global int *res_g) { \n  int gid = get_global_id(0); \n  res_g[gid] = a_g[gid] + b_g[gid];} \n\"\"\").build()\n```", "```py\nprogram.vectorSum(queue, vector_a.shape, None, a_g, b_g, res_g)\n```", "```py\nassert(la.norm(res_np - (vector_a + vector_b))) < 1e-5\n```", "```py\n(base) C:\\>python vectorSumPyopencl.py \nPyOPENCL SUM OF TWO VECTORS\nPlatform Selected = Intel(R) OpenCL\nDevice Selected = Intel(R) HD Graphics 5500\nVECTOR LENGTH = 100\nINPUT VECTOR A\n [45 46 0 97 96 98 83 7 51 21 72 70 59 65 79 92 98 24 56 6 70 64 59 0\n 96 78 15 21 4 89 14 66 53 20 34 64 48 20 8 53 82 66 19 53 11 17 39 11\n 89 97 51 53 7 4 92 82 90 78 31 18 72 52 44 17 98 3 36 69 25 87 86 68\n 85 16 58 4 57 64 97 11 81 36 37 21 51 22 17 6 66 12 80 50 77 94 6 70\n 21 86 80 69]\n INPUT VECTOR B\n[25 8 76 57 86 96 58 89 26 31 28 92 67 47 72 64 13 93 96 91 91 36 1 75\n 2 40 60 49 24 40 23 35 80 60 61 27 82 38 66 81 95 79 96 23 73 19 5 43\n 2 47 17 88 46 76 64 82 31 73 43 17 35 28 48 89 8 61 23 17 56 7 84 36\n 95 60 34 9 4 5 74 59 6 89 84 98 25 50 38 2 3 43 64 96 47 79 12 82\n 72 0 78 5]\n OUTPUT VECTOR RESULT A + B\n[70 54 76 154 182 194 141 96 77 52 100 162 126 112 151 156 111 117 152 \n 97 161 100 60 75 98 118 75 70 28 129 37 101 133 80 95 91 130 58 74 134 \n 177 145 115 76 84 36 44 54 91 144 68 141 53 80 156 164 121 151 74 35 \n 107 80 92 106 106 64 59 86 81 94 170 104 80 76 92 13 61 69 171 70 87 \n 125 121 119 76 72 55 8 69 55 144 146 124 173 18 152 93 86 158 74] \n```", "```py\nimport pyopencl as cl\nimport pyopencl.array as cl_array\nimport numpy as np\n```", "```py\ncontext = cl.create_some_context()\nqueue = cl.CommandQueue(context)\n```", "```py\nvector_dim = 100 \nvector_a=cl_array.to_device(queue,np.random.randint(100,\\\nsize=vector_dim)) \nvector_b = cl_array.to_device(queue,np.random.randint(100,\\ \nsize=vector_dim)) \nresult_vector = cl_array.empty_like(vector_a) \n```", "```py\nelementwiseSum = cl.elementwise.ElementwiseKernel(context, \"int *a,\\\nint *b, int *c\", \"c[i] = a[i] + b[i]\", \"sum\")\nelementwiseSum(vector_a, vector_b, result_vector)\n```", "```py\nprint (\"PyOpenCL ELEMENTWISE SUM OF TWO VECTORS\")\nprint (\"VECTOR LENGTH = %s\" %vector_dimension)\nprint (\"INPUT VECTOR A\")\nprint (vector_a)\nprint (\"INPUT VECTOR B\")\nprint (vector_b)\nprint (\"OUTPUT VECTOR RESULT A + B \")\nprint (result_vector)\n```", "```py\nChoose platform:\n[0] <pyopencl.Platform 'NVIDIA CUDA' at 0x1c0a25aecf0>\n[1] <pyopencl.Platform 'Intel(R) OpenCL' at 0x1c0a2608400>\n```", "```py\nqueue = cl.CommandQueue(context)\n```", "```py\ncl.array_to_device(queue,array)\n```", "```py\nelementwiseSum = cl.elementwise.ElementwiseKernel(context,\\\n \"int *a, int *b, int *c\", \"c[i] = a[i] + b[i]\", \"sum\")\n```", "```py\nelementwiseSum(vector_a, vector_b, result_vector)\n```", "```py\n(base) C:\\>python elementwisePyopencl.py\n\nChoose platform:\n[0] <pyopencl.Platform 'NVIDIA CUDA' at 0x1c0a25aecf0>\n[1] <pyopencl.Platform 'Intel(R) OpenCL' at 0x1c0a2608400>\nChoice [0]:1 \nChoose device(s):\n[0] <pyopencl.Device 'Intel(R) HD Graphics 5500' on 'Intel(R) OpenCL' at 0x1c0a1640db0>\n[1] <pyopencl.Device 'Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz' on 'Intel(R) OpenCL' at 0x1c0a15e53f0>\nChoice, comma-separated [0]:0 PyOpenCL ELEMENTWISE SUM OF TWO VECTORS\nVECTOR LENGTH = 100\nINPUT VECTOR A\n[24 64 73 37 40 4 41 85 19 90 32 51 6 89 98 56 97 53 34 91 82 89 97 2\n 54 65 90 90 91 75 30 8 62 94 63 69 31 99 8 18 28 7 81 72 14 53 91 80\n 76 39 8 47 25 45 26 56 23 47 41 18 89 17 82 84 10 75 56 89 71 56 66 61\n 58 54 27 88 16 20 9 61 68 63 74 84 18 82 67 30 15 25 25 3 93 36 24 27\n 70 5 78 15] \nINPUT VECTOR B\n[49 18 69 43 51 72 37 50 79 34 97 49 51 29 89 81 33 7 47 93 70 52 63 90\n 99 95 58 33 41 70 84 87 20 83 74 43 78 34 94 47 89 4 30 36 34 56 32 31\n 56 22 50 52 68 98 52 80 14 98 43 60 20 49 15 38 74 89 99 29 96 65 89 41\n 72 53 89 31 34 64 0 47 87 70 98 86 41 25 34 10 44 36 54 52 54 86 33 38\n 25 49 75 53] \nOUTPUT VECTOR RESULT A + B\n[73 82 142 80 91 76 78 135 98 124 129 100 57 118 187 137 130 60 81 184 \n 152 141 160 92 153 160 148 123 132 145 114 95 82 177 137 112 109 133 \n 102 65 117 11 111 108 48 109 123 111 132 61 58 99 93 143 78 136 37 145 \n 84 78 109 66 97 122 84 164 155 118 167 121 155 102 130 107 116 119 50 \n 84 9 108 155 133 172 170 59 107 101 40 59 61 79 55 147 122 57 65 \n 95 54 153 68] \n```", "```py\nElementwiseKernel(arguments,operation,name,optional_parameters)\n```", "```py\nimport pycuda.autoinit \nimport numpy \nfrom pycuda.elementwise import ElementwiseKernel \nimport numpy.linalg as la \n\nvector_dimension=100 \ninput_vector_a = np.random.randint(100,size= vector_dimension) \ninput_vector_b = np.random.randint(100,size= vector_dimension) \noutput_vector_c = gpuarray.empty_like(input_vector_a) \n\nelementwiseSum = ElementwiseKernel(\" int *a, int * b, int *c\",\\ \n                             \"c[i] = a[i] + b[i]\",\" elementwiseSum \") \nelementwiseSum(input_vector_a, input_vector_b,output_vector_c) \n\nprint (\"PyCUDA ELEMENTWISE SUM OF TWO VECTORS\") \nprint (\"VECTOR LENGTH = %s\" %vector_dimension) \nprint (\"INPUT VECTOR A\") \nprint (vector_a) \nprint (\"INPUT VECTOR B\") \nprint (vector_b) \nprint (\"OUTPUT VECTOR RESULT A + B \") \nprint (result_vector) \n```", "```py\nfrom time import time \nimport pyopencl as cl   \nimport numpy as np    \nimport deviceInfoPyopencl as device_info \nimport numpy.linalg as la \n```", "```py\na = np.random.rand(10000).astype(np.float32) \nb = np.random.rand(10000).astype(np.float32) \n```", "```py\ndef test_cpu_vector_sum(a, b): \n    c_cpu = np.empty_like(a) \n    cpu_start_time = time() \n    for i in range(10000): \n            for j in range(10000): \n                    c_cpu[i] = a[i] + b[i] \n    cpu_end_time = time() \n    print(\"CPU Time: {0} s\".format(cpu_end_time - cpu_start_time)) \n    return c_cpu \n```", "```py\ndef test_gpu_vector_sum(a, b): \n    platform = cl.get_platforms()[0] \n    device = platform.get_devices()[0] \n    context = cl.Context([device]) \n    queue = cl.CommandQueue(context,properties=\\\n cl.command_queue_properties.PROFILING_ENABLE)\n```", "```py\n a_buffer = cl.Buffer(context,cl.mem_flags.READ_ONLY \\ \n                | cl.mem_flags.COPY_HOST_PTR, hostbuf=a) \n    b_buffer = cl.Buffer(context,cl.mem_flags.READ_ONLY \\ \n                | cl.mem_flags.COPY_HOST_PTR, hostbuf=b) \n    c_buffer = cl.Buffer(context,cl.mem_flags.WRITE_ONLY, b.nbytes) \n```", "```py\n program = cl.Program(context, \"\"\" \n    __kernel void sum(__global const float *a,\\ \n                      __global const float *b,\\ \n                      __global float *c){ \n        int i = get_global_id(0); \n        int j; \n        for(j = 0; j < 10000; j++){ \n            c[i] = a[i] + b[i];} \n    }\"\"\").build() \n```", "```py\n gpu_start_time = time() \n    event = program.sum(queue, a.shape, None,a_buffer, b_buffer,\\ \n c_buffer) \n    event.wait() \n    elapsed = 1e-9*(event.profile.end - event.profile.start) \n    print(\"GPU Kernel evaluation Time: {0} s\".format(elapsed)) \n    c_gpu = np.empty_like(a) \n    cl._enqueue_read_buffer(queue, c_buffer, c_gpu).wait() \n    gpu_end_time = time() \n    print(\"GPU Time: {0} s\".format(gpu_end_time - gpu_start_time)) \n    return c_gpu \n```", "```py\nif __name__ == \"__main__\": \n    device_info.print_device_info() \n    cpu_result = test_cpu_vector_sum(a, b) \n    gpu_result = test_gpu_vector_sum(a, b) \n    assert (la.norm(cpu_result - gpu_result)) < 1e-5 \n```", "```py\n cpu_start_time = time() \n               for i in range(10000): \n                         for j in range(10000): \n                             c_cpu[i] = a[i] + b[i] \n               cpu_end_time = time() \n```", "```py\n CPU Time = cpu_end_time - cpu_start_time \n```", "```py\n __kernel void sum(__global const float *a, \n                      __global const float *b, \n                      __global float *c){ \n        int i=get_global_id(0); \n        int j; \n        for(j=0;j< 10000;j++){ \n            c[i]=a[i]+b[i];} \n```", "```py\n(base) C:\\>python testApplicationPyopencl.py \n\n============================================================\nOpenCL Platforms and Devices\n============================================================\nPlatform - Name: NVIDIA CUDA\nPlatform - Vendor: NVIDIA Corporation\nPlatform - Version: OpenCL 1.2 CUDA 10.1.152\nPlatform - Profile: FULL_PROFILE\n --------------------------------------------------------\n Device - Name: GeForce 840M\n Device - Type: GPU\n Device - Max Clock Speed: 1124 Mhz\n Device - Compute Units: 3\n Device - Local Memory: 48 KB\n Device - Constant Memory: 64 KB\n Device - Global Memory: 2 GB\n Device - Max Buffer/Image Size: 512 MB\n Device - Max Work Group Size: 1024\n============================================================\nPlatform - Name: Intel(R) OpenCL\nPlatform - Vendor: Intel(R) Corporation\nPlatform - Version: OpenCL 2.0\nPlatform - Profile: FULL_PROFILE\n --------------------------------------------------------\n Device - Name: Intel(R) HD Graphics 5500\n Device - Type: GPU\n Device - Max Clock Speed: 950 Mhz\n Device - Compute Units: 24\n Device - Local Memory: 64 KB\n Device - Constant Memory: 64 KB\n Device - Global Memory: 3 GB\n Device - Max Buffer/Image Size: 808 MB\n Device - Max Work Group Size: 256\n --------------------------------------------------------\n Device - Name: Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz\n Device - Type: CPU\n Device - Max Clock Speed: 2400 Mhz\n Device - Compute Units: 4\n Device - Local Memory: 32 KB\n Device - Constant Memory: 128 KB\n Device - Global Memory: 8 GB\n Device - Max Buffer/Image Size: 2026 MB\n Device - Max Work Group Size: 8192\n\nCPU Time: 39.505873918533325 s\nGPU Kernel evaluation Time: 0.013606592 s\nGPU Time: 0.019981861114501953 s \n```", "```py\n(base) C:\\> conda install numba\n```", "```py\n(base) C:\\> conda install cudatoolkit\n```", "```py\n(base) C:\\> python\nPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>\n```", "```py\n>>> import numba.cuda.api\n>>> import numba.cuda.cudadrv.libs\n>>> numba.cuda.cudadrv.libs.test()\n```", "```py\nFinding cublas from Conda environment\n located at C:\\Users\\Giancarlo\\Anaconda3\\Library\\bin\\cublas64_10.dll\n trying to open library... ok\nFinding cusparse from Conda environment\n located at C:\\Users\\Giancarlo\\Anaconda3\\Library\\bin\\cusparse64_10.dll\n trying to open library... ok\nFinding cufft from Conda environment\n located at C:\\Users\\Giancarlo\\Anaconda3\\Library\\bin\\cufft64_10.dll\n trying to open library... ok\nFinding curand from Conda environment\n located at C:\\Users\\Giancarlo\\Anaconda3\\Library\\bin\\curand64_10.dll\n trying to open library... ok\nFinding nvvm from Conda environment\n located at C:\\Users\\Giancarlo\\Anaconda3\\Library\\bin\\nvvm64_33_0.dll\n trying to open library... ok\nFinding libdevice from Conda environment\n searching for compute_20... ok\n searching for compute_30... ok\n searching for compute_35... ok\n searching for compute_50... ok\nTrue\n\n```", "```py\n>>> numba.cuda.api.detect()\n```", "```py\nFound 1 CUDA devices\nid 0 b'GeForce 840M' [SUPPORTED]\n compute capability: 5.0\n pci device id: 0\n pci bus id: 8\nSummary:\n 1/1 devices are supported\nTrue\n```", "```py\nfrom numba import guvectorize \nimport numpy as np \n```", "```py\n@guvectorize(['void(int64[:,:], int64[:,:], int64[:,:])'], \n             '(m,n),(n,p)->(m,p)') \ndef matmul(A, B, C): \n    m, n = A.shape \n    n, p = B.shape \n    for i in range(m): \n        for j in range(p): \n            C[i, j] = 0 \n            for k in range(n): \n                C[i, j] += A[i, k] * B[k, j] \n```", "```py\ndim = 10 \nA = np.random.randint(dim,size=(dim, dim)) \nB = np.random.randint(dim,size=(dim, dim)) \n```", "```py\nC = matmul(A, B) \n```", "```py\nprint(\"INPUT MATRIX A\") \nprint(\":\\n%s\" % A) \nprint(\"INPUT MATRIX B\") \nprint(\":\\n%s\" % B) \nprint(\"RESULT MATRIX C = A*B\") \nprint(\":\\n%s\" % C) \n```", "```py\n for i in range(m): \n            for j in range(p): \n                C[i, j] = 0 \n                for k in range(n): \n                      C[i, j] += A[i, k] * B[k, j] \n```", "```py\ndim = 10\nA = np.random.randint(dim,size=(dim, dim))\nB = np.random.randint(dim,size=(dim, dim))\n```", "```py\nC = matmul(A, B)\nprint(\"RESULT MATRIX C = A*B\")\nprint(\":\\n%s\" % C)\n```", "```py\n(base) C:\\>python matMulNumba.py\n```", "```py\nINPUT MATRIX A\n:\n[[8 7 1 3 1 0 4 9 2 2]\n [3 6 2 7 7 9 8 4 4 9]\n [8 9 9 9 1 1 1 1 8 0]\n [0 5 0 7 1 3 2 0 7 3]\n [4 2 6 4 1 2 9 1 0 5]\n [3 0 6 5 1 0 4 3 7 4]\n [0 9 7 2 1 4 3 3 7 3]\n [1 7 2 7 1 8 0 3 4 1]\n [5 1 5 0 7 7 2 3 0 9]\n [4 6 3 6 0 3 3 4 1 2]]\nINPUT MATRIX B\n:\n[[2 1 4 6 6 4 9 9 5 2]\n [8 6 7 6 5 9 2 1 0 9]\n [4 1 2 4 8 2 9 5 1 4]\n [9 9 1 5 0 5 1 1 7 1]\n [8 7 8 3 9 1 4 3 1 5]\n [7 2 5 8 3 5 8 5 6 2]\n [5 3 1 4 3 7 2 9 9 5]\n [8 7 9 3 4 1 7 8 0 4]\n [3 0 4 2 3 8 8 8 6 2]\n [8 6 7 1 8 3 0 8 8 9]]\nRESULT MATRIX C = A*B\n:\n[[225 172 201 161 170 172 189 230 127 169]\n [400 277 289 251 278 276 240 324 295 273]\n [257 171 177 217 208 254 265 224 176 174]\n [187 130 116 117 94 175 105 128 152 114]\n [199 133 117 143 168 156 143 214 188 157]\n [180 118 124 113 152 149 175 213 167 122]\n [238 142 186 165 188 215 202 200 139 192]\n [237 158 162 176 122 185 169 140 137 130]\n [249 160 220 159 249 125 201 241 169 191]\n [209 152 142 154 131 160 147 161 132 137]]\n```", "```py\nimport numpy \nfrom numba import cuda \n\n@cuda.reduce \ndef sum_reduce(a, b): \n    return a + b \n\nA = (numpy.arange(10000, dtype=numpy.int64)) + 1\nprint(A) \ngot = sum_reduce(A)\nprint(got) \n```", "```py\n(base) C:\\>python reduceNumba.py\n```", "```py\nvector to reduce = [ 1 2 3 ... 9998 9999 10000]\nresult = 50005000\n```"]
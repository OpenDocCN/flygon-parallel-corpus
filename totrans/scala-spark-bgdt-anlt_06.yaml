- en: Start Working with Spark â€“ REPL and RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"All this modern technology just makes people try to do everything at once."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Bill Watterson'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how Spark works; then, you will be introduced
    to RDDs, the basic abstractions behind Apache Spark, and you'll learn that they
    are simply distributed collections exposing Scala-like APIs. You will then see
    how to download Spark and how to make it run locally via the Spark shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Dig deeper into Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Spark shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions and Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and Saving data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dig deeper into Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a fast in-memory data processing engine with elegant and expressive
    development APIs to allow data workers to efficiently execute streaming machine
    learning or SQL workloads that require fast interactive access to datasets. Apache
    Spark consists of Spark core and a set of libraries. The core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    application development.
  prefs: []
  type: TYPE_NORMAL
- en: Additional libraries built on top of the core allow the workloads for streaming,
    SQL, Graph processing, and machine learning. SparkML, for instance, is designed
    for Data science and its abstraction makes Data science easier.
  prefs: []
  type: TYPE_NORMAL
- en: In order to plan and carry out the distributed computations, Spark uses the
    concept of a job, which is executed across the worker nodes using Stages and Tasks.
    Spark consists of a driver, which orchestrates the execution across a cluster
    of worker nodes. The driver is also responsible for tracking all the worker nodes
    as well as the work currently being performed by each of the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look into the various components a little more. The key components are
    the Driver and the Executors which are all JVM processes (Java processes):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Driver**: The Driver program contains the applications, main program. If
    you are using the Spark shell, that becomes the Driver program and the Driver
    launches the executors across the cluster and also controls the task executions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**: Next are the executors which are processes running on the worker
    nodes in your cluster. Inside the executor, the individual tasks or computations
    are run. There could be one or more executors in each worker node and, similarly,
    there could be multiple tasks inside each executor. When Driver connects to the
    cluster manager, the cluster manager assigns resources to run executors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster manager could be a standalone cluster manager, YARN, or Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Cluster Manager** is responsible for the scheduling and allocation of
    resources across the compute nodes forming the cluster. Typically, this is done
    by having a manager process which knows and manages a cluster of resources and
    allocates the resources to a requesting process such as Spark. We will look at
    the three different cluster managers: standalone, YARN, and Mesos further down
    in the next sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how Spark works at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00292.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The main entry point to a Spark program is called the `SparkContext`. The `SparkContext`
    is inside the **Driver** component and represents the connection to the cluster
    along with the code to run the scheduler and task distribution and orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark 2.x, a new variable called `SparkSession` has been introduced. `SparkContext`,
    `SQLContext`, and `HiveContext` are now member variables of the `SparkSession`.
  prefs: []
  type: TYPE_NORMAL
- en: When you start the **Driver** program, the commands are issued to the cluster
    using the `SparkContext`, and then the **executors** will execute the instructions.
    Once the execution is completed, the **Driver** program completes the job. You
    can, at this point, issue more commands and execute more Jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to maintain and reuse the `SparkContext` is a key advantage of the
    Apache Spark architecture, unlike the Hadoop framework where every `MapReduce`
    job or Hive query or Pig Script starts entire processing from scratch for each
    task we want to execute that too using expensive disk instead of memory.
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkContext` can be used to create RDDs, accumulators, and broadcast variables
    on the cluster. Only one `SparkContext` may be active per JVM/Java process. You
    must `stop()` the active `SparkContext` before creating a new one.
  prefs: []
  type: TYPE_NORMAL
- en: The **Driver** parses the code, and serializes the byte level code across to
    the executors to be executed. When we perform any computations, the computations
    will actually be done at the local level by each node, using in-memory processing.
  prefs: []
  type: TYPE_NORMAL
- en: The process of parsing the code and planning the execution is the key aspect
    implemented by the **Driver** process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how Spark **Driver** coordinates the computations across the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00298.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The **Directed Acyclic Graph** (**DAG**) is the secret sauce of Spark framework.
    The **Driver** process creates a DAG of tasks for a piece of code you try to run
    using the distributed processing framework. Then, the DAG is actually executed
    in stages and tasks by the task scheduler by communicating with the **Cluster
    Manager** for resources to run the executors. A DAG represents a job, and a job
    is split into subsets, also called stages, and each stage is executed as tasks
    using one core per task.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration of a simple job and how the DAG is split into stages and tasks
    is shown in the following two illustrations; the first one shows the job itself,
    and the second diagram shows the stages in the job and the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00301.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram now breaks down the job/DAG into stages and tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00304.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The number of stages and what the stages consist of is determined by the kind
    of operations. Usually, any transformation comes into the same stage as the one
    before, but every operation such as reduce or shuffle always creates a new stage
    of execution. Tasks are part of a stage and are directly related to the cores
    executing the operations on the executors.
  prefs: []
  type: TYPE_NORMAL
- en: If you use YARN or Mesos as the cluster manager, you can use dynamic YARN scheduler
    to increase the number of executors when more work needs to be done, as well as
    killing idle executors.
  prefs: []
  type: TYPE_NORMAL
- en: The driver, hence, manages the fault tolerance of the entire execution process.
    Once the job is completed by the Driver, the output can be written to a file,
    database, or simply to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the code in the Driver program itself has to be completely serializable
    including all the variables and objects.
  prefs: []
  type: TYPE_NORMAL
- en: The often seen exception is a not a serializable exception, which is a result
    of including global variables from outside the block.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the Driver process takes care of the entire execution process while monitoring
    and managing the resources used, such as executors, stages, and tasks, making
    sure everything is working as planned and recovering from failures such as task
    failures on executor nodes or entire executor nodes as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a cross-platform framework, which can be deployed on Linux,
    Windows, and a Mac Machine as long as we have Java installed on the machine. In
    this section, we will look at how to install Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark can be downloaded from [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the pre-requisites that must be available on the machine:'
  prefs: []
  type: TYPE_NORMAL
- en: Java 8+ (mandatory as all Spark software runs as JVM processes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.4+ (optional and used only when you want to use PySpark)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R 3.1+ (optional and used only when you want to use SparkR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala 2.11+ (optional and used only to write programs for Spark)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark can be deployed in three primary deployment modes, which we will look
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark standalone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark on YARN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark on Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark standalone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark standalone uses a built-in scheduler without depending on any external
    scheduler such as YARN or Mesos. To install Spark in standalone mode, you have
    to copy the spark binary install package onto all the machines in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In standalone mode, the client can interact with the cluster, either through
    spark-submit or Spark shell. In either case, the Driver communicates with the
    Spark master Node to get the worker nodes, where executors can be started for
    this application.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple clients interacting with the cluster create their own executors on
    the Worker Nodes. Also, each client will have its own Driver component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the standalone deployment of Spark using Master node and worker
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00307.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now download and install Spark in standalone mode using a Linux/Mac:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download Apache Spark from the link [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00313.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Extract the package in your local directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Change directory to the newly created directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set environment variables for `JAVA_HOME` and `SPARK_HOME` by implementing
    the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`JAVA_HOME` should be where you have Java installed. On my Mac terminal, this
    is set as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`SPARK_HOME` should be the newly extracted folder. On my Mac terminal, this
    is set as:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Run Spark shell to see if this works. If it does not work, check the `JAVA_HOME`
    and `SPARK_HOME` environment variable: `./bin/spark-shell`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will now see the shell as shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00316.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see the Scala/ Spark shell at the end and now you are ready to interact
    with the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a Spark-shell connected to an automatically setup local cluster
    running Spark. This is the quickest way to launch Spark on a local machine. However,
    you can still control the workers/executors as well as connect to any cluster
    (standalone/YARN/Mesos). This is the power of Spark, enabling you to quickly move
    from interactive testing to testing on a cluster and subsequently deploying your
    jobs on a large cluster. The seamless integration offers a lot of benefits, which
    you cannot realize using Hadoop and other technologies.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to the official documentation in case you want to understand all
    the settings [http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to start the Spark shell as in the following snippet.
    We will see more options in a later section, showing Spark shell in more detail.:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Default shell on local machine automatically picks local machine as master:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Default shell on local machine specifying local machine as master with `n`
    threads:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Default shell on local machine connecting to a specified spark master:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Default shell on local machine connecting to a YARN cluster using client mode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Default shell on local machine connecting to a YARN cluster using cluster mode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Spark Driver also has a Web UI, which helps you to understand everything about
    the Spark cluster, the executors running, the jobs and tasks, environment variables,
    and cache. The most important use, of course, is to monitor the jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Launch the Web UI for the local Spark cluster at `http://127.0.0.1:4040/jobs/`
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Jobs tab in the Web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00322.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the tab showing all the executors of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Spark on YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In YARN mode, the client communicates with YARN resource manager and gets containers
    to run the Spark execution. You can regard it as something like a mini Spark-cluster
    deployed just for you.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple clients interacting with the cluster create their own executors on
    the cluster nodes (node managers). Also, each client will have its own Driver
    component.
  prefs: []
  type: TYPE_NORMAL
- en: When running using YARN, Spark can run either in YARN-client mode or YARN-cluster
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: YARN client mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In YARN client mode, the Driver runs on a node outside the cluster (typically
    where the client is). Driver first contacts the resource manager requesting resources
    to run the Spark job. The resource manager allocates a container (container zero)
    and responds to the Driver. The Driver then launches the Spark application master
    in the container zero. The Spark application master then creates the executors
    on the containers allocated by the resource manager. The YARN containers can be
    on any node in the cluster controlled by node manager. So, all allocations are
    managed by resource manager.
  prefs: []
  type: TYPE_NORMAL
- en: Even the Spark application master needs to talk to resource manager to get subsequent
    containers to launch executors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the YARN-client mode deployment of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00203.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: YARN cluster mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the YARN cluster mode, the Driver runs on a node inside the cluster (typically
    where the application master is). Client first contacts the resource manager requesting
    resources to run the Spark job. The resource manager allocates a container (container
    zero) and responds to the client. The client then submits the code to the cluster
    and then launches the Driver and Spark application master in the container zero.
    The Driver runs along with the application master and the Spark application master,
    and then creates the executors on the containers allocated by the resource manager.
    The YARN containers can be on any node in the cluster controlled by the node manager.
    So, all allocations are managed by the resource manager.
  prefs: []
  type: TYPE_NORMAL
- en: Even the Spark application master needs to talk to the resource manager to get
    subsequent containers to launch executors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Yarn-cluster mode deployment of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00206.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There is no shell mode in YARN cluster mode, since the Driver itself is running
    inside YARN.
  prefs: []
  type: TYPE_NORMAL
- en: Spark on Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mesos deployment is similar to Spark standalone mode and the Driver communicates
    with the Mesos Master, which then allocates the resources needed to run the executors.
    As seen in standalone mode, the Driver then communicates with the executors to
    run the job. Thus, the Driver in Mesos deployment first talks to the master and
    then secures the container's request on all the Mesos slave nodes.
  prefs: []
  type: TYPE_NORMAL
- en: When the containers are allocated to the Spark job, the Driver then gets the
    executors started up and then runs the code in the executors. When the Spark job
    is completed and Driver exits, the Mesos master is notified, and all the resources
    in the form of containers on the Mesos slave nodes are reclaimed.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple clients interacting with the cluster create their own executors on
    the slave nodes. Also, each client will have its own Driver component. Both client
    and cluster mode are possible just like YARN mode
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the mesos-based deployment of Spark depicting the **Driver**
    connecting to **Mesos Master Node**, which also has the cluster manager of all
    the resources on all the Mesos slaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Introduction to RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Resilient Distributed Dataset** (**RDD**) is an immutable, distributed collection
    of objects. Spark RDDs are resilient or fault tolerant, which enables Spark to
    recover the RDD in the face of failures. Immutability makes the RDDs read-only
    once created. Transformations allow operations on the RDD to create a new RDD
    but the original RDD is never modified once created. This makes RDDs immune to
    race conditions and other synchronization problems.
  prefs: []
  type: TYPE_NORMAL
- en: The distributed nature of the RDDs works because an RDD only contains a reference
    to the data, whereas the actual data is contained within partitions across the
    nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, a RDD is a distributed collection of elements spread out across
    multiple nodes in the cluster. We can simplify a RDD to better understand by thinking
    of a RDD as a large array of integers distributed across machines.
  prefs: []
  type: TYPE_NORMAL
- en: A RDD is actually a dataset that has been partitioned across the cluster and
    the partitioned data could be from **HDFS** (**Hadoop Distributed File System**),
    HBase table, Cassandra table, Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Internally, each RDD is characterized by five main properties:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of partitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function for computing each split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of dependencies on other RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, a partitioner for key-value RDDs (for example, to say that the RDD
    is hash partitioned)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, a list of preferred locations to compute each split on (for example,
    block locations for an HDFS file)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Within your program, the driver treats the RDD object as a handle to the distributed
    data. It is analogous to a pointer to the data, rather than the actual data used,
    to reach the actual data when it is required.
  prefs: []
  type: TYPE_NORMAL
- en: The RDD by default uses the hash partitioner to partition the data across the
    cluster. The number of partitions is independent of the number of nodes in the
    cluster. It could very well happen that a single node in the cluster has several
    partitions of data. The number of partitions of data that exist is entirely dependent
    on how many nodes your cluster has and the size of the data. If you look at the
    execution of tasks on the nodes, then a task running on an executor on the worker
    node could be processing the data which is available on the same local node or
    a remote node. This is called the locality of the data, and the executing task
    chooses the most local data possible.
  prefs: []
  type: TYPE_NORMAL
- en: The locality affects the performance of your job significantly. The order of
    preference of locality by default can be shown as
  prefs: []
  type: TYPE_NORMAL
- en: '`PROCESS_LOCAL > NODE_LOCAL > NO_PREF > RACK_LOCAL > ANY`'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no guarantee of how many partitions a node might get. This affects
    the processing efficiency of any executor, because if you have too many partitions
    on a single node processing multiple partitions, then the time taken to process
    all the partitions also grows, overloading the cores on the executor, and thus
    slowing down the entire stage of processing, which directly slows down the entire
    job. In fact, partitioning is one of the main tuning factors to improve the performance
    of a Spark job. Refer to the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look further into what an RDD will look like when we load data. The
    following is an example of how Spark uses different workers to load different
    partitions or splits of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00218.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: No matter how the RDD is created, the initial RDD is typically called the base
    RDD and any subsequent RDDs created by the various operations are part of the
    lineage of the RDDs. This is another very important aspect to remember, as the
    secret to fault tolerance and recovery is that the **Driver** maintains the lineage
    of the RDDs and can execute the lineage to recover any lost blocks of the RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example showing multiple RDDs created as a result of operations.
    We start with the **Base RDD,** which has 24 items and derive another RDD **carsRDD**
    that contains only items (3) which match cars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The number of partitions does not change during such operations, as each executor
    applies the filter transformation in-memory, generating a new RDD partition corresponding
    to the original RDD partition.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how to create RDDs
  prefs: []
  type: TYPE_NORMAL
- en: RDD Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An RDD is the fundamental object used in Apache Spark. They are immutable collections
    representing datasets and have the inbuilt capability of reliability and failure
    recovery. By nature, RDDs create new RDDs upon any operation such as transformation
    or action. RDDs also store the lineage which is used to recover from failures.
    We have also seen in the previous chapter some details about how RDDs can be created
    and what kind of operations can be applied to RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An RDD can be created in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing a collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from an external source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation of an existing RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing a collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelizing a collection can be done by calling `parallelize()` on the collection
    inside the driver program. The driver, when it tries to parallelize a collection,
    splits the collection into partitions and distributes the data partitions across
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an RDD to create an RDD from a sequence of numbers using the
    SparkContext and the `parallelize()` function. The `parallelize()` function essentially
    splits the Sequence of numbers into a distributed collection otherwise known as
    an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Reading data from an external source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A second method for creating an RDD is by reading data from an external distributed
    source such as Amazon S3, Cassandra, HDFS, and so on. For example, if you are
    creating an RDD from HDFS, then the distributed blocks in HDFS are all read by
    the individual nodes in the Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the nodes in the Spark cluster is essentially doing its own input-output
    operations and each node is independently reading one or more blocks from the
    HDFS blocks. In general, Spark makes the best effort to put as much RDD as possible
    into memory. There is the capability to `cache` the data to reduce the input-output
    operations by enabling nodes in the spark cluster to avoid repeated reading operations,
    say from the HDFS blocks, which might be remote to the Spark cluster. There are
    a whole bunch of caching strategies that can be used within your Spark program,
    which we will examine later in a section for caching.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an RDD of text lines loading from a text file using the Spark
    Context and the `textFile()` function. The `textFile` function loads the input
    data as a text file (each newline `\n` terminated portion becomes an element in
    the RDD). The function call also automatically uses HadoopRDD (shown in next chapter)
    to detect and load the data in the form of several partitions as needed, distributed
    across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Transformation of an existing RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDDs, by nature, are immutable; hence, your RDDs could be created by applying
    transformations on any existing RDD. Filter is one typical example of a transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a simple `rdd` of integers and transformation by multiplying
    each integer by `2`. Again, we use the `SparkContext` and parallelize function
    to create a sequence of integers into an RDD by distributing the Sequence in the
    form of partitions. Then, we use the `map()` function to transform the RDD into
    another RDD by multiplying each number by `2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Streaming API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDDs can also be created via spark streaming. These RDDs are called Discretized
    Stream RDDs (DStream RDDs).
  prefs: []
  type: TYPE_NORMAL
- en: We will look at this further in [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will create RDDs and explore some of the operations
    using Spark-Shell.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Spark shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark shell provides a simple way to perform interactive analysis of data. It
    also enables you to learn the Spark APIs by quickly trying out various APIs. In
    addition, the similarity to Scala shell and support for Scala APIs also lets you
    also adapt quickly to Scala language constructs and make better use of Spark APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Spark shell implements the concept of **read-evaluate-print-loop** (**REPL**),
    which allows you to interact with the shell by typing in code which is evaluated.
    The result is then printed on the console, without needing to be compiled, so
    building executable code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start it by running the following in the directory where you installed Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Spark shell launches and the Spark shell automatically creates the `SparkSession`
    and `SparkContext` objects. The `SparkSession` is available as a Spark and the
    `SparkContext` is available as sc.
  prefs: []
  type: TYPE_NORMAL
- en: '`spark-shell` can be launched with several options as shown in the following
    snippet (the most important ones are in bold):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can also submit Spark code in the form of executable Java jars so that the
    job is executed in a cluster. Usually, you do this once you have reached a workable
    solution using the shell.
  prefs: []
  type: TYPE_NORMAL
- en: Use `./bin/spark-submit` when submitting a Spark job to a cluster (local, YARN,
    and Mesos).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are Shell Commands (the most important ones are in bold):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the spark-shell, we will now load some data as an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, we are running the commands one by one. Alternately, we can also
    paste the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will go deeper into the operations.
  prefs: []
  type: TYPE_NORMAL
- en: Actions and Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDDs are immutable and every operation creates a new RDD. Now, the two main
    operations that you can perform on an RDD are **Transformations** and **Actions**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformations** change the elements in the RDD such as splitting the input
    element, filtering out elements, and performing calculations of some sort. Several
    transformations can be performed in a sequence; however no execution takes place
    during the planning.'
  prefs: []
  type: TYPE_NORMAL
- en: For transformations, Spark adds them to a DAG of computation and, only when
    driver requests some data, does this DAG actually gets executed. This is called
    *lazy* evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The reasoning behind the lazy evaluation is that Spark can look at all the transformations
    and plan the execution, making use of the understanding the Driver has of all
    the operations. For instance, if a filter transformation is applied immediately
    after some other transformation, Spark will optimize the execution so that each
    Executor performs the transformations on each partition of data efficiently. Now,
    this is possible only when Spark is waiting until something needs to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Actions** are operations, which actually trigger the computations. Until
    an action operation is encountered, the execution plan within the spark program
    is created in the form of a DAG and does nothing. Clearly, there could be several
    transformations of all sorts within the execution plan, but nothing happens until
    you perform an action.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a depiction of the various operations on some arbitrary data
    where we just wanted to remove all pens and bikes and just count cars**.** Each
    print statement is an action which triggers the execution of all the transformation
    steps in the DAG based execution plan until that point as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00230.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For example, an action count on a directed acyclic graph of transformations
    triggers the execution of the transformation all the way up to the base RDD. If
    there is another action performed, then there is a new chain of executions that
    could take place. This is a clear case of why any caching that could be done at
    different stages in the directed acyclic graph will greatly speed up the next
    execution of the program. Another way that the execution is optimized is through
    the reuse of the shuffle files from the previous execution.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the collect action that collects or pulls all the data from
    all the nodes to the driver. You could use a partial function when invoking collect
    to selectively pull the data.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transformations** creates a new RDD from an existing RDD by applying transformation
    logic to each of the elements in the existing RDD. Some of the transformation
    functions involve splitting the element, filtering out elements, and performing
    calculations of some sort. Several transformations can be performed in a sequence.
    However, no execution takes place during the planning.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations can be divided into four categories, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: General transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**General transformations** are transformation functions that handle most of
    the general purpose use cases, applying the transformational logic to existing
    RDDs and generating a new RDD. The common operations of aggregation, filters and
    so on are all known as general transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of general transformation functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`map`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sortByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combineByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Math/Statistical transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematical or statistical transformations are transformation functions which
    handle some statistical functionality, and which usually apply some mathematical
    or statistical operation on existing RDDs, generating a new RDD. Sampling is a
    great example of this and is used often in Spark programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of such transformations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sampleByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '``randomSplit``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set theory/relational transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set theory/relational transformations are transformation functions, which handle
    transformations like Joins of datasets and other relational algebraic functionality
    such as `cogroup`. These functions work by applying the transformational logic
    to existing RDDs and generating a new RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of such transformations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cogroup`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subtractByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fullOuterJoin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leftOuterJoin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rightOuterJoin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data structure-based transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data structure-based transformations are transformation functions which operate
    on the underlying data structures of the RDD, the partitions in the RDD. In these
    functions, you can directly work on partitions without directly touching the elements/data
    inside the RDD. These are essential in any Spark program beyond the simple programs
    where you need more control of the partitions and distribution of partitions in
    the cluster. Typically, performance improvements can be realized by redistributing
    the data partitions according to the cluster state and the size of the data, and
    the exact use case requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of such transformations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`partitionBy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repartition`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zipwithIndex`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coalesce`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the list of transformation functions as available in the latest
    Spark 2.1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| `map(func)` | Return a new distributed dataset formed by passing each element
    of the source through a function `func`. |'
  prefs: []
  type: TYPE_TB
- en: '| `filter(func)` | Return a new dataset formed by selecting those elements
    of the source on which func returns true. |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMap(func)` | Similar to map, but each input item can be mapped to 0
    or more output items (so `func` should return a `Seq` rather than a single item).
    |'
  prefs: []
  type: TYPE_TB
- en: '| `mapPartitions(func)` | Similar to map, but runs separately on each partition
    (block) of the RDD, so `func` must be of type `Iterator<T> => Iterator<U>` when
    running on an RDD of type `T`. |'
  prefs: []
  type: TYPE_TB
- en: '| `mapPartitionsWithIndex(func)` | Similar to `mapPartitions`, but also provides
    `func` with an integer value representing the index of the partition, so `func`
    must be of type `(Int, Iterator<T>) => Iterator<U>` when running on an RDD of
    type `T`. |'
  prefs: []
  type: TYPE_TB
- en: '| `sample(withReplacement, fraction, seed)` | Sample a fraction fraction of
    the data, with or without replacement, using a given random number generator seed.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `union(otherDataset)` | Return a new dataset that contains the union of the
    elements in the source dataset and the argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `intersection(otherDataset)` | Return a new RDD that contains the intersection
    of elements in the source dataset and the argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `distinct([numTasks]))` | Return a new dataset that contains the distinct
    elements of the source dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| `groupByKey([numTasks])` | When called on a dataset of `(K, V)` pairs, returns
    a dataset of `(K, Iterable<V>)` pairs. Note: If you are grouping in order to perform
    an aggregation (such as a sum or average) over each key, using `reduceByKey` or
    `aggregateByKey` will yield much better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: By default, the level of parallelism in the output depends on the number
    of partitions of the parent RDD. You can pass an optional `numTasks` argument
    to set a different number of tasks. |'
  prefs: []
  type: TYPE_NORMAL
- en: '| reduceByKey(func, [numTasks]) | When called on a dataset of `(K, V)` pairs,
    returns a dataset of `(K, V)` pairs where the values for each key are aggregated
    using the given `reduce` function `func`, which must be of type `(V,V) => V`.
    As in `groupByKey`, the number of reduce tasks is configurable through an optional
    second argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])` | When called on a
    dataset of `(K, V)` pairs, returns a dataset of `(K, U)` pairs where the values
    for each key are aggregated using the given combine functions and a neutral *zero*
    value. Allows an aggregated value type that is different than the input value
    type, while avoiding unnecessary allocations. As in `groupByKey`, the number of
    reduce tasks is configurable through an optional second argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `sortByKey([ascending], [numTasks])` | When called on a dataset of `(K, V)`
    pairs where `K` implements ordered, returns a dataset of `(K, V)` pairs sorted
    by keys in ascending or descending order, as specified in the boolean ascending
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `join(otherDataset, [numTasks])` | When called on datasets of type `(K, V)`
    and `(K, W)`, returns a dataset of `(K, (V, W))` pairs with all pairs of elements
    for each key. Outer joins are supported through `leftOuterJoin`, `rightOuterJoin`,
    and `fullOuterJoin`. |'
  prefs: []
  type: TYPE_TB
- en: '| `cogroup(otherDataset, [numTasks])` | When called on datasets of type `(K,
    V)` and `(K, W)`, returns a dataset of `(K, (Iterable<V>, Iterable<W>))` tuples.
    This operation is also called `groupWith`. |'
  prefs: []
  type: TYPE_TB
- en: '| `cartesian(otherDataset)` | When called on datasets of types `T` and `U`,
    returns a dataset of `(T, U)` pairs (all pairs of elements). |'
  prefs: []
  type: TYPE_TB
- en: '| `pipe(command, [envVars])` | Pipe each partition of the RDD through a shell
    command, for example, a Perl or bash script. RDD elements are written to the process''s
    `stdin`, and lines output to its `stdout` are returned as an RDD of strings. |'
  prefs: []
  type: TYPE_TB
- en: '| `coalesce(numPartitions)` | Decrease the number of partitions in the RDD
    to `numPartitions`. Useful for running operations more efficiently after filtering
    down a large dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| `repartition(numPartitions)` | Reshuffle the data in the RDD randomly to
    create either more or fewer partitions and balance it across them. This always
    shuffles all data over the network. |'
  prefs: []
  type: TYPE_TB
- en: '| `repartitionAndSortWithinPartitions(partitioner)` | Repartition the RDD according
    to the given partitioner and, within each resulting partition, sort records by
    their keys. This is more efficient than calling `repartition` and then sorting
    within each partition because it can push the sorting down into the shuffle machinery.
    |'
  prefs: []
  type: TYPE_TB
- en: 'We will illustrate the most common transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: map function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`map` applies transformation function to input partitions to generate output
    partitions in the output RDD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following snippet, this is how we can map an RDD of a text
    file to an RDD with lengths of the lines of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains of how `map()` works. You can see that each
    partition of the RDD results in a new partition in a new RDD essentially applying
    the transformation to all elements of the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00236.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: flatMap function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`flatMap()` applies transformation function to input partitions to generate
    output partitions in the output RDD just like `map()` function. However, `flatMap()`
    also flattens any collection in the input RDD elements.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains how `flatMap()` works. You can see that each
    partition of the RDD results in a new partition in a new RDD, essentially applying
    the transformation to all elements of the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00239.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: filter function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`filter` applies transformation function to input partitions to generate filtered
    output partitions in the output RDD.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The following diagram explains how `filter` works. You can see that each partition
    of the RDD results in a new partition in a new RDD, essentially applying the filter
    transformation on all elements of the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the partitions do not change, and some partitions could be empty too,
    when applying filter
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00242.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: coalesce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`coalesce` applies a `transformation` function to input partitions to combine
    the input partitions into fewer partitions in the output RDD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code snippet, this is how we can combine all partitions
    to a single partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains how `coalesce` works. You can see that a new
    RDD is created from the original RDD essentially reducing the number of partitions
    by combining them as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00248.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: repartition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`repartition` applies a `transformation` function to input partitions to `repartition`
    the input into fewer or more output partitions in the output RDD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code snippet, this is how we can map an RDD of a
    text file to an RDD with more partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains how `repartition` works. You can see that a
    new RDD is created from the original RDD, essentially redistributing the partitions
    by combining/splitting them as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00254.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Action triggers the entire **DAG** (**Directed Acyclic Graph**) of transformations
    built so far to be materialized by running the code blocks and functions. All
    operations are now executed as the DAG specifies.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of action operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Driver**: One kind of action is the driver action such as collect count,
    count by key, and so on. Each such action performs some calculations on the remote
    executor and pulls the data back into the driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver-based action has the problem that actions on large datasets can easily
    overwhelm the memory available on the driver taking down the application, so you
    should use the driver involved actions judiciously
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed**: Another kind of action is a distributed action, which is executed
    on the nodes in the cluster. An example of such a distributed action is `saveAsTextfile`.
    This is the most common action operation due to the desirable distributed nature
    of the operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the list of action functions as available in the latest Spark
    2.1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Action | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| `reduce(func)` | Aggregate the elements of the dataset using a function `func`
    (which takes two arguments and returns one). The function should be commutative
    and associative so that it can be computed correctly in parallel. |'
  prefs: []
  type: TYPE_TB
- en: '| `collect()` | Return all the elements of the dataset as an array at the driver
    program. This is usually useful after a filter or other operation that returns
    a sufficiently small subset of the data. |'
  prefs: []
  type: TYPE_TB
- en: '| `count()` | Return the number of elements in the dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| `first()` | Return the first element of the dataset (similar to `take(1)`).
    |'
  prefs: []
  type: TYPE_TB
- en: '| `take(n)` | Return an array with the first `n` elements of the dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| `takeSample(withReplacement, num, [seed])` | Return an array with a random
    sample of `num` elements of the dataset, with or without replacement, optionally
    pre-specifying a random number generator seed. |'
  prefs: []
  type: TYPE_TB
- en: '| `takeOrdered(n, [ordering])` | Return the first `n` elements of the RDD using
    either their natural order or a custom comparator. |'
  prefs: []
  type: TYPE_TB
- en: '| `saveAsTextFile(path)` | Write the elements of the dataset as a text file
    (or set of text files) in a given directory in the local filesystem, HDFS or any
    other Hadoop-supported file system. Spark will call `toString` on each element
    to convert it to a line of text in the file. |'
  prefs: []
  type: TYPE_TB
- en: '| `saveAsSequenceFile(path)` (Java and Scala) | Write the elements of the dataset
    as a Hadoop SequenceFile in a given path in the local filesystem, HDFS, or any
    other Hadoop-supported file system. This is available on RDDs of key-value pairs
    that implement Hadoop''s `Writable` interface. In Scala, it is also available
    on types that are implicitly convertible to `Writable` (Spark includes conversions
    for basic types like `Int`, `Double`, `String`, and so on). |'
  prefs: []
  type: TYPE_TB
- en: '| `saveAsObjectFile(path)` (Java and Scala) | Write the elements of the dataset
    in a simple format using Java serialization, which can then be loaded using `SparkContext.objectFile()`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `countByKey()` | Only available on RDDs of type `(K, V)`. Returns a hashmap
    of `(K, Int)` pairs with the count of each key. |'
  prefs: []
  type: TYPE_TB
- en: '| `foreach(func)` | Run a function `func` on each element of the dataset. This
    is usually done for side effects such as updating an accumulator ([http://spark.apache.org/docs/latest/programming-guide.html#accumulators](http://spark.apache.org/docs/latest/programming-guide.html#accumulators))
    or interacting with external storage systems. Note: modifying variables other
    than accumulators outside of the `foreach()` may result in undefined behavior.
    See understanding closures ([http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka))
    [for more details.](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka)
    |'
  prefs: []
  type: TYPE_TB
- en: reduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`reduce()` applies the reduce function to all the elements in the RDD and sends
    it to the Driver.'
  prefs: []
  type: TYPE_NORMAL
- en: The following is example code to illustrate this. You can use `SparkContext`
    and the parallelize function to create an RDD from a sequence of integers. Then
    you can add up all the numbers of the RDD using the `reduce` function on the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an action, the results are printed as soon as you run the `reduce`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown below is the code to build a simple RDD from a small array of numbers
    and then perform a reduce operation on the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The following diagram is an illustration of `reduce()`. Driver runs the reduce
    function on the executors and collects the results in the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00257.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: count
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`count()` simply counts the number of elements in the RDD and sends it to the
    Driver.'
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example of this function. We created an RDD from a Sequence
    of integers using SparkContext and parallelize function and then called count
    on the RDD to print the number of elements in the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The following is an illustration of `count()`. The Driver asks each of the executor/task
    to count the number of elements in the partition being handled by the task and
    then adds up the counts from all the tasks together at the Driver level.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00260.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: collect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`collect()` simply collects all elements in the RDD and sends it to the Driver.'
  prefs: []
  type: TYPE_NORMAL
- en: Shown here is an example showing what collect function essentially does. When
    you call collect on an RDD, the Driver collects all the elements of the RDD by
    pulling them into the Driver.
  prefs: []
  type: TYPE_NORMAL
- en: Calling collect on large RDDs will cause out-of-memory issues on the Driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown below is the code to collect the content of the RDD and display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The following is an illustration of `collect()`. Using collect, the Driver is
    pulling all the elements of the RDD from all partitions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caching enables Spark to persist data across computations and operations. In
    fact, this is one of the most important technique in Spark to speed up computations,
    particularly when dealing with iterative computations.
  prefs: []
  type: TYPE_NORMAL
- en: Caching works by storing the RDD as much as possible in the memory. If there
    is not enough memory then the current data in storage is evicted, as per LRU policy.
    If the data being asked to cache is larger than the memory available, the performance
    will come down because Disk will be used instead of memory.
  prefs: []
  type: TYPE_NORMAL
- en: You can mark an RDD as cached using either `persist()` or `cache()`
  prefs: []
  type: TYPE_NORMAL
- en: '`cache()` is simply a synonym for persist`(MEMORY_ONLY)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`persist` can use memory or disk or both:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the possible values for Storage level:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage Level | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_ONLY` | Stores RDD as deserialized Java objects in the JVM. If the
    RDD does not fit in memory, some partitions will not be cached and will be recomputed
    on the fly each time they''re needed. This is the default level. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_AND_DISK` | Stores RDD as deserialized Java objects in the JVM. If
    the RDD does not fit in memory, store the partitions that don''t fit on disk,
    and read them from there when they''re needed. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_ONLY_SER` (Java and Scala) | Stores RDD as serialized Java objects
    (one byte array per partition). This is generally more space-efficient than deserialized
    objects, especially when using a fast serializer, but more CPU-intensive to read.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_AND_DISK_SER` (Java and Scala) | Similar to `MEMORY_ONLY_SER`, but
    spill partitions that don''t fit in memory to disk instead of recomputing them
    on the fly each time they''re needed. |'
  prefs: []
  type: TYPE_TB
- en: '| `DISK_ONLY` | Store the RDD partitions only on disk. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, and so on. | Same as the preceding
    levels, but replicate each partition on two cluster nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| `OFF_HEAP` (experimental) | Similar to `MEMORY_ONLY_SER`, but store the data
    in off-heap memory. This requires off-heap memory to be enabled. |'
  prefs: []
  type: TYPE_TB
- en: Storage level to choose depends on the situation
  prefs: []
  type: TYPE_NORMAL
- en: If RDDs fit into memory, use `MEMORY_ONLY` as that's the fastest option for
    execution performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try `MEMORY_ONLY_SER` is there are serializable objects being used in order
    to make the objects smaller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DISK` should not be used unless your computations are expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use replicated storage for best fault tolerance if you can spare the additional
    memory needed. This will prevent recomputation of lost partitions for best availability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unpersist()` simply frees up the cached content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of how to call `persist()` function using different
    types of storage (memory or disk):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The following is an illustration of the performance improvement we get by caching.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the WebUI to look at the improvement achieved as shown in the following
    screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Loading and saving data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loading data into an RDD and Saving an RDD onto an output system both support
    several different methods. We will cover the most common ones in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loading data into an RDD can be done by using `SparkContext`. Some of the most
    common methods are:.
  prefs: []
  type: TYPE_NORMAL
- en: '`textFile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wholeTextFiles`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load` from a JDBC datasource'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: textFile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`textFile()` can be used to load textFiles into an RDD and each line becomes
    an element in the RDD.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of loading a `textfile` into an RDD using `textFile()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: wholeTextFiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`wholeTextFiles()` can be used to load multiple text files into a paired RDD
    containing pairs `<filename, textOfFile>` representing the filename and the entire
    content of the file. This is useful when loading multiple small text files and
    is different from `textFile` API because when whole `TextFiles()` is used, the
    entire content of the file is loaded as a single record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of loading a `textfile` into an RDD using `wholeTextFiles()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Load from a JDBC Datasource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can load data from an external data source which supports **Java Database
    Connectivity** (**JDBC**). Using a JDBC driver, you can connect to a relational
    database such as Mysql and load the content of a table into Spark as shown in
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of loading from a JDBC datasource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Saving RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Saving data from an RDD into a file system can be done by either:'
  prefs: []
  type: TYPE_NORMAL
- en: '`saveAsTextFile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`saveAsObjectFile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following is an example of saving an RDD to a text file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: There are many more ways of loading and saving data, particularly when integrating
    with HBase, Cassandra and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the internals of Apache Spark, what RDDs are,
    DAGs and lineages of RDDs, Transformations, and Actions. We also looked at various
    deployment modes of Apache Spark using standalone, YARN, and Mesos deployments.
    We also did a local install on our local machine and then looked at Spark shell
    and how it can be used to interact with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we also looked at loading data into RDDs and saving RDDs to external
    systems as well as the secret sauce of Spark's phenomenal performance, the caching
    functionality, and how we can use memory and/or disk to optimize the performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dig deeper into RDD API and how it all works in
    [Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c), *Special RDD
    Operations*.
  prefs: []
  type: TYPE_NORMAL

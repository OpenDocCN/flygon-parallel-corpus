- en: Special RDD Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"It''s supposed to be automatic, but actually you have to push this button."'
  prefs: []
  type: TYPE_NORMAL
- en: '- John Brunner'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learn how RDDs can be tailored to different needs, and
    how these RDDs provide new functionalities (and dangers!) Moreover, we investigate
    other useful objects that Spark provides, such as broadcast variables and accumulators.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning and shuffling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broadcast variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Resilient Distributed Datasets** (**RDDs**) are the fundamental object used
    in Apache Spark. RDDs are immutable collections representing datasets and have
    the inbuilt capability of reliability and failure recovery. By nature, RDDs create
    new RDDs upon any operation such as transformation or action. They also store
    the lineage, which is used to recover from failures. We have also seen in the
    previous chapter some details about how RDDs can be created and what kind of operations
    can be applied to RDDs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simply example of the RDD lineage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start looking at the simplest RDD again by creating a RDD from a sequence
    of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example shows RDD of integers and any operation done on the RDD
    results in another RDD. For example, if we multiply each element by `3`, the result
    is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do one more operation, adding `2` to each element and also print all
    three RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'An interesting thing to look at is the lineage of each RDD using the `toDebugString`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the lineage shown in the Spark web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: RDD does not need to be the same datatype as the first RDD (integer). The following
    is a RDD which writes a different datatype of a tuple of (string, integer).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The following is a RDD of the `StatePopulation` file where each record is converted
    to `upperCase`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a diagram of the preceding transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Pair RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pair RDDs are RDDs consisting of key-value tuples which suits many use cases
    such as aggregation, sorting, and joining data. The keys and values can be simple
    types such as integers and strings or more complex types such as case classes,
    arrays, lists, and other types of collections. The key-value based extensible
    data model offers many advantages and is the fundamental concept behind the MapReduce
    paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a `PairRDD` can be done easily by applying transformation to any RDD
    to convert the RDD to an RDD of key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's read the `statesPopulation.csv` into an RDD using the `SparkContext`,
    which is available as `sc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a basic RDD of the state population and how
    `PairRDD` looks like for the same RDD splitting the records into tuples (pairs)
    of state and population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a diagram of the preceding example showing how the RDD elements
    are converted to `(key - value)` pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00341.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: DoubleRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DoubleRDD is an RDD consisting of a collection of double values. Due to this
    property, many statistical functions are available to use with the DoubleRDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of DoubleRDD where we create an RDD from a sequence
    of double numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a diagram of the DoubleRDD and how you can run a `sum()` function
    on the DoubleRDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00371.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: SequenceFileRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`SequenceFileRDD` is created from a `SequenceFile` which is a format of files
    in the Hadoop File System. The `SequenceFile` can be compressed or uncompressed.'
  prefs: []
  type: TYPE_NORMAL
- en: Map Reduce processes can use SequenceFiles, which are pairs of Keys and Values.
    Key and Value are of Hadoop writable datatypes, such as Text, IntWritable, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a `SequenceFileRDD`, which shows how we can
    write and read `SequenceFile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a diagram of **SequenceFileRDD** as seen in the preceding
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: CoGroupedRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CoGroupedRDD` is an RDD that cogroups its parents. Both parent RDDs have to
    be pairRDDs for this to work, as a cogroup essentially generates a pairRDD consisting
    of the common key and list of values from both parent RDDs. Take a look at the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of a CoGroupedRDD where we create a cogroup of
    two pairRDDs, one having pairs of State, Population and the other having pairs
    of State, Year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a diagram of the cogroup of **pairRDD** and **pairRDD2** by
    creating pairs of values for each key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: ShuffledRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ShuffledRDD` shuffles the RDD elements by key so as to accumulate values for
    the same key on the same executor to allow an aggregation or combining logic.
    A very good example is to look at what happens when `reduceByKey()` is called
    on a PairRDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a `reduceByKey` operation on the `pairRDD` to aggregate the
    records by the State:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram, is an illustration of the shuffling by Key to send the
    records of the same Key(State) to the same partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: UnionRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`UnionRDD` is the result of a union operation of two RDDs. Union simply creates
    an RDD with elements from both RDDs as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is an illustration of a union of two RDDs where the elements
    from both **RDD 1** and **RDD 2** are combined into a new RDD **UnionRDD**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00305.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: HadoopRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`HadoopRDD` provides core functionality for reading data stored in HDFS using
    the MapReduce API from the Hadoop 1.x libraries. `HadoopRDD` is the default used
    and can be seen when loading data from any file system into an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When loading the state population records from the CSV, the underlying base
    RDD is actually `HadoopRDD` as in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is an illustration of a **HadoopRDD** created by loading
    a textfile from the file system into an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: NewHadoopRDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`NewHadoopRDD` provides core functionality for reading data stored in HDFS,
    HBase tables, Amazon S3 using the new MapReduce API from Hadoop 2.x `libraries.NewHadoopRDD`
    can read from many different formats thus is used to interact with several external
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Prior to `NewHadoopRDD`, `HadoopRDD` was the only available option which used
    the old MapReduce API from Hadoop 1.x
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The simplest example is to use SparkContext''s `wholeTextFiles` function to
    create `WholeTextFileRDD`. Now, `WholeTextFileRDD` actually extends `NewHadoopRDD`
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at another example where we will use the function `newAPIHadoopFile`
    using the `SparkContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Aggregations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aggregation techniques allow you to combine the elements in the RDD in arbitrary
    ways to perform some computation. In fact, aggregation is the most important part
    of big data analytics. Without aggregation, we would not have any way to generate
    reports and analysis like *Top States by Population*, which seems to be a logical
    question asked when given a dataset of all State populations for the past 200
    years. Another simpler example is that of a need to just count the number of elements
    in the RDD, which asks the executors to count the number of elements in each partition
    and send to the Driver, which then adds the subsets to compute the total number
    of elements in the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, our primary focus is on the aggregation functions used to collect
    and combine data by key. As seen earlier in this chapter, a PairRDD is an RDD
    of (key - value) pairs where key and value are arbitrary and can be customized
    as per the use case.
  prefs: []
  type: TYPE_NORMAL
- en: In our example of state populations, a PairRDD could be the pairs of `<State,
    <Population, Year>>` which means `State` is taken as the key and the tuple `<Population,
    Year>` is considered the value. This way of breaking down the key and value can
    generate aggregations such as *Top Years by Population per State*. On the contrary,
    in case our aggregations are done around Year say *Top States by Population per
    Year*, we can use a `pairRDD` of pairs of `<Year, <State, Population>>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the sample code to generate a `pairRDD` from the `StatePopulation`
    dataset both with `State` as the key as well as the `Year` as the key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can generate a `pairRDD` using `State` as the key and a tuple of `<Year,
    Population>` as the value as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we can also generate a `PairRDD` using `Year` as the
    key and a tuple of `<State, Population>` as the value as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now look into how we can use the common aggregation functions on the
    `pairRDD` of `<State, <Year, Population>>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduceByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregateByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combineByKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: groupByKey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`groupByKey` groups the values for each key in the RDD into a single sequence.
    `groupByKey` also allows controlling the partitioning of the resulting key-value
    pair RDD by passing a partitioner. By default, a `HashPartitioner` is used but
    a custom partitioner can be given as an argument. The ordering of elements within
    each group is not guaranteed, and may even differ each time the resulting RDD
    is evaluated.'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupByKey` is an expensive operation due to all the data shuffling needed.
    `reduceByKey` or `aggregateByKey` provide much better performance. We will look
    at this later in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupByKey` can be invoked either using a custom partitioner or just using
    the default `HashPartitioner` as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As currently implemented, `groupByKey` must be able to hold all the key-value
    pairs for any key in memory. If a key has too many values, it can result in an
    `OutOfMemoryError`.
  prefs: []
  type: TYPE_NORMAL
- en: '`groupByKey` works by sending all elements of the partitions to the partition
    based on the partitioner so that all pairs of (key - value) for the same key are
    collected in the same partition. Once this is done, the aggregation operation
    can be done easily.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown here is an illustration of what happens when `groupByKey` is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: reduceByKey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`groupByKey` involves a lot of shuffling and `reduceByKey` tends to improve
    the performance by not sending all elements of the `PairRDD` using shuffles, rather
    using a local combiner to first do some basic aggregations locally and then send
    the resultant elements as in `groupByKey`. This greatly reduces the data transferred,
    as we don''t need to send everything over. `reduceBykey` works by merging the
    values for each key using an associative and commutative reduce function. Of course,
    first, this will'
  prefs: []
  type: TYPE_NORMAL
- en: also perform the merging locally on each mapper before sending results to a
    reducer.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with Hadoop MapReduce, this is very similar to a combiner
    in MapReduce programming.
  prefs: []
  type: TYPE_NORMAL
- en: '`reduceByKey` can be invoked either using a custom partitioner or just using
    the default HashPartitioner as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`reduceByKey` works by sending all elements of the partitions to the partition
    based on the `partitioner` so that all pairs of (key - value) for the same Key
    are collected in the same partition. But before the shuffle, local aggregation
    is also done reducing the data to be shuffled. Once this is done, the aggregation
    operation can be done easily in the final partition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of what happens when `reduceBykey`
    is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: aggregateByKey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`aggregateByKey` is quite similar to `reduceByKey`, except that `aggregateByKey`
    allows more flexibility and customization of how to aggregate within partitions
    and between partitions to allow much more sophisticated use cases such as generating
    a list of all `<Year, Population>` pairs as well as total population for each
    State in one function call.'
  prefs: []
  type: TYPE_NORMAL
- en: '`aggregateByKey` works by aggregating the values of each key, using given combine
    functions and a neutral initial/zero value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This function can return a different result type, `U`, than the type of the
    values in this RDD `V`, which is the biggest difference. Thus, we need one operation
    for merging a `V` into a `U` and one operation for merging two `U`''s. The former
    operation is used for merging values within a partition, and the latter is used
    for merging values between partitions. To avoid memory allocation, both of these
    functions are allowed to modify and return their first argument instead of creating
    a new `U`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`aggregateByKey` works by performing an aggregation within the partition operating
    on all elements of each partition and then applies another aggregation logic when
    combining the partitions themselves. Ultimately, all pairs of (key - value) for
    the same Key are collected in the same partition; however, the aggregation as
    to how it is done and the output generated is not fixed as in `groupByKey` and
    `reduceByKey`, but is more flexible and customizable when using `aggregateByKey`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of what happens when `aggregateByKey`
    is called. Instead of adding up the counts as in `groupByKey` and `reduceByKey`,
    here we are generating lists of values for each Key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: combineByKey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`combineByKey` is very similar to `aggregateByKey`; in fact, `combineByKey`
    internally invokes `combineByKeyWithClassTag`, which is also invoked by `aggregateByKey`.
    As in `aggregateByKey`, the `combineByKey` also works by applying an operation
    within each partition and then between combiners.'
  prefs: []
  type: TYPE_NORMAL
- en: '`combineByKey` turns an `RDD[K,V]` into an `RDD[K,C]`, where `C` is a list
    of Vs collected or combined under the name key `K`.'
  prefs: []
  type: TYPE_NORMAL
- en: There are three functions expected when you call combineByKey.
  prefs: []
  type: TYPE_NORMAL
- en: '`createCombiner`, which turns a `V` into `C`, which is a one element list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mergeValue` to merge a `V` into a `C` by appending the `V` to the end of the
    list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mergeCombiners` to combine two Cs into one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In `aggregateByKey`, the first argument is simply a zero value but in `combineByKey`,
    we provide the initial function which takes the current value as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '`combineByKey` can be invoked either using a custom partitioner or just using
    the default HashPartitioner as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`combineByKey` works by performing an aggregation within the partition operating
    on all elements of each partition and then applies another aggregation logic when
    combining the partitions themselves. Ultimately, all pairs of (key - value) for
    the same Key are collected in the same partition however the aggregation as to
    how it is done and the output generated is not fixed as in `groupByKey` and `reduceByKey`,
    but is more flexible and customizable when using `combineByKey`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of what happens when `combineBykey`
    is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of groupByKey, reduceByKey, combineByKey, and aggregateByKey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider the example of StatePopulation RDD generating a `pairRDD` of
    `<State, <Year, Population>>`.
  prefs: []
  type: TYPE_NORMAL
- en: '`groupByKey` as seen in the preceding section will do `HashPartitioning` of
    the `PairRDD` by generating a hashcode of the keys and then shuffling the data
    to collect the values for each key in the same partition. This obviously results
    in too much shuffling.'
  prefs: []
  type: TYPE_NORMAL
- en: '`reduceByKey` improves upon `groupByKey` using a local combiner logic to minimize
    the data sent in a shuffle phase. The result will be the same as `groupByKey`,
    but will be much more performant.'
  prefs: []
  type: TYPE_NORMAL
- en: '`aggregateByKey` is very similar to `reduceByKey` in how it works but with
    one big difference, which makes it the most powerful one among the three. `aggregateBykey`
    does not need to operate on the same datatype and can do different aggregation
    within the partition and do a different aggregation between partitions.'
  prefs: []
  type: TYPE_NORMAL
- en: '`combineByKey` is very similar in performance to `aggregateByKey` except for
    the initial function to create the combiner.'
  prefs: []
  type: TYPE_NORMAL
- en: The function to use depends on your use case but when in doubt just refer to
    this section on *Aggregation* to choose the right function for your use case.
    Also, pay close attention to the next section as *Partitioning and shuffling*
    are covered in that section.
  prefs: []
  type: TYPE_NORMAL
- en: The following is the code showing all four ways of calculating total population
    by state.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Initialize the RDD:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Convert to pair RDD:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3\. groupByKey - Grouping the values and then adding up populations:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4\. reduceByKey - Reduce the values by key simply adding the populations:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5\. aggregateBykey - aggregate the populations under each key and adds
    them up:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6\. combineByKey - combine within partitions and then merging combiners:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you see, all four aggregations result in the same output. It's just how they
    work that is different.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning and shuffling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how Apache Spark can handle distributed computing much better than
    Hadoop. We also saw the inner workings, mainly the fundamental data structure
    known as **Resilient Distributed Dataset** (**RDD**). RDDs are immutable collections
    representing datasets and have the inbuilt capability of reliability and failure
    recovery. RDDs operate on data not as a single blob of data, rather RDDs manage
    and operate data in partitions spread across the cluster. Hence, the concept of
    data partitioning is critical to the proper functioning of Apache Spark Jobs and
    can have a big effect on the performance as well as how the resources are utilized.
  prefs: []
  type: TYPE_NORMAL
- en: RDD consists of partitions of data and all operations are performed on the partitions
    of data in the RDD. Several operations like transformations are functions executed
    by an executor on the specific partition of data being operated on. However, not
    all operations can be done by just performing isolated operations on the partitions
    of data by the respective executors. Operations like aggregations (seen in the
    preceding section) require data to be moved across the cluster in a phase known
    as **shuffling**. In this section, we will look deeper into the concepts of partitioning
    and shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start looking at a simple RDD of integers by executing the following code.
    Spark Context's `parallelize` function creates an RDD from the Sequence of integers.
    Then, using the `getNumPartitions()` function, we can get the number of partitions
    of this RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The RDD can be visualized as shown in the following diagram, which shows the
    8 partitions in the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The number of partitions is important because this number directly influences
    the number of tasks that will be running RDD transformations. If the number of
    partitions is too small, then we will use only a few CPUs/cores on a lot of data
    thus having a slower performance and leaving the cluster underutilized. On the
    other hand, if the number of partitions is too large then you will use more resources
    than you actually need and in a multi-tenant environment could be causing starvation
    of resources for other Jobs being run by you or others in your team.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Partitioning of RDDs is done by partitioners. Partitioners assign a partition
    index to the elements in the RDD. All elements in the same partition will have
    the same partition index.
  prefs: []
  type: TYPE_NORMAL
- en: Spark comes with two partitioners the `HashPartitioner` and the `RangePartitioner`.
    In addition to these, you can also implement a custom partitioner.
  prefs: []
  type: TYPE_NORMAL
- en: HashPartitioner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`HashPartitioner` is the default partitioner in Spark and works by calculating
    a hash value for each key of the RDD elements. All the elements with the same
    hashcode end up in the same partition as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of the String `hashCode()` function and how we
    can generate `partitionIndex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The default number of partitions is either from the Spark configuration parameter
    `spark.default.parallelism` or the number of cores in the cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of how hash partitioning works. We
    have an RDD with 3 elements **a**, **b**, and **e**. Using String hashcode we
    get the `partitionIndex` for each element based on the number of partitions set
    at 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: RangePartitioner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`RangePartitioner` works by partitioning the RDD into roughly equal ranges.
    Since the range has to know the starting and ending keys for any partition, the
    RDD needs to be sorted first before a `RangePartitioner` can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: '`RangePartitioning` first needs reasonable boundaries for the partitions based
    on the RDD and then create a function from key K to the `partitionIndex` where
    the element belongs. Finally, we need to repartition the RDD, based on the `RangePartitioner`
    to distribute the RDD elements correctly as per the ranges we determined.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of how we can use `RangePartitioning` of a `PairRDD`.
    We also can see how the partitions changed after we repartition the RDD using
    a `RangePartitioner`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is an illustration of the `RangePartitioner` as seen
    in the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Shuffling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whatever the partitioner used, many operations will cause a repartitioning of
    data across the partitions of an RDD. New partitions can be created or several
    partitions can be collapsed/coalesced. All the data movement necessary for the
    repartitioning is called **shuffling,** and this is an important concept to understand
    when writing a Spark Job. The shuffling can cause a lot of performance lag as
    the computations are no longer in memory on the same executor but rather the executors
    are exchanging data over the wire.
  prefs: []
  type: TYPE_NORMAL
- en: A good example is the example of `groupByKey()`, we saw earlier in the *Aggregations*
    section. Obviously, lot of data was flowing between executors to make sure all
    values for a key are collected onto the same executor to perform the `groupBy`
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffling also determines the Spark Job execution process and influences how
    the Job is split into Stages. As we have seen in this chapter and the previous
    chapter, Spark holds a DAG of RDDs, which represent the lineage of the RDDs such
    that not only does Spark use the lineage to plan the execution of the job but
    also any loss of executors can be recovered from. When an RDD is undergoing a
    transformation, an attempt is made to make sure the operations are performed on
    the same node as the data. However, often we use join operations, reduce, group,
    or aggregate operations among others, which cause repartitioning intentionally
    or unintentionally. This shuffling in turn determines where a particular stage
    in the processing has ended and a new stage has begun.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of how a Spark Job is split into stages.
    This example shows a `pairRDD` being filtered, transformed using map before invoking
    `groupByKey` followed by one last transformation using `map()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The more shuffling we have, the more stages occur in the job execution affecting
    the performance. There are two key aspects which are used by Spark Driver to determine
    the stages. This is done by defining two types of dependencies of the RDDs, the
    narrow dependencies and the wide dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Narrow Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an RDD can be derived from another RDD using a simple one-to-one transformation
    such as a `filter()` function, `map()` function, `flatMap()` function, and so
    on, then the child RDD is said to depend on the parent RDD on a one-to-one basis.
    This dependency is known as narrow dependency as the data can be transformed on
    the same node as the one containing the original RDD/parent RDD partition without
    requiring any data transfer over the wire between other executors.
  prefs: []
  type: TYPE_NORMAL
- en: Narrow dependencies are in the same stage of the job execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of how a narrow dependency transforms
    one RDD to another RDD, applying one-to-one transformation on the RDD elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Wide Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an RDD can be derived from one or more RDDs by transferring data over the
    wire or exchanging data to repartition or redistribute the data using functions,
    such as `aggregateByKey`, `reduceByKey` and so on, then the child RDD is said
    to depend on the parent RDDs participating in a shuffle operation. This dependency
    is known as a Wide dependency as the data cannot be transformed on the same node
    as the one containing the original RDD/parent RDD partition thus requiring data
    transfer over the wire between other executors.
  prefs: []
  type: TYPE_NORMAL
- en: Wide dependencies introduce new stages in the job execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of how wide dependency transforms
    one RDD to another RDD shuffling data between executors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00155.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Broadcast variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Broadcast variables are shared variables across all executors. Broadcast variables
    are created once in the Driver and then are read only on executors. While it is
    simple to understand simple datatypes broadcasted, such as an `Integer`, broadcast
    is much bigger than simple variables conceptually. Entire datasets can be broadcasted
    in a Spark cluster so that executors have access to the broadcasted data. All
    the tasks running within an executor all have access to the broadcast variables.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast uses various optimized methods to make the broadcasted data accessible
    to all executors. This is an important challenge to solve as if the size of the
    datasets broadcasted is significant, you cannot expect 100s or 1000s of executors
    to connect to the Driver and pull the dataset. Rather, the executors pull the
    data via HTTP connection and the more recent addition which is similar to BitTorrent
    where the dataset itself is distributed like a torrent amongst the cluster. This
    enables a much more scalable method to distribute the broadcasted variables to
    all executors rather than having each executor pull the data from the Driver one
    by one which can cause failures on the Driver when you have a lot of executors.
  prefs: []
  type: TYPE_NORMAL
- en: The driver can only broadcast the data it has and you cannot broadcast RDDs
    by using references. This is because only Driver knows how to interpret RDDs and
    executors only know the particular partitions of data they are handling.
  prefs: []
  type: TYPE_NORMAL
- en: If you look deeper into how broadcast works, you will see that the mechanism
    works by first having the Driver divide the serialized object into small chunks
    and then stores those chunks in the BlockManager of the driver. When the code
    is serialized to be run on the executors, then each executor first attempts to
    fetch the object from its own internal BlockManager. If the broadcast variable
    was fetched before, it will find it and use it. However, if it does not exist,
    the executor then uses remote fetches to fetch the small chunks from the driver
    and/or other executors if available. Once it gets the chunks, it puts the chunks
    in its own BlockManager, ready for any other executors to fetch from. This prevents
    the driver from being the bottleneck in sending out multiple copies of the broadcast
    data (one per executor).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of how broadcast works in a Spark
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Broadcast variables can be both created and destroyed too. We will look into
    the creation and destruction of broadcast variables. There is also a way to remove
    broadcasted variables from memory which we will also look at.
  prefs: []
  type: TYPE_NORMAL
- en: Creating broadcast variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a broadcast variable can be done using the Spark Context's `broadcast()`
    function on any data of any data type provided that the data/variable is serializable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how we can broadcast an Integer variable and then use the broadcast
    variable inside a transformation operation executed on the executors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Broadcast variables can also be created on more than just primitive data types
    as shown in the next example where we will broadcast a `HashMap` from the Driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simple transformation of an integer RDD by multiplying each
    element with another integer by looking up the HashMap. The RDD of 1,2,3 is transformed
    to 1 X 2 , 2 X 3, 3 X 4 = 2,6,12 :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Cleaning broadcast variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Broadcast variables do occupy memory on all executors and depending on the size
    of the data contained in the broadcasted variable, this could cause resource issues
    at some point. There is a way to remove broadcasted variables from the memory
    of all executors.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `unpersist()` on a broadcast variable removed the data of the broadcast
    variable from the memory cache of all executors to free up resources. If the variable
    is used again, then the data is retransmitted to the executors in order for it
    to be used again. The Driver, however, holds onto the memory as if the Driver
    does not have the data, then broadcast variable is no longer valid.
  prefs: []
  type: TYPE_NORMAL
- en: We look at destroying broadcast variables next.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example of how `unpersist()` can be invoked on a broadcast
    variable. After calling `unpersist` if we access the broadcast variable again,
    it works as usual but behind the scenes, the executors are pulling the data for
    the variable again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Destroying broadcast variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can also destroy broadcast variables, completely removing them from all
    executors and the Driver too making them inaccessible. This can be quite helpful
    in managing the resources optimally across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `destroy()` on a broadcast variable destroys all data and metadata related
    to the specified broadcast variable. Once a broadcast variable has been destroyed,
    it cannot be used again and will have to be recreated all over again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of destroying broadcast variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If an attempt is made to use a destroyed broadcast variable, an exception is
    thrown
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of an attempt to reuse a destroyed broadcast variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Thus, broadcast functionality can be use to greatly improve the flexibility
    and performance of Spark jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Accumulators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accumulators are shared variables across executors typically used to add counters
    to your Spark program. If you have a Spark program and would like to know errors
    or total records processed or both, you can do it in two ways. One way is to add
    extra logic to just count errors or total records, which becomes complicated when
    handling all possible computations. The other way is to leave the logic and code
    flow fairly intact and add Accumulators.
  prefs: []
  type: TYPE_NORMAL
- en: Accumulators can only be updated by adding to the value.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example of creating and using a long Accumulator using Spark
    Context and the `longAccumulator` function to initialize a newly created accumulator
    variable to zero. As the accumulator is used inside the map transformation, the
    Accumulator is incremented. At the end of the operation, the Accumulator holds
    a value of 351.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'There are inbuilt accumulators which can be used for many use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LongAccumulator`: for computing sum, count, and average of 64-bit integers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DoubleAccumulator`: for computing sum, count, and averages for double precision
    floating numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CollectionAccumulator[T]` : for collecting a list of elements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the preceding Accumulators are built on top of the `AccumulatorV2` class.
    By following the same logic, we can potentially build very complex and customized
    Accumulators to use in our project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build a custom accumulator by extending the `AccumulatorV2` class. The
    following is an example showing the necessary functions to implement. `AccumulatorV2[Int,
    Int]` shown in the following code means that the Input and Output are both of
    Integer type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will look at a practical example of a custom accumulator. Again, we
    shall use the `statesPopulation` CSV file for this. Our goal is to accumulate
    the sum of year and sum of population in a custom accumulator.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Import the package containing the AccumulatorV2 class:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Case class to contain the Year and Population:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3\. StateAccumulator class extends AccumulatorV2:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4\. Create a new StateAccumulator and register the same with SparkContext:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5\. Read the statesPopulation.csv as an RDD:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6\. Use the StateAccumulator:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7\. Now, we can examine the value of the StateAccumulator:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we examined accumulators and how to build a custom accumulator.
    Thus, using the preceding illustrated example, you can create complex accumulators
    to meet your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the many types of RDDs, such as `shuffledRDD`,
    `pairRDD`, `sequenceFileRDD`, `HadoopRDD`, and so on. We also looked at the three
    main types of aggregations, `groupByKey`, `reduceByKey`, and `aggregateByKey`.
    We looked into how partitioning works and why it is important to have a proper
    plan around partitioning to increase the performance. We also looked at shuffling
    and the concepts of narrow and wide dependencies which are basic tenets of how
    Spark jobs are broken into stages. Finally, we looked at the important concepts
    of broadcast variables and accumulators.
  prefs: []
  type: TYPE_NORMAL
- en: The true power of the flexibility of RDDs makes it easy to adapt to most use
    cases and perform the necessary operations to accomplish the goal.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will switch gears to the higher layer of abstraction
    added to the RDDs as part of the Tungsten initiative known as DataFrames and Spark
    SQL and how it all comes together in the [Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduce a Little Structure â€“ Spark SQL*.
  prefs: []
  type: TYPE_NORMAL

["```scala\nval df = spark.createDataFrame(\nSeq((0, Array(\"Jason\", \"David\")),\n(1, Array(\"David\", \"Martin\")),\n(2, Array(\"Martin\", \"Jason\")),\n(3, Array(\"Jason\", \"Daiel\")),\n(4, Array(\"Daiel\", \"Martin\")),\n(5, Array(\"Moahmed\", \"Jason\")),\n(6, Array(\"David\", \"David\")),\n(7, Array(\"Jason\", \"Martin\")))).toDF(\"id\", \"name\")\ndf.show(false)\n\n```", "```scala\nval cvModel: CountVectorizerModel = new CountVectorizer()\n                           .setInputCol(\"name\")\n                           .setOutputCol(\"features\")\n                           .setVocabSize(3)\n                           .setMinDF(2)\n                           .fit(df)\n\n```", "```scala\nval feature = cvModel.transform(df)\nspark.stop()\n\n```", "```scala\nfeature.show(false)\n\n```", "```scala\nval sentence = spark.createDataFrame(Seq(\n (0, \"Tokenization,is the process of enchanting words,from the raw text\"),\n (1, \" If you want,to have more advance tokenization,RegexTokenizer,\n       is a good option\"),\n (2, \" Here,will provide a sample example on how to tockenize sentences\"),\n (3, \"This way,you can find all matching occurrences\"))).toDF(\"id\",\n                                                        \"sentence\")\n\n```", "```scala\nval tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\") \n\n```", "```scala\nval countTokens = udf { (words: Seq[String]) => words.length } \n\n```", "```scala\nval tokenized = tokenizer.transform(sentence) \n\n```", "```scala\ntokenized.select(\"sentence\", \"words\")\n.withColumn(\"tokens\", countTokens(col(\"words\")))\n.show(false) \n\n```", "```scala\nval regexTokenizer = new RegexTokenizer()\n                     .setInputCol(\"sentence\")\n                     .setOutputCol(\"words\")\n                     .setPattern(\"\\\\W+\")\n                     .setGaps(true)\n\n```", "```scala\nval regexTokenized = regexTokenizer.transform(sentence) \nregexTokenized.select(\"sentence\", \"words\") \n              .withColumn(\"tokens\", countTokens(col(\"words\")))\n              .show(false)\n\n```", "```scala\nval remover = new StopWordsRemover()\n             .setInputCol(\"words\")\n             .setOutputCol(\"filtered\")\n\n```", "```scala\nval newDF = remover.transform(regexTokenized)\n newDF.select(\"id\", \"filtered\").show(false)\n\n```", "```scala\nval df = spark.createDataFrame(\n Seq((0, \"Jason\", \"Germany\"),\n (1, \"David\", \"France\"),\n (2, \"Martin\", \"Spain\"),\n (3, \"Jason\", \"USA\"),\n (4, \"Daiel\", \"UK\"),\n (5, \"Moahmed\", \"Bangladesh\"),\n (6, \"David\", \"Ireland\"),\n (7, \"Jason\", \"Netherlands\"))).toDF(\"id\", \"name\", \"address\")\n\n```", "```scala\nval indexer = new StringIndexer()\n .setInputCol(\"name\")\n .setOutputCol(\"label\")\n .fit(df)\n\n```", "```scala\nval indexed = indexer.transform(df)\n\n```", "```scala\nindexed.show(false)\n\n```", "```scala\nval indexer = new StringIndexer()\n                  .setInputCol(\"name\")\n                  .setOutputCol(\"categoryIndex\")\n                  .fit(df)\nval indexed = indexer.transform(df)\nval encoder = new OneHotEncoder()\n                  .setInputCol(\"categoryIndex\")\n                  .setOutputCol(\"categoryVec\")\n\n```", "```scala\nval encoded = encoder.transform(indexed)\nencoded.show()\n\n```", "```scala\nval data = Array(\n Vectors.dense(3.5, 2.0, 5.0, 6.3, 5.60, 2.4),\n Vectors.dense(4.40, 0.10, 3.0, 9.0, 7.0, 8.75),\n Vectors.dense(3.20, 2.40, 0.0, 6.0, 7.4, 3.34) )\n\n```", "```scala\nval df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\ndf.show(false)\n\n```", "```scala\nval pca = new PCA()\n .setInputCol(\"features\")\n .setOutputCol(\"pcaFeatures\")\n .setK(4) \n .fit(df)\n\n```", "```scala\nval result = pca.transform(df).select(\"pcaFeatures\") \nresult.show(false)\n\n```", "```scala\nval data = MLUtils.loadLibSVMFile(spark.sparkContext, \"data/mnist.bz2\") \n\n```", "```scala\nval featureSize = data.first().features.size\nprintln(\"Feature Size: \" + featureSize)\n\n```", "```scala\nFeature Size: 780\n\n```", "```scala\nval splits = data.randomSplit(Array(0.75, 0.25), seed = 12345L)\nval (training, test) = (splits(0), splits(1))\n\n```", "```scala\nval pca = new PCA(featureSize/2).fit(data.map(_.features))\nval training_pca = training.map(p => p.copy(features = pca.transform(p.features)))\nval test_pca = test.map(p => p.copy(features = pca.transform(p.features))) \n\n```", "```scala\nval numIterations = 20\nval stepSize = 0.0001\nval model = LinearRegressionWithSGD.train(training, numIterations)\nval model_pca = LinearRegressionWithSGD.train(training_pca, numIterations)\n\n```", "```scala\nval valuesAndPreds = test.map { point =>\n                      val score = model.predict(point.features)\n                      (score, point.label)\n                     }\n\n```", "```scala\nval valuesAndPreds_pca = test_pca.map { point =>\n                         val score = model_pca.predict(point.features)\n                         (score, point.label)\n                       }\n\n```", "```scala\nval MSE = valuesAndPreds.map { case (v, p) => math.pow(v - p 2) }.mean()\nval MSE_pca = valuesAndPreds_pca.map { case (v, p) => math.pow(v - p, 2) }.mean()\nprintln(\"Mean Squared Error = \" + MSE)\nprintln(\"PCA Mean Squared Error = \" + MSE_pca)\n\n```", "```scala\nMean Squared Error = 2.9164359135973043E78\nPCA Mean Squared Error = 2.9156682256149184E78\n\n```", "```scala\nprintln(\"Model coefficients:\"+ model.toString())\nprintln(\"Model with PCA coefficients:\"+ model_pca.toString())\n\n```", "```scala\nModel coefficients: intercept = 0.0, numFeatures = 780\nModel with PCA coefficients: intercept = 0.0, numFeatures = 390\n\n```", "```scala\n0\\. Sample code number id number\n1\\. Clump Thickness 1 - 10\n2\\. Uniformity of Cell Size 1 - 10\n3\\. Uniformity of Cell Shape 1 - 10\n4\\. Marginal Adhesion 1 - 10\n5\\. Single Epithelial Cell Size 1 - 10\n6\\. Bare Nuclei 1 - 10\n7\\. Bland Chromatin 1 - 10\n8\\. Normal Nucleoli 1 - 10\n9\\. Mitoses 1 - 10\n10\\. Class: (2 for benign, 4 for malignant)\n\n```", "```scala\nval rdd = spark.sparkContext.textFile(\"data/wbcd.csv\") \nval cancerRDD = parseRDD(rdd).map(parseCancer) \n\n```", "```scala\ndef parseRDD(rdd: RDD[String]): RDD[Array[Double]] = { \n  rdd.map(_.split(\",\")).filter(_(6) != \"?\").map(_.drop(1)).map(_.map(_.toDouble)) \n} \n\n```", "```scala\ndef parseCancer(line: Array[Double]): Cancer = { \n  Cancer(if (line(9) == 4.0) 1 else 0, line(0), line(1), line(2), line(3), line(4), line(5), line(6), line(7), line(8)) \n}  \n\n```", "```scala\ncase class Cancer(cancer_class: Double, thickness: Double, size: Double, shape: Double, madh: Double, epsize: Double, bnuc: Double, bchrom: Double, nNuc: Double, mit: Double)\n\n```", "```scala\nimport spark.sqlContext.implicits._\nval cancerDF = cancerRDD.toDF().cache() \ncancerDF.show() \n\n```", "```scala\nval featureCols = Array(\"thickness\", \"size\", \"shape\", \"madh\", \"epsize\", \"bnuc\", \"bchrom\", \"nNuc\", \"mit\") \n\n```", "```scala\nval assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\") \n\n```", "```scala\nval df2 = assembler.transform(cancerDF) \n\n```", "```scala\ndf2.show() \n\n```", "```scala\nval labelIndexer = new StringIndexer().setInputCol(\"cancer_class\").setOutputCol(\"label\")\nval df3 = labelIndexer.fit(df2).transform(df2)\ndf3.show() \n\n```", "```scala\nval splitSeed = 1234567 \nval Array(trainingData, testData) = df3.randomSplit(Array(0.7, 0.3), splitSeed)\n\n```", "```scala\nval lr = new LogisticRegression().setMaxIter(50).setRegParam(0.01).setElasticNetParam(0.01) \nval model = lr.fit(trainingData)  \n\n```", "```scala\nval predictions = model.transform(testData) \npredictions.show() \n\n```", "```scala\nval trainingSummary = model.summary \nval objectiveHistory = trainingSummary.objectiveHistory \nobjectiveHistory.foreach(loss => println(loss))\n\n```", "```scala\n    0.6562291876496595\n    0.6087867761081431\n    0.538972588904556\n    0.4928455913405332\n    0.46269258074999386\n    0.3527914819973198\n    0.20206901337404978\n    0.16459454874996993\n    0.13783437051276512\n    0.11478053164710095\n    0.11420433621438157\n    0.11138884788059378\n    0.11041889032338036\n    0.10849477236373875\n    0.10818880537879513\n    0.10682868640074723\n    0.10641395229253267\n    0.10555411704574749\n    0.10505186414044905\n    0.10470425580130915\n    0.10376219754747162\n    0.10331139609033112\n    0.10276173290225406\n    0.10245982201904923\n    0.10198833366394071\n    0.10168248313103552\n    0.10163242551955443\n    0.10162826209311404\n    0.10162119367292953\n    0.10161235376791203\n    0.1016114803209495\n    0.10161090505556039\n    0.1016107261254795\n    0.10161056082112738\n    0.10161050381332608\n    0.10161048515341387\n    0.10161043900301985\n    0.10161042057436288\n    0.10161040971267737\n    0.10161040846923354\n    0.10161040625542347\n    0.10161040595207525\n    0.10161040575664354\n    0.10161040565870835\n    0.10161040519559975\n    0.10161040489834573\n    0.10161040445215266\n    0.1016104043469577\n    0.1016104042793553\n    0.1016104042606048\n    0.10161040423579716 \n\n```", "```scala\nval binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n\n```", "```scala\nval roc = binarySummary.roc \nroc.show() \nprintln(\"Area Under ROC: \" + binarySummary.areaUnderROC)\n\n```", "```scala\nArea Under ROC: 0.9959095884623509\n\n```", "```scala\nimport org.apache.spark.sql.functions._\n\n// Calculate the performance metrics\nval lp = predictions.select(\"label\", \"prediction\") \nval counttotal = predictions.count() \nval correct = lp.filter($\"label\" === $\"prediction\").count() \nval wrong = lp.filter(not($\"label\" === $\"prediction\")).count() \nval truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() \nval falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() \nval falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() \nval ratioWrong = wrong.toDouble / counttotal.toDouble \nval ratioCorrect = correct.toDouble / counttotal.toDouble \n\nprintln(\"Total Count: \" + counttotal) \nprintln(\"Correctly Predicted: \" + correct) \nprintln(\"Wrongly Identified: \" + wrong) \nprintln(\"True Positive: \" + truep) \nprintln(\"False Negative: \" + falseN) \nprintln(\"False Positive: \" + falseP) \nprintln(\"ratioWrong: \" + ratioWrong) \nprintln(\"ratioCorrect: \" + ratioCorrect) \n\n```", "```scala\nval fMeasure = binarySummary.fMeasureByThreshold \nval fm = fMeasure.col(\"F-Measure\") \nval maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0) \nval bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).select(\"threshold\").head().getDouble(0) \nmodel.setThreshold(bestThreshold) \n\n```", "```scala\nval evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\") \nval accuracy = evaluator.evaluate(predictions) \nprintln(\"Accuracy: \" + accuracy)     \n\n```", "```scala\nAccuracy: 0.9963975418520874\n\n```", "```scala\n// Load training data in LIBSVM format.\n val data = MLUtils.loadLibSVMFile(spark.sparkContext, \"data/mnist.bz2\")\n\n```", "```scala\nval splits = data.randomSplit(Array(0.75, 0.25), seed = 12345L)\nval training = splits(0).cache()\nval test = splits(1)\n\n```", "```scala\nval model = new LogisticRegressionWithLBFGS()\n           .setNumClasses(10)\n           .setIntercept(true)\n           .setValidateData(true)\n           .run(training)\n\n```", "```scala\nmodel.clearThreshold()\n\n```", "```scala\nval scoreAndLabels = test.map { point =>\n  val score = model.predict(point.features)\n  (score, point.label)\n}\n\n```", "```scala\nval metrics = new MulticlassMetrics(scoreAndLabels)\n\n```", "```scala\nprintln(\"Confusion matrix:\")\nprintln(metrics.confusionMatrix)\n\n```", "```scala\nval accuracy = metrics.accuracy\nprintln(\"Summary Statistics\")\nprintln(s\"Accuracy = $accuracy\")\n// Precision by label\nval labels = metrics.labels\nlabels.foreach { l =>\n  println(s\"Precision($l) = \" + metrics.precision(l))\n}\n// Recall by label\nlabels.foreach { l =>\n  println(s\"Recall($l) = \" + metrics.recall(l))\n}\n// False positive rate by label\nlabels.foreach { l =>\n  println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n}\n// F-measure by label\nlabels.foreach { l =>\n  println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n}\n\n```", "```scala\nSummary Statistics\n ----------------------\n Accuracy = 0.9203609775377116\n Precision(0.0) = 0.9606815203145478\n Precision(1.0) = 0.9595732734418866\n .\n .\n Precision(8.0) = 0.8942172073342737\n Precision(9.0) = 0.9027210884353741\n\n Recall(0.0) = 0.9638395792241946\n Recall(1.0) = 0.9732346241457859\n .\n .\n Recall(8.0) = 0.8720770288858322\n Recall(9.0) = 0.8936026936026936\n\n FPR(0.0) = 0.004392386530014641\n FPR(1.0) = 0.005363128491620112\n .\n .\n FPR(8.0) = 0.010927369417935456\n FPR(9.0) = 0.010441004672897197\n\n F1-Score(0.0) = 0.9622579586478502\n F1-Score(1.0) = 0.966355668645745\n .\n .\n F1-Score(9.0) = 0.8981387478849409\n\n```", "```scala\nprintln(s\"Weighted precision: ${metrics.weightedPrecision}\")\nprintln(s\"Weighted recall: ${metrics.weightedRecall}\")\nprintln(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\nprintln(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\") \n\n```", "```scala\nWeighted precision: 0.920104303076327\n Weighted recall: 0.9203609775377117\n Weighted F1 score: 0.9201934861645358\n Weighted false positive rate: 0.008752250453215607\n\n```", "```scala\n// Load training data in LIBSVM format.\n val data = MLUtils.loadLibSVMFile(spark.sparkContext, \"data/mnist.bz2\")\n\n```", "```scala\nval splits = data.randomSplit(Array(0.75, 0.25), seed = 12345L)\nval training = splits(0).cache()\nval test = splits(1)\n\n```", "```scala\nval numClasses = 10 //number of classes in the MNIST dataset\nval categoricalFeaturesInfo = Map[Int, Int]()\nval numTrees = 50 // Use more in practice.More is better\nval featureSubsetStrategy = \"auto\" // Let the algorithm choose.\nval impurity = \"gini\" // see above notes on RandomForest for explanation\nval maxDepth = 30 // More is better in practice\nval maxBins = 32 // More is better in practice \nval model = RandomForest.trainClassifier(training, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n\n```", "```scala\nval scoreAndLabels = test.map { point =>\n  val score = model.predict(point.features)\n  (score, point.label)\n}\n\n```", "```scala\nval metrics = new MulticlassMetrics(scoreAndLabels)\n\n```", "```scala\nprintln(\"Confusion matrix:\")\nprintln(metrics.confusionMatrix)\n\n```", "```scala\nval accuracy = metrics.accuracy\nprintln(\"Summary Statistics\")\nprintln(s\"Accuracy = $accuracy\")\n// Precision by label\nval labels = metrics.labels\nlabels.foreach { l =>\n  println(s\"Precision($l) = \" + metrics.precision(l))\n}\n// Recall by label\nlabels.foreach { l =>\n  println(s\"Recall($l) = \" + metrics.recall(l))\n}\n// False positive rate by label\nlabels.foreach { l =>\n  println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n}\n// F-measure by label\nlabels.foreach { l =>\n  println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n} \n\n```", "```scala\nSummary Statistics:\n ------------------------------\n Precision(0.0) = 0.9861932938856016\n Precision(1.0) = 0.9891799544419134\n .\n .\n Precision(8.0) = 0.9546079779917469\n Precision(9.0) = 0.9474747474747475\n\n Recall(0.0) = 0.9778357235984355\n Recall(1.0) = 0.9897435897435898\n .\n .\n Recall(8.0) = 0.9442176870748299\n Recall(9.0) = 0.9449294828744124\n\n FPR(0.0) = 0.0015387997362057595\n FPR(1.0) = 0.0014151646059883808\n .\n .\n FPR(8.0) = 0.0048136532710962\n FPR(9.0) = 0.0056967572304995615\n\n F1-Score(0.0) = 0.9819967266775778\n F1-Score(1.0) = 0.9894616918256907\n .\n .\n F1-Score(8.0) = 0.9493844049247605\n F1-Score(9.0) = 0.9462004034969739\n\n```", "```scala\nprintln(s\"Weighted precision: ${metrics.weightedPrecision}\")\nprintln(s\"Weighted recall: ${metrics.weightedRecall}\")\nprintln(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\nprintln(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")\nval testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / test.count()\nprintln(\"Accuracy = \" + (1-testErr) * 100 + \" %\")\n\n```", "```scala\nOverall statistics\n ----------------------------\n Weighted precision: 0.966513107682512\n Weighted recall: 0.9664712469534286\n Weighted F1 score: 0.9664794711607312\n Weighted false positive rate: 0.003675328222679072\n Accuracy = 96.64712469534287 %\n\n```"]
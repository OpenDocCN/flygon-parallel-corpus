- en: Advanced Machine Learning Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Hyperparameter optimization or model selection is the problem of choosing
    a set of hyperparameters [when defined as?] for a learning algorithm, usually
    with the goal of optimizing a measure of the algorithm''s performance on an independent
    dataset."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Machine learning model tuning quote'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will provide some theoretical and practical aspects of some
    advanced topics of machine learning (ML) with Spark. We will see how to tune machine
    learning models for better and optimized performance using grid search, cross-validation
    and hyperparameter tuning. In the later section, we will cover how to develop
    a scalable recommendation system using the ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modeling application as a text clustering
    technique will be demonstrated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning best practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning of ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling using latent dirichlet allocation (LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A recommendation system using collaborative filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, it is recommended to consider the error rate rather than only the
    accuracy. For example, suppose an ML system with 99% accuracy and 50% errors is
    worse than the one with 90% accuracy but 25% errors. So far, we have discussed
    the following machine learning topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**: This is for predicting values that are linearly separable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: This is for finding unusual data points often done using
    a clustering algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: This is for discovering the hidden structure in the dataset
    for clustering homogeneous data points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary classification**: This is for predicting two categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: This is for predicting three or more categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, we have also seen that there are some good algorithms for these tasks.
    However, choosing the right algorithm for your problem type is a tricky task for
    achieving higher and outstanding accuracy in your ML algorithms. For this, we
    need to adopt some good practices through the stages, that is, from data collection,
    feature engineering, model building, evaluating, tuning, and deployment. Considering
    these, in this section, we will provide some practical recommendation while developing
    your ML application using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A straight line cutting across a curving scatter plot would be a good example
    of under-fitting, as we can see in the diagram here. However, if the line fits
    the data too well, there evolves an opposite problem called **overfitting**. When
    we say a model overfits a dataset, we mean it may have a low error rate for the
    training data, but it does not generalize well to the overall population in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpeg)**Figure 1**: Overfitting-underfitting trade-off (source:
    The book, "Deep Learning" by Adam Gibson, Josh Patterson)'
  prefs: []
  type: TYPE_IMG
- en: 'More technically, if you evaluate your model on the training data instead of
    test or validated data, you probably won''t be able to articulate whether your
    model is overfitting or not. The common symptoms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive accuracy of the data used for training can be over accurate (that
    is, sometimes even 100%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model might show better performance compared to the random prediction for
    new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We like to fit a dataset to a distribution because if the dataset is reasonably
    close to the distribution, we can make assumptions based on the theoretical distribution
    of how we operate with the data. Consequently, the normal distribution in the
    data allows us to assume that sampling distributions of statistics are normally
    distributed under specified conditions. The normal distribution is defined by
    its mean and standard deviation and has generally the same shape across all variations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)**Figure 2**: Normal distribution in the data helps overcoming
    both the over-fitting and underfitting (source: The book, "Deep Learning" by Adam
    Gibson, Josh Patterson)'
  prefs: []
  type: TYPE_IMG
- en: 'Sometimes, the ML model itself becomes underfit for a particular tuning or
    data point, which means the model become too simplistic. Our recommendation (like
    that of others, we believe) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset into two sets to detect overfitting situations--the first
    one is for training and model selection called the training set, and the second
    one is the test set for evaluating the model started in place of the ML workflow
    section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you also could avoid the overfitting by consuming simpler models
    (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling
    the regularization parameters of your ML model (if available).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the model with a correct data value of parameters to avoid both the overfitting
    as well as underfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, solving underfitting is the priority, but most of the machines learning
    practitioners suggest spending more time and effort attempting not to overfit
    the line to the data. Also, many machine learning practitioners, on the other
    hand, have recommended splitting the large-scale dataset into three sets: a training
    set (50%), validation set (25%), and test set (25%). They also suggested to build
    the model using the training set and to calculate the prediction errors using
    the validation set. The test set was recommended to be used to assess the generalization
    error of the final model. If the amount of labeled data available on the other
    hand during the supervised learning is smaller, it is not recommended to split
    the datasets. In that case, use cross validation. More specifically, divide the
    dataset into 10 parts of (roughly) equal size; after that, for each of these 10
    parts, train the classifier iteratively and use the 10th part to test the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay tuned with Spark MLlib and Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step of the pipeline designing is to create the building blocks (as
    a directed or undirected graph consisting of nodes and edges) and make a linking
    between those blocks. Nevertheless, as a data scientist, you should be focused
    on scaling and optimizing nodes (primitives) too so that you are able to scale
    up your application for handling large-scale datasets in the later stage to make
    your ML pipeline performing consistently. The pipeline process will also help
    you to make your model adaptive for new datasets. However, some of these primitives
    might be explicitly defined to particular domains and data types (for example,
    text, image, and video, audio, and spatiotemporal).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these types of data, the primitives should also be working for general
    purpose domain statistics or mathematics. The casting of your ML model in terms
    of these primitives will make your workflow more transparent, interpretable, accessible,
    and explainable.
  prefs: []
  type: TYPE_NORMAL
- en: A recent example would be the ML-matrix, which is a distributed matrix library
    that can be used on top of Spark. Refer to the JIRA issue at [https://issues.apache.org/jira/browse/SPARK-3434](https://issues.apache.org/jira/browse/SPARK-3434).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.jpeg)**Figure 3:** Stay tune and interoperate ML and MLlib'
  prefs: []
  type: TYPE_NORMAL
- en: As we already stated in the previous section, as a developer, you can seamlessly
    combine the implementation techniques in Spark MLlib along with the algorithms
    developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable
    ML applications on top of RDD, DataFrame, and datasets as shown in *Figure 3*.
    Therefore, the recommendation here is to stay in tune or synchronized with the
    latest technologies around you for the betterment of your ML application.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right algorithm for your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"What machine learning algorithm should I use?" is a very frequently asked
    question for the naive machine learning practitioners but the answer is always
    *it depends*. More elaborately:'
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the volume, quality, complexity, and the nature of the data you
    have to be tested/used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on external environments and parameters like your computing system's
    configuration or underlying infrastructures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on what you want to do with the answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on how the mathematical and statistical formulation of the algorithm
    was translated into machine instructions for the computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It depends on how much time do you have
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reality is, even the most experienced data scientists or data engineers
    can''t give a straight recommendation about which ML algorithm will perform the
    best before trying them all together. Most of the statements of agreement/disagreement
    begins with "It depends...hmm..." Habitually, you might wonder if there are cheat
    sheets of machine learning algorithms, and if so, how should you use that cheat
    sheet? Several data scientists have said that the only sure way to find the very
    best algorithm is to try all of them; therefore, there is no shortcut dude! Let''s
    make it clearer; suppose you do have a set of data and you want to do some clustering.
    Technically, this could be either a classification or regression problem that
    you want o apply on your dataset if your data is labeled. However, if you have
    a unlabeled dataset, it is the clustering technique that you will be using. Now,
    the concerns that evolve in your mind are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Which factors should I consider before choosing an appropriate algorithm? Or
    should I just choose an algorithm randomly?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I choose any data pre-processing algorithm or tools that can be applied
    to my data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What sort of feature engineering techniques should I be using to extract the
    useful features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What factors can improve the performance of my ML model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I adopt my ML application for new data types?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I scale up my ML application for large-scale datasets? And so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will try to answer these questions with our little machine
    learning knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations when choosing an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recommendation or suggestions we provide here are for the novice data scientist
    who is just learning machine learning. These will use useful to expert the data
    scientists too, who is trying to choose an optimal algorithm to start with Spark
    ML APIs. Don''t worry, we will guide you to the direction! We also recommend going
    with the following algorithmic properties when choosing an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Whether getting the best score is the goal or an approximate
    solution (*good enough*) in terms of precision, recall, f1 score or AUC and so
    on, while trading off overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training time**: The amount of time available to train the model (including
    the model building, evaluation, and tanning time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity**: An aspect of model complexity in terms of how the problem is
    modeled. Since most of the non-linear models are often more complex to understand
    and tune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of parameters**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of features**: The problem of having more attributes than instances,
    the *p>>n* problem. This often requires specialized handling or specialized techniques
    using dimensionality reduction or better feature engineering approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting the most accurate results from your ML application isn''t always indispensable.
    Depending on what you want to use it for, sometimes an approximation is adequate
    enough. If the situation is something like this, you may be able to reduce the
    processing time drastically by incorporating the better-estimated methods. When
    you become familiar of the workflow with the Spark machine learning APIs, you
    will enjoy the advantage of having more approximation methods, because these approximation
    methods will tend to avoid the overfitting problem of your ML model automatically.
    Now, suppose you have two binary classification algorithms that perform as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classifier** | **Precision** | **Recall** |'
  prefs: []
  type: TYPE_TB
- en: '| X | 96% | 89% |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 99% | 84% |'
  prefs: []
  type: TYPE_TB
- en: 'Here, none of the classifiers is obviously superior, so it doesn''t immediately
    guide you toward picking the optimal one. F1-score which is the harmonic mean
    of precision and recall helps you. Let''s calculate it and place it in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classifier** | **Precision** | **Recall** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| X | 96% | 89% | 92.36% |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 99% | 84% | 90.885% |'
  prefs: []
  type: TYPE_TB
- en: Therefore, having an F1-score helps make a decision for selecting from a large
    number of classifiers. It gives a clear preference ranking among all of them,
    and therefore a clear direction for progress--that is classifier **X**.
  prefs: []
  type: TYPE_NORMAL
- en: Training time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training time is often closely related to the model training and the accuracy.
    In addition, often you will discover that some of the algorithms are elusive to
    the number of data points compared to others. However, when your time is inadequate
    but the training set is large with a lot of features, you can choose the simplest
    one. In this case, you might have to compromise with the accuracy. But it will
    fulfill your minimum requirements at least.
  prefs: []
  type: TYPE_NORMAL
- en: Linearity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many machine learning algorithms developed recently that make use
    of linearity (also available in the Spark MLlib and Spark ML). For example, linear
    classification algorithms undertake that classes can be separated by plotting
    a differentiating straight line or using higher-dimensional equivalents. Linear
    regression algorithms, on the other hand, assume that data trends simply follow
    a straight line. This assumption is not naive for some machine learning problems;
    however, there might be some other cases where the accuracy will be down. Despite
    their hazards, linear algorithms are very popular with data engineers and data
    scientists as the first line of the outbreak. Moreover, these algorithms also
    tend to be simple and fast, to train your models during the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: Inspect your data when choosing an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will find many machine learning datasets available at the UC Irvine Machine
    Learning Repository. The following data properties should also be prioritized:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parameters or data properties are the handholds for a data scientist when setting
    up an algorithm. They are numbers that affect the algorithm's performance, such
    as error tolerance or a number of iterations, or options between variants of how
    the algorithm acts. The training time and accuracy of the algorithm can sometimes
    be quite sensitive making it difficult get the right settings. Typically, algorithms
    with a large number of parameters require more trial and error to find an optimal
    combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the fact that this is a great way to span the parameter space, the
    model building or train time increases exponentially with the increased number
    of parameters. This is a dilemma as well as a time-performance trade-off. The
    positive sides are:'
  prefs: []
  type: TYPE_NORMAL
- en: Having many parameters characteristically indicates the greater flexibility
    of the ML algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your ML application achieves much better accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large is your training set?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your training set is smaller, high bias with low variance classifiers, such
    as Naive Bayes have an advantage over low bias with high variance classifiers
    (also can be used for regression) such as the **k-nearest neighbors algorithm**
    (**kNN**).
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias, Variance, and the kNN model:** In reality, *increasing k* will *decrease
    the variance,* but *increase the bias*. On the other hand, *decreasing k* will
    *increase variance* and *decrease bias*. As *k* increases, this variability is
    reduced. But if we increase *k* too much, then we no longer follow the true boundary
    line and we observe high bias. This is the nature of the Bias-Variance Trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen the over and underfitting issue already. Now, you can assume that
    dealing with bias and variance is like dealing with over- and underfitting. Bias
    is reduced and variance is increased in relation to model complexity. As more
    and more parameters are added to a model, the complexity of the model rises and
    variance becomes our primary concern while bias steadily falls. In other words,
    bias has a negative first-order derivative in response to model complexity, while
    variance has a positive slope. Refer to the following figure for a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.jpeg)**Figure 4:** Bias and variance contributing to total error'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the latter will overfit. But low bias with high variance classifiers,
    on the other hand, starts to win out as your training set grows linearly or exponentially,
    since they have lower asymptotic errors. High bias classifiers aren't powerful
    enough to provide accurate models.
  prefs: []
  type: TYPE_NORMAL
- en: Number of features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For certain types of experimental dataset, the number of extracted features
    can be very large compared to the number of data points itself. This is often
    the case with genomics, biomedical, or textual data. A large number of features
    can swamp down some learning algorithms, making training time ridiculously higher.
    **Support Vector Machines** (**SVMs**) are particularly well suited in this case
    for its high accuracy, nice theoretical guarantees regarding overfitting, and
    with an appropriate kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '**The SVM and the Kernel**: The task is to find a set of weight and bias such
    that the margin can maximize the function:'
  prefs: []
  type: TYPE_NORMAL
- en: y = w*¥(x) +b,
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *w* is the weight, *¥* is the feature vector, and *b* is the bias. Now
    if *y> 0*, then we classify datum to class *1*, else to class *0*, whereas, the
    feature vector *¥(x)* makes the data linearly separable. However, using the kernel
    makes the calculation process faster and easier, especially when the feature vector
    *¥* consisting of very high dimensional data. Let''s see a concrete example. Suppose
    we have the following value of *x* and *y*: *x = (x1, x2, x3)* and *y = (y1, y2,
    y3)*, then for the function *f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1,
    x3x2, x3x3)*, the kernel is *K(x, y ) = (<x, y>)²*. Following the above, if *x*
    *= (1, 2, 3)* and *y = (4, 5, 6)*, then we have the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
  prefs: []
  type: TYPE_NORMAL
- en: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
  prefs: []
  type: TYPE_NORMAL
- en: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple linear algebra that maps a 3-dimensional space to a 9 dimensional.
    On the other hand, the kernel is a similarity measure used for SVMs. Therefore,
    choosing an appropriate kernel value based on the prior knowledge of invariances
    is suggested. The choice of the kernel and kernel and regularization parameters
    can be automated by optimizing a cross-validation based model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, an automated choice of kernels and kernel parameters is a tricky
    issue, as it is very easy to overfit the model selection criterion. This might
    result in a worse model than you started with. Now, if we use the kernel function
    *K(x, y), this gives the same value but with much simpler calculation -i.e. (4
    + 10 + 18) ^2 = 32^2 = 1024*.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning of ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tuning an algorithm is simply a process that one goes through in order to enable
    the algorithm to perform optimally in terms of runtime and memory usage. In Bayesian
    statistics, a hyperparameter is a parameter of a prior distribution. In terms
    of machine learning, the term hyperparameter refers to those parameters that cannot
    be directly learned from the regular training process. Hyperparameters are usually
    fixed before the actual training process begins. This is done by setting different
    values for those hyperparameters, training different models, and deciding which
    ones work best by testing them. Here are some typical examples of such parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of leaves, bins, or depth of a tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of latent factors in a matrix factorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of hidden layers in a deep neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of clusters in a k-means clustering and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will discuss how to perform hyperparameter tuning using
    the cross-validation technique and grid searching.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning is a technique for choosing the right combination of
    hyperparameters based on the performance of presented data. It is one of the fundamental
    requirements to obtain meaningful and accurate results from machine learning algorithms
    in practice. The following figure shows the model tuning process, consideration,
    and workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00343.jpeg)**Figure 5**: The model tuning process, consideration, and
    workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we have two hyperparameters to tune for a pipeline presented
    in *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, a Spark ML pipeline model
    using a logistic regression estimator (dash lines only happen during pipeline
    fitting). We can see that we have put three candidate values for each. Therefore,
    there would be nine combinations in total. However, only four are shown in the
    diagram, namely, Tokenizer, HashingTF, Transformer, and Logistic Regression (LR).
    Now, we want to find the one that will eventually lead to the model with the best
    evaluation result. The fitted model consists of the Tokenizer, the HashingTF feature
    extractor, and the fitted logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: If you recall *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, the dashed line, however,
    happens only during the pipeline fitting. As mentioned earlier, the fitted pipeline
    model is a Transformer. The Transformer can be used for prediction, model validation,
    and model inspection. In addition, we also argued that one ill-fated distinguishing
    characteristic of the ML algorithms is that typically they have many hyperparameters
    that need to be tuned for better performance. For example, the degree of regularizations
    in these hyperparameters is distinctive from the model parameters optimized by
    the Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, it is really hard to guess or measure the best combination
    of hyperparameters without expert knowledge of the data and the algorithm to use.
    Since the complex dataset is based on the ML problem type, the size of the pipeline
    and the number of hyperparameters may grow exponentially (or linearly); the hyperparameter
    tuning becomes cumbersome even for an ML expert, not to mention that the result
    of the tuning parameters may become unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Spark API documentation, a unique and uniform API is used for
    specifying Spark ML estimators and Transformers. A `ParamMap` is a set of (parameter,
    value) pairs with a Param as a named parameter with self-contained documentation
    provided by Spark. Technically, there are two ways for passing the parameters
    to an algorithm as specified in the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting parameters**: If an LR is an instance of Logistic Regression (that
    is, Estimator), you can call the `setMaxIter()` method as follows: `LR.setMaxIter(5)`.
    It essentially fits the model pointing the regression instance as follows: `LR.fit()`.
    In this particular example, there would be at most five iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The second option**: This one involves passing a `ParamMaps` to `fit()` or
    `transform()` (refer *Figure 5* for details). In this circumstance, any parameters
    will be overridden by the `ParamMaps` previously specified via setter methods
    in the ML application-specific codes or algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid search parameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you selected your hyperparameters after necessary feature engineering.
    In this regard, a full grid search of the space of hyperparameters and features
    is computationally too intensive. Therefore, you need to perform a fold of the
    K-fold cross-validation instead of a full-grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the required hyperparameters using cross validation on the training set
    of the fold, using all the available features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the required features using those hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the computation for each fold in K
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final model is constructed on all the data using the N most prevalent features
    that were selected from each fold of CV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interesting thing is that the hyperparameters would also be tuned again
    using all the data in a cross-validation loop. Would there be a large downside
    from this method as compared to a full-grid search? In essence, I am doing a line
    search in each dimension of free parameters (finding the best value in one dimension,
    holding that constant, then finding the best in the next dimension), rather than
    every single combination of parameter settings. The most important downside for
    searching along single parameters instead of optimizing them all together, is
    that you ignore interactions.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite common that, for instance, more than one parameter influences model
    complexity. In that case, you need to look at their interaction in order to successfully
    optimize the hyperparameters. Depending on how large your dataset is and how many
    models you compare, optimization strategies that return the maximum observed performance
    may run into trouble (this is true for both grid search and your strategy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is that searching through a large number of performance estimates
    for the maximum skims the variance of the performance estimate: you may just end
    up with a model and training/test split combination that accidentally happens
    to look good. Even worse, you may get several perfect-looking combinations, and
    the optimization then cannot know which model to choose and thus becomes unstable.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation (also called the **rotation estimation** (**RE**)) is a model
    validation technique for assessing the quality of the statistical analysis and
    results. The target is to make the model generalize toward an independent test
    set. One of the perfect uses of the cross-validation technique is making a prediction
    from a machine learning model. It will help if you want to estimate how a predictive
    model will perform accurately in practice when you deploy it as an ML application.
    During the cross-validation process, a model is usually trained with a dataset
    of a known type. Conversely, it is tested using a dataset of unknown type.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, cross-validation helps to describe a dataset to test the model
    in the training phase using the validation set. There are two types of cross-validation
    that can be typed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-exhaustive cross-validation**: This includes K-fold cross-validation
    and repeated random sub-sampling cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In most of the cases, the researcher/data scientist/data engineer uses 10-fold
    cross-validation instead of testing on a validation set. This is the most widely
    used cross-validation technique across the use cases and problem type as explained
    by the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00372.gif)**Figure 6:** Cross-validation basically splits your complete
    available training data into a number of folds. This parameter can be specified.
    Then the whole pipeline is run once for every fold and one machine learning model
    is trained for each fold. Finally, the different machine learning models obtained
    are joined by a voting scheme for classifiers or by averaging for regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, to reduce the variability, multiple iterations of cross-validation
    are performed using different partitions; finally, the validation results are
    averaged over the rounds. The following figure shows an example of hyperparameter
    tuning using the logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpeg)**Figure 7:** An example of hyperparameter tuning using
    the logistic regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using cross-validation instead of conventional validation has two main advantages
    outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, if there is not enough data available to partition across the separate
    training and test sets, there's the chance of losing significant modeling or testing
    capability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the K-fold cross-validation estimator has a lower variance than a
    single hold-out set estimator. This low variance limits the variability and is
    again very important if the amount of available data is limited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In these circumstances, a fair way to properly estimate the model prediction
    and related performance are to use cross-validation as a powerful general technique
    for model selection and validation. If we need to perform manual features and
    a parameter selection for the model tuning, after that, we can perform a model
    evaluation with a 10-fold cross-validation on the entire dataset. What would be
    the best strategy? We would suggest you go for the strategy that provides an optimistic
    score as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the dataset into training, say 80%, and testing 20% or whatever you chose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the K-fold cross-validation on the training set to tune your model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the CV until you find your model optimized and therefore tuned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, use your model to predict on the testing set to get an estimate of out
    of model errors.
  prefs: []
  type: TYPE_NORMAL
- en: Credit risk analysis – An example of hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show a practical example of machine learning hyperparameter
    tuning in terms of grid searching and cross-validation technique. More specifically,
    at first, we will develop a credit risk pipeline that is commonly used in financial
    institutions such as banks and credit unions. Later on, we will look at how to
    improve the prediction accuracy by hyperparameter tuning. Before diving into the
    example, let's take a quick overview of what credit risk analysis is and why it
    is important?
  prefs: []
  type: TYPE_NORMAL
- en: What is credit risk analysis? Why is it important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When an applicant applies for loans and a bank receives that application, based
    on the applicant''s profile, the bank has to make a decision whether to approve
    the loan application or not. In this regard, there are two types of risk associated
    with the bank''s decision on the loan application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The applicant is a good credit risk**: That means the client or applicant
    is more likely to repay the loan. Then, if the loan is not approved, the bank
    can potentially suffer the loss of business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The applicant is a bad credit risk**: That means that the client or applicant
    is most likely not to repay the loan. In that case, approving the loan to the
    client will result in financial loss to the bank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The institution says that the second one is riskier than that of the first one,
    as the bank has a higher chance of not getting reimbursed the borrowed amount.
    Therefore, most banks or credit unions evaluate the risks associated with lending
    money to a client, applicant, or customer. In business analytics, minimizing the
    risk tends to maximize the profit to the bank itself.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, maximizing the profit and minimizing the loss from a financial
    perspective is important. Often, the bank makes a decision about approving a loan
    application based on different factors and parameters of an applicant, such as
    demographic and socio-economic conditions regarding their loan application.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The German credit dataset was downloaded from the UCI Machine Learning Repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
    Although a detailed description of the dataset is available in the link, we provide
    some brief insights here in **Table 3**. The data contains credit-related data
    on 21 variables and the classification of whether an applicant is considered a
    good or a bad credit risk for 1000 loan applicants (that is, binary classification
    problem).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows details about each variable that was considered before
    making the dataset available online:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Entry** | **Variable** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | creditability | Capable of repaying: has value either 1.0 or 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | balance | Current balance |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | duration | Duration of the loan being applied for |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | history | Is there any bad loan history? |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | purpose | Purpose of the loan |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | amount | Amount being applied for |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | savings | Monthly saving |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | employment | Employment status |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | instPercent | Interest percent |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | sexMarried | Sex and marriage status |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | guarantors | Are there any guarantors? |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | residenceDuration | Duration of residence at the current address |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | assets | Net assets |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | age | Age of the applicant |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | concCredit | Concurrent credit |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | apartment | Residential status |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | credits | Current credits |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | occupation | Occupation |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | dependents | Number of dependents |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | hasPhone | If the applicant uses a phone |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | foreign | If the applicant is a foreigner |'
  prefs: []
  type: TYPE_TB
- en: Note that, although *Table 3* describes the variables with an associated header,
    there is no associated header in the dataset. In *Table 3*, we have shown the
    variable, position, and associated significance of each variable.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step example with Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will provide a step-by-step example of credit risk prediction using
    the Random Forest classifier. The steps include from data ingestion, some statistical
    analysis, training set preparation, and finally model evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.** Load and parse the dataset into RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For the preceding line, the `parseRDD()` method is used to split the entry
    with `,` and then converted all of them as `Double` value (that is, numeric).
    This method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the `parseCredit()` method is used to parse the dataset
    based on the `Credit` case class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Credit` case class goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Prepare the DataFrame for the ML pipeline** - Get the DataFrame
    for the ML pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Save them as a temporary view for making the query easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a snap of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `show()` method prints the credit DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.gif)**Figure 8:** A snap of the credit dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3\. Observing related statistics** - First, let''s see some aggregate
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the statistics about the balance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see the creditability per average balance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.gif)**Figure 9:** Some statistics of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Feature vectors and labels creation** - As you can see, the credibility
    column is the response column, and, for the result, we need to create the feature
    vector without considering this column. Now, let''s create the feature column
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assemble all the features of these selected columns using `VectorAssembler()`
    API as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see what the feature vectors look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line shows the features created by the VectorAssembler transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00360.gif)**Figure 10:** Generating features for ML models using VectorAssembler'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a new column as a label from the old response column creditability
    using `StringIndexer` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line shows the features and labels created by the `VectorAssembler`
    transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00274.gif)**Figure 11:** Corresponding labels and feature for ML models
    using VectorAssembler'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5.** Prepare the training and test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6\. Train the random forest model** - At first, instantiate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For an explanation of the preceding parameters, refer to the random forest
    algorithm section in this chapter. Now, let''s train the model using the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7.** Compute the raw prediction for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the top 20 rows of this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line shows the DataFrame containing the label, raw prediction,
    probablity, and actual prediciton:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.gif)**Figure 12:** The DataFrame containing raw and actual prediction
    for test set'
  prefs: []
  type: TYPE_NORMAL
- en: Now after seeing the prediction from the last column, a bank can make a decision
    about the applications for which the application should be accepted.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Model evaluation before tuning** - Instantiate the binary evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the accuracy of the prediction for the test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy before pipeline fitting: `0.751921784149243`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, the accuracy is 75%, which is not that good. Let''s compute other
    important performance metrics for the binary classifier like **area under receiver
    operating characteristic** (**AUROC**) and **area under precision recall curve**
    (**AUPRC**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printlnMetric()` method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s compute a few more performance metrics using the `RegressionMetrics
    ()` API for the random forest model we used during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see how our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Not that bad! However, not satisfactory either, right? Let's tune the model
    using grid search and cross-validation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9\. Model tuning using grid search and cross-validation** - First, let''s
    use the `ParamGridBuilder` API to construct a grid of parameters to search over
    the param grid consisting of 20 to 70 trees with `maxBins` between 25 and 30,
    `maxDepth` between 5 and 10, and impurity as entropy and gini:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the cross-validator model using the training set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the raw prediction for the test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10\. Evaluation of the model after tuning** - Let''s see the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s above 83%. Good improvement indeed! Let''s see the two other metrics
    computing AUROC and AUPRC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now based on the `RegressionMetrics` API, compute the other metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11\. Finding the best cross-validated model** - Finally, let''s find
    the best cross-validated model information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: A recommendation system with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recommender system tries to predict potential items a user might be interested
    in based on a history from other users. Model-based collaborative filtering is
    commonly used in many companies such as Netflix. It is to be noted that Netflix
    is an American entertainment company founded by Reed Hastings and Marc Randolph
    on August 29, 1997, in Scotts Valley, California. It specializes in and provides
    streaming media and video-on-demand online and DVD by mail. In 2013, Netflix expanded
    into film and television production, as well as online distribution. As of 2017,
    the company has its headquarters in Los Gatos, California (source Wikipedia).
    Netflix is a recommender system for a real-time movie recommendation. In this
    section, we will see a complete example of how it works toward recommending movies
    for new users.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based recommendation with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The implementation in Spark MLlib supports model-based collaborative filtering.
    In the model based collaborative filtering technique, users and products are described
    by a small set of factors, also called the **latent factors** (**LFs**). From
    the following figure, you can get some idea of a different recommender system.
    *Figure 13* justifies why are going to use model-based collaborative filtering
    for the movie recommendation example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00284.gif)**Figure 13**: A comparative view of a different recommendation
    system'
  prefs: []
  type: TYPE_NORMAL
- en: 'The LFs are then used for predicting the missing entries. Spark API provides
    the implementation of the alternating least squares (also known as the ALS widely)
    algorithm, which is used to learn these latent factors by considering six parameters,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '*numBlocks*: This is the number of blocks used to parallelize computation (set
    to -1 to auto-configure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*rank*: This is the number of latent factors in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*iterations*: This is the number of iterations of ALS to run. ALS typically
    converges to a reasonable solution in 20 iterations or less.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*lambda*: This specifies the regularization parameter in ALS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*implicitPrefs*: This specifies whether to use the *explicit feedback* ALS
    variant or one adapted for *implicit feedback* data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*alpha*: This is a parameter applicable to the implicit feedback variant of
    ALS that governs the *baseline* confidence in preference observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that to construct an ALS instance with default parameters; you can set
    the value based on your requirements. The default values are as follows: `numBlocks:
    -1`, `rank: 10`, `iterations: 10`, `lambda: 0.01`, `implicitPrefs: false`, and
    `alpha: 1.0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The movie and the corresponding rating dataset were downloaded from the MovieLens
    Website ([https://movielens.org](https://movielens.org)). According to the data
    description on the MovieLens Website, all the ratings are described in the `ratings.csv`
    file. Each row of this file followed by the header represents one rating for one
    movie by one user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CSV dataset has the following columns: **userId**, **movieId**, **rating**,
    and **timestamp**, as shown in *Figure 14*. The rows are ordered first by the
    **userId**, and within the user, by **movieId**. Ratings are made on a five-star
    scale, with half-star increments (0.5 stars up to 5.0 stars). The timestamps represent
    the seconds since midnight Coordinated Universal Time (UTC) on January 1, 1970,
    where we have 105,339 ratings from the 668 users on 10,325 movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00244.gif)**Figure 14:** A snap of the ratings dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the movie information is contained in the `movies.csv` file.
    Each row apart from the header information represents one movie containing the
    columns: movieId, title, and genres (see *Figure 14*). Movie titles are either
    created or inserted manually or imported from the website of the movie database
    at [https://www.themoviedb.org/](https://www.themoviedb.org/). The release year,
    however, is shown in the bracket. Since movie titles are inserted manually, some
    errors or inconsistencies may exist in these titles. Readers are, therefore, recommended
    to check the IMDb database ([https://www.ibdb.com/](https://www.ibdb.com/)) to
    make sure if there are no inconsistencies or incorrect titles with their corresponding
    release year.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Genres are a separated list, and are selected from the following genre categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Action, Adventure, Animation, Children's, Comedy, Crime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentary, Drama, Fantasy, Film-Noir, Horror, Musical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mystery, Romance, Sci-Fi, Thriller, Western, War
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00356.gif)**Figure 15**: The title and genres for the top 20 movies'
  prefs: []
  type: TYPE_NORMAL
- en: Movie recommendation using ALS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will show you how to recommend the movie for other users
    through a step-by-step example from data collection to movie recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Load, parse and explore the movie and rating Dataset** - Here is
    the code illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This code segment should return you the DataFrame of the ratings. On the other
    hand, the following code segment shows you the DataFrame of movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Register both DataFrames as temp tables to make querying easier**
    - To register both Datasets, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This will help to make the in-memory querying faster by creating a temporary
    view as a table in min-memory. The lifetime of the temporary table using the `createOrReplaceTempView
    ()` method is tied to the `[[SparkSession]]` that was used to create this DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3\. Explore and query for related statistics** - Let''s check the ratings-related
    statistics. Just use the following code lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's
    get the maximum and minimum ratings along with the count of users who have rated
    a movie. However, you need to perform a SQL query on the ratings table we just
    created in-memory in the previous step. Making a query here is simple, and it
    is similar to making a query from a MySQL database or RDBMS. However, if you are
    not familiar with SQL-based queries, you are recommended to look at the SQL query
    specification to find out how to perform a selection using `SELECT` from a particular
    table, how to perform the ordering using `ORDER`, and how to perform a joining
    operation using the `JOIN` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, if you know the SQL query, you should get a new dataset by using a complex
    SQL query as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.gif)**Figure 16:** max, min ratings along with the count of users
    who have rated a movie'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an insight, we need to know more about the users and their ratings.
    Now, let''s find the top most active users and how many times they rated a movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00262.jpeg)**Figure 17:** top 10 most active users and how many times
    they rated a movie'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a particular user, and find the movies that, say user
    668, rated higher than 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00035.gif)**Figure 18:** movies that user 668 rated higher than 4'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Prepare training and test rating data and see the counts** - The
    following code splits ratings RDD into training data RDD (75%) and test data RDD
    (25%). Seed here is optional but required for the reproducibility purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: You should find that there are 78,792 ratings in the training and 26,547 ratings
    in the test
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5\. Prepare the data for building the recommendation model using ALS**
    - The ALS algorithm takes the RDD of `Rating` for the training purpose. The following
    code illustrates for building the recommendation model using APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ratingsRDD` is an RDD of ratings that contains the `userId`, `movieId`,
    and corresponding ratings from the training dataset that we prepared in the previous
    step. On the other hand, a test RDD is also required for evaluating the model.
    The following `testRDD` also contains the same information coming from the test
    DataFrame we prepared in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6\. Build an ALS user product matrix** - Build an ALS user matrix model
    based on the `ratingsRDD` by specifying the maximal iteration, a number of blocks,
    alpha, rank, lambda, seed, and `implicitPrefs`. Essentially, this technique predicts
    missing ratings for specific users for specific movies based on ratings for those
    movies from other users who did similar ratings for other movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we iterated the model for learning 15 times. With this setting, we
    got good prediction accuracy. Readers are suggested to apply the hyperparameter
    tuning to get to know the optimum values for these parameters. Furthermore, set
    the number of blocks for both user blocks and product blocks to parallelize the
    computation into a pass -1 for an auto-configured number of blocks. The value
    is -1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7\. Making predictions** - Let''s get the top six movie predictions
    for user 668\. The following source code can be used to make the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment produces the following output containing the rating
    prediction with `UserID`, `MovieID`, and corresponding `Rating` for that movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00376.gif)**Figure 19**: top six movie predictions for user 668'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Evaluating the model** - In order to verify the quality of the models,
    **Root Mean Squared Error** (**RMSE**) is used to measure the differences between
    values predicted by a model and the values actually observed. By default, the
    smaller the calculated error, the better the model. In order to test the quality
    of the model, the test data is used (which was split above in step 4). According
    to many machine learning practitioners, the RMSE is a good measure of accuracy,
    but only to compare forecasting errors of different models for a particular variable
    and not between variables, as it is scale-dependent. The following line of code
    calculates the RMSE value for the model that was trained using the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'It is to be noted that the `computeRmse()` is a UDF, that goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The preceding method computes the RMSE to evaluate the model. Less the RMSE,
    the better the model and its prediction capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the earlier setting, we got the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The performance of the preceding model could be increased further we believe.
    Interested readers should refer to this URL for more on tuning the ML-based ALS
    models [https://spark.apache.org/docs/preview/ml-collaborative-filtering.html](https://spark.apache.org/docs/preview/ml-collaborative-filtering.html).
  prefs: []
  type: TYPE_NORMAL
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. In
    the next section, we will show an example of topic modeling using the **latent
    dirichlet allocation** (**LDA**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modelling - A best practice for text clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. The
    dataset that will be used for this example is just in plain text, however, in
    an unstructured format. Now the challenging part is finding useful patterns about
    the data using LDA called topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: How does LDA work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is a topic model which infers topics from a collection of text documents.
    LDA can be thought of as a clustering algorithm where topics correspond to cluster
    centers, and documents correspond to examples (rows) in a dataset. Topics and
    documents both exist in a feature space, where feature vectors are vectors of
    word counts (bag of words). Instead of estimating a clustering using a traditional
    distance, LDA uses a function based on a statistical model of how text documents
    are generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA supports different inference algorithms via `setOptimizer` function. `EMLDAOptimizer`
    learns clustering using expectation-maximization on the likelihood function and
    yields comprehensive results, while `OnlineLDAOptimizer` uses iterative mini-batch
    sampling for online variational inference and is generally memory friendly. LDA
    takes in a collection of documents as vectors of word counts and the following
    parameters (set using the builder pattern):'
  prefs: []
  type: TYPE_NORMAL
- en: '`k`: Number of topics (that is, cluster centers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer`: Optimizer to use for learning the LDA model, either `EMLDAOptimizer`
    or `OnlineLDAOptimizer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docConcentration`: Dirichlet parameter for prior over documents'' distributions
    over topics. Larger values encourage smoother inferred distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topicConcentration`: Dirichlet parameter for prior over topics'' distributions
    over terms (words). Larger values encourage smoother inferred distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIterations`: Limit on the number of iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`checkpointInterval`: If using checkpointing (set in the Spark configuration),
    this parameter specifies the frequency with which checkpoints will be created.
    If `maxIterations` is large, using checkpointing can help reduce shuffle file
    sizes on disk and help with failure recovery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Particularly, we would like to discuss the topics people talk about most from
    the large collection of texts. Since the release of Spark 1.3, MLlib supports
    the LDA, which is one of the most successfully used topic modeling techniques
    in the area of text mining and **Natural Language Processing** (**NLP**). Moreover,
    LDA is also the first MLlib algorithm to adopt Spark GraphX.
  prefs: []
  type: TYPE_NORMAL
- en: To get more information about how the theory behind the LDA works, please refer
    to David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent, Dirichlet Allocation,
    *Journal of Machine Learning Research 3* (2003) 993-1022.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the topic distribution from randomly generated tweet
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 20**: The topic distribution and how it looks like'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at an example of topic modeling using the LDA
    algorithm of Spark MLlib with unstructured raw tweets datasets. Note that here
    we have used LDA, which is one of the most popular topic modeling algorithms commonly
    used for text mining. We could use more robust topic modeling algorithms such
    as **Probabilistic Latent Sentiment Analysis** (**pLSA**), **pachinko allocation
    model** (**PAM**), or **hierarchical dirichlet process** (**HDP**) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: However, pLSA has the overfitting problem. On the other hand, both HDP and PAM
    are more complex topic modeling algorithms used for complex text mining such as
    mining topics from high dimensional text data or documents of unstructured text.
    Moreover, to this date, Spark has implemented only one topic modeling algorithm,
    that is LDA. Therefore, we have to use LDA reasonably.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this subsection, we represent a semi-automated technique of topic modeling
    using Spark. Using other options as defaults, we train LDA on the dataset downloaded
    from the GitHub URL at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
    The following steps show the topic modeling from data reading to printing the
    topics, along with their term-weights. Here''s the short workflow of the topic
    modeling pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual computation on topic modeling is done in the `LDAforTM` class. The
    `Params` is a case class, which is used for loading the parameters to train the
    LDA model. Finally, we train the LDA model using the parameters setting via the
    `Params` class. Now, we will explain each step broadly with step-by-step source
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Creating a Spark session** - Let''s create a Spark session by defining
    number of computing core, SQL warehouse, and application name as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Creating vocabulary, tokens count to train LDA after text pre-processing**
    - At first, load the documents, and prepare them for LDA as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The pre-process method is used to process the raw texts. At first, let''s read
    the whole texts using the `wholeTextFiles()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, paths are the path of the text files. Then, we need
    to prepare a morphological RDD from the raw text based on the lemma texts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the `getLemmaText()` method from the `helperForLDA` class supplies the
    lemma texts after filtering the special characters such as `("""[! @ # $ % ^ &
    * ( ) _ + - − , " '' ; : . ` ? --]` as regular expressions using the `filterSpaecialChatacters()`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is to be noted that the `Morphology()` class computes the base form of English
    words, by removing just inflections (not derivational morphology). That is, it
    only does noun plurals, pronoun case, and verb endings, and not things like comparative
    adjectives or derived nominals. This comes from the Stanford NLP group. To use
    this, you should have the following import in the main class file: `edu.stanford.nlp.process.Morphology`.
    In the `pom.xml` file, you will have to include the following entries as dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The `filterSpecialCharacters()` goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`def filterSpecialCharacters(document: String) = document.replaceAll("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]""", " ")`. Once we have the RDD
    with special characters removed in hand, we can create a DataFrame for building
    the text analytics pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the DataFrame consist of only of the documents tag. A snapshot of the DataFrame
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00332.gif)**Figure 21**: Raw texts'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you examine the preceding DataFrame carefully, you will see that we
    still need to tokenize the items. Moreover, there are stop words in a DataFrame
    such as this, so we need to remove them as well. At first, let''s tokenize them
    using the `RegexTokenizer` API as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s remove all the stop words as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we also need to apply count victories to find only the important
    features from the tokens. This will help make the pipeline chained at the pipeline
    stage. Let''s do it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create the pipeline by chaining the transformers (`tokenizer`, `stopWordsRemover`,
    and `countVectorizer`) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s fit and transform the pipeline towards the vocabulary and number of
    tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, return the vocabulary and token count pairs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see the statistics of the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '****Step 4\. Instantiate the LDA model before training****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Set the NLP optimizer**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For better and optimized results from the LDA model, we need to set the optimizer
    for the LDA model. Here we use the `EMLDAOPtimizer` optimizer. You can also use
    the `OnlineLDAOptimizer()` optimizer. However, you need to add (1.0/actualCorpusSize)
    to `MiniBatchFraction` to be more robust on tiny datasets. The whole operation
    goes as follows. First, instantiate the `EMLDAOptimizer` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now set the optimizer using the `setOptimizer()` method from the LDA API as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Params` case class is used to define the parameters to training the LDA
    model. This goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'For a better result you can set these parameters in a naive way. Alternatively,
    you should go with the cross-validation for even better performance. Now if you
    want to checkpoint the current parameters, use the following line of codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6.** Training the LDA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: For the texts we have, the LDA model took 6.309715286 sec to train. Note that
    these timing codes are optional. Here we provide them for reference purposes,
    only to get an idea of the training time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7\. Measuring the likelihood of the data** - Now, of to get some more
    statistics about the data such as maximum likelihood or log-likelihood, we can
    use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code calculates the average log likelihood if the LDA model is
    an instance of the distributed version of the LDA model. We get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The likelihood is used after data are available to describe a function of a
    parameter (or parameter vector) for a given outcome. This helps especially for
    estimating a parameter from a set of statistics. For more information on the likelihood
    measurement, interested readers should refer to [https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Prepare the topics of interests** - Prepare the top five topics
    with each topic having 10 terms. Include the terms and their corresponding weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9\. Topic modeling** - Print the top ten topics, showing the top-weighted
    terms for each topic. Also, include the total weight in each topic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see the output of our LDA model toward topics modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can see that the topic of the input documents
    is topic 5 having the most weight of `0.31363611105890865`. This topic discusses
    the terms love, long, shore, shower, ring, bring, bear and so on. Now, for a better
    understanding of the flow, here''s the complete source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Scalability of LDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous example shows how to perform topic modeling using the LDA algorithm
    as a standalone application. The parallelization of LDA is not straightforward,
    and there have been many research papers proposing different strategies. The key
    obstacle in this regard is that all methods involve a large amount of communication.
    According to the blog on the Databricks website ([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)),
    here are the statistics of the dataset and related training and test sets that
    were used during the experimentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training set size: 4.6 million documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vocabulary size: 1.1 million terms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training set size: 1.1 billion tokens (~239 words/document)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100 topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the preceding setting, the timing result was 176 secs/iteration on average
    over 10 iterations. From these statistics, it is clear that LDA is quite scalable
    for a very large number of the corpus as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided theoretical and practical aspects of some advanced
    topics of machine learning with Spark. We also provided some recommendations about
    the best practice in machine learning. Following that, we have seen how to tune
    machine learning models for better and optimized performance using grid search,
    cross-validation, and hyperparameter tuning. In the later section, we have seen
    how to develop a scalable recommendation system using the ALS, which is an example
    of a model-based recommendation system using a model-based collaborative filtering
    approach. Finally, we have seen how to develop a topic modeling application as
    a text clustering technique.
  prefs: []
  type: TYPE_NORMAL
- en: For additional aspects and topics on machine learning best practice, interested
    readers can refer to the book titled *Large Scale Machine Learning with Spark*
    at [https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark.](https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark)
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will enter into more advanced use of Spark. Although
    we have discussed and provided a comparative analysis on binary and multiclass
    classification, we will get to know more about other multinomial classification
    algorithms with Spark such as Naive Bayes, decision trees, and the One-vs-Rest
    classifier.
  prefs: []
  type: TYPE_NORMAL

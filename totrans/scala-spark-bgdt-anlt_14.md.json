["```scala\npackage com.chapter13.Clustering\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.SQLContext\n\n```", "```scala\nval spark = SparkSession\n                 .builder\n                 .master(\"local[*]\")\n                 .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n                 .appName(\"KMeans\")\n                 .getOrCreate()\n\n```", "```scala\n//Start parsing the dataset\nval start = System.currentTimeMillis()\nval dataPath = \"data/Saratoga NY Homes.txt\"\n//val dataPath = args(0)\nval landDF = parseRDD(spark.sparkContext.textFile(dataPath))\n                                 .map(parseLand).toDF().cache()\nlandDF.show()\n\n```", "```scala\nimport spark.sqlContext.implicits._\n\n```", "```scala\n// function to create a  Land class from an Array of Double\ndef parseLand(line: Array[Double]): Land = {\n  Land(line(0), line(1), line(2), line(3), line(4), line(5),\n   line(6), line(7), line(8), line(9), line(10),\n   line(11), line(12), line(13), line(14), line(15)\n  )\n}\n\n```", "```scala\ncase class Land(\n  Price: Double, LotSize: Double, Waterfront: Double, Age: Double,\n  LandValue: Double, NewConstruct: Double, CentralAir: Double, \n  FuelType: Double, HeatType: Double, SewerType: Double, \n  LivingArea: Double, PctCollege: Double, Bedrooms: Double,\n  Fireplaces: Double, Bathrooms: Double, rooms: Double\n)\n\n```", "```scala\n// method to transform an RDD of Strings into an RDD of Double\ndef parseRDD(rdd: RDD[String]): RDD[Array[Double]] = {\n  rdd.map(_.split(\",\")).map(_.map(_.toDouble))\n}\n\n```", "```scala\nval rowsRDD = landDF.rdd.map(r => (\n  r.getDouble(0), r.getDouble(1), r.getDouble(2),\n  r.getDouble(3), r.getDouble(4), r.getDouble(5),\n  r.getDouble(6), r.getDouble(7), r.getDouble(8),\n  r.getDouble(9), r.getDouble(10), r.getDouble(11),\n  r.getDouble(12), r.getDouble(13), r.getDouble(14),\n  r.getDouble(15))\n)\nrowsRDD.cache()\n\n```", "```scala\n// Get the prediction from the model with the ID so we can\n   link them back to other information\nval predictions = rowsRDD.map{r => (\n  r._1, model.predict(Vectors.dense(\n    r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9,\n    r._10, r._11, r._12, r._13, r._14, r._15, r._16\n  )\n))}\n\n```", "```scala\nval numClusters = 5\nval numIterations = 20\nval run = 10\nval model = KMeans.train(numericHome, numClusters,numIterations, run,\n                         KMeans.K_MEANS_PARALLEL)\n\n```", "```scala\n// Evaluate clustering by computing Within Set Sum of Squared Errors\nval WCSSS = model.computeCost(landRDD)\nprintln(\"Within-Cluster Sum of Squares = \" + WCSSS)\n\n```", "```scala\nWithin-Cluster Sum of Squares = 1.455560123603583E12 \n\n```", "```scala\n// Get the prediction from the model with the ID so we can link them\n   back to other information\nval predictions = rowsRDD.map{r => (\n  r._1, model.predict(Vectors.dense(\n    r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9, r._10,\n    r._11, r._12, r._13, r._14, r._15, r._16\n  )\n))}\n\n```", "```scala\nval predictions = rowsRDD.map{r => (\n  r._1, model.predict(Vectors.dense(\n    r._1, r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9, r._10,\n    r._11, r._12, r._13, r._14, r._15, r._16\n  )\n))}\n\n```", "```scala\nimport spark.sqlContext.implicits._val predCluster = predictions.toDF(\"Price\", \"CLUSTER\")\npredCluster.show()\n\n```", "```scala\nval newDF = landDF.join(predCluster, \"Price\") \nnewDF.show()\n\n```", "```scala\nnewDF.filter(\"CLUSTER = 0\").show() \nnewDF.filter(\"CLUSTER = 1\").show()\nnewDF.filter(\"CLUSTER = 2\").show()\nnewDF.filter(\"CLUSTER = 3\").show()\nnewDF.filter(\"CLUSTER = 4\").show()\n\n```", "```scala\nnewDF.filter(\"CLUSTER = 0\").describe().show()\nnewDF.filter(\"CLUSTER = 1\").describe().show()\nnewDF.filter(\"CLUSTER = 2\").describe().show()\nnewDF.filter(\"CLUSTER = 3\").describe().show() \nnewDF.filter(\"CLUSTER = 4\").describe().show()\n\n```", "```scala\nval numClusters = 5 \nval numIterations = 20 \nval seed = 12345 \nval model = KMeans.train(landRDD, numClusters, numIterations, seed)\n\n```", "```scala\nspark.stop()\n\n```", "```scala\n// Cluster the data into two classes using KMeans \nval bkm = new BisectingKMeans() \n                 .setK(5) // Number of clusters of the similar houses\n                 .setMaxIterations(20)// Number of max iteration\n                 .setSeed(12345) // Setting seed to disallow randomness \nval model = bkm.run(landRDD)\n\n```", "```scala\nval WCSSS = model.computeCost(landRDD)\nprintln(\"Within-Cluster Sum of Squares = \" + WCSSS) // Less is better    \n\n```", "```scala\nval K = 5 \nval maxIteration = 20 \nval model = new GaussianMixture()\n                .setK(K)// Number of desired clusters\n                .setMaxIterations(maxIteration)//Maximum iterations\n                .setConvergenceTol(0.05) // Convergence tolerance. \n                .setSeed(12345) // setting seed to disallow randomness\n                .run(landRDD) // fit the model using the training set\n\n```", "```scala\n// output parameters of max-likelihood model\nfor (i <- 0 until model.K) {\n  println(\"Cluster \" + i)\n  println(\"Weight=%f\\nMU=%s\\nSigma=\\n%s\\n\" format(model.weights(i),   \n           model.gaussians(i).mu, model.gaussians(i).sigma))\n}\n\n```", "```scala\nval WCSSS = model.computeCost(landRDD) // land RDD is the training set \nprintln(\"Within-Cluster Sum of Squares = \" + WCSSS) // Less is better \n\n```", "```scala\nWithin-Cluster Sum of Squares of Bisecting K-means = 2.096980212594632E11 \nWithin-Cluster Sum of Squares of K-means = 1.455560123603583E12\n\n```", "```scala\nval start = System.currentTimeMillis() \nval numClusters = 5 \nval numIterations = 20  \nval seed = 12345 \nval runs = 50 \nval model = KMeans.train(landRDD, numClusters, numIterations, seed) \nval end = System.currentTimeMillis()\nprintln(\"Model building and prediction time: \"+ {end - start} + \"ms\")\n\n```", "```scala\nModel building and prediction time for Bisecting K-means: 2680ms \nModel building and prediction time for Gaussian Mixture: 2193ms \nModel building and prediction time for K-means: 3741ms\n\n```", "```scala\n# Run application as standalone mode on 8 cores \nSPARK_HOME/bin/spark-submit \\   \n--class org.apache.spark.examples.KMeansDemo \\   \n--master local[8] \\   \nKMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \\   \nSaratoga_NY_Homes.txt\n\n# Run on a YARN cluster \nexport HADOOP_CONF_DIR=XXX \nSPARK_HOME/bin/spark-submit \\   \n--class org.apache.spark.examples.KMeansDemo \\   \n--master yarn \\   \n--deploy-mode cluster \\  # can be client for client mode   \n--executor-memory 20G \\   \n--num-executors 50 \\   \nKMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \\   \nSaratoga_NY_Homes.txt\n\n# Run on a Mesos cluster in cluster deploy mode with supervising \nSPARK_HOME/bin/spark-submit \\  \n--class org.apache.spark.examples.KMeansDemo \\  \n--master mesos://207.184.161.138:7077 \\ # Use your IP aadress   \n--deploy-mode cluster \\   \n--supervise \\   \n--executor-memory 20G \\   \n--total-executor-cores 100 \\   \nKMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \\   \nSaratoga_NY_Homes.txt\n\n```"]
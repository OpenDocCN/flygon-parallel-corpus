- en: Text Analytics Using Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Programs must be written for people to read, and only incidentally for machines
    to execute."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Harold Abelson'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the wonderful field of text analytics using
    Spark ML. Text analytics is a wide area in machine learning and is useful in many
    use cases, such as sentiment analysis, chat bots, email spam detection, and natural
    language processing. We will learn how to use Spark for text analysis with a focus
    on use cases of text classification using a 10,000 sample set of Twitter data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding text analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers and Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StopWordsRemover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGrams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CountVectorizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling using LDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding text analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have explored the world of machine learning and Apache Spark''s support
    for machine learning in the last few chapters. As we discussed, machine learning
    has a workflow, which is explained in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading or ingesting data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cleansing the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracting features from the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training a model on the data to generate desired outcomes based on features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate or predict some outcome based on the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A simplified view of a typical pipeline is as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00310.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Hence, there are several stages of transformation of data possible before the
    model is trained and then subsequently deployed. Moreover, we should expect refinement
    of the features and model attributes. We could even explore a completely different
    algorithm repeating the entire sequence of tasks as part of a new workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline of steps can be created using several steps of transformation, and
    for this purpose, we use a **domain specific language** (**DSL**) to define the
    nodes (data transformation steps) to create a **DAG** (**Directed Acyclic Graph**)
    of nodes. Hence, the ML pipeline is a sequence of Transformers and Estimators
    to fit a Pipeline model to an input dataset. Each stage in the pipeline is known
    as *Pipeline stage*, which are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you look at a line of text, we see sentences, phrases, words, nouns, verbs,
    punctuation, and so on, which when put together, have a meaning and purpose. Humans
    are very good at understanding sentences, words, and slangs and annotations or
    contexts extremely well. This comes from years of practice and learning how to
    read/write, proper grammar, punctuation, exclamations, and so on. So, how can
    we write a computer program to try to replicate this kind of capability?
  prefs: []
  type: TYPE_NORMAL
- en: Text analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text analytics is the way to unlock the meaning from a collection of text. By
    using various techniques and algorithms to process and analyze the text data,
    we can uncover patterns and themes in the data. The goal of all this is to make
    sense of the unstructured text in order to derive contextual meaning and relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Text analytics utilizes several broad categories of techniques, which we will
    cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing the political opinions of people on Facebook, Twitter, and other social
    media is a good example of sentiment analysis. Similarly, analyzing the reviews
    of restaurants on Yelp is also another great example of sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) frameworks and libraries, such as
    OpenNLP and Stanford NLP, are typically used to implement sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling is a useful technique for detecting the topics or themes in a
    corpus of documents. This is an unsupervised algorithm, which can find themes
    in a set of documents. An example is to detect topics covered in a news article.
    Another example is to detect the ideas in a patent application.
  prefs: []
  type: TYPE_NORMAL
- en: The **latent dirichlet allocation** (**LDA**) is a popular clustering model
    using unsupervised algorithm, while **latent semantic analysis** (**LSA**) uses
    a probabilistic model on co-occurrence data.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF (term frequency - inverse document frequency)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TF-IDF measures how frequently words appear in documents and the relative frequency
    across the set of documents. This information can be used in building classifiers
    and predictive models. The examples are spam classification, chat conversations,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition (NER)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Named entity recognition detects the usage of words and nouns in sentences to
    extract information about persons, organizations, locations, and so on. This gives
    important contextual information on the actual content of the documents rather
    than just treating words as the primary entities.
  prefs: []
  type: TYPE_NORMAL
- en: Stanford NLP and OpenNLP have implementation for NER algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Event extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event extraction expands on the NER establishing relationships around the entities
    detected. This can be used to make inferences on the relationship between two
    entities. Hence, there is an additional layer of semantic understanding to make
    sense of the document content.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transformer** is a function object that transforms one dataset to another
    by applying the transformation logic (function) to the input dataset yielding
    an output dataset. There are two types of Transformers the standard Transformer
    and the Estimator Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: Standard Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A standard Transformer transforms the input dataset into the output dataset,
    explicitly applying transformation function to the input data. There is no dependency
    on the input data other than reading the input column and generating the output
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such Transformers are invoked as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Examples of standard Transformers are as follows and will be explained in detail
    in the subsequent sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tokenizer`: This splits sentences into words using space as the delimiter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RegexTokenizer`: This splits sentences into words using regular expressions
    to split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StopWordsRemover`: This removes commonly used stop words from the list of
    words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Binarizer`: This converts the strings to binary numbers 0/1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NGram`: This creates N word phrases from the sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HashingTF`: This creates Term frequency counts using hash table to index the
    words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SQLTransformer`: This implements the transformations, which are defined by
    SQL statements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorAssembler`: This combines a given list of columns into a single vector
    column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The diagram of a standard Transformer is as follows, where the input column
    from an input dataset is transformed into an output column generating the output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00247.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Estimator Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Estimator Transformer transforms the input dataset into the output dataset
    by first generating a Transformer based on the input dataset. Then the Transformer
    processes the input data, reading the input column and generating the output column
    in the output dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such Transformers are invoked as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The examples of Estimator Transformers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The diagram of an Estimator Transformer is as follows, where the input column
    from an input dataset is transformed into an output column generating the output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next few sections, we will look deeper into text analytics using a simple
    example dataset, which consists of lines of text (sentences), as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00330.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The upcoming code is used to load the text data into the input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a sequence of sentences called lines using a sequence of pairs of
    ID and text as shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, invoke the `createDataFrame()` function to create a DataFrame from the
    sequence of sentences we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now you can see the newly created dataset, which shows the Sentence DataFrame
    containing two column IDs and sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tokenizer** converts the input string into lowercase and then splits the
    string with whitespaces into individual tokens. A given sentence is split into
    words either using the default space delimiter or using a customer regular expression
    based Tokenizer. In either case, the input column is transformed into an output
    column. In particular, the input column is usually a String and the output column
    is a Sequence of Words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenizers are available by importing two packages shown next, the `Tokenizer`
    and the `RegexTokenize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize a `Tokenizer` specifying the input column and
    the output column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column words, which contain the sequence of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, if you wanted to set up a regular expression based `Tokenizer`,
    you have to use the `RegexTokenizer` instead of `Tokenizer`. For this, you need
    to initialize a `RegexTokenizer` specifying the input column and the output column
    along with the regex pattern to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `regexWordsDF`, which contain the sequence of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of a `Tokenizer` is as follows, wherein the sentence from the input
    text is split into words using the space delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: StopWordsRemover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`StopWordsRemover` is a Transformer that takes a `String` array of words and
    returns a `String` array after removing all the defined stop words. Some examples
    of stop words are I, you, my, and, or, and so on which are fairly commonly used
    in the English language. You can override or extend the set of stop words to suit
    the purpose of the use case. Without this cleansing process, the subsequent algorithms
    might be biased because of the common words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to invoke `StopWordsRemover`, you need to import the following package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize a `StopWordsRemover` , specifying the input column
    and the output column. Here, we are choosing the words column created by the `Tokenizer`
    and generate an output column for the filtered words after removal of stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `filteredWords`, which contains the sequence of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing just the sentence and the `filteredWords`,
    which contains the sequence of filtered words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of the `StopWordsRemover` is as follows, which shows the words
    filtered to remove stop words such as I, should, some, and before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Stop words are set by default, but can be overridden or amended very easily,
    as shown in the following code snippet, where we will remove hello from the filtered
    words considering hello as a stop word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: NGrams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NGrams are word combinations created as sequences of words. N stands for the
    number of words in the sequence. For example, 2-gram is two words together, 3-gram
    is three words together. `setN()` is used to specify the value of `N`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate NGrams, you need to import the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize an `NGram` generator specifying the input column
    and the output column. Here, we are choosing the filtered words column created
    by the `StopWordsRemover` and generating an output column for the filtered words
    after removal of stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column ID, sentence,
    and the output column `ngram`, which contain the sequence of n-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the sentence and 2-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of an NGram is as follows, which shows 2-grams generated from the
    sentence after tokenizing and removing stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TF-IDF stands for term frequency-inverse document frequency, which measures
    how important a word is to a document in a collection of documents. It is used
    extensively in informational retrieval and reflects the weightage of the word
    in the document. The TF-IDF value increases in proportion to the number of occurrences
    of the words otherwise known as frequency of the word/term and consists of two
    key elements, the term frequency and the inverse document frequency.
  prefs: []
  type: TYPE_NORMAL
- en: TF is the term frequency, which is the frequency of a word/term in the document.
  prefs: []
  type: TYPE_NORMAL
- en: For a term *t*, *tf* measures the number of times term *t* occurs in document
    *d*. *tf* is implemented in Spark using hashing where a term is mapped into an
    index by applying a hash function.
  prefs: []
  type: TYPE_NORMAL
- en: 'IDF is the inverse document frequency, which represents the information a term
    provides about the tendency of the term to appear in documents. IDF is a log-scaled
    inverse function of documents containing the term:'
  prefs: []
  type: TYPE_NORMAL
- en: IDF = TotalDocuments/Documents containing Term
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have *TF* and *IDF*, we can compute the *TF-IDF* value by multiplying
    the *TF* and *IDF*:'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF = TF * IDF
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at how we can generate *TF* using the HashingTF Transformer
    in Spark ML.
  prefs: []
  type: TYPE_NORMAL
- en: HashingTF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**HashingTF** is a Transformer, which takes a set of terms and converts them
    into vectors of fixed length by hashing each term using a hash function to generate
    an index for each term. Then, term frequencies are generated using the indices
    of the hash table.'
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, the HashingTF uses the **MurmurHash3** algorithm to hash terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use `HashingTF`, you need to import the following package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize a `HashingTF` specifying the input column and
    the output column. Here, we choose the filtered words column created by the `StopWordsRemover`
    Transformer and generate an output column `rawFeaturesDF`. We also choose the
    number of features as 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `rawFeaturesDF`, which contains the features represented
    by a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the preceding output to have a better understanding. If you just
    look at columns `filteredWords` and `rawFeatures` alone, you can see that,
  prefs: []
  type: TYPE_NORMAL
- en: The array of words `[hello, there, like, book, and far]` is transformed to raw
    feature vector `(100,[30,48,70,93],[2.0,1.0,1.0,1.0])`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The array of words `(book, stores, coffee, go, book, and store)` is transformed
    to raw feature vector `(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, what does the vector represent here? The underlying logic is that each word
    is hashed into an integer and counted for the number of occurrences in the word
    array.
  prefs: []
  type: TYPE_NORMAL
- en: Spark internally uses a `hashMap` for this `mutable.HashMap.empty[Int, Double]`,
    which stores the hash value of each word as `Integer` key and the number of occurrences
    as double value. Double is used so that we can use it in conjunction with IDF
    (we'll talk about it in the next section). Using this map, the array `[book, stores,
    coffee, go, book, store]` can be seen as `[hashFunc(book), hashFunc(stores), hashFunc(coffee),
    hashFunc(go), hashFunc(book), hashFunc(store)]`*,* which is equal to `[43,48,51,77,93]`*.*
    Then, if you count the number of occurrences too, that is, `book-2, coffee-1,go-1,store-1,stores-1`.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the preceding information, we can generate a vector `(numFeatures,
    hashValues, Frequencies)`**,** which in this case will be `(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])`.
  prefs: []
  type: TYPE_NORMAL
- en: Inverse Document Frequency (IDF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Inverse Document Frequency** (**IDF**) is an estimator, which is fit onto
    a dataset and then generates features by scaling the input features. Hence, IDF
    works on output of a HashingTF Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to invoke IDF, you need to import the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize an `IDF` specifying the input column and the
    output column. Here, we are choosing the words column `rawFeatures` created by
    the HashingTF and generate an output column feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `fit()` function on the input dataset yields an output Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, invoking the `transform()` function on the input dataset yields an
    output dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column ID and the output
    column features, which contain the vector of scaled features produced by HashingTF
    in the previous transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    `rawFeatures`, and the output column features, which contain the vector of scaled
    features produced by HashingTF in the previous transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of the TF-IDF is as follows, which shows the generation of **TF-IDF
    Features**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2Vec is a sophisticated neural network style natural language processing
    tool and uses a technique called **skip-grams** to convert a sentence of words
    into an embedded vector representation. Let''s look at an example of how this
    can be used by looking at a collection of sentences about animals:'
  prefs: []
  type: TYPE_NORMAL
- en: A dog was barking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some cows were grazing the grass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dogs usually bark randomly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cow likes grass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using neural network with a hidden layer (machine learning algorithm used in
    many unsupervised learning applications), we can learn (with enough examples)
    that *dog* and *barking* are related, *cow* and *grass* are related in the sense
    that they appear close to each other a lot, which is measured by probabilities.
    The output of `Word2vec` is a vector of `Double` features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to invoke `Word2vec`, you need to import the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize a `Word2vec` Transformer specifying the input
    column and the output column. Here, we are choosing the words column created by
    the `Tokenizer` and generate an output column for the word vector of size 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `fit()` function on the input dataset yields an output Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, invoking the `transform()` function on the input dataset yields an
    output dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `wordvector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of the **Word2Vec Features** is as follows, which shows the words
    being converted into a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00347.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: CountVectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CountVectorizer` is used to convert a collection of text documents to vectors
    of token counts essentially producing sparse representations for the documents
    over the vocabulary. The end result is a vector of features, which can then be
    passed to other algorithms. Later on, we will see how to use the output from the
    `CountVectorizer` in LDA algorithm to perform topic detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to invoke `CountVectorizer`, you need to import the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you need to initialize a `CountVectorizer` Transformer specifying the
    input column and the output column. Here, we are choosing the `filteredWords`
    column created by the `StopWordRemover` and generate output column features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, invoking the `fit()` function on the input dataset yields an output Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Further, invoking the `transform()` function on the input dataset yields an
    output dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of a `CountVectorizer` is as follows, which shows the features
    generated from `StopWordsRemover` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00205.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Topic modeling using LDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LDA is a topic model, which infers topics from a collection of text documents.
    LDA can be thought of as an unsupervised clustering algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Topics correspond to cluster centers and documents correspond to rows in a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topics and documents both exist in a feature space, where feature vectors are
    vectors of word counts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rather than estimating a clustering using a traditional distance, LDA uses a
    function based on a statistical model of how text documents are generated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to invoke LDA, you need to import the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 1.** First, you need to initialize an LDA model setting 10 topics and
    10 iterations of clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2.** Next invoking the `fit()` function on the input dataset yields
    an output transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3.** Extract `logLikelihood`, which calculates a lower bound on the
    provided documents given the inferred topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4.** Extract `logPerplexity`, which calculates an upper bound on the
    perplexity of the provided documents given the inferred topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5.** Now, we can use `describeTopics()` to get the topics generated
    by LDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6.** The following is the output dataset showing the `topic`, `termIndices`,
    and `termWeights` computed by LDA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The diagram of an LDA is as follows, which shows the topics created from the
    features of TF-IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Implementing text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification is one of the most widely used paradigms in the field of
    machine learning and is useful in use cases such as spam detection and email classification
    and just like any other machine learning algorithm, the workflow is built of Transformers
    and algorithms. In the field of text processing, preprocessing steps such as stop-word
    removal, stemming, tokenizing, n-gram extraction, TF-IDF feature weighting come
    into play. Once the desired processing is complete, the models are trained to
    classify the documents into two or more classes.
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification is the classification of inputting two output classes
    such as spam/not spam and a given credit card transaction is fraudulent or not.
    Multiclass classification can generate multiple output classes such as hot, cold,
    freezing, and rainy. There is another technique called Multilabel classification,
    which can generate multiple labels such as speed, safety, and fuel efficiency
    can be produced from descriptions of car features.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, we will using a 10k sample dataset of tweets and we will use
    the preceding techniques on this dataset. Then, we will tokenize the text lines
    into words, remove stop words, and then use `CountVectorizer` to build a vector
    of the words (features).
  prefs: []
  type: TYPE_NORMAL
- en: Then we will split the data into training (80%)-testing (20%) and train a Logistic
    Regression model. Finally, we will evaluate against the test data and look at
    how it is performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps in the workflow are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 1.** Load the input text data containing 10k tweets along with label
    and ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2.** Convert the input lines to a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3.** Transform the data into words using a `Tokenizer` with white space
    delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4.** Remove stop words and create a new DataFrame with the filtered
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5.** Create a feature vector from the filtered words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6.** Create the `inputData` DataFrame with just a label and the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7.** Split the data using a random split into 80% training and 20% testing
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8.** Create a Logistic Regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9.** Create a Logistic Regression model by fitting the `trainingData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10.** Examine the model summary especially `areaUnderROC`, which should
    be *> 0.90* for a good model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11.** Transform both training and testing datasets using the trained
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 12.** Count the number of records with matching label and prediction
    columns. They should match for correct model evaluation else they will mismatch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The results can be put into a table as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Dataset** | **Total** | **label == prediction** | **label != prediction**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Training** | 8048 | 8029 ( 99.76%) | 19 (0.24%) |'
  prefs: []
  type: TYPE_TB
- en: '| **Testing** | 1951 | 1334 (68.35%) | 617 (31.65%) |'
  prefs: []
  type: TYPE_TB
- en: While training data produced excellent matches, the testing data only had 68.35%
    match. Hence, there is room for improvement which can be done by exploring the
    model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is an easy-to-understand method for predicting a binary
    outcome using a linear combination of inputs and randomized noise in the form
    of a logistic random variable. Hence, Logistic Regression model can be tuned using
    several parameters. (The full set of parameters and how to tune such a Logistic
    Regression model is out of scope for this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Some parameters that can be used to tune the model are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model hyperparameters include the following parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`elasticNetParam`: This parameter specifies how you would like to mix L1 and
    L2 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParam`: This parameter determines how the inputs should be regularized
    before being passed in the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training parameters include the following parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIter`: This is total number of interactions before stopping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weightCol`: This is the name of the weight column to weigh certain rows more
    than others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prediction parameters include the following parameter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold`: This is the probability threshold for binary prediction. This
    determines the minimum probability for a given class to be predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now seen how to build a simple classification model, so any new tweet
    can be labeled based on the training set. Logistic Regression is only one of the
    models that can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other models which can be used in place of Logistic Regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient Boosted Trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer Perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have introduced the world of text analytics using Spark
    ML with emphasis on text classification. We have learned about Transformers and
    Estimators. We have seen how Tokenizers can be used to break sentences into words,
    how to remove stop words, and generate n-grams. We also saw how to implement `HashingTF`
    and `IDF` to generate TF-IDF-based features. We also looked at `Word2Vec` to convert
    sequences of words into vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we also looked at LDA, a popular technique used to generate topics from
    documents without knowing much about the actual text. Finally, we implemented
    text classification on the set of 10k tweets from the Twitter dataset to see how
    it all comes together using Transformers, Estimators, and the Logistic Regression
    model to perform binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dig even deeper toward tuning Spark applications
    for better performance.
  prefs: []
  type: TYPE_NORMAL

- en: Practical Machine Learning with Spark Using Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and installing the JDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and installing IntelliJ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and installing Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring IntelliJ to work with Spark and run Spark ML sample codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a sample ML code from Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying data sources for practical machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running your first program using Apache Spark 2.0 with the IntelliJ IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to add graphics to your Spark program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the recent advancements in cluster computing coupled with the rise of big
    data, the field of machine learning has been pushed to the forefront of computing.
    The need for an interactive platform that enables data science at scale has long been
    a dream that is now a reality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following three areas together have enabled and accelerated interactive
    data science at scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Spark**: A unified technology platform for data science that combines
    a fast compute engine and fault-tolerant data structures into a well-designed
    and integrated offering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning**: A field of artificial intelligence that enables machines
    to mimic some of the tasks originally reserved exclusively for the human brain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scala**: A modern JVM-based language that builds on traditional languages,
    but unites functional and object-oriented concepts without the verboseness of
    other languages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we need to set up the development environment, which will consist of
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ community edition IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recipes in this chapter will give you detailed instructions for installing
    and configuring the IntelliJ IDE, Scala plugin, and Spark. After the development
    environment is set up, we'll proceed to run one of the Spark ML sample codes to
    test the setup.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is emerging as the de facto platform and trade language for big
    data analytics and as a complement to the **Hadoop** paradigm. Spark enables a
    data scientist to work in the manner that is most conducive to their workflow
    right out of the box. Spark's approach is to process the workload in a completely
    distributed manner without the need for **MapReduce** (**MR**) or repeated writing
    of the intermediate results to a disk.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides an easy-to-use distributed framework in a unified technology
    stack, which has made it the platform of choice for data science projects, which
    more often than not require an iterative algorithm that eventually merges toward
    a solution. These algorithms, due to their inner workings, generate a large amount
    of intermediate results that need to go from one stage to the next during the
    intermediate steps. The need for an interactive tool with a robust native distributed
    **machine learning library** (**MLlib**) rules out a disk-based approach for most
    of the data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: Spark has a different approach toward cluster computing. It solves the problem
    as a technology stack rather than as an ecosystem. A large number of centrally
    managed libraries combined with a lightning-fast compute engine that can support
    fault-tolerant data structures has poised Spark to take over Hadoop as the preferred
    big data platform for analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark has a modular approach, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of machine learning is to produce machines and devices that can mimic
    human intelligence and automate some of the tasks that have been traditionally
    reserved for a human brain. Machine learning algorithms are designed to go through
    very large data sets in a relatively short time and approximate answers that would
    have taken a human much longer to process.
  prefs: []
  type: TYPE_NORMAL
- en: The field of machine learning can be classified into many forms and at a high
    level, it can be classified as supervised and unsupervised learning. Supervised
    learning algorithms are a class of ML algorithms that use a training set (that
    is, labeled data) to compute a probabilistic distribution or graphical model that
    in turn allows them to classify the new data points without further human intervention.
    Unsupervised learning is a type of machine learning algorithm used to draw inferences
    from datasets consisting of input data without labeled responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of the box, Spark offers a rich set of ML algorithms that can be deployed
    on large datasets without any further coding. The following figure depicts Spark''s
    MLlib algorithms as a mind map. Spark''s MLlib is designed to take advantage of
    parallelism while having fault-tolerant distributed data structures. Spark refers
    to such data structures as **Resilient Distributed Datasets** or **RDDs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scala** is a modern programming language that is emerging as an alternative
    to traditional programming languages such as **Java** and **C++**. Scala is a
    JVM-based language that not only offers a concise syntax without the traditional
    boilerplate code, but also incorporates both object-oriented and functional programming
    into an extremely crisp and extraordinarily powerful type-safe language.'
  prefs: []
  type: TYPE_NORMAL
- en: Scala takes a flexible and expressive approach, which makes it perfect for interacting
    with Spark's MLlib. The fact that Spark itself is written in Scala provides a
    strong evidence that the Scala language is a full-service programming language
    that can be used to create sophisticated system code with heavy performance needs.
  prefs: []
  type: TYPE_NORMAL
- en: Scala builds on Java's tradition by addressing some of its shortcomings, while
    avoiding an all-or-nothing approach. Scala code compiles into Java bytecode, which
    in turn makes it possible to coexist with rich Java libraries interchangeably.
    The ability to use Java libraries with Scala and vice versa provides continuity
    and a rich environment for software engineers to build modern and complex machine
    learning systems without being fully disconnected from the Java tradition and
    code base.
  prefs: []
  type: TYPE_NORMAL
- en: Scala fully supports a feature-rich functional programming paradigm with standard
    support for lambda, currying, type interface, immutability, lazy evaluation, and
    a pattern-matching paradigm reminiscent of Perl without the cryptic syntax. Scala
    is an excellent match for machine learning programming due to its support for
    algebra-friendly data types, anonymous functions, covariance, contra-variance,
    and higher-order functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a hello world program in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running `HelloWorld` in Scala looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Apache Spark Machine Learning Cookbook takes a practical approach by offering
    a multi-disciplinary view with the developer in mind. This book focuses on the
    interactions and cohesiveness of **machine learning**, **Apache Spark**, and **Scala**.
    We also take an extra step and teach you how to set up and run a comprehensive
    development environment familiar to a developer and provide code snippets that
    you have to run in an interactive shell without the modern facilities that an
    IDE provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Software versions and libraries used in this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table provides a detailed list of software versions and libraries
    used in this book. If you follow the installation instructions covered in this
    chapter, it will include most of the items listed here. Any other JAR or library
    files that may be required for specific recipes are covered via additional installation
    instructions in the respective recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Core systems** | **Version** |'
  prefs: []
  type: TYPE_TB
- en: '| Spark | 2.0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 1.8 |'
  prefs: []
  type: TYPE_TB
- en: '| IntelliJ IDEA | 2016.2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Scala-sdk | 2.11.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Miscellaneous JARs that will be required are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Miscellaneous JARs** | **Version** |'
  prefs: []
  type: TYPE_TB
- en: '| `bliki-core` | 3.0.19 |'
  prefs: []
  type: TYPE_TB
- en: '| `breeze-viz` | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| `Cloud9` | 1.5.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `Hadoop-streaming` | 2.2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `JCommon` | 1.0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| `JFreeChart` | 1.0.19 |'
  prefs: []
  type: TYPE_TB
- en: '| `lucene-analyzers-common` | 6.0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `Lucene-Core` | 6.0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `scopt` | 3.3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `spark-streaming-flume-assembly` | 2.0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| `spark-streaming-kafka-0-8-assembly` | 2.0.0 |'
  prefs: []
  type: TYPE_TB
- en: We have additionally tested all the recipes in this book on Spark 2.1.1 and
    found that the programs executed as expected. It is recommended for learning purposes
    you use the software versions and libraries listed in these tables.
  prefs: []
  type: TYPE_NORMAL
- en: To stay current with the rapidly changing Spark landscape and documentation,
    the API links to the Spark documentation mentioned throughout this book point
    to the latest version of Spark 2.x.x, but the API references in the recipes are
    explicitly for Spark 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the Spark documentation links provided in this book will point to the latest
    documentation on Spark''s website. If you prefer to look for documentation for
    a specific version of Spark (for example, Spark 2.0.0), look for relevant documentation
    on the Spark website using the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We've made the code as simple as possible for clarity purposes rather than demonstrating
    the advanced features of Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and installing the JDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to download the JDK development environment that is required
    for Scala/Spark development.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are ready to download and install the JDK, access the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.oracle.com/technetwork/java/javase/downloads/index.html](http://www.oracle.com/technetwork/java/javase/downloads/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After successful download, follow the on-screen instructions to install the
    JDK.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and installing IntelliJ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IntelliJ Community Edition is a lightweight IDE for Java SE, Groovy, Scala,
    and Kotlin development. To complete setting up your machine learning with the
    Spark development environment, the IntelliJ IDE needs to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are ready to download and install IntelliJ, access the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.jetbrains.com/idea/download/](https://www.jetbrains.com/idea/download/)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, we are using IntelliJ version 15.x or later (for example,
    version 2016.2.4) to test the examples in the book, but feel free to download
    the latest version. Once the installation file is downloaded, double-click on
    the downloaded file (`.exe`) and begin to install the IDE. Leave all the installation
    options at the default settings if you do not want to make any changes. Follow
    the on-screen instructions to complete the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Downloading and installing Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now proceed to download and install Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are ready to download and install Spark, access the Apache website
    at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the Apache website and select the required download parameters, as shown
    in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Make sure to accept the default choices (click on Next) and proceed with the
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring IntelliJ to work with Spark and run Spark ML sample codes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to run some configurations to ensure that the project settings are correct
    before being able to run the samples that are provided by Spark or any of the
    programs listed this book.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to be particularly careful when configuring the project structure and
    global libraries. After we set everything up, we proceed to run the sample ML
    code provided by the Spark team to verify the setup. Sample code can be found
    under the Spark directory or can be obtained by downloading the Spark source code
    with samples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for configuring IntelliJ to work with Spark MLlib
    and for running the sample ML code provided by Spark in the examples directory.
    The examples directory can be found in your home directory for Spark. Use the
    Scala samples to proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Project Structure... option, as shown in the following screenshot,
    to configure project settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Verify the settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Configure Global Libraries. Select Scala SDK as your global library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the JARs for the new Scala SDK and let the download complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the project name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Verify the settings and additional libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Add dependency JARs. Select modules under the Project Settings in the left-hand
    pane and click on dependencies to choose the required JARs, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the JAR files provided by Spark. Choose Spark''s default installation
    directory and then select the `lib` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We then select the JAR files for examples that are provided for Spark out of
    the box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Add required JARs by verifying that you selected and imported all the JARs
    listed under `External Libraries` in the the left-hand pane:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Spark 2.0 uses Scala 2.11\. Two new streaming JARs, Flume and Kafka, are needed
    to run the examples, and can be downloaded from the following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-flume-assembly_2.11/2.0.0/spark-streaming-flume-assembly_2.11-2.0.0.jar](https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-flume-assembly_2.11/2.0.0/spark-streaming-flume-assembly_2.11-2.0.0.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.0.0/spark-streaming-kafka-0-8-assembly_2.11-2.0.0.jar](https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.0.0/spark-streaming-kafka-0-8-assembly_2.11-2.0.0.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next step is to download and install the Flume and Kafka JARs. For the
    purposes of this book, we have used the Maven repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Download and install the Kafka assembly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Download and install the Flume assembly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the download is complete, move the downloaded JAR files to the `lib`
    directory of Spark. We used the `C` drive when we installed Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Open your IDE and verify that all the JARs under the `External Libraries` folder
    on the left, as shown in the following screenshot, are present in your setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Build the example projects in Spark to verify the setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Verify that the build was successful:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prior to Spark 2.0, we needed another library from Google called **Guava**
    for facilitating I/O and for providing a set of rich methods of defining tables
    and then letting Spark broadcast them across the cluster. Due to dependency issues
    that were hard to work around, Spark 2.0 no longer uses the Guava library. Make
    sure you use the Guava library if you are using Spark versions prior to 2.0 (required
    in version 1.5.2). The Guava library can be accessed at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/google/guava/wiki](https://github.com/google/guava/wiki)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may want to use Guava version 15.0, which can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mvnrepository.com/artifact/com.google.guava/guava/15.0](https://mvnrepository.com/artifact/com.google.guava/guava/15.0)'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using installation instructions from previous blogs, make sure to
    exclude the Guava library from the installation set.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If there are other third-party libraries or JARs required for the completion
    of the Spark installation, you can find those in the following Maven repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://repo1.maven.org/maven2/org/apache/spark/](https://repo1.maven.org/maven2/org/apache/spark/)'
  prefs: []
  type: TYPE_NORMAL
- en: Running a sample ML code from Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can verify the setup by simply downloading the sample code from the Spark
    source tree and importing it into IntelliJ to make sure it runs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first run the logistic regression code from the samples to verify installation.
    In the next section, we proceed to write our own version of the same program and
    examine the output in order to understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the source directory and pick one of the ML sample code files to run.
    We've selected the logistic regression example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you cannot find the source code in your directory, you can always download
    the Spark source, unzip, and then extract the examples directory accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting the example, select Edit Configurations..., as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Configurations tab, define the following options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'VM options: The choice shown allows you to run a standalone Spark cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Program arguments: What we are supposed to pass into the program'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Run the logistic regression by going to Run ''LogisticRegressionExample'',
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Verify the exit code and make sure it is as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Identifying data sources for practical machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting data for machine learning projects was a challenge in the past. However,
    now there is a rich set of public data sources specifically suitable for machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the university and government sources, there are many other open
    sources of data that can be used to learn and code your own examples and projects.
    We will list the data sources and show you how to best obtain and download data
    for each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of open source data worth exploring if you would like
    to develop applications in this field:'
  prefs: []
  type: TYPE_NORMAL
- en: '*UCI machine learning repository*: This is an extensive library with search
    functionality. At the time of writing, there were more than 350 datasets. You
    can click on the [https://archive.ics.uci.edu/ml/index.html](https://archive.ics.uci.edu/ml/index.html)
    link to see all the datasets or look for a specific set using a simple search
    (*Ctrl* + *F*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kaggle datasets*: You need to create an account, but you can download any
    sets for learning as well as for competing in machine learning competitions. The
    [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions) link
    provides details for exploring and learning more about Kaggle, and the inner workings
    of machine learning competitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLdata.org*: A public site open to all with a repository of datasets for machine
    learning enthusiasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google Trends*: You can find statistics on search volume (as a proportion
    of total search) for any given term since 2004 on [http://www.google.com/trends/explore](http://www.google.com/trends/explore).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The CIA World Factbook*: The [https://www.cia.gov/library/publications/the-world-factbook/](https://www.cia.gov/library/publications/the-world-factbook/) link
    provides information on the history, population, economy, government, infrastructure,
    and military of 267 countries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Other sources for machine learning data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SMS spam data: [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Financial dataset from Lending Club [https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research data from Yahoo [http://webscope.sandbox.yahoo.com/index.php](http://webscope.sandbox.yahoo.com/index.php)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon AWS public dataset [http://aws.amazon.com/public-data-sets/](http://aws.amazon.com/public-data-sets/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeled visual data from Image Net [http://www.image-net.org](http://www.image-net.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Census datasets [http://www.census.gov](http://www.census.gov)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiled YouTube dataset [http://netsg.cs.sfu.ca/youtubedata/](http://netsg.cs.sfu.ca/youtubedata/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collected rating data from the MovieLens site [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enron dataset available to the public [http://www.cs.cmu.edu/~enron/](http://www.cs.cmu.edu/~enron/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset for the classic book elements of statistical learning [http://statweb.stanford.edu/~tibs/ElemStatLearn/data.htmlIMDB](http://statweb.stanford.edu/~tibs/ElemStatLearn/data.htmlIMDB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Movie dataset [http://www.imdb.com/interfaces](http://www.imdb.com/interfaces)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Million Song dataset [http://labrosa.ee.columbia.edu/millionsong/](http://labrosa.ee.columbia.edu/millionsong/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset for speech and audio [http://labrosa.ee.columbia.edu/projects/](http://labrosa.ee.columbia.edu/projects/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face recognition data [http://www.face-rec.org/databases/](http://www.face-rec.org/databases/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social science data [http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies](http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bulk datasets from Cornell University [http://arxiv.org/help/bulk_data_s3](http://arxiv.org/help/bulk_data_s3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Guttenberg datasets [http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs](http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets from World Bank [http://data.worldbank.org](http://data.worldbank.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lexical database from World Net [http://wordnet.princeton.edu](http://wordnet.princeton.edu)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collision data from NYPD [http://nypd.openscrape.com/#/](http://nypd.openscrape.com/#/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset for congressional row calls and others [http://voteview.com/dwnl.htm](http://voteview.com/dwnl.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large graph datasets from Stanford [http://snap.stanford.edu/data/index.html](http://snap.stanford.edu/data/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rich set of data from datahub [https://datahub.io/dataset](https://datahub.io/dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yelp's academic dataset [https://www.yelp.com/academic_dataset](https://www.yelp.com/academic_dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source of data from GitHub [https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset archives from Reddit [https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some specialized datasets (for example, text analytics in Spanish,
    and gene and IMF data) that might be of some interest to you:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets from Colombia (in Spanish): [http://www.datos.gov.co/frm/buscador/frmBuscador.aspx](http://www.datos.gov.co/frm/buscador/frmBuscador.aspx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset from cancer studies [http://www.broadinstitute.org/cgi-bin/cancer/datasets.cgi](http://www.broadinstitute.org/cgi-bin/cancer/datasets.cgi)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research data from Pew [http://www.pewinternet.org/datasets/](http://www.pewinternet.org/datasets/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data from the state of Illinois/USA [https://data.illinois.gov](https://data.illinois.gov)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data from freebase.com [http://www.freebase.com](http://www.freebase.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets from the UN and its associated agencies [http://data.un.org](http://data.un.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: International Monetary Fund datasets [http://www.imf.org/external/data.htm](http://www.imf.org/external/data.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UK government data [https://data.gov.uk](https://data.gov.uk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open data from Estonia [http://pub.stat.ee/px-web.2001/Dialog/statfile1.asp](http://pub.stat.ee/px-web.2001/Dialog/statfile1.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many ML libraries in R containing data that can be exported as CSV [https://www.r-project.org](https://www.r-project.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gene expression datasets [http://www.ncbi.nlm.nih.gov/geo/](http://www.ncbi.nlm.nih.gov/geo/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running your first program using Apache Spark 2.0 with the IntelliJ IDE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of this program is to get you comfortable with compiling and running
    a recipe using the Spark 2.0 development environment you just set up. We will
    explore the components and steps in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to write our own version of the Spark 2.0.0 program and examine
    the output so we can understand how it works. To emphasize, this short recipe
    is only a simple RDD program with Scala sugar syntax to make sure you have set
    up your environment correctly before starting to work with more complicated recipes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the sample code for the book, find the `myFirstSpark20.scala` file,
    and place the code in the following directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We installed Spark 2.0 in the `C:\spark-2.0.0-bin-hadoop2.7\` directory on a
    Windows machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Place the `myFirstSpark20.scala` file in the `C:\spark-2.0.0-bin-hadoop2.7\examples\src\main\scala\spark\ml\cookbook\chapter1` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Mac users note that we installed Spark 2.0 in the `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/`
    directory on a Mac machine.
  prefs: []
  type: TYPE_NORMAL
- en: Place the `myFirstSpark20.scala` file in the `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/src/main/scala/spark/ml/cookbook/chapter1`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Set output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session by specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `myFirstSpark20` object will run in local mode. The previous code block
    is a typical way to start creating a `SparkSession` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create two array variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then let Spark create two RDDs based on the array created before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we let Spark operate on the `RDD`; the `zip()` function will create a
    new `RDD` from the two RDDs mentioned before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console output at runtime (more details on how to run the program in
    the IntelliJ IDE in the following steps), you will see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we sum up the value for `xRDD` and `yRDD` and calculate the new `zipedRDD`
    sum value. We also calculate the item count for `zipedRDD`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the value calculated previously in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the program is complete, the layout of `myFirstSpark20.scala` in the IntelliJ
    project explorer will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Make sure there is no compiling error. You can test this by rebuilding the
    project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the rebuild is complete, there should be a build completed message on
    the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can run the previous program by right-clicking on `the myFirstSpark20` object
    in the project explorer and selecting the context menu option (shown in the next
    screenshot) called `Run myFirstSpark20`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can also use the Run menu from the menu bar to perform the same action.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the program is successfully executed, you will see the following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is also shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Mac users with IntelliJ will be able to perform this action using the same context
    menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place the code in the correct path.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we wrote our first Scala program, `myFirstSpark20.scala`, and
    displayed the steps to execute the program in IntelliJ. We placed the code in
    the path described in the steps for both Windows and Mac.
  prefs: []
  type: TYPE_NORMAL
- en: In the `myFirstSpark20` code, we saw a typical way to create a `SparkSession`
    object and how to configure it to run in local mode using the `master()` function.
    We created two RDDs out of the array objects and used a simple `zip()` function
    to create a new RDD.
  prefs: []
  type: TYPE_NORMAL
- en: We also did a simple sum calculation on the RDDs that were created and then
    displayed the result in the console. Finally, we exited and released the resource
    by calling `spark.stop()`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can be downloaded from [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for Spark 2.0 related to RDD can be found at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More information about JetBrain IntelliJ can be found at [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to add graphics to your Spark program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we discuss how to use JFreeChart to add a graphic chart to your
    Spark 2.0.0 program.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set up the JFreeChart library. JFreeChart JARs can be downloaded from the [https://sourceforge.net/projects/jfreechart/files/](https://sourceforge.net/projects/jfreechart/files/)
    site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The JFreeChart version we have covered in this book is JFreeChart 1.0.19, as
    can be seen in the following screenshot. It can be downloaded from the [https://sourceforge.net/projects/jfreechart/files/1.%20JFreeChart/1.0.19/jfreechart-1.0.19.zip/download](https://sourceforge.net/projects/jfreechart/files/1.%20JFreeChart/1.0.19/jfreechart-1.0.19.zip/download) site:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Once the ZIP file is downloaded, extract it. We extracted the ZIP file under
    `C:\` for a Windows machine, then proceed to find the `lib` directory under the
    extracted destination directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then find the two libraries we need (JFreeChart requires JCommon), `JFreeChart-1.0.19.jar`
    and `JCommon-1.0.23`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now we copy the two previously mentioned JARs into the `C:\spark-2.0.0-bin-hadoop2.7\examples\jars\`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This directory, as mentioned in the previous setup section, is in the classpath
    for the IntelliJ IDE project setting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In macOS, you need to place the previous two JARs in the `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples\jars\`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the sample code for the book, find `MyChart.scala`, and place the code
    in the following directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We installed Spark 2.0 in the `C:\spark-2.0.0-bin-hadoop2.7\` directory in Windows.
    Place `MyChart.scala` in the `C:\spark-2.0.0-bin-hadoop2.7\examples\src\main\scala\spark\ml\cookbook\chapter1`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import necessary JFreeChart packages for the graphics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `myChart` object will run in local mode. The previous code block is a typical
    start to creating a `SparkSession` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then create an RDD using a random number and ZIP the number with its index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the RDD in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We then create a data series for JFreeChart to display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a chart object from JFreeChart''s `ChartFactory` and set up
    the basic configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the plot object from the chart and prepare it to display graphics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We configure the plot first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `configurePlot` function is defined as follows; it sets up some basic color
    schema for the graphical part:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We now show the `chart`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `show()` function is defined as follows. It is a very standard frame-based
    graphic-displaying function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once `show(chart)` is executed successfully, the following frame will pop up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we wrote `MyChart.scala` and saw the steps for executing the
    program in IntelliJ. We placed code in the path described in the steps for both
    Windows and Mac.
  prefs: []
  type: TYPE_NORMAL
- en: In the code, we saw a typical way to create the `SparkSession` object and how
    to use the `master()` function. We created an RDD out of an array of random integers
    in the range of 1 to 15 and zipped it with the Index.
  prefs: []
  type: TYPE_NORMAL
- en: We then used JFreeChart to compose a basic chart that contains a simple *x*
    and *y* axis, and supplied the chart with the dataset we generated from the original
    RDD in the previous steps.
  prefs: []
  type: TYPE_NORMAL
- en: We set up the schema for the chart and called the `show()` function in JFreeChart
    to show a Frame with the *x* and *y* axes displayed as a linear graphical chart.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we exited and released the resource by calling `spark.stop()`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More about JFreeChart can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.jfree.org/jfreechart/](http://www.jfree.org/jfreechart/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.jfree.org/jfreechart/api/javadoc/index.html](http://www.jfree.org/jfreechart/api/javadoc/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Additional examples about the features and capabilities of JFreeChart can be
    found at the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: "[http://www.jfree.org/jfreechart/samples.html\uFEFF](http://www.jfree.org/jfreechart/samples.html)"
  prefs: []
  type: TYPE_NORMAL

- en: Spark&#x27;s Three Data Musketeers for Machine Learning - Perfect Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using internal data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using external data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with Spark 2.0 using the filter() API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with the super useful flatMap() API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with set operation APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with the zip() API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join transformation with paired key-value RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce and grouping transformation with paired key-value RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrames from Scala data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating on DataFrames programmatically without SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading DataFrames and setup from an external source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using DataFrames with standard SQL language - SparkSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the Dataset API using a Scala sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and using Datasets from RDDs and back again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with JSON using the Dataset API and SQL together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming with the Dataset API using domain objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three workhorses of Spark for efficient processing of data at scale are
    RDD, DataFrames, and the Dataset API. While each can stand on its own merit, the
    new paradigm shift favors Dataset as the unifying data API to meet all data wrangling
    needs in a single interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new Spark 2.0 Dataset API is a type-safe collection of domain objects that
    can be operated on via transformation (similar to RDDs'' filter, `map`, `flatMap()`,
    and so on) in parallel using functional or relational operations. For backward
    compatibility, Dataset has a view called **DataFrame**, which is a collection
    of rows that are untyped. In this chapter, we demonstrate all three API sets.
    The figure ahead summarizes the pros and cons of the key components of Spark for
    data wrangling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.gif)'
  prefs: []
  type: TYPE_IMG
- en: An advanced developer in machine learning must understand and be able to use
    all three API sets without any issues, for algorithmic augmentation or legacy
    reasons. While we recommend that every developer should migrate toward the high-level
    Dataset API, you will still need to know RDDs for programming against the Spark
    core system. For example, it is very common for investment banking and hedge funds
    to read leading journals in machine learning, mathematical programming, finance,
    statistics, or artificial intelligence and then code the research in low-level
    APIs to gain competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs - what started it all...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RDD API is a critical toolkit for Spark developers since it favors low-level
    control over the data within a functional programming paradigm. What makes RDDs
    powerful also makes it harder to work with for new programmers. While it may be
    easy to understand the RDD API and manual optimization techniques (for example,
    `filter()` before a `groupBy()` operation), writing advanced code would require
    consistent practice and fluency.
  prefs: []
  type: TYPE_NORMAL
- en: When data files, blocks, or data structures are converted to RDDs, the data
    is broken down into smaller units called **partitions** (similar to splits in
    Hadoop) and distributed among the nodes so they can be operated on in parallel
    at the same time. Spark provides this functionality right out of the box at scale
    without any additional coding. The framework will take care of all the details
    for you and you can concentrate on writing code without worrying about the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To appreciate the genius and yet the elegance of the underlying RDDs, one must
    read the original paper on this subject, which was deemed as the best work on
    this subject. The paper can be accessed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of RDDs in Spark that can simplify programming. The following
    mind map depicts a partial taxonomy of RDDs. It is suggested that a programmer
    on Spark know the types of RDDs available out of the box at minimum, even the
    less-known ones such as **RandomRDD** ,**VertexRDD**, **HadoopRDD**, **JdbcRDD**,
    and **UnionRDD**, in order to avoid unnecessary coding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.gif)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame - a natural evolution to unite API and SQL via a high-level API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark developer community has always strived to provide an easy-to-use high-level
    API for the community starting from the AMPlab days at Berkley. The next evolution
    in the Data API materialized when Michael Armbrust gave the community the SparkSQL
    and Catalyst optimizer, which made data virtualization possible with Spark using
    a simple and well-understood SQL interface. The DataFrame API was a natural evolution
    to take advantage of SparkSQL by organizing data into named columns like relational
    tables.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame API made data wrangling via SQL available to a multitude of data
    scientists and developers familiar with DataFrames in R (data.frame) or Python/Pandas
    (pandas.DataFrame).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset - a high-level unifying Data API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dataset is an immutable collection of objects which are modelled/mapped to
    a traditional relational schema. There are four attributes that distinguish it
    as the preferred method going forward. We particularly find the Dataset API appealing
    since we find it familiar to RDDs with the usual transformational operators (for
    example, `filter()`, `map()`, `flatMap()`, and so on). The Dataset will follow
    a lazy execution paradigm similar to RDD. The best way to try to reconcile DataFrames
    and Datasets is to think of a DataFrame as an alias that can be thought of as
    `Dataset[Row]`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strong type safety**: We now have both compile-time (syntax errors) and runtime
    safety in a unified Data API, which helps the ML developer not only during development,
    but can also help guard against mishaps during runtime. Developers hit by unexpected
    runtime errors using DataFrame or RDD Lambda either in Scala or Python (due to
    flaws in data) will better understand and appreciate this new contribution from
    the Spark community and Databricks ([https://databricks.com](https://databricks.com)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tungsten Memory Management enabled**: Tungsten brings Apache Spark closer
    to bare metal (that is, leveraging the `sun.misc.Unsafe interface`). The encoders
    facilitate mapping of JVM objects to tabular format (see the following figure).
    If you use the Dataset API, Spark will map the JVM objects to internal Tungsten
    off-heap binary format, which is more efficient. While the details of Tungsten
    internals are beyond the scope of a cookbook on machine learning, it is worth
    mentioning that the benchmarking shows significant improvement using off-head
    memory management versus JVM objects. It is noteworthy to mention that the concept
    of off-heap memory management has always been intrinsic in Apache Flink before
    it became available in Spark. Spark developers realized the importance of project
    Tungsten since Spark 1.4, 1.5, and 1.6 to its current state in Spark 2.0+. Again,
    we emphasize that even though DataFrame will be supported as of writing this,
    and has been covered in detail (most prod systems are still pre-Spark 2.0), we
    encourage you to start thinking in the Dataset paradigm. The following figure
    shows how RDD, DataFrame, and DataSet relate to the project Tungsten evolutionary
    roadmap:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Encoders**: Encoders are Spark''s serialization and deserialization (that
    is, SerDe) framework in Spark 2.0\. Encoders seamlessly handle the mapping of
    JVM objects to tabular format that you can get under the cover and modify if desired
    (expert level).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike standard Java serialization and other serialization schemes (for example,
    Kryo), the encoders do not use runtime reflection to discover object internals
    to serialize on the fly. Instead, encoder code is generated and compiled during
    compile time to bytecode for a given object, which will result in much faster
    operation (no reflection is used) to serialize and de-serialize the object. The
    reflection at runtime for object internals (for example, lookup of fields and
    their format) imposes extra overhead that is not present using Spark 2.0\. The
    ability to use Kryo, standard java serialization, or any other serialization technique
    still remains an option (edge cases and backward compatibility) if needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoders for standard data types and objects (made of standard data types)
    are available in Tungsten out of the box. Using a quick informal program benchmark,
    serializing objects back and forth using Kryo serialization, which is popular
    with Hadoop MapReduce developers, versus encoders, revealed a significant 4x to
    8x improvement. When we looked at the source code and probed under the covers,
    we realized that the encoders actually use runtime code generation (at bytecode
    level!) to pack and unpack objects. For completeness, we mention that the objects
    also seemed to be smaller, but further details and the reasons as to why it is
    so, is beyond the scope of this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Encoder[T] is an internal artifact made of the DataSet[T], which is just
    a schema of records. You can create your own custom encoders as needed in Scala
    using tuples of underlying data (for example, Long, Double, and Int). Before you
    embark on the custom encoder journey (for example, want to store custom objects
    in DataSet[T]), make sure you take a look at `[Encoders.scala](https://github.com/apache/spark/blob/v2.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala#L270-L316)`
    and `[SQLImplicits.scala](https://github.com/apache/spark/blob/v2.0.0/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L77-L96)`
    in Spark's source directory. The plan and strategic direction for Spark is to
    provide a public API in future releases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Catalyst optimizer friendly**: Using Catalyst, the API gestures are translated
    into logical query plans which use a catalog (user-defined functions) and ultimately
    translate the logical plan to a physical plan, which is often much more efficient
    than proposed by the original scheme (even if you try to put `groupBy()` before
    `filter()`, it is smart enough to arrange it the other way around). For better
    clarity, see the following figure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Noteworthy for pre-Spark 2.0 users:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` is now the single entry point into the system. SQLContext and
    HiveContext are replaced by SparkSession.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Java users, be sure to replace DataFrame with `Dataset<Row>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the new catalog interface via `SparkSession` to execute `cacheTable()`,
    `dropTempView()`, `createExternalTable()`, `ListTable()`, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DataFrame and DataSet API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unionALL()` is deprecated; you should use now `union()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explode()` should be replaced by `functions.explode()` plus `select()` or
    `flatMap()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`registerTempTable` has been deprecated and replaced by `createOrReplaceTempView()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using internal data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are four ways to create RDDs in Spark. They range from the `parallelize()`
    method for simple testing and debugging within the client driver code to streaming
    RDDs for near-realtime responses. In this recipe, we provide you with several
    examples to demonstrate RDD creation using internal sources. The streaming case
    will be covered in the streaming Spark example in [Chapter 13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77),
    *Streaming Machine Learning System*, so we can address it in a meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameter so Spark can run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We declare two local data structures to hold the data prior to using any distributed
    RDDs. It should be noted that the data here will be held in the driver's heap
    space via local data structures. We make an explicit mention here, due to the
    multitude of problems programmers encounter when using large data sets for testing
    using the `parallelize()` technique. Ensure that you have enough space to hold
    the data locally in the driver if you use this technique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We use the `parallelize()` function to take the local data and distribute it
    across the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the difference between the two data structures as seen
    by Spark. This can be done by printing the two data structure handles: a local
    array and a cluster parallel collection (that is, RDD).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Spark tries to set the number of partitions (that is, splits in Hadoop) itself
    based on the configuration of the cluster, but there are times when we need to
    set the number of partitions manually. The `parallelize()` function offers a second
    parameter that allows you to set the number of partitions manually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the first two lines, Spark has chosen two partitions by default, and, in
    the next two lines, we have set the number of partitions to 4 and 8, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data held in the client driver is parallelized and distributed across the
    cluster using the number of portioned RDDs (the second parameter) as the guideline.
    The resulting RDD is the magic of Spark that started it all (refer to Matei Zaharia's
    original white paper).
  prefs: []
  type: TYPE_NORMAL
- en: The resulting RDDs are now fully distributed data structures with fault tolerance
    and lineage that can be operated on in parallel using Spark framework.
  prefs: []
  type: TYPE_NORMAL
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    Spark RDDs. We then proceed to split and tokenize the data and print the number
    of total words using Spark's operators (for example, `map`, `flatMap()`, and so
    on).
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using external data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we provide you with several examples to demonstrate RDD creation
    using external sources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We obtain the data from the Gutenberg project. This is a great source for accessing
    actual text, ranging from the complete works of *Shakespeare* to *Charles Dickens*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the text from the following sources and store it in your local directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Source: [http://www.gutenberg.org](http://www.gutenberg.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selected book: *A Tale of Two Cities by Charles Dickens*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again, we use `SparkContext`, available via `SparkSession`, and its function
    `textFile()` to read the external data source and parallelize it across the cluster.
    Remarkably, all the work is done for the developer behind the scenes by Spark
    using one single call to load a wide variety of formats (for example, text, S3,
    and HDFS), which parallelizes the data across the cluster using the `protocol:filepath`
    combination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To demonstrate, we load the book, which is stored as ASCII, text using the `textFile()`
    method from `SparkContext` via `SparkSession`, which, in turn goes to work behind
    the scenes and creates portioned RDDs across the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Even though we have not covered the Spark transformation operator, we'll look
    at a small code snippet which will break the file into words using blanks as a
    separator. In a real-life situation, a regular expression will be needed to cover
    all the edge cases with all the whitespace variations (refer to the *Transforming
    RDDs with Spark using filter() APIs* recipe in this chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a lambda function to receive each line as it is read and split it into
    words using blanks as separator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a flatMap to break the array of lists of words (that is, each group of
    words from a line corresponds to a distinct array/list for that line). In short,
    what we want is a list of words and not a list of a list of words for each line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    an RDD and then proceed to tokenize the words by using whitespace as the separator
    in a lambda expression using `.split()` and `.flatmap()` of RDD itself. We then
    proceed to use the `.count()` method of RDDs to output the total number of words.
    While this is simple, you have to bear in mind that the operation takes place
    using the distributed parallel framework of Spark with only a couple of lines.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating RDDs with external data sources, whether it is a text file, Hadoop
    HDFS, sequence file, Casandra, or Parquet file is remarkably simple. Once again,
    we use `SparkSession` (`SparkContext` prior to Spark 2.0) to get a handle to the
    cluster. Once the function (for example, textFile Protocol: file path) is executed,
    the data is broken into smaller pieces (partitions) and automatically flows to
    the cluster, which becomes available to the computations as fault-tolerant distributed
    collections that can be operated on in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of variations that one must consider when working with real-life
    situations. The best advice based on our own experience is to consult the documentation
    before writing your own functions or connectors. Spark either supports your data
    source right out of the box, or the vendor has a connector that can be downloaded
    to do the same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another situation that we often see is many small files that are generated (usually
    within `HDFS` directories) that need to be parallelized as RDDs for consumption.
    `SparkContext` has a method named `wholeTextFiles()` which lets you read a directory
    containing multiple files and returns each of them as (filename, content) key-value
    pairs. We found this to be very useful in multi-stage machine learning situations
    using lambda architecture, where the model parameters are calculated as a batch
    and then updated in Spark every day.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, we read multiple files and then print the first file for examination.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spark.sparkContext.wholeTextFiles()` function is used to read a large
    number of small files and present them as (K,V), or key-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark documentation for the `textFile()` and `wholeTextFiles()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  prefs: []
  type: TYPE_NORMAL
- en: The `textFile()` API is a single abstraction for interfacing to external data
    sources. The formulation of protocol/path is enough to invoke the right decoder.
    We'll demonstrate reading from an ASCII text file, Amazon AWS S3, and HDFS with
    code snippets that the user would leverage to build their own system.
  prefs: []
  type: TYPE_NORMAL
- en: The path can be expressed as a simple path (for example, local text file) to
    a complete URI with the required protocol (for example, s3n for AWS storage buckets)
    to complete resource path with server and port configuration (for example, to
    read HDFS file from a Hadoop cluster).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `textFile()` method supports full directories, regex wildcards, and compressed
    formats as well. Take a look at this example code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `textFile()` method has an optional parameter at the end that defines the
    minimum number of partitions required by RDDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, we explicitly direct Spark to break the file into 13 partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You also have the option of specifying a URI to read and create RDDs from other
    sources such as HDFS, and S3 by specifying a complete URI (protocol:path). The
    following examples demonstrate the point:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading and creating files from Amazon S3 buckets. A word of caution is that
    the AWS inline credentials in the URI will break if the AWS secret key has a forward
    slash. See this sample file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Reading from HDFS is very similar. In this example, we are reading from a local
    Hadoop cluster, but, in a real-world situation, the port number will be different
    and set by administrator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Transforming RDDs with Spark 2.0 using the filter() API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `filter()` method of RDD which is used to select
    a subset of the base RDD and return a new filtered RDD. The format is similar
    to `map()`, but a lambda function selects which members are to be included in
    the resulting RDD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following lines for the examples to compile. The `pow()` function will
    allow us to raise any number to any power (for example, square the number):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We create some data and `parallelize()` it to get our base RDD. We also use
    `textFile()`to create the initial (for example, base RDD) from our text file that
    we downloaded earlier from the [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)
    link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `filter()` function to the RDDs to demonstrate the `filter()` function
    transformation. We use the `filter()` function to select the odd members from
    the original RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `filter()` function iterates (in parallel) through members of the RDD and
    applies the mod function (%) and compares it to 1\. In short, if there is a reminder
    after dividing by 2, then it must be an odd number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a second variation of the previous line, but here we demonstrate the
    use of `_` (underscore), which acts as a wildcard. We use this notation in Scala
    to abbreviate the obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Another example combines map and filter together. This code snippet first squares
    every number and then applies the `filter` function to select the odd numbers
    from the original RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use the `filter()` method to identify the lines that are
    fewer than 30 characters. The resulting RDD will only contain the short lines.
    A quick examination of counts and output verify the results. The RDD transformation
    functions can be chained together, as long as the format complies with the function
    syntax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.gif)'
  prefs: []
  type: TYPE_IMG
- en: In this example we use the `contain()` method to filter out sentences that contain
    the word `two` in any upper/lowercase combination. We use several methods chained
    together to find the desired sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `filter()` API is demonstrated using several examples. In the first example
    we went through an RDD and output odd numbers by using a lambda expression `.filter
    ( i => (i%2) == 1)` which takes advantage of the mod (modulus) function.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example we made it a bit interesting by mapping the result to
    a square function using a lambda expression `num.map(pow(_,2)).filter(_ %2 ==
    1)`.
  prefs: []
  type: TYPE_NORMAL
- en: In the third example, we went through the text and filtered out short lines
    (for example, lines under 30 character) using the lambda expression `.filter(_.length
    < 30).filter(_.length > 0)` to print short versus total number of lines (`.count()`
    ) as output.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `filter()` API walks through the parallelized distributed collection (that
    is, RDDs) and applies the selection criteria supplied to `filter()` as a lambda
    in order to include or exclude the element from the resulting RDD. The combination
    uses `map()`, which transforms each element and `filter()`, which selects a subset
    is a powerful combination in Spark ML programming.
  prefs: []
  type: TYPE_NORMAL
- en: We will see later with the `DataFrame` API how a similar `Filter()` API can
    be used to achieve the same effect using a higher-level framework used in R and
    Python (pandas).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `.filter()`, which is a method call of RDD, is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `BloomFilter()`--for the sake of completeness, be aware that
    there is also a bloom filter function already in existence and it is suggested
    that you avoid coding this yourselves. We will tackle this in [C](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77)hapter
    13, *Spark Streaming and Machine Learning Library*, to match Spark's view and
    layout. The link for this same is [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with the super useful flatMap() API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine the `flatMap()` method which is often a source of
    confusion for beginners; however, on closer examination we demonstrate that it
    is a clear concept that applies the lambda function to each element just like
    map, and then flattens the resulting RDD as a single structure (rather than having
    a list of lists, we create a single list made of all sublist with sublist elements).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `textFile()` function to create the initial (that is, base RDD) from
    our text file that we downloaded earlier from [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the map function to the RDDs to demonstrate the `map()` function transformation.
    To start with, we are doing it the wrong way to make a point: we first attempt
    to separate all the words based on the regular expression *[\s\W]+]* using just
    `map()` to demonstrate that the resulting RDD is a list of lists in which each
    list corresponds to a line and the tokenized word within that line. This example
    demonstrates what could cause confusion for beginners when using `flatMap()`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following line trims each line and then splits the line into words. The
    resulting RDD (that is, wordRDD2) will be a list of lists of words rather than
    a single list of words for the whole file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We use the `flatMap()` method to not only map, but also flatten the list of
    lists so we end up with an RDD which is made of words themselves. We trim and
    split the words (that is, tokenize) and then filter for words greater than zero
    and then map it to upper case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this case, after flattening the list using `flatMap()`, we can get a list
    of the words back as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this short example, we read a text file and then split the words (that is,
    tokenize it) using the `flatMap(_.trim.split("""[\s\W]+""")` lambda expression
    to have a single RDD with the tokenized content. Additionally we use the `filter
    ()` API `filter(_.length > 0)` to exclude the empty lines and the lambda expression
    `.map(_.toUpperCase())` in a `.map()` API to map to uppercase before outputting
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: There are cases where we do not want to get a list back for every element of
    base RDD (for example, get a list for words corresponding to a line). We sometimes
    prefer to have a single flattened list that is flat and corresponds to every word
    in the document. In short, rather than a list of lists, we want a single list
    containing all the elements.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The function `glom()` is a function that lets you model each partition in the
    RDD as an array rather than a row list. While it is possible to produce the results
    in most cases, `glom()` allows you to reduce the shuffling between partitions.
  prefs: []
  type: TYPE_NORMAL
- en: While at the surface, both method 1 and 2 mentioned in the text below look similar
    for calculating the minimum numbers in an RDD, the `glom()` function will cause
    much less data shuffling across the network by first applying `min()` to all the
    partitions, and then sending over the resulting data. The best way to see the
    difference is to use this on 10M+ RDDs and watch the IO and CPU usage accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method is to find the minimum value without using `glom()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the preceding code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The second method is to find the minimum value using `glom(`, which causes a
    local application of the min function to a partition and then sends the results
    across via a shuffle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the preceding code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `flatMap()`, `PairFlatMap()`, and other variations under RDD
    is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the `FlatMap()` function under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the `PairFlatMap()` function - very handy variation for paired
    data elements is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `flatMap()` method applies the supplied function (lambda or named function
    via def) to every element, flattens the structure, and produces a new RDD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with set operation APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore set operations on RDDs, such as `intersection()`,
    `union()`, `subtract(),` and `distinct()` and `Cartesian()`. Let's implement the
    usual set operations in a distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the data structures and RDD for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `intersection()` function to the RDDs to demonstrate the transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `union()` function to the RDDs to demonstrate the transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `subract()` function to the RDDs to demonstrate the transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `distinct()` function to the RDDs to demonstrate the transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `distinct()` function to the RDDs to demonstrate the transformation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we started with three sets of number Arrays (odd, even, and
    their combo) and then proceeded to pass them as parameters into the set operation
    API. We covered how to use `intersection()`, `union()`, `subtract()`, `distinct()`,
    and `cartesian()` RDD operators.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the RDD set operators are easy to use, one must be careful with the data
    shuffling that Spark has to perform in the background to complete some of these
    operations (for example, intersection).
  prefs: []
  type: TYPE_NORMAL
- en: It is worth nothing that the union operator does not remove duplicates from
    the resulting RDD set.
  prefs: []
  type: TYPE_NORMAL
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `groupBy()` and `reduceBy()` methods, which allow
    us to group values corresponding to a key. It is an expensive operation due to
    internal shuffling. We first demonstrate `groupby()` in more detail and then cover
    `reduceBy()` to show the similarity in coding these while stressing the advantage
    of the `reduceBy()` operator.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the packages for setting up logging level for `log4j`. This step is
    optional, but we highly recommend it (change the level appropriately as you move
    through the development cycle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the Spark context and application parameter so Spark can run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Set up the data structures and RDD for the example. In this example we create
    an RDD using range facilities and divide them into three partitions (that is,
    explicit parameter set). It simply creates numbers 1 through 12 and puts them
    into 3 partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `groupBy()` function to the RDDs to demonstrate the transformation.
    In the example, we take the partitioned RDD of ranges and label them as odd/even
    using the `mod` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have seen how to code `groupBy()`, we switch gears and demonstrate
    `reduceByKey()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To see the difference in coding, while producing the same output more efficiently,
    we set up an array with two letters (that is, `a` and `b`) so we can show aggregation
    by summing them up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we use a Spark context to produce a parallelized RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `groupBy()` function first using the usual Scala syntax `(_+_)`
    to traverse the RDD and sum up, while aggregating by the type of alphabet (that
    is, considered key):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `reduceByKey()` function first using the usual Scala syntax `(_+_)`
    to traverse the RDD and sum up while aggregating by type of alphabet (that is,
    considered key)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We output the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we created numbers one through twelve and placed them in three
    partitions. We then proceeded to break them into odd/even using a simple mod operation
    while. The `groupBy()` is used to aggregate them into two groups of odd/even.
    This is a typical aggregation problem that should look familiar to SQL users.
    Later in this chapter we revisit this operation using `DataFrame` which also takes
    advantage of the better optimization techniques provided by the SparkSQL engine.
    In the later part, we demonstrate the similarity of `groupBy()` and `reduceByKey()`.
    We set up an array of alphabets (that is, `a` and `b`) and then convert them into
    RDD. We then proceed to aggregate them based on key (that is, unique letters -
    only two in this case) and print the total in each group.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the direction for Spark which favors the Dataset/DataFrame paradigm over
    low level RDD coding, one must seriously consider the reasoning for doing `groupBy()`
    on an RDD. While there are legitimate situations for which the operation is needed,
    the readers are advised to reformulate their solution to take advantage of the
    SparkSQL subsystem and its optimizer called **Catalyst**.
  prefs: []
  type: TYPE_NORMAL
- en: The Catalyst optimizer takes into account Scala's powerful features such as
    **pattern matching** and **quasiquotes** while building an optimized query plan.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation on Scala pattern matching is available at [http://docs.scala-lang.org/tutorials/tour/pattern-matching.html](http://docs.scala-lang.org/tutorials/tour/pattern-matching.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The documentation on Scala quasiquotes is available at [http://docs.scala-lang.org/overviews/quasiquotes/intro.html](http://docs.scala-lang.org/overviews/quasiquotes/intro.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runtime efficiency consideration: The `groupBy()` function groups data by keys.
    The operation causes internal shuffling which can explode the execution time;
    one must always prefer to use the `reduceByKey()` family of operations to a straight
    `groupBy()` method call. The `groupBy()` method is an expensive operation due
    to shuffling. Each group is made of keys and items that belong to that key. The
    ordering of values corresponding to the key will not be guaranteed by Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an explanation of the two operations, see the Databricks knowledge base
    blog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://databricks.gitbooks.io/databricks-Spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for `groupBy()` and `reduceByKey()` operations under RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming RDDs with the zip() API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we explore the `zip()` function. For those of us working in Python
    or Scala, `zip()` is a familiar method that lets you pair items before applying
    an inline function. Using Spark, it can be used to facilitate RDD arithmetic between
    pairs. Conceptually, it combines the two RDDs in such a way that each member of
    one RDD is paired with the second RDD that occupies the same position (that is,
    it lines up the two RDDs and makes pairs out of the members).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Set up the data structures and RDD for the example. In this example we create
    two RDDs from `Array[]` and let Spark decide on the number of partitions (that
    is, the second parameter in the `parallize()` method is not set).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `zip()` function to the RDDs to demonstrate the transformation.
    In the example, we take the partitioned RDD of ranges and label them as odd/even
    using the mod function. We use the `zip()` function to pair elements from the
    two RDDs (SignalNoiseRDD and SignalStrengthRDD) so we can apply a `map()` function
    and compute their ratio (noise to signal ratio). We can use this technique to
    perform almost all types of arithmetic or non-arithmetic operations involving
    individual members of two RDDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pairing of two RDD members act as a tuple or a row. The individual members
    of the pair created by `zip()` can be accessed by their position (for example,
    `._1` and `._2`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we first set up two arrays representing signal noise and signal
    strength. They are simply a set of measured numbers that we could have received
    from the IoT platform. We then proceeded to pair the two separate arrays so each
    member looks like they have been input originally as a pair of (x, y). We then
    proceed to divide the pair and produce the noise to signal ratio using the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: The `zip()` method has many variations that involve partitions. The developers
    should familiarize themselves with variations of the `zip()` method with partition
    (for example, `zipPartitions`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `zip()` and `zipPartitions()` operations under RDD is available
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join transformation with paired key-value RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduce the `KeyValueRDD` pair RDD and the supporting join
    operations such as `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`
    as an alternative to the more traditional and more expensive set operations available
    via the set operation API, such as `intersection()`, `union()`, `subtraction()`,
    `distinct()`, `cartesian()`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We'll demonstrate `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`,
    to explain the power and flexibility of key-value pair RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Set up the data structures and RDD for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Turn the List into RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: We can access the `keys` and `values` inside a pair RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `mapValues()` function to the pair RDDs to demonstrate the transformation.
    In this example we use the map function to lift up the value by adding 100 to
    every element. This is a popular technique to introduce noise to the data (that
    is, jittering).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `join()` function to the RDDs to demonstrate the transformation.
    We use `join()` to join the two RDDs. We join the two RDDs based on keys (that
    is, north, south, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `leftOuterJoin()` function to the RDDs to demonstrate the transformation.
    The `leftOuterjoin` acts like a relational left outer join. Spark replaces the
    absence of a membership with `None` rather than `NULL`, which is common in relational
    systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: We'll apply `rightOuterJoin()` to the RDDs to demonstrate the transformation.
    This is similar to a right outer join in relational systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We then apply the `fullOuterJoin()` function to the RDDs to demonstrate the
    transformation. This is similar to full outer join in relational systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we declared three lists representing typical data available
    in relational tables, which could be imported using a connector to Casandra or
    RedShift (not shown here to simplify the recipe). We used two of the three lists
    representing city names (that is, data tables) and joined them with the first
    list, which represents directions (for example, defining tables). The first step
    is to define three lists of paired values. We then parallelized them into key-value
    RDDs so we can perform join operations between the first RDD (that is, directions)
    and the other two RDDs representing city names. We applied the join function to
    the RDDs to demonstrate the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrated `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`
    to show the power and flexibility when combined with key-value pair RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `join()` and its variations under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  prefs: []
  type: TYPE_NORMAL
- en: Reduce and grouping transformation with paired key-value RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore reduce and group by key. The `reduceByKey()` and
    `groupbyKey()` operations are much more efficient and preferred to `reduce()`
    and `groupBy()` in most cases. The functions provide convenient facilities to
    aggregate values and combine them by key with less shuffling, which is problematic
    on large data sets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the data structures and RDD for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: We apply `groupByKey()` to demonstrate the transformation. In this example,
    we group all the buy and sell signals together while operating in a distributed
    setting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `reduceByKey()` function to the pair of RDDs to demonstrate the
    transformation. In this example the function is to sum up the total volume for
    the buy and sell signals. The Scala notation of `(_+_)` simply denotes adding
    two members at the time and producing a single result from it. Just like `reduce()`,
    we can apply any function (that is, inline for simple functions and named functions
    for more complex cases).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example we declared a list of items as being sold or purchased and their
    corresponding price (that is, typical commercial transaction). We then proceeded
    to calculate the sum using Scala shorthand notation `(_+_)`. In the last step,
    we provided the total for each key group (that is, `Buy` or `Sell`). The key-value
    RDD is a powerful construct that can reduce coding while providing the functionality
    needed to group paired values into aggregated buckets. The `groupByKey()` and
    `reduceByKey()` functions mimic the same aggregation functionality, while `reduceByKey()`
    is more efficient due to less shuffling of the data while final results are being
    assembled.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `groupByKey()` and `reduceByKey()` operations under RDD is
    available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames from Scala data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `DataFrame` API, which provides a higher level
    of abstraction than RDDs for working with data. The API is similar to R and Python
    data frame facilities (pandas).
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` simplifies coding and lets you use standard SQL to retrieve and
    manipulate data. Spark keeps additional information about DataFrames, which helps
    the API to manipulate the frames with ease. Every `DataFrame` will have a schema
    (either inferred from data or explicitly defined) which allows us to view the
    frame like an SQL table. The secret sauce of SparkSQL and DataFrame is that the
    catalyst optimizer will work behind the scenes to optimize access by rearranging
    calls in the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the Scala data structures as two `List()` objects and a sequence
    (that is, `Seq()`). We then proceed to turn the `List` structures into RDDs for
    conversion to `DataFrames` for the next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: We take a list which is turned into an RDD using the `parallelize()` method
    and use the `toDF()` method of the RDD to turn it into a DataFrame. The `show()`
    method allows us to view the DataFrame, which is similar to a SQL table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: In the following code snippet, we take a generic Scala **Seq** (**Sequence**)
    data structure and use `createDataFrame()` explicitly to create a DataFrame while
    naming the columns at the same time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: In the next two steps, we use the `show()` method to see the contents and then
    proceed to use `printscheme()` to show the inferred scheme based on types. In
    this example, the DataFrame correctly identified the integer and double in the
    Seq as the valid type for the two columns of numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we took two lists and a Seq data structure and converted them
    to DataFrame and used `df1.show()` and `df1.printSchema()` to display contents
    and schema for the table.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames can be created from both internal and external sources. Just like
    SQL tables, the DataFrames have schemas associated with them that can either be
    inferred or explicitly defined using Scala case classes or the `map()` function
    to explicitly convert while ingesting the data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure completeness, we include the `import` statement that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits import statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example code for Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Operating on DataFrames programmatically without SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to manipulate DataFrame with code and method
    calls only (without SQL). The DataFrames have their own methods that allow you
    to perform SQL-like operations using a programmatic approach. We demonstrate some
    of these commands such as `select()`, `show()`, and `explain()` to get the point
    across that the DataFrame itself is capable of wrangling and manipulating the
    data without using SQL.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'We are creating an RDD from an external source, which is a comma-separated
    text file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Here is a quick look at what the customer data file would look like
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: After creating the RDD for the corresponding customer data file, we proceed
    to explicitly parse and convert the data types using a `map()` function from the
    RDD. In this example, we want to make sure the last field (that is, age) is represented
    as an integer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: In the third step, we convert the RDD to a DataFrame using a `toDF()` call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the DataFrame ready, we want to display the contents quickly for
    visual verification and also print and verify the schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: Having the DataFrame ready and inspected, we proceed to demonstrate DataFrame
    access and manipulation via `show()`, `select()`, `sort()`, `groupBy()`, and `explain()`
    APIs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use the `filter()` method to list customers that are more than 25 years
    old:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: We use the `select()` method to display the names of customers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `select()` to list multiple columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'We use an alternative syntax to display and refer to fields within the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `select()` and a predicate, to list customers'' name and city where the
    age is less than 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We use `sort()` and `groupBy()` to sort and group customers by their city of
    residence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also ask for a plan of execution: this command will be more relevant
    with upcoming recipes in which we use SQL to access and manipulate the DataFrame.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we loaded data from a text file into an RDD and then converted
    it to a DataFrame structure using the `.toDF()` API. We then proceeded to mimic
    SQL queries using built-in methods such as `select()`, `filter()`, `show()`, and
    `explain()` that help us to programmatically explore the data (no SQL). The `explain()`
    command shows the query plan which can be awfully useful to remove the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames provide multiple approaches to data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: For those comfortable with the DataFrame API and packages from R ([https://cran.r-project.org](https://cran.r-project.org))
    like dplyr or an older version, we have a programmatic API with an extensive set
    of methods that lets you do all your data wrangling via the API.
  prefs: []
  type: TYPE_NORMAL
- en: For those more comfortable with SQL, you can simply use SQL to retrieve and
    manipulate data as if you were using Squirrel or Toad to query the database.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example `import` statement for Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: Loading DataFrames and setup from an external source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine data manipulation using SQL. Spark's approach to
    provide both a pragmatic and SQL interface works very well in production settings
    in which we not only require machine learning, but also access to existing data
    sources using SQL to ensure compatibility and familiarity with existing SQL-based
    systems. DataFrame with SQL makes for an elegant process toward integration in
    real-life settings.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the imports related to DataFrame and the required data structures and
    create the RDDs as needed for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logging level to warning and `Error` to cut down on output. See
    the previous step for package requirement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: We create the DataFrame corresponding to the `customer` file. In this step,
    we first create an RDD and then proceed to use the `toDF()` to convert the RDD
    to DataFrame and name the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: 'Customer data contents for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the preceding code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.gif)'
  prefs: []
  type: TYPE_IMG
- en: We create the DataFrame corresponding to the `product` file. In this step, we
    first create an RDD and then proceed to use the `toDF()` to convert the RDD to
    DataFrame and name the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert `prodRDD` to DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: Using SQL select, we display contents of the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Product data contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.gif)'
  prefs: []
  type: TYPE_IMG
- en: We create the DataFrame corresponding to the `sales` file. In this step we first
    create an RDD and then proceed to use `toDF()` to convert the RDD to DataFrame
    and name the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the `saleRDD` to DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: We use SQL select to display the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sales data contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We print schemas for the customer, product, and sales DataFrames to verify
    schema after column definition and type conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we first loaded data into an RDD and then converted it into
    a DataFrame using the `toDF()` method. The DataFrame is very good at inferring
    types, but there are occasions that require manual intervention. We used the `map()`
    function after creating the RDD (lazy initialization paradigm applies) to massage
    the data either by type conversion or calling on more complicated user defined
    functions (referenced in the `map()` method) to do the conversion or data wrangling.
    Finally, we proceeded to examine the schema for each of the three DataFrames using
    `show()` and `printSchema()`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example `import` statement for Spark 1.5.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: Using DataFrames with standard SQL language - SparkSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use DataFrame SQL capabilities to perform
    basic CRUD operations, but there is nothing limiting you from using the SQL interface
    provided by Spark to any level of sophistication (that is, DML) desired.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and `ERROR` to cut down on output. See the
    previous step for package requirement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: We will be using the DataFrames created in the previous recipe to demonstrate
    the SQL capabilities of DataFrames. You can refer to the previous steps for details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: Before we can use the DataFrame for queries via SQL, we have to register the
    DataFrame as a temp table so the SQL statements can refer to it without any Scala/Spark
    syntax. This step may cause confusion for many beginners as we are not creating
    any table (temp or permanent), but the call `registerTempTable()` (pre-Spark 2.0)
    and `createOrReplaceTempView()` (Spark 2.0+) creates a name in SQL land that the
    SQL statements can refer to without additional UDF or any domain-specific query
    language. In short, there is additional metadata that is kept by Spark in the
    background (`registerTempTable()` call), which facilitates querying in the execution
    phase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `CustDf` DataFrame as a name which SQL statements recognize as `customers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `prodDf` DataFrame as a name which SQL statements recognize as `product`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `saleDf` DataFrame as a name which SQL statements recognize as `sales`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: Now that everything is ready, let's demonstrate the power of DataFrames with
    standard SQL. For those who prefer not to work with SQL, the programmatic way
    is always at your fingertips.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example we see how to select a column from the customers table (it is
    not a SQL table underneath, but you can certainly abstract it as such).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Select multiple columns from the customer table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We print the schema for customer, product, and sales DataFrames to verify it
    after column definition and type conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.gif)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we join the sales and product tables and list all the customers
    that have purchased a product at more than 20% discount. This SQL joins the sales
    and product tables and then uses a simple formula to find products that are sold
    at a deep discount. To reiterate, the key aspect of DataFrame is that we use standard
    SQL without any special syntax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.gif)'
  prefs: []
  type: TYPE_IMG
- en: We can always use the `explain()` method to examine the physical query plan
    that Spark SQL used to execute the query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic workflow for DataFrame using SQL is to first populate the DataFrame
    either through internal Scala data structures or via external data sources first,
    and then use the `createOrReplaceTempView()` call to register the DataFrame as
    a SQL-like artifact.
  prefs: []
  type: TYPE_NORMAL
- en: When you use DataFrames, you have the benefit of additional metadata that Spark
    stores (whether API or SQL approach) which can benefit you during the coding and
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: While RDDs are still the workhorses of core Spark, the trend is toward the DataFrame
    approach which has successfully shown its capabilities in languages such as Python/Pandas
    or R.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There has been a change for registration of a DataFrame as a table. Refer to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For versions prior to Spark 2.0.0: `registerTempTable()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Spark version 2.0.0 and previous: `createOrReplaceTempView()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pre-Spark 2.0.0 to register a DataFrame as a SQL table like artifact:'
  prefs: []
  type: TYPE_NORMAL
- en: Before we can use the DataFrame for queries via SQL, we have to register the
    DataFrame as a temp table so the SQL statements can refer to it without any Scala/Spark
    syntax. This step may cause confusion for many beginners as we are not creating
    any table (temp or permanent), but the call `registerTempTable()` creates a name
    in SQL land that the SQL statements can refer to without additional UDF or without
    any domain-specific query language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Register the `CustDf` DataFrame as a name which SQL statements recognize as
    `customers`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the `prodDf` DataFrame as a name which SQL statements recognize as
    `product`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the `saleDf` DataFrame as a name which SQL statements recognize as
    `sales`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, please double check to make
    sure you have included implicits `import` statement.
  prefs: []
  type: TYPE_NORMAL
- en: Example `import` statement for Spark 1.5.2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: DataFrame is an extensive subsystem and deserves an entire book on its own.
    It makes complex data manipulation at scale available to SQL programmers.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the Dataset API using a Scala Sequence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine the new Dataset and how it works with the *seq* Scala
    data structure. We often see a relationship between the LabelPoint data structure
    used with ML libraries and a Scala sequence (that is, seq data structure) that
    play nicely with dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset is being positioned as a unifying API going forward. It is important
    to note that DataFrame is still available as an alias described as `Dataset[Row]`.
    We have covered the SQL examples extensively via DataFrame recipes, so we concentrate
    our efforts on other variations for dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for a Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala `case class` to model data for processing, and the `Car` class
    will represent electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: Configure output level to `ERROR` to reduce Spark's logging output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: Create a SparkSession yielding access to the Spark cluster, including the underlying
    session object attributes and functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: Import Spark implicits, therefore adding in behavior with only an import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will create a Dataset from the car data sequence utilizing the Spark
    session's `createDataset()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: Let's print out the results as confirmation that our method invocation transformed
    the sequence into a Spark Dataset by invoking the show method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00075.gif)'
  prefs: []
  type: TYPE_IMG
- en: Print out the Dataset's implied column names. We can now use class attribute
    names as column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: Let's show the automatically generated schema, and validate inferred data types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will filter the Dataset on price referring to the `Car` class attribute
    price as a column and show results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00076.gif)'
  prefs: []
  type: TYPE_IMG
- en: We close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduced Spark's Dataset feature which first appeared in
    Spark 1.6 and which was further refined in subsequent releases. First, we created
    an instance of a Dataset from a Scala sequence with the help of the `createDataset()`
    method belonging to the Spark session. The next step was to print out meta information
    about the generated Datatset to establish that the creation transpired as expected.
    Finally, snippets of Spark SQL were used to filter the Dataset by the price column
    for any price greater than $50, 000.00 and show the final results of execution.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset has a view called [DataFrame](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D),
    which is a Dataset of [row](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/Row.html)s
    which is untyped. The Dataset still retains all the transformation abilities of
    RDD such as `filter()`, `map()`, `flatMap()`, and so on. This is one of the reasons
    we find Datasets easy to use if we have programmed in Spark using RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KeyValue grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and using Datasets from RDDs and back again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to use RDD and interact with Dataset to build
    a multi-stage machine learning pipeline. Even though the Dataset (conceptually
    thought of as RDD with strong type-safety) is the way forward, you still have
    to be able to interact with other machine learning algorithms or codes that return/operate
    on RDD for either legacy or coding reasons. In this recipe, we also explore how
    to create and convert from Dataset to RDD and back.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala case class to model data for processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: Set output level to `ERROR` to reduce Spark's logging output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: Next, we retrieve a reference to the Spark context from the Spark session, because
    we will need it later to generate an RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: Import Spark implicits, therefore adding in behavior with only an import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: Let's make an RDD from the car data sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will create a Dataset from the RDD containing the car data by making
    use of Spark's session `createDataset()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: Let's print out the Dataset to validate that creation happened as we would expect
    via the `show` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will print out the implied column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: Let's show the automatically generated schema, and validate that the inferred
    data types are correct.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's group the Dataset by make, and count the number of makes in our dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.gif)'
  prefs: []
  type: TYPE_IMG
- en: The next step will use Spark's SQL on the Dataset, filtering by make for the
    value of Tesla, and transforming the resulting Dataset back into an RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: Finally, display the contents of the RDD, taking advantage of the `foreach()`
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: We close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we transformed an RDD into a Dataset and finally transformed
    it back to an RDD. We began with a Scala sequence which was changed into an RDD.
    After the creation of the RDD, invocation of Spark's session `createDataset()`
    method occurred, passing the RDD as an argument while receiving a Dataset as the
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the Dataset was grouped by the make column, counting the existence of
    various makes of cars. The next step involved filtering the Dataset for makes
    of Tesla and transforming the results back to an RDD. Finally, we displayed the
    resulting RDD by way of the RDD `foreach()` method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Dataset source file in Spark is only about 2500+ lines of Scala code. It
    is a very nice piece of code which can be leveraged for specialization under Apache
    license. We list the following URL and encourage you to at least scan the file
    and understand how buffering comes into play when using Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Source code for Datasets hosted on GitHub is available at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KeyValue grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with JSON using the Dataset API and SQL together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to use JSON with Dataset. The JSON format has
    rapidly become the de-facto standard for data interoperability in the last 5 years.
  prefs: []
  type: TYPE_NORMAL
- en: We explore how Dataset uses JSON and executes API commands like `select()`.
    We then progress by creating a view (that is, `createOrReplaceTempView()`) and
    then execute a SQL query to demonstrate how to query against a JSON file using
    API and SQL with ease.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use a JSON data file named `cars.json` which has been created for this
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for the Spark session to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala `case class` to model data for processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: Set output level to `ERROR` to reduce Spark's logging output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: Initialize a Spark session creating an entry point for access to the Spark cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: Import Spark implicits, therefore adding in behavior with only an import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will load the JSON data file into memory, specifying the class type
    as `Car`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: Let's print out the data from our generated Dataset of type `Car`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00079.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will display column names of the Dataset to verify that the cars' JSON
    attribute names were processed correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: Let's see the automatically generated schema and validate the inferred data
    types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: In this step, we will select the Dataset's `make` column, removing duplicates
    by applying the `distinct` method and showing the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00080.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, create a view on the cars Dataset so we can execute a literal Spark SQL
    query string against the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we execute a Spark SQL query filtering the Dataset for electric cars,
    and returning only three of the defined columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00081.gif)'
  prefs: []
  type: TYPE_IMG
- en: We close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is extremely straightforward to read a **JavaScript Object Notation** (**JSON**)
    data file and to transform it into a Dataset with Spark. JSON has become a widely
    used data format over the past several years and Spark's support for the format
    is substantial.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part, we demonstrated loading JSON into a Dataset by means of built
    in JSON parsing functionality in Spark's session. You should take note of Spark's
    built-in functionality that transforms the JSON data into the car case class.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we demonstrated Spark SQL being applied on the Dataset to
    wrangle the said data into a desirable state. We utilized the Dataset's select
    method to retrieve the `make` column and apply the `distinct` method for the removal
    of duplicates. Next, we set up a view on the cars Dataset, so we can apply SQL
    queries against it. Finally, we used the session's SQL method to execute a literal
    SQL query string against the Dataset, retrieving any items which are of kind electric.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully understand and master the Dataset API, be sure to understand the concept
    of `Row` and `Encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets follow the *lazy execution* paradigm, meaning that execution only occurs
    by invoking actions in Spark. When we execute an action, the Catalyst query optimizer
    produces a logical plan and generates a physical plan for optimized execution
    in a parallel distributed manner. See the figure in the introduction for all the
    detailed steps.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for `Row` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for `Encoder` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for KeyValue grouped Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for relational grouped Dataset [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, be sure to download and explore the Dataset source file, which is about
    2500+ lines from GitHub. Exploring the Spark source code is the best way to learn
    advanced programming in Scala, Scala Annotations, and Spark 2.0 itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Noteworthy for Pre-Spark 2.0 users:'
  prefs: []
  type: TYPE_NORMAL
- en: SparkSession is the single entry point into the system. SQLContext and HiveContext
    are replaced by SparkSession.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Java users, be sure to replace DataFrame with `Dataset<Row>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the new catalog interface via SparkSession to execute `cacheTable()`, `dropTempView()`,
    `createExternalTable()`, and `ListTable()`, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame and DataSet API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unionALL()` is deprecated and you should now use `union()` instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explode()` should be replaced by `functions.explode()` plus `select()` or
    `flatMap()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`registerTempTable` has been deprecated and replaced by `createOrReplaceTempView()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Dataset()` API source code (that is, `Dataset.scala` ) can be found via
    GitHub at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming with the Dataset API using domain objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how functional programming works with Dataset. We
    use the Dataset and functional programming to separate the cars (domain object)
    by their models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use package instruction to provide the right path
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala case to contain our data for processing, and our car class will
    represent electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a `Seq` populated with electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: Set output level to `ERROR` to reduce Spark's output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: Create a SparkSession yielding access to the Spark cluster and underlying session
    object attributes such as the SparkContext and SparkSQLContext.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: Import spark implicits, therefore adding in behavior with only an import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: Now we will create a Dataset from the car data Seq utilizing the SparkSessions's
    `createDataset()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: Display the Dataset to understand how to transform data in subsequent steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.gif)'
  prefs: []
  type: TYPE_IMG
- en: Now we construct a functional sequence of steps to transform the original Dataset
    into data grouped by make with all various models attached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: Let's display results from our previous sequence of functional logic for validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.gif)'
  prefs: []
  type: TYPE_IMG
- en: We close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we use a Scala sequence data structure to hold the original
    data, which is a series of cars and their attributes. Using `createDataset()`*,*
    we create a DataSet and populate it. We then proceed to use the 'make' attribute
    with `groupBy` and `mapGroups()` to list cars by their models using a functional
    paradigm with DataSet. Using this form of functional programming with domain objects
    was not impossible before DataSet (for example, the case class with RDD or UDF
    with DataFrame), but the DataSet construct makes this easy and intrinsic.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Be sure to include the `implicits` statement in all your DataSet coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The documentation for Datasets can be accessed at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  prefs: []
  type: TYPE_NORMAL

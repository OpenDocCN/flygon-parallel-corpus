- en: Common Recipes for Implementing a Robust Machine Learning System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark's basic statistical API to help you build your own algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipelines for real-life machine learning applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting data for training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common operations with the new Dataset API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LabeledPoint data structure for Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to Spark cluster in Spark 2.0+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to Spark cluster pre-Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New model export and PMML markup in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression model evaluation using Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification model evaluation using Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilabel classification model evaluation using Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass classification model evaluation using Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Scala Breeze library to do graphics in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In every line of business ranging from running a small business to creating
    and managing a mission critical application, there are a number of tasks that
    are common and need to be included as a part of almost every workflow that is
    required during the course of executing the functions. This is true even for building
    robust machine learning systems. In Spark machine learning, some of these tasks
    range from splitting the data for model development (train, test, validate) to
    normalizing input feature vector data to creating ML pipelines via the Spark API.
    We provide a set of recipes in this chapter to enable the reader to think about
    what is actually required to implement an end-to-end machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter attempts to demonstrate a number of common tasks which are present
    in any robust Spark machine learning system implementation. To avoid redundant
    references these common tasks in every recipe covered in this book, we have factored
    out such common tasks as short recipes in this chapter, which can be leveraged
    as needed while reading the other chapters. These recipes can either stand alone
    or be included as pipeline subtasks in a larger system. Please note that these
    common recipes are emphasized in the larger context of machine learning algorithms
    in later chapters, while also including them as independent recipes in this chapter
    for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Spark's basic statistical API to help you build your own algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we cover Spark's multivariate statistical summary (that is,
    *Statistics.colStats*) such as correlation, stratified sampling, hypothesis testing,
    random data generation, kernel density estimators, and much more, which can be
    applied to extremely large datasets while taking advantage of both parallelism
    and resiliency via RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s retrieve the Spark session underlying the SparkContext to use when generating
    RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create a RDD with the handcrafted data to illustrate usage of summary
    statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We use Spark''s statistics objects by invoking the method `colStats()` and
    passing the RDD as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `colStats()` method will return a `MultivariateStatisticalSummary`, which
    contains the computed summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created an RDD from dense vector data followed by the generation of summary
    statistics on it using the statistics object. Once the `colStats()` method returned,
    we retrieved summary statistics such as the mean, variance, minimum, maximum,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It cannot be emphasized enough how efficient the statistical API is on large
    datasets. These APIs will provide you with basic elements to implement any statistical
    learning algorithm from scratch. Based on our research and experience with half
    versus full matrix factorization, we encourage you to first read the source code
    and make sure that there isn't an equivalent functionality already implemented
    in Spark before implementing your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we only demonstrate a basic statistics summary here, Spark comes equipped
    out of the box with:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlation: `Statistics.corr(seriesX, seriesY, "type of correlation")`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson (default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spearman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stratified sampling - RDD API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a replacement RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a replacement - requires an additional pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hypothesis testing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector - `Statistics.chiSqTest( vector )`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix - `Statistics.chiSqTest( dense matrix )`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kolmogorov-Smirnov** (**KS**) test for equality - one or two-sided:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Statistics.kolmogorovSmirnovTest(RDD, "norm", 0, 1)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random data generator - `normalRDD()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal - can specify a parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lots of option plus `map()`s to generate any distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel density estimator - `KernelDensity().estimate( data )`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick reference to the *Goodness of fit* concept in statistics can be found
    at [https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit)
    link.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for more multivariate statistical summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipelines for real-life machine learning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the first of two recipes which cover the ML pipeline in Spark 2.0\.
    For a more advanced treatment of ML pipelines with additional details such as
    API calls and parameter extraction, see later chapters in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we attempt to have a single pipeline that can tokenize text,
    use HashingTF (an old trick) to map term frequencies, run a regression to fit
    a model, and then predict which group a new term belongs to (for example, news
    filtering, gesture classification, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a training set DataFrame with several random text documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a tokenizer to parse the text documents into individual terms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a HashingTF for transforming terms into feature vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a logistic regression class to generate a model to predict which group
    a new text document belongs to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we construct a data pipeline with an array of three stages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we train the model so we can make predictions later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a test dataset to validate our trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we transform the test set using the trained model, generating predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00084.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we investigated constructing a simple machine learning pipeline
    with Spark. We began with creating a DataFrame comprised of two groups of text
    documents and then proceeded to set up a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: First, we created a tokenizer to parse text documents into terms followed by
    the creation of the HashingTF to convert the terms into features. Then, we created
    a logistic regression object to predict which group a new text document belongs
    to.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we constructed the pipeline by passing an array of arguments to it,
    specifying three stages of execution. You will notice each subsequent stage provides
    the result as a specified column, while using the previous stage's output column
    as the input.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we trained the model by invoking `fit()` on the pipeline object and
    defining a set of test data for verification. Next, we transformed the test set
    with the model, producing which of the defined two groups the text documents in
    the test set belong to.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pipeline in Spark ML was inspired by scikit-learn in Python, which is referenced
    here for completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ML pipelines make it easy to combine multiple algorithms used to implement
    a production task in Spark. It would be unusual to see a use case in a real-life
    situation that is made of a single algorithm. Often a number of cooperating ML
    algorithms work together to achieve a complex use case. For example, in LDA-based
    systems (for example, news briefings) or human emotion detection, there are a
    number of steps before and after the core system to be implemented as a single
    pipe to produce any meaningful and production-worthy system. See the following
    link for a real-life use case requiring a pipeline to implement a robust system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.thinkmind.org/index.php?view=article&articleid=achi_2013_15_50_20241](https://www.thinkmind.org/index.php?view=article&articleid=achi_2013_15_50_20241)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for more multivariate statistical summary:'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline docs are available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline model that is useful when we load and save the `.load()`, `.save()
    methods`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline stage information is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HashingTF, a nice old trick to map a sequence to their term frequency in text
    analytics is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate normalizing (scaling) the data prior to importing
    the data into an ML algorithm. There are a good number of ML algorithms such as
    **Support Vector Machine** (**SVM**) that work better with scaled input vectors
    rather than with the raw values.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the UCI Machine Learning Repository and download the [http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to parse wine data into a tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the wine data into memory, taking only the first four columns and
    converting the latter three into a new feature vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate a DataFrame with two columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will print out the DataFrame schema and display data contained within
    the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we generate the scaling model and transform the feature into a common
    range between a negative and positive one displaying the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we explored feature scaling which is a critical step in most
    machine learning algorithms such as **classifiers**. We started out by loading
    the wine data files, extracted an identifier, and used the next three columns
    to create a feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we created a `MinMaxScaler` object, configuring a minimum and maximum
    range to scale our values into. We invoked the scaling model by executing the
    `fit()` method on the scaler class, and then we used the model to scale the values
    in our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we displayed the resulting DataFrame and we noticed feature vector
    values ranges are between negative 1 and positive 1.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The roots of normalizing and scaling can be better understood by examining
    the concept of **unit vectors** in introductory linear algebra. Please see the
    following links for some common references for unit vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to unit vectors at [https://en.wikipedia.org/wiki/Unit_vector](https://en.wikipedia.org/wiki/Unit_vector)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For scalar, you can refer to [https://en.wikipedia.org/wiki/Scalar_(mathematics)](https://en.wikipedia.org/wiki/Scalar_(mathematics))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of input sensitive algorithms, such as SVM, it is recommended that
    the algorithm be trained on scaled values (for example, range from 0 to 1) of
    the features rather than the absolute values as represented by the original vector.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `MinMaxScaler` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)
  prefs: []
  type: TYPE_NORMAL
- en: We want to emphasize that `MinMaxScaler` is an extensive API that extends the
    `Estimator` (a concept from the ML pipeline) and when used correctly can lead
    to achieving coding efficiency and high accuracy results.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data for training and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn to use Spark's API to split your available input
    data into different datasets that can be used for training and validation phases.
    It is common to use an 80/20 split, but other variations of splitting the data
    can be considered as well based on your preference.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the UCI Machine Learning Repository and download the [http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip](http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip) file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin with loading a data file by way of the Spark session''s `csv()` method
    to parse and load data into a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now, we count how many items the CSV loader parsed and loaded into memory. We
    will need this value later to reconcile data splitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we utilize the dataset''s `randomSplit` method to split the data into
    two buckets with allocations of 80% and 20% of data each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `randomSplit` method returns an array with two sets of data, the first
    set with 80% of data being the training set and the next with 20% being the testing
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s generate counts for both training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we reconcile the values, and notice that the original row count is `415606`
    and the final summation of the training and testing sets equals `415606`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began by loading the data file `newsCorpora.csv` and then by way of the `randomSplit()`
    method attached to the dataset object, we split the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To validate the result, we must set up a Delphi technique in which the test
    data is absolutely unknown to the model. See Kaggle competitions for details at [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Three types of datasets are needed for a robust ML system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dataset**: This is used to fit a model to sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation dataset**: This is used to estimate the delta or prediction error
    for the fitted model (trained by training set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test dataset**: This is used to assess the model generalization error once
    a final model is selected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `randomSplit()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D).
  prefs: []
  type: TYPE_NORMAL
- en: The `randomSplit()` is a method call within an RDD. While the number of RDD
    method calls can be overwhelming, mastering this Spark concept and API is a must.
  prefs: []
  type: TYPE_NORMAL
- en: 'API signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Randomly splits this RDD with the provided weights.
  prefs: []
  type: TYPE_NORMAL
- en: Common operations with the new Dataset API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we cover the Dataset API, which is the way forward for data
    wrangling in Spark 2.0 and beyond. In [Chapter 3](part0116.html#3EK180-4d291c9fed174a6992fd24938c2f9c77),
    *Spark's Three Data Musketeers for Machine Learning - Perfect Together* we covered
    three detailed recipes for dataset, and in this chapter we cover some of the common,
    repetitive operations that are required to work with these new API sets. Additionally,
    we demonstrate the query plan generated by the Spark SQL Catalyst optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use a JSON data file named `cars.json`, which has been created for
    this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala `case class` to model the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a dataset from a Scala list and print out the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will load a CSV into memory and transform it into a dataset of type
    `Team`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we demonstrate a transversal of the teams dataset by use of the `map` function,
    yielding a new dataset of city names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Display the execution plan for retrieving city names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save the `teams` dataset to a JSON file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we created a dataset from a Scala list and displayed the output to validate
    the creation of the dataset as expected. Second, we loaded a **comma-separated
    value** (**CSV**) file into memory, transforming it into a dataset of type `Team`.
    Third, we executed the `map()` function over our dataset to build a list of team
    city names and printed out the execution plan used to generate the dataset. Finally,
    we persisted the `teams` dataset we previously loaded into a JSON formatted file
    for future use.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please take a note of some interesting points on datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets use *lazy* evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets take advantage of the Spark SQL Catalyst optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets take advantage of the tungsten off-heap memory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are plenty of systems that will remain pre-Spark 2.0 for the next 2 year
    so you must still learn and master RDDs and DataFrame for practical reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we explore the subtle differences in creating RDD, DataFrame,
    and Dataset from a text file and their relationship to each other via a short
    sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Assume `spark` is the session name
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define a `case class` to host the data used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In the following block, we let Spark *create a dataset* object from a text file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The text file contains very simple data (each line contains an ID and name
    separated by a comma):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the file in and parse the data in the file. The dataset object is created
    by Spark. We confirm the type in the console and then display the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we create an RDD with the same data file, in a very similar way as the
    preceding step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We then confirm that it is an RDD and display the data in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Note that the method is very similar but different.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'DataFrame is another common data structure utilized by Spark communities. We
    show a similar way to create a DataFrame using the similar method based on the
    same data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We then confirm that it is a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Note that `DataFrame = Dataset[Row]`, so the type is Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We create an RDD, DataFrame, and Dataset object using a similar method from
    the same text file and confirm the type using the `getClass` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Please note that they are very similar and sometimes confusing. Spark 2.0 has
    transformed DataFrame into an alias for `Dataset[Row]`, making it truly a dataset.
    We showed the preceding methods to let the user pick an example to create their
    own datatype flavor.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for datatypes is available at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you are unsure as to what kind of data structure you have at hand (sometimes
    the difference is not obvious), use the `getClass` method to verify.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0 has transformed DataFrame into an alias for `Dataset[Row]`. While
    RDD and Dataram remain fully viable for near future, it is best to learn and code
    new projects using the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for RDD and Dataset is available at the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LabeledPoint data structure for Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LabeledPoint** is a data structure that has been around since the early days
    for packaging a feature vector along with a label so it can be used in unsupervised
    learning algorithms. We demonstrate a short recipe that uses LabeledPoint, the
    **Seq** data structure, and DataFrame to run a logistic regression for binary
    classification of the data. The emphasis here is on LabeledPoint, and the regression
    algorithms are covered in more depth in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and
    [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II*.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the LabeledPoint, using the `SparseVector` and `DenseVector`. In
    the following code blocks, the first four LabeledPoints are created by the `DenseVector`,
    the last two LabeledPoints are created by the `SparseVector`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The DataFrame objects are created from the preceding LabeledPoint.
  prefs: []
  type: TYPE_NORMAL
- en: We verify the raw data count and process data count.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can operate a `show()` function call to the DataFrame created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: You will see the following in the console:![](img/00092.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We create a simple LogisticRegression model from the data structure we just
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console, it will show the following `model` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used a LabeledPoint data structure to model features and drive training of
    a logistics regression model. We began by defining a group of LabeledPoints, which
    are used to create a DataFrame for further processing. Then, we created a logistic
    regression object and passed LabeledPoint DataFrame as an argument to it so we
    could train our model. Spark ML APIs are designed to work well with the LabeledPoint
    format and require minimal intervention.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A LabeledPoint is a popular structure used to package data as a `Vector` +
    a `Label` which can be purposed for supervised machine learning algorithms. A
    typical layout of the LabeledPoint is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Please note that not only dense but also sparse vectors can be used with LabeledPoint,
    which will make a huge difference in efficiency especially if you have a large
    and sparse dataset housed in the driver during testing and development.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LabeledPoint API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseVector API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparseVector API documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to Spark cluster in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to get access to a Spark cluster using a
    single point access named `SparkSession`. Spark 2.0 abstracts multiple contexts
    (such as SQLContext, HiveContext) into a single entry point, `SparkSession`, which
    allows you to get access to all Spark subsystems in a unified way.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for SparkContext to get access to the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark 2.x, `SparkSession` is more commonly used instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and `SparkSession` so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code utilizes the `master()` function to set the cluster type
    to `local`. A comment is provided to show how to run the local cluster running
    on a specific port.
  prefs: []
  type: TYPE_NORMAL
- en: The `-D` option value will be overridden by the cluster master parameter set
    in the code if both exist.
  prefs: []
  type: TYPE_NORMAL
- en: In a `SparkSession` object, we typically use the `master()` function, while
    pre-Spark 2.0, in the `SparkConf` object, uses the `setMaster()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the three sample ways to connect to a cluster in different
    modes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running in `local` mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Running in cluster mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing the master value in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We read a CSV file in and parse the CSV file into Spark using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We show the DataFrame in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: And you will see the following in the console:![](img/00094.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we show how to connect to a Spark cluster using local and remote
    options for an application. First, we create a `SparkSession` object which will
    grant us access to a Spark cluster by specifying whether the cluster is local
    or remote using the `master()` function. You can also specify the master location
    by passing a JVM argument when starting your client program. In addition, you
    can configure an application name and a working data directory. Next, you invoked
    the `getOrCreate()` method to create a new `SparkSession` or hand you a reference
    to an already existing session. Finally, we execute a small sample program to
    prove our `SparkSession` object creation is valid.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark session has numerous parameters and APIs that can be set and exercised,
    but it is worth consulting the Spark documentation since some of the methods/parameters
    are marked with the status Experimental or left blank - for non-experimental statuses
    (15 minimum as of our last examination).
  prefs: []
  type: TYPE_NORMAL
- en: Another change to be aware of is to use `spark.sql.warehouse.dir` for the location
    of the tables. Spark 2.0 uses `spark.sql.warehouse.dir` to set warehouse locations
    to store tables rather than `hive.metastore.warehouse.dir`. The default value
    for `spark.sql.warehouse.dir` is `System.getProperty("user.dir")`.
  prefs: []
  type: TYPE_NORMAL
- en: Also see `spark-defaults.conf` for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also noteworthy are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of our favorite and interesting APIs from the Spark 2.0 documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The version of Spark on which this application is running:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Def **sql**(sqlText: String): [DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executes a SQL query using Spark, returning the result as a [DataFrame](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D)
    - **Preferred Spark 2.0**
  prefs: []
  type: TYPE_NORMAL
- en: 'Val **sqlContext**: [SQLContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SQLContext.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A wrapped version of this session in the form of a [SQLContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SQLContext.html),
    for backward compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'lazy val **conf**: [RuntimeConfig](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/RuntimeConfig.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime configuration interface for Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'lazy val **catalog**: [Catalog](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/catalog/Catalog.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interface through which the user may create, drop, alter, or query underlying
    databases, tables, functions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Def newSession(): SparkSession**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starts a new session with isolated SQL configurations and temporary tables;
    registered functions are isolated, but share the underlying [SparkContext](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/SparkContext.html)
    and cached data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Def **udf**: [UDFRegistration](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/UDFRegistration.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A collection of methods for registering user-defined functions (UDF).
  prefs: []
  type: TYPE_NORMAL
- en: We can create both DataFrame and Dataset directly via the Spark session. It
    works, but is marked as experimental in Spark 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: If you are going to do any SQL related work, SparkSession is now the entry point
    to Spark SQL. SparkSession is the first object that you have to create in order
    to create Spark SQL applications.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `SparkSession` API documents is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession).
  prefs: []
  type: TYPE_NORMAL
- en: Getting access to Spark cluster pre-Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a *pre-Spark 2.0 recipe*, but it will be helpful for developers who
    want to quickly compare and contrast the cluster access for porting pre-Spark
    2.0 programs to Spark 2.0's new paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code utilizes the `setMaster()` function to set the cluster master
    location. As you can see, we are running the code in `local` mode.
  prefs: []
  type: TYPE_NORMAL
- en: The `-D` option value will be overridden by the cluster master parameter set
    in the code if both exist).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the three sample ways to connect to the cluster in different
    modes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running in local mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Running in cluster mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing the master value in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We use the preceding SparkContext to read a CSV file in and parse the CSV file
    into Spark using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'We take the sample result and print them in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: And you will see the following in the console:![](img/00096.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we show how to connect to a Spark cluster using the local and
    remote modes prior to Spark 2.0\. First, we create a `SparkConf` object and configure
    all the required parameters. We will specify the master location, application
    name, and working data directory. Next, we create a SparkContext passing the `SparkConf`
    as an argument to access a Spark cluster. Also, you can specify the master location
    my passing a JVM argument when starting your client program. Finally, we execute
    a small sample program to prove our SparkContext is functioning correctly.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to Spark 2.0, getting access to a Spark cluster was done via **SparkContext**.
  prefs: []
  type: TYPE_NORMAL
- en: The access to the subsystems such as SQL was per-specific names context (for
    example, SQLContext**)**.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0 changed how we gain access to a cluster by creating a single unified
    access point (namely, `SparkSession`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for SparkContext is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext).
  prefs: []
  type: TYPE_NORMAL
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to get hold of SparkContext using a SparkSession
    object in Spark 2.0\. This recipe will demonstrate the creation, usage, and back
    and forth conversion of RDD to Dataset. The reason this is important is that even
    though we prefer Dataset going forward, we must still be able to use and augment
    the legacy (pre-Spark 2.0) code mostly utilizing RDD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'We first show how to use `sparkContext` to create RDD. The following code samples
    were very common in Spark 1.x:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the `SparkContext` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We first create `rdd1` from the `makeRDD` method and display the RDD in the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: We then use the `parallelize` method to generate `rdd2`, and display the data
    in the RDD in the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we show the way to use the `session` object to create the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: We generated `dataset1` and `dataset2` using different methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For dataset1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For dataset2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We show the way to retrieve the underlying RDD from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The following block shows a way to convert RDD to Dataset object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created RDD using the SparkContext; this was widely used in Spark 1.x. We
    also demonstrated a way to create Dataset in Spark 2.0 using the Session object.
    The conversion back and forth is necessary to deal with pre-Spark 2.0 code in
    production today.
  prefs: []
  type: TYPE_NORMAL
- en: The technical message from this recipe is that while DataSet is the preferred
    method of data wrangling going forward, we can always use the API to go back and
    forth to RDD and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More about the datatypes can be found at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for SparkContext and SparkSession is available at the following
    websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New model export and PMML markup in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the model export facility available in Spark 2.0
    to use **Predictive Model Markup Language** (**PMML**). This standard XML-based
    language allows you to export and run your models on other systems (some limitations
    apply). You can explore the *There's more...* section for more information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data from a text file; the data file contains a sample dataset
    for a KMeans model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the parameters for the KMeans model, and train the model using the
    preceding datasets and parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: We have effectively created a simple KMeans model (by setting the number of
    clusters to 2) from the data structure we just created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console, it will show the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'We then export the PMML to an XML file in the data directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you spend time to train a model, the next step will be to persist the
    model for future use. In this recipe, we began by training a KMeans model to generate
    model info for persistence in later steps. Once we have the trained model, we
    invoke the `toPMML()` method on the model converting it into PMML for storage.
    The invocation of the method generates an XML document, then the XML document
    text can easily be persisted to a file.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PMML is a standard developed by the **Data Mining Group** (**DMG**). The standard
    enables inter-platform interoperability by letting you build on one system and
    then deploy to another system in production. The PMML standard has gained momentum
    and has been adopted by most vendors. At its core, the standard is based on an
    XML document with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Header with general information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary describing field level definitions used by the third component (the
    model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model structure and parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As of this writing, the Spark 2.0 Machine Library support for PMML exporting
    is currently limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KMeans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can export the model to the following file types in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Local filesystem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Distributed filesystem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Output stream--acting as a pipe:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `PMMLExportable` API documents at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable).
  prefs: []
  type: TYPE_NORMAL
- en: Regression model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to evaluate a regression model (a regression
    decision tree in this example). Spark provides the **RegressionMetrics** facility
    which has basic statistical facilities such as **Mean Squared Error** (**MSE**),
    R-Squared, and so on, right out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: The objective in this recipe is to understand the evaluation metrics provided
    by Spark out of the box. It is best to concentrate on step 8 since we cover regression
    in more detail in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and
    [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II* and
    throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: We utilize the Wisconsin breast cancer dataset as an example dataset for the
    regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Wisconsin breast cancer** dataset was obtained from the University of
    Wisconsin Hospital from Dr. William H Wolberg. The dataset was gained periodically
    as Dr.Wolberg reported his clinical cases.
  prefs: []
  type: TYPE_NORMAL
- en: More details on the dataset can be found in [Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77), *Optimization
    - Going Down the Hill with Gradient Descent*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: We load the data into Spark and filter the missing values in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the dataset in the ratio of 70:30 to create two datasets, one used
    for training the model, and the other for testing the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the parameters and using the `DecisionTree` model, after the training
    dataset, we use the test dataset to do the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate the `RegressionMetrics` object and start the evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the statistics value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explored the generation of regression metrics to help us
    evaluate our regression model. We began to load a breast cancer data file and
    then split it in a 70/30 ratio to create training and test datasets. Next, we
    trained a `DecisionTree` regression model and utilized it to make predictions
    on our test set. Finally, we took the predictions and generated regression metrics
    which gave us the squared error, R-squared, mean absolute error, and explained
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use `RegressionMetrics()` to produce the following statistical measures:'
  prefs: []
  type: TYPE_NORMAL
- en: MSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explained variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation on regression validation is available at [https://en.wikipedia.org/wiki/Regression_validation](https://en.wikipedia.org/wiki/Regression_validation).
  prefs: []
  type: TYPE_NORMAL
- en: R-Squared/coefficient of determination is available at [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wisconsin breast cancer dataset could be downloaded at [ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression metrics documents are available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate the use of the `BinaryClassificationMetrics`
    facility in Spark 2.0 and its application to evaluating a model that has a binary
    outcome (for example, a logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: The purpose here is not to showcase the regression itself, but to demonstrate
    how to go about evaluating it using common metrics such as **receiver operating
    characteristic** (**ROC**), Area Under ROC Curve, thresholds, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We recommend that you concentrate on step 8 since we cover regression in more
    detail in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and
    [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part II*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'We download the dataset, originally from the UCI, and modify it to fit the
    need for the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is a modified dataset. The original adult dataset has 14 features,
    among which six are continuous and eight are categorical. In this dataset, continuous
    features are discretized into quantiles, and each quantile is represented by a
    binary feature. We modified the data to fit the purpose of the code. Details of
    the dataset feature can be found at the [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
    UCI site.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the dataset into training and test parts in a ratio of 60:40 random
    split, then get the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the prediction using the model created by the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `BinaryClassificationMetrics` object from the predication, and
    start the evaluation on the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the precision by `Threashold` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the `recallByThreshold` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the `fmeasureByThreshold` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the `Area Under Precision Recall Curve` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the Area Under ROC curve in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we investigated the evaluation of metrics for binary classification.
    First, we loaded the data, which is in the `libsvm` format, and split it in the
    ratio of 60:40, resulting in the creation of a training and a test set of data.
    Next, we trained a logistic regression model followed by generating predictions
    from our test set.
  prefs: []
  type: TYPE_NORMAL
- en: Once we had our predictions, we created a binary classification metrics object.
    Finally, we retrieved the true positive rate, positive predictive value, receiver
    operating curve, area under receiver operating curve, area under precision recall
    curve, and F-measure to evaluate our model for fitness.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark provides the following metrics to facilitate evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: TPR - True Positive Rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PPV - Positive Predictive Value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F - F-Measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC - Receiver Operating Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUROC - Area Under Receiver Operating Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUORC - Area Under Precision-Recall Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following links should provide a good introductory material for the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for the original dataset information is available at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets.html](http://archive.ics.uci.edu/ml/datasets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for binary classification metrics is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics).
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore `MulticlassMetrics`, which allows you to evaluate
    a model that classifies the output to more than two labels (for example, red,
    blue, green, purple, do-not-know). It highlights the use of confusion matrix (`confusionMatrix`)
    and model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: 'We download the dataset, originally from the UCI, and modify it to fit the
    need of the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is a modified dataset. The original Iris Plant dataset has four
    features. We modified the data to fit the purpose of the code. Details of the
    dataset features can be found at the UCI site.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the dataset into training and test parts in a ratio of 60% versus
    40% random split, then get the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the raw score on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `MulticlassMetrics` object from the predication, and start the
    evaluation on the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the confusion matrix in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the overall statistics in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the precision by label value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the recall by label in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the false positive rate by label in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the F-measure by label in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the weighted statistics value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explored generating evaluation metrics for a multi-classification
    model. First, we loaded the Iris data into memory and split it in a ratio 60:40\.
    Second, we trained a logistic regression model with the number of classifications
    set to three. Third, we made predictions with the test dataset and utilized `MultiClassMetric`
    to generate evaluation measurements. Finally, we evaluated metrics such as the
    model accuracy, weighted precision, weighted recall, weighted F1 score, weighted
    false positive rate, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the scope of the book does not allow for a complete treatment of the confusion
    matrix, a short explanation and a link are provided as a quick reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix is just a fancy name for an error matrix. It is mostly
    used in unsupervised learning to visualize the performance. It is a layout that
    captures actual versus predicted outcomes with an identical set of labels in two
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion Matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To get a quick introduction to the confusion matrix in unsupervised and supervised
    statistical learning systems, see [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for original dataset information is available at the following
    websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for multiclass classification metrics is available at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilabel classification model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore multilabel classification `MultilabelMetrics` in
    Spark 2.0 which should not be mixed up with the previous recipe dealing with multiclass
    classification `MulticlassMetrics`. The key to exploring this recipe is to concentrate
    on evaluation metrics such as Hamming loss, accuracy, f1-measure, and so on, and
    what they measure.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the dataset for the evaluation model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `MultilabelMetrics` object from the predication, and start the
    evaluation on the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the overall statistics summary in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the individual label value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the micro statistics value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the Hamming loss and subset accuracy from the metrics in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: We then close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we investigated generating evaluation metrics for the multilabel
    classification model. We began with manually creating a dataset for the model
    evaluation. Next, we passed our dataset as an argument to the `MultilabelMetrics`
    and generated evaluation metrics. Finally, we printed out various metrics such
    as micro recall, micro precision, micro f1-measure, Hamming loss, subset accuracy,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that the multilabel and multiclass classifications sound similar, but they
    are two different things.
  prefs: []
  type: TYPE_NORMAL
- en: All multilabel `MultilabelMetrics()` method is trying to accomplish is to map
    a number of inputs (x) to a binary vector (y) rather than numerical values in
    a typical classification system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important metrics associated with the multilabel classification are (see
    the preceding code):'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamming loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A full explanation of each parameter is out of scope, but the following link
    provides a short treatment for the multilabel metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Multi-label_classification](https://en.wikipedia.org/wiki/Multi-label_classification)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for multilabel classification metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Scala Breeze library to do graphics in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the functions `scatter()` and `plot()` from the
    Scala Breeze linear algebra library (part of) to draw a scatter plot from a two-dimensional
    data. Once the results are computed on the Spark cluster, either the actionable
    data can be used in the driver for drawing or a JPEG or GIF can be generated in
    the backend and pushed forward for efficiency and speed (popular with GPU-based
    analytical databases such as MapD)
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to download the necessary ScalaNLP library. Download the JAR
    from the Maven repository available at [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Place the JAR in the `C:\spark-2.0.0-bin-hadoop2.7\examples\jars` directory
    on a Windows machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In macOS, please put the JAR in its correct path. For our setting examples,
    the path is `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/jars/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the sample screenshot showing the JARs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session by specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create the figure object, and set the parameter for the figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: We create a dataset from random numbers, and display the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset will be used later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We collect the dataset, and set up the *x* and *y* axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the photo part, we convert the datatype to double, and derive the value
    to `y2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use the Breeze library''s scatter method to put the data into the chart,
    and plot the diagonal line with the plot method from Breeze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: We set the label for both the *x* axis and *y* axis and refresh the figure object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the generated Breeze chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we created a dataset in Spark from random numbers. We then created
    a Breeze figure and set up the basic parameters. We derived *x*, *y* data from
    the created dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We used Breeze's `scatter()` and `plot()` functions to do graphics using the
    Breeze library.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One can use Breeze as an alternative to more complicated and powerful charting
    libraries such as JFreeChart, demonstrated in the previous chapter. The ScalaNLP
    project tends to be optimized with Scala goodies such as implicit conversions
    that make the coding relatively easier.
  prefs: []
  type: TYPE_NORMAL
- en: The Breeze graphics JAR file can be downloaded at [http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  prefs: []
  type: TYPE_NORMAL
- en: More about Breeze graphics can be found at [https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart).
  prefs: []
  type: TYPE_NORMAL
- en: The API document (please note, the API document is not necessarily up-to-date)
    can be found at [http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package).
  prefs: []
  type: TYPE_NORMAL
- en: Note that once you are in the root package, you need click on Breeze to see
    the details.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information on Breeze, see the original material on GitHub at [https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze).
  prefs: []
  type: TYPE_NORMAL
- en: Note that once you are in the root package, you need to click on Breeze to see
    the details.
  prefs: []
  type: TYPE_NORMAL
- en: For more information regarding the Breeze API documentation, please download
    the [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar)
    JAR.
  prefs: []
  type: TYPE_NORMAL

- en: Spark Streaming and Machine Learning Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Structured streaming for near real-time machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming DataFrames for real-time machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Datasets for real-time machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data and debugging with queueStream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and understanding the famous Iris data for unsupervised classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans for a real-time online classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading wine quality data for streaming regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming linear regression for a real-time regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading Pima Diabetes data for supervised classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming logistic regression for an on-line classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark streaming is an evolving journey toward unification and structuring of
    the APIs in order to address the concerns of batch versus stream. Spark streaming
    has been available since Spark 1.3 with **Discretized Stream** (**DStream**).
    The new direction is to abstract the underlying framework using an unbounded table
    model in which the users can query the table using SQL or functional programming
    and write the output to another output table in multiple modes (complete, delta,
    and append output). The Spark SQL Catalyst optimizer and Tungsten (off-heap memory
    manager) are now an intrinsic part of the Spark streaming, which leads to a much
    efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we not only cover the streaming facilities available in Spark's
    machine library out of the box, but also provide four introductory recipes that
    we found useful as we journeyed toward our better understanding of Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts what is covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00284.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Spark 2.0+ builds on the success of the previous generation by abstracting away
    some of the framework's inner workings and presenting it to the developer without
    worrying about *end-to-end write only once* semantics. It is a journey from DStream
    based on RDD to a structured streaming paradigm in which your world of streaming
    can be viewed as infinite tables with multiple modes for output.
  prefs: []
  type: TYPE_NORMAL
- en: The state management has evolved from `updateStateByKey` (Spark 1.3 to Spark
    1.5) to `mapWithState` (Spark 1.6+) to the third generation state management with
    structured streaming (Spark 2.0+).
  prefs: []
  type: TYPE_NORMAL
- en: A modern ML streaming system is a complex continuous application that needs
    to not only combine various ML steps into a pipeline, but also interact with other
    subsystems to provide a real-life useful, end-to-end information system.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we were wrapping up the book, Databricks, the company that empowers the
    Spark community, made the following announcement at Spark Summit West 2017 regarding
    the future direction of Spark streaming (not prod release yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Today, we are excited to propose a new extension,* *continuous processing**,
    that also eliminates micro-batches from execution. As we demonstrated at Spark
    Summit this morning, this new execution mode lets users achieve sub-millisecond
    end-to-end latency for many important workloads - with no change to their Spark
    application."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html](https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a minimum viable streaming system that is the
    foundation of most streaming systems (over simplified for presentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00285.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As seen in the preceding figure, any real-life system must interact with batch
    (for example, offline learning of the model parameters) while the faster subsystem
    concentrates on real-time response to external events (that is, online learning).
  prefs: []
  type: TYPE_NORMAL
- en: Spark's structured streaming full integration with ML library is on the horizon,
    but meanwhile we can create and use streaming DataFrames and streaming Datasets
    to compensate, as will be seen in some of the following recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new structured streaming has several advantages, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Unification of Batch and Stream APIs (no need to translate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming with more concise expressive language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault-tolerant state management (third generation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Significantly simplified programming model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trigger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data stream as a unbounded table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure depicts the basic concepts beyond a data stream being
    modeled as an infinite unbounded table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00286.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The pre-Spark 2.0 paradigm advanced the DStream construct, which modeled the
    stream as a set of discrete data structures (RDDs) that was very difficult to
    deal with when we had late arrivals. The inherent late arrival problem made it
    difficult to build systems that had a real-time chargeback model (very prominent
    in the cloud) due to the uncertainty around the actual charges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the DStream model in a visual way so it can be
    compared accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In comparison, by using the new model, there are fewer concepts that a developer
    needs to worry about and there is no need to translate the code from a batch model
    (often ETL like code) to a real-time stream model.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, due to the timeline and legacy, one must know both models (DStream
    and structured streaming) for a while before all pre-Spark 2.0 code is replaced.
    We found the new structured streaming model particularly simple compared to DStream
    and have tried to demonstrate and highlight the differences in the four introductory
    recipes covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Structured streaming for near real-time machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the new structured streaming paradigm introduced
    in Spark 2.0\. We explore real-time streaming using sockets and structured streaming
    API to vote and tabulate the votes accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: We also explore the newly introduced subsystem by simulating a stream of randomly
    generated votes to pick the most unpopular comic book villain.
  prefs: []
  type: TYPE_NORMAL
- en: There are two distinct programs (`VoteCountStream.scala` and `CountStreamproducer.scala`)
    that make up this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala class to generate voting data onto a client socket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an array containing literal string values of people to vote for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will override the `Threads` class `run` method to randomly simulate
    a vote for a particular villain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a Scala singleton object to accept connections on a defined
    port `9999` and generate voting data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to start up the data generation server, so our streaming example
    can process the streaming vote data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set output level to `ERROR` to reduce Spark''s output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` yielding access to the Spark cluster and underlying
    session object attributes such as the `SparkContext` and `SparkSQLContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Import spark implicits, therefore adding in behavior with only an import:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a streaming DataFrame by connecting to localhost on port `9999`, which
    utilizes a Spark socket source as the source of streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we group streaming data by villain name and count to simulate
    user votes streaming in real time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define a streaming query to trigger every 10 seconds, dump the whole
    result set into the console, and invoke it by calling the `start()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first output batch is displayed here as batch `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00288.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'An additional batch result is displayed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00289.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, wait for termination of the streaming query or stop the process using
    the `SparkSession` API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we created a simple data generation server to simulate a stream
    of voting data and then counted the vote. The following figure provides a high-level
    depiction of this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00290.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: First, we began by executing the data generation server. Second, we defined
    a socket data source, which allows us to connect to the data generation server.
    Third, we constructed a simple Spark expression to group by villain (that is,
    bad superheroes) and count all currently received votes. Finally, we configured
    a threshold trigger of 10 seconds to execute our streaming query, which dumps
    the accumulated results onto the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two short programs involved in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CountStreamproducer.scala`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The producer - data generation server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulates the voting for itself and broadcasts it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VoteCountStream.scala`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consumer - consumes and aggregates/tabulates the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receives and count votes for our villain superhero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic of how to program using Spark streaming and structured streaming in
    Spark is out of scope for this book, but we felt it is necessary to share some
    programs to introduce the concepts before drilling down into ML streaming offering
    for Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a solid introduction to streaming, please consult the following documentation
    on Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Information of Spark 2.0+ structured streaming is available at [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information of Spark 1.6 streaming is available at [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for structured streaming is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for DStream (pre-Spark 2.0) is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `DataStreamReader` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `DataStreamWriter` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `StreamingQuery` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming DataFrames for real-time machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the concept of a streaming DataFrame. We create a
    DataFrame consisting of the name and age of individuals, which we will be streaming
    across a wire. A streaming DataFrame is a popular technique to use with Spark
    ML since we do not have a full integration between Spark structured ML at the
    time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: We limit this recipe to only the extent of demonstrating a streaming DataFrame
    and leave it up to the reader to adapt this to their own custom ML pipelines.
    While streaming DataFrame is not available out of the box in Spark 2.1.0, it will
    be a natural evolution to see it in later versions of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` as an entry point to the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the person data file to infer a data schema without hand coding
    the structure types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now configure a streaming DataFrame for ingestion of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us execute a simple data transform, by filtering on age greater than `60`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We now output the transformed streaming data to the console, which will trigger
    every second:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We start our defined streaming query and wait for data to appear in the stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the result of our streaming query will appear in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00291.gif)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we first discover the underlying schema for a person object
    using a quick method (using a JSON object) as described in step 6\. The resulting
    DataFrame will know the schema that we subsequently impose on the streaming input
    (simulated via streaming a file) and treated as a streaming DataFrame as seen
    in step 7.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to treat the stream as a DataFrame and act on it using a functional
    or SQL paradigm is a powerful concept that can be seen in step 8\. We then proceed
    to output the result using `writestream()` with `append` mode and a 1-second batch
    interval trigger.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The combination of DataFrames and structured programming is a powerful concept
    that helps us to separate the data layer from the stream, which makes the programming
    significantly easier. One of the biggest drawbacks with DStream (pre-Spark 2.0)
    was its inability to isolate the user from details of the underlying details of
    stream/RDD implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation for DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrameReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrameWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for Spark data stream reader and writer:'
  prefs: []
  type: TYPE_NORMAL
- en: DataStreamReader: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataStreamWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Datasets for real-time machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we create a streaming Dataset to demonstrate the use of Datasets
    with a Spark 2.0 structured programming paradigm. We stream stock prices from
    a file using a Dataset and apply a filter to select the day's stock that closed
    above $100.
  prefs: []
  type: TYPE_NORMAL
- en: The recipe demonstrates how streams can be used to filter and to act on the
    incoming data using a simple structured streaming programming model. While it
    is similar to a DataFrame, there are some differences in the syntax. The recipe
    is written in a generalized manner so the user can customize it for their own
    Spark ML programming projects.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala `case class` to model streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `SparkSession` to use as an entry point to the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, load the general electric CSV file inferring the schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following in console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the general electric CSV file into a dataset of type `StockPrice`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will filter the stream for any close price greater than $100 USD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We now output the transformed streaming data to the console that will trigger
    every second:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We start our defined streaming query and wait for data to appear in the stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the result of our streaming query will appear in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00292.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be utilizing the market data of closing prices for **General
    Electric** (**GE**) dating back to 1972\. To simplify the data, we have preprocessed
    for the purposes of this recipe. We use the same method from the previous recipe, *Streaming
    DataFrames for real-time machine learning*, by peeking into the JSON object to
    discover the schema (step 7), which we impose on the stream in step 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to use the schema to make the stream look like
    a simple table that you can read from on the fly. This is a powerful concept that
    makes stream programming accessible to more programmers. The `schema(s.schema)` and
    `as[StockPrice]`from the following code snippet are required to create the streaming
    Dataset, which has a schema associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for all the APIs available under Dataset at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    website[.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following documentation is helpful while exploring the streaming Dataset
    concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamQuery`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data and debugging with queueStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the concept of `queueStream()`*,* which is a valuable
    tool while trying to get a streaming program to work during the development cycle.
    We found the `queueStream()` API very useful and felt that other developers can
    benefit from a recipe that fully demonstrates its usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by simulating a user browsing various URLs associated with different
    web pages using the program `ClickGenerator.scala` and then proceed to consume
    and tabulate the data (user behavior/visits) using the `ClickStream.scala` program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00293.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We use Spark's streaming API with `Dstream()`, which will require the use of
    a streaming context. We are calling this out explicitly to highlight one of the
    differences between Spark streaming and the Spark structured streaming programming
    model.
  prefs: []
  type: TYPE_NORMAL
- en: There are two distinct programs (`ClickGenerator.scala` and `ClickStream.scala`)
    that make up this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala `case class` to model click events by users that contains user
    identifier, IP address, time of event, URL, and HTTP status code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Define status codes for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define URLs for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define IP address range for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Define timestamp range for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Define user identifier range for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to generate one or more pseudo random events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to parse a pseudo random `ClickEvent` from a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark streaming context with 1-second duration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a mutable queue to append our generated data onto:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Spark queue stream from the streaming context passing in a reference
    of our data queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Process any data received by the queue stream and count the total number of
    each particular link users have clicked upon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the `12` URLs and their totals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Start our streaming context to receive micro-batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop 10 times generating 100 pseudo random events on each iteration and append
    them our mutable queue so they materialize in the streaming queue abstraction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this recipe, we introduced Spark Streaming using a technique many overlook,
    which allows us to craft a streaming application utilizing Spark's `QueueInputDStream`
    class. The `QueueInputDStream` class is not only a beneficial tool for understanding
    Spark streaming, but also for debugging during the development cycle. In the beginning
    steps, we set up a few data structures, in order to generate pseudo random `clickstream`
    event data for stream processing at a later stage.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that in step 12, we are creating a streaming context instead
    of a SparkContext. The streaming context is what we use for Spark streaming applications.
    Next, the creation of a queue and queue stream is done to receive streaming data.
    Now steps 15 and 16 resemble a general Spark application manipulating RDDs. The
    next step starts the streaming context processing. After the streaming context
    is started, we append data to the queue and the processing begins with micro-batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation for some of the related topics is mentioned here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamingContext` and `queueStream()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DStream`:[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InputDStream`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At its core, `queueStream()` is just a queue of RDDs that we have after the
    Spark streaming (pre-2.0) turns into RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for structured streaming (Spark 2.0+): [https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for streaming (pre-Spark 2.0): [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and understanding the famous Iris data for unsupervised classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we download and inspect the well-known Iris dataset in preparation
    for the upcoming streaming KMeans recipe, which lets you see classification/clustering
    in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: The data is housed on the UCI machine learning repository, which is a great
    source of data to prototype algorithms on. You will notice that R bloggers tend
    to love this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can start by downloading the dataset using either two of the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first step of data exploration by examining how the data in
    `iris.data` is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take a look at the iris data to know how it is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data is made of 150 observations. Each observation is made of four numerical
    features (measured in centimeters) and a label that signifies which class each
    Iris belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features/attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label/class**:'
  prefs: []
  type: TYPE_NORMAL
- en: Iris Setosa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris Versicolour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris Virginic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image depicts an Iris flower with Petal and Sepal marked for
    clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00294.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following link explores the Iris dataset in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming KMeans for a real-time on-line classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the streaming version of KMeans in Spark used in
    unsupervised learning schemes. The purpose of streaming KMeans algorithm is to
    classify or group a set of data points into a number of clusters based on their
    similarity factor.
  prefs: []
  type: TYPE_NORMAL
- en: There are two implementations of the KMeans classification method, one for static/offline
    data and another version for continuously arriving, real-time updating data.
  prefs: []
  type: TYPE_NORMAL
- en: We will be streaming iris dataset clustering as new data streams into our streaming
    context.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by defining a function to load iris data into memory, filtering out
    blank lines, attaching an identifier to each element, and finally returning tuple
    of type string and long:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a parser to take the string portion of our tuple and create a label
    point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a lookup map to convert the identifier back to the text label feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark streaming context with 1-second duration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the Iris data and build a lookup map to display the final output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Create mutable queues to append streaming data onto:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark streaming queues to receive data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Create streaming KMeans object to cluster data into three groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up KMeans model to accept streaming training data to build a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up KMeans model to predict clustering group values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Start streaming context so it will process data when received:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert Iris data into label points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split label point data into training dataset and test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Append training data to streaming queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we split test data into four groups and append to streaming queues for
    processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The configured streaming queues print out the following results of clustered
    prediction groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we begin by loading the iris dataset and using the `zip()` API
    to pair data with a unique identifier to the data for generating *labeled points* data
    structure for use with the KMeans algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the mutable queues and `QueueInputDStream` are created for appending data
    to simulate streaming. Once the `QueueInputDStream` starts receiving data then
    the streaming k-mean clustering begins to dynamically cluster data and printing
    out results. The interesting thing you will notice here is we are streaming the
    training dataset on one queue stream and the test data on another queue stream.
    As we append data to our queues, the KMeans clustering algorithm is processing
    our incoming data and dynamically generating clusters.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for *StreamingKMeans()*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamingKMeans`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamingKMeansModel`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hyper parameters defined via a builder pattern or `streamingKMeans` are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Please refer to the *Building a KMeans classifying system in Spark* recipe in
    [Chapter 8](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77), *Unsupervised
    Clustering with Apache Spark 2.0 *for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading wine quality data for streaming regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we download and inspect the wine quality dataset from the UCI
    machine learning repository to prepare data for Spark's streaming linear regression
    algorithm from MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    specified data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start by downloading the dataset using either of the following three
    commands. The first one is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'This command is the third way to do the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `winequality-white.csv` is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take a look at the wine quality data to know how it is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data is comprised of 1,599 red wines and 4,898 white wines with 11 features
    and an output label that can be used during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of features/attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatile acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citric acid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual sugar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chlorides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sulphates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alcohol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the output label:'
  prefs: []
  type: TYPE_NORMAL
- en: quality (a numeric value between 0 to 10)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following link lists datasets for popular machine learning algorithms. A
    new dataset can be chosen to experiment with as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative datasets are available at [https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research).
  prefs: []
  type: TYPE_NORMAL
- en: We selected the Iris dataset so we can use continuous numerical features for
    a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming linear regression for a real-time regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the wine quality dataset from UCI and Spark's streaming
    linear regression algorithm from MLlib to predict the quality of a wine based
    on a group of wine features.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between this recipe and the traditional regression recipes we
    saw before is the use of Spark ML streaming to score the quality of the wine in
    real time using a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the wine quality CSV using the Databricks CSV API into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the DataFrame into an `rdd` and `zip` a unique identifier onto it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a lookup map to compare predicted quality against actual quality value
    later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert wine quality into label points for use with the machine learning library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a mutable queue for appending data to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark streaming queues to receive streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure streaming linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Train regression model and predict final values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Split label point data into training set and test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Append data to training data queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split test data in half and append to queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Once data is received by the queue stream, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00295.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Close the program by stopping the Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by loading the wine quality dataset into a DataFrame via Databrick's
    `spark-csv` library. The next step was to attach a unique identifier to each row
    in our dataset to later match the predicted quality to the actual quality. The
    raw data was converted to labeled points so it can be used as input for the streaming
    linear regression algorithm. In steps 9 and 10, we created instances of mutable
    queues and Spark's `QueueInputDStream` class to be used as a conduit into the
    regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We then created the streaming linear regression model, which will predict wine
    quality for our final results. We customarily created training and test datasets
    from the original data and appended them to the appropriate queue to start our
    model processing streaming data. The final results for each micro-batch displays
    the unique generated identifier, predicted quality value, and quality value contained
    in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `StreamingLinearRegressionWithSGD()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyper parameters for `StreamingLinearRegressionWithSGD()`*:*
  prefs: []
  type: TYPE_NORMAL
- en: '`setInitialWeights(Vectors.*zeros*())`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setNumIterations()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setStepSize()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMiniBatchFraction()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also a `StreamingLinearRegression()` API that does not use the **stochastic
    gradient descent** (**SGD**) version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following link provides a quick reference for linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Pima Diabetes data for supervised classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we download and inspect the Pima Diabetes dataset from the UCI
    machine learning repository. We will use the dataset later with Spark's streaming
    logistic regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    the specified data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start by downloading the dataset using either two of the following
    commands. The first command is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an alternative that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `pima-indians-diabetes.data` is formatted (from Mac or Linux Terminal):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take a look at the Pima Diabetes data to understand how it is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have 768 observations for the dataset. Each line/record is comprised of 10
    features and a label value that can used for a supervised learning model (that
    is, logistic regression). The label/class is either a `1`, meaning tested positive
    for diabetes, and `0` if the test came back negative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Features/Attributes:**'
  prefs: []
  type: TYPE_NORMAL
- en: Number of times pregnant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diastolic blood pressure (mm Hg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triceps skin fold thickness (mm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2-hour serum insulin (mu U/ml)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Body mass index (weight in kg/(height in m)^2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diabetes pedigree function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age (years)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class variable (0 or 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We found the following alternative datasets from Princeton University very
    helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.princeton.edu/wws509/datasets](http://data.princeton.edu/wws509/datasets)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that you can use to explore this recipe has to be structured in
    a way that the label (prediction class) has to be binary (tested positive/negative
    for diabetes).
  prefs: []
  type: TYPE_NORMAL
- en: Streaming logistic regression for an on-line classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be using the Pima Diabetes dataset we downloaded in
    the previous recipe and Spark's streaming logistic regression algorithm with SGD
    to predict whether a Pima with various features will test positive as a diabetic.
    It is an on-line classifier that learns and predicts based on the streamed data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` object as an entry point to the cluster and a `StreamingContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the Pima data file into a Dataset of type string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a RDD from our raw Dataset by generating a tuple consisting of the last
    item into a record as the label and everything else as a sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the preprocessed data into label points for use with the machine learning
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Create mutable queues for appending data to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark streaming queues to receive streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the streaming logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the regression model and predict final values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Split label point data into training set and test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Append data to training data queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split test data in half and append to the queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Once data is received by the queue stream, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the program by stopping the Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we loaded the Pima Diabetes Dataset into a Dataset and parsed it into
    a tuple by taking every element as a feature except the last one, which we used
    as a label. Second, we morphed the RDD of tuples into labeled points so it can
    be used as input to the streaming logistic regression algorithm. Third, we created
    instances of mutable queues and Spark's `QueueInputDStream` class to be used as
    a pathway into the logistic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, we created the streaming logistic regression model, which will predict
    wine quality for our final results. Finally, we customarily created training and
    test datasets from original data and appended it to the appropriate queue to trigger
    the model's processing of streaming data. The final results for each micro-batch
    displays the original label and predicted label of 1.0 for testing true positive
    as a diabetic or 0.0 as true negative.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `StreamingLogisticRegressionWithSGD()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)
  prefs: []
  type: TYPE_NORMAL
- en: "[\uFEFF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)"
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hyper parameters for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setInitialWeights()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setNumIterations()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setStepSize()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMiniBatchFraction()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

- en: Chapter 1. Getting Started with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will set up Spark and configure it. This chapter is divided
    into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark from binaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the Spark source code with Maven
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching Spark on Amazon EC2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster in standalone mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster with Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster with YARN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Tachyon as an off-heap storage layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a general-purpose cluster computing system to process big data
    workloads. What sets Spark apart from its predecessors, such as MapReduce, is
    its speed, ease-of-use, and sophisticated analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark was originally developed at AMPLab, UC Berkeley, in 2009\. It was
    made open source in 2010 under the BSD license and switched to the Apache 2.0
    license in 2013\. Toward the later part of 2013, the creators of Spark founded
    Databricks to focus on Spark's development and future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Talking about speed, Spark can achieve sub-second latency on big data workloads.
    To achieve such low latency, Spark makes use of the memory for storage. In MapReduce,
    memory is primarily used for actual computation. Spark uses memory both to compute
    and store objects.
  prefs: []
  type: TYPE_NORMAL
- en: Spark also provides a unified runtime connecting to various big data storage
    sources, such as HDFS, Cassandra, HBase, and S3\. It also provides a rich set
    of higher-level libraries for different big data compute tasks, such as machine
    learning, SQL processing, graph processing, and real-time streaming. These libraries
    make development faster and can be combined in an arbitrary fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Though Spark is written in Scala, and this book only focuses on recipes in Scala,
    Spark also supports Java and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is an open source community project, and everyone uses the pure open source
    Apache distributions for deployments, unlike Hadoop, which has multiple distributions
    available with vendor enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the Spark ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Spark runtime runs on top of a variety of cluster managers, including YARN
    (Hadoop's compute framework), Mesos, and Spark's own cluster manager called **standalone
    mode**. Tachyon is a memory-centric distributed file system that enables reliable
    file sharing at memory speed across cluster frameworks. In short, it is an off-heap
    storage layer in memory, which helps share data across jobs and users. Mesos is
    a cluster manager, which is evolving into a data center operating system. YARN
    is Hadoop's compute framework that has a robust resource management feature that
    Spark can seamlessly use.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark from binaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can be either built from the source code or precompiled binaries can be
    downloaded from [http://spark.apache.org](http://spark.apache.org). For a standard
    use case, binaries are good enough, and this recipe will focus on installing Spark
    using binaries.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the recipes in this book are developed using Ubuntu Linux but should work
    fine on any POSIX environment. Spark expects Java to be installed and the `JAVA_HOME`
    environment variable to be set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Linux/Unix systems, there are certain standards for the location of files
    and directories, which we are going to follow in this book. The following is a
    quick cheat sheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Directory | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `/bin` | Essential command binaries |'
  prefs: []
  type: TYPE_TB
- en: '| `/etc` | Host-specific system configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `/opt` | Add-on application software packages |'
  prefs: []
  type: TYPE_TB
- en: '| `/var` | Variable data |'
  prefs: []
  type: TYPE_TB
- en: '| `/tmp` | Temporary files |'
  prefs: []
  type: TYPE_TB
- en: '| `/home` | User home directories |'
  prefs: []
  type: TYPE_TB
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing this, Spark's current version is 1.4\. Please check the
    latest version from Spark's download page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Binaries are developed with a most recent and stable version of Hadoop. To use
    a specific version of Hadoop, the recommended approach is to build from sources,
    which will be covered in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the installation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal and download binaries using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Unpack binaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Rename the folder containing binaries by stripping the version information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the configuration folder to the `/etc` folder so that it can be made a
    symbolic link later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create your company-specific installation directory under `/opt`. As the recipes
    in this book are tested on `infoobjects` sandbox, we are going to use `infoobjects`
    as directory name. Create the `/opt/infoobjects` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the `spark` directory to `/opt/infoobjects` as it''s an add-on software
    package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the ownership of the `spark` home directory to `root`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Change permissions of the `spark` home directory, `0755 = user:read-write-execute
    group:read-execute world:read-execute`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Move to the `spark` home directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the symbolic link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Append to `PATH` in `.bashrc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Open a new terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `log` directory in `/var`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Make `hduser` the owner of the Spark `log` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Spark `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure Spark with the help of the following command lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Building the Spark source code with Maven
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Installing Spark using binaries works fine in most cases. For advanced cases,
    such as the following (but not limited to), compiling from the source code is
    a better option:'
  prefs: []
  type: TYPE_NORMAL
- en: Compiling for a specific Hadoop version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding the Hive integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding the YARN integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the prerequisites for this recipe to work:'
  prefs: []
  type: TYPE_NORMAL
- en: Java 1.6 or a later version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven 3.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the steps to build the Spark source code with Maven:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase `MaxPermSize` for heap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a new terminal window and download the Spark source code from GitHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Unpack the archive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Move to the `spark` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the sources with these flags: Yarn enabled, Hadoop version 2.4, Hive
    enabled, and skipping tests for faster compilation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the `conf` folder to the `etc` folder so that it can be made a symbolic
    link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the `spark` directory to `/opt` as it''s an add-on software package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the ownership of the `spark` home directory to `root`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the permissions of the `spark` home directory `0755 = user:rwx group:r-x
    world:r-x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Move to the `spark` home directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a symbolic link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Put the Spark executable in the path by editing `.bashrc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `log` directory in `/var`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Make `hduser` the owner of the Spark `log` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Spark `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure Spark with the help of the following command lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Launching Spark on Amazon EC2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Elastic Compute Cloud** (**Amazon EC2**) is a web service that provides
    resizable compute instances in the cloud. Amazon EC2 provides the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: On-demand delivery of IT resources via the Internet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provision of as many instances as you like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Payment for the hours you use instances like your utility bill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No setup cost, no installation, and no overhead at all
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you no longer need instances, you either shut down or terminate and walk
    away
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of these instances on all familiar operating systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EC2 provides different types of instances to meet all compute needs, such as
    general-purpose instances, micro instances, memory-optimized instances, storage-optimized
    instances, and others. They have a free tier of micro-instances to try.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `spark-ec2` script comes bundled with Spark and makes it easy to launch,
    manage, and shut down clusters on Amazon EC2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you start, you need to do the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Amazon AWS account ([http://aws.amazon.com](http://aws.amazon.com)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Security Credentials** under your account name in the top-right corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Access Keys** and **Create New Access Key**:![Getting ready](img/3056_01_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note down the access key ID and secret access key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now go to **Services** | **EC2**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Key Pairs** in left-hand menu under NETWORK & SECURITY.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create Key Pair** and enter `kp-spark` as key-pair name:![Getting
    ready](img/3056_01_15.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the private key file and copy it in the `/home/hduser/keypairs folder`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set permissions on key file to `600`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set environment variables to reflect access key ID and secret access key (please
    replace sample values with your own values):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark comes bundled with scripts to launch the Spark cluster on Amazon EC2\.
    Let''s launch the cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the cluster with the example value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`<key-pair>`: This is the name of EC2 key-pair created in AWS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<key-file>`: This is the private key file you downloaded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<num-slaves>`: This is the number of slave nodes to launch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<cluster-name>`: This is the name of the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sometimes, the default availability zones are not available; in that case,
    retry sending the request by specifying the specific availability zone you are
    requesting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If your application needs to retain data after the instance shuts down, attach
    EBS volume to it (for example, a 10 GB space):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If you use Amazon spot instances, here''s the way to do it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spot instances allow you to name your own price for Amazon EC2 computing capacity.
    You simply bid on spare Amazon EC2 instances and run them whenever your bid exceeds
    the current spot price, which varies in real-time based on supply and demand (source:
    [amazon.com](http://amazon.com)).'
  prefs: []
  type: TYPE_NORMAL
- en: After everything is launched, check the status of the cluster by going to the
    web UI URL that will be printed at the end.![How to do it...](img/3056_01_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the status of the cluster:![How to do it...](img/3056_01_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, to access the Spark cluster on EC2, let''s connect to the master node
    using **secure shell protocol** (**SSH**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Check directories in the master node and see what they do:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Directory | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `ephemeral-hdfs` | This is the Hadoop instance for which data is ephemeral
    and gets deleted when you stop or restart the machine. |'
  prefs: []
  type: TYPE_TB
- en: '| `persistent-hdfs` | Each node has a very small amount of persistent storage
    (approximately 3 GB). If you use this instance, data will be retained in that
    space. |'
  prefs: []
  type: TYPE_TB
- en: '| `hadoop-native` | These are native libraries to support Hadoop, such as snappy
    compression libraries. |'
  prefs: []
  type: TYPE_TB
- en: '| `Scala` | This is Scala installation. |'
  prefs: []
  type: TYPE_TB
- en: '| `shark` | This is Shark installation (Shark is no longer supported and is
    replaced by Spark SQL). |'
  prefs: []
  type: TYPE_TB
- en: '| `spark` | This is Spark installation |'
  prefs: []
  type: TYPE_TB
- en: '| `spark-ec2` | These are files to support this cluster deployment. |'
  prefs: []
  type: TYPE_TB
- en: '| `tachyon` | This is Tachyon installation |'
  prefs: []
  type: TYPE_TB
- en: 'Check the HDFS version in an ephemeral instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the HDFS version in persistent instance with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the configuration level in logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The default log level information is too verbose, so let's change it to Error:![How
    to do it...](img/3056_01_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `log4.properties` file by renaming the template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Open `log4j.properties` in vi or your favorite editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Change second line from `| log4j.rootCategory=INFO, console` to `| log4j.rootCategory=ERROR,
    console`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy the configuration to all slave nodes after the change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Destroy the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[http://aws.amazon.com/ec2](http://aws.amazon.com/ec2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying on a cluster in standalone mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compute resources in a distributed environment need to be managed so that resource
    utilization is efficient and every job gets a fair chance to run. Spark comes
    along with its own cluster manager conveniently called **standalone mode**. Spark
    also supports working with YARN and Mesos cluster managers.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster manager that should be chosen is mostly driven by both legacy concerns
    and whether other frameworks, such as MapReduce, are sharing the same compute
    resource pool. If your cluster has legacy MapReduce jobs running, and all of them
    cannot be converted to Spark jobs, it is a good idea to use YARN as the cluster
    manager. Mesos is emerging as a data center operating system to conveniently manage
    jobs across frameworks, and is very compatible with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: If the Spark framework is the only framework in your cluster, then standalone
    mode is good enough. As Spark evolves as technology, you will see more and more
    use cases of Spark being used as the standalone framework serving all big data
    compute needs. For example, some jobs may be using Apache Mahout at present because
    MLlib does not have a specific machine-learning library, which the job needs.
    As soon as MLlib gets this library, this particular job can be moved to Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider a cluster of six nodes as an example setup: one master and
    five slaves (replace them with actual node names in your cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Spark''s standalone mode is the default, all you need to do is to have
    Spark binaries installed on both master and slave machines. Put `/opt/infoobjects/spark/sbin`
    in path on every node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the standalone master server (SSH to master first):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Master, by default, starts on port 7077, which slaves use to connect to it.
    It also has a web UI at port 8088.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please SSH to master node and start slaves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '| Argument (for fine-grained configuration, the following parameters work with
    both master and slaves) | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `-i <ipaddress>,-ip <ipaddress>` | IP address/DNS service listens on |'
  prefs: []
  type: TYPE_TB
- en: '| `-p <port>, --port <port>` | Port service listens on |'
  prefs: []
  type: TYPE_TB
- en: '| `--webui-port <port>` | Port for web UI (by default, 8080 for master and
    8081 for worker) |'
  prefs: []
  type: TYPE_TB
- en: '| `-c <cores>,--cores <cores>` | Total CPU cores Spark applications that can
    be used on a machine (worker only) |'
  prefs: []
  type: TYPE_TB
- en: '| `-m <memory>,--memory <memory>` | Total RAM Spark applications that can be
    used on a machine (worker only) |'
  prefs: []
  type: TYPE_TB
- en: '| `-d <dir>,--work-dir <dir>` | The directory to use for scratch space and
    job output logs |'
  prefs: []
  type: TYPE_TB
- en: Rather than manually starting master and slave daemons on each node, it can
    also be accomplished using cluster launch scripts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, create the `conf/slaves` file on a master node and add one line per
    slave hostname (using an example of five slaves nodes, replace with the DNS of
    slave nodes in your cluster):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the slave machine is set up, you can call the following scripts to start/stop
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Script name | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `start-master.sh` | Starts a master instance on the host machine |'
  prefs: []
  type: TYPE_TB
- en: '| `start-slaves.sh` | Starts a slave instance on each node in the slaves file
    |'
  prefs: []
  type: TYPE_TB
- en: '| `start-all.sh` | Starts both master and slaves |'
  prefs: []
  type: TYPE_TB
- en: '| `stop-master.sh` | Stops the master instance on the host machine |'
  prefs: []
  type: TYPE_TB
- en: '| `stop-slaves.sh` | Stops the slave instance on all nodes in the slaves file
    |'
  prefs: []
  type: TYPE_TB
- en: '| `stop-all.sh` | Stops both master and slaves |'
  prefs: []
  type: TYPE_TB
- en: 'Connect an application to the cluster through the Scala code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect to the cluster through Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In standalone mode, Spark follows the master slave architecture, very much like
    Hadoop, MapReduce, and YARN. The compute master daemon is called **Spark master**
    and runs on one master node. Spark master can be made highly available using ZooKeeper.
    You can also add more standby masters on the fly, if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compute slave daemon is called **worker** and is on each slave node. The
    worker daemon does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reports the availability of compute resources on a slave node, such as the number
    of cores, memory, and others, to Spark master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spawns the executor when asked to do so by Spark master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restarts the executor if it dies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is, at most, one executor per application per slave machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both Spark master and worker are very lightweight. Typically, memory allocation
    between 500 MB to 1 GB is sufficient. This value can be set in `conf/spark-env.sh`
    by setting the `SPARK_DAEMON_MEMORY` parameter. For example, the following configuration
    will set the memory to 1 gigabits for both master and worker daemon. Make sure
    you have `sudo` as the super user before running it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, each slave node has one worker instance running on it. Sometimes,
    you may have a few machines that are more powerful than others. In that case,
    you can spawn more than one worker on that machine by the following configuration
    (only on those machines):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark worker, by default, uses all cores on the slave machine for its executors.
    If you would like to limit the number of cores the worker can use, you can set
    it to that number (for example, 12) by the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark worker, by default, uses all the available RAM (1 GB for executors).
    Note that you cannot allocate how much memory each specific executor will use
    (you can control this from the driver configuration). To assign another value
    for the total memory (for example, 24 GB) to be used by all executors combined,
    execute the following setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some settings you can do at the driver level:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify the maximum number of CPU cores to be used by a given application
    across the cluster, you can set the `spark.cores.max` configuration in Spark submit
    or Spark shell as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'To specify the amount of memory each executor should be allocated (the minimum
    recommendation is 8 GB), you can set the `spark.executor.memory` configuration
    in Spark submit or Spark shell as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram depicts the high-level architecture of a Spark cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3056_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/spark-standalone.html](http://spark.apache.org/docs/latest/spark-standalone.html)
    to find more configuration options'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying on a cluster with Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mesos is slowly emerging as a data center operating system to manage all compute
    resources across a data center. Mesos runs on any computer running the Linux operating
    system. Mesos is built using the same principles as Linux kernel. Let's see how
    we can install Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mesosphere provides a binary distribution of Mesos. The most recent package
    for the Mesos distribution can be installed from the Mesosphere repositories by
    performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute Mesos on Ubuntu OS with the trusty version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Mesos:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: To connect Spark to Mesos to integrate Spark with Mesos, make Spark binaries
    available to Mesos and configure the Spark driver to connect to Mesos.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Spark binaries from the first recipe and upload to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The master URL for single master Mesos is `mesos://host:5050`, and for the ZooKeeper
    managed Mesos cluster, it is `mesos://zk://host:2181`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the following variables in `spark-env.sh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Run from the Scala program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Run from the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mesos has two run modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-grained**: In fine-grained (default) mode, every Spark task runs as
    a separate Mesos task'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coarse-grained**: This mode will launch only one long-running Spark task
    on each Mesos machine'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run in the coarse-grained mode, set the `spark.mesos.coarse` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Deploying on a cluster with YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Yet another resource negotiator** (**YARN**) is Hadoop''s compute framework
    that runs on top of HDFS, which is Hadoop''s storage layer.'
  prefs: []
  type: TYPE_NORMAL
- en: YARN follows the master slave architecture. The master daemon is called `ResourceManager`
    and the slave daemon is called `NodeManager`. Besides this application, life cycle
    management is done by `ApplicationMaster`, which can be spawned on any slave node
    and is alive for the lifetime of an application.
  prefs: []
  type: TYPE_NORMAL
- en: When Spark is run on YARN, `ResourceManager` performs the role of Spark master
    and `NodeManagers` work as executor nodes.
  prefs: []
  type: TYPE_NORMAL
- en: While running Spark with YARN, each Spark executor is run as YARN container.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running Spark on YARN requires a binary distribution of Spark that has YARN
    support. In both Spark installation recipes, we have taken care of it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run Spark on YARN, the first step is to set the configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see this in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command launches YARN Spark in the `yarn-client` mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command launches Spark shell in the `yarn-client` mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The command to launch in the `yarn-cluster` mode is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark applications on YARN run in two modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`yarn-client`: Spark Driver runs in the client process outside of YARN cluster,
    and `ApplicationMaster` is only used to negotiate resources from ResourceManager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yarn-cluster`: Spark Driver runs in `ApplicationMaster` spawned by `NodeManager`
    on a slave node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `yarn-cluster` mode is recommended for production deployments, while the
    y`arn-client` mode is good for development and debugging when you would like to
    see immediate output. There is no need to specify Spark master in either mode
    as it's picked from the Hadoop configuration, and the master parameter is either
    `yarn-client` or `yarn-cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how Spark is run with YARN in the client mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows how Spark is run with YARN in the cluster mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the YARN mode, the following configuration parameters can be set:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--num-executors`: Configure how many executors will be allocated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--executor-memory`: RAM per executor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--executor-cores`: CPU cores per executor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Tachyon as an off-heap storage layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark RDDs are a great way to store datasets in memory while ending up with
    multiple copies of the same data in different applications. Tachyon solves some
    of the challenges with Spark RDD management. A few of them are:'
  prefs: []
  type: TYPE_NORMAL
- en: RDD only exists for the duration of the Spark application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same process performs the compute and RDD in-memory storage; so, if a process
    crashes, in-memory storage also goes away
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different jobs cannot share an RDD even if they are for the same underlying
    data, for example, an HDFS block that leads to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow writes to disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplication of data in memory, higher memory footprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the output of one application needs to be shared with the other application,
    it's slow due to the replication in the disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tachyon provides an off-heap memory layer to solve these problems. This layer,
    being off-heap, is immune to process crashes and is also not subject to garbage
    collection. This also lets RDDs be shared across applications and outlive a specific
    job or session; in essence, one single copy of data resides in memory, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Tachyon as an off-heap storage layer](img/3056_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s download and compile Tachyon (Tachyon, by default, comes configured
    for Hadoop 1.0.4, so it needs to be compiled from sources for the right Hadoop
    version). Replace the version with the current version. The current version at
    the time of writing this book is 0.6.4:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Unarchive the source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the version from the `tachyon` source folder name for convenience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the directory to the `tachyon` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Comment the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Uncomment the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the following properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Replace `${tachyon.home}` with `/var/log/tachyon`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new `core-site.xml` file in the `conf` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `<tachyon home>/bin` to the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the shell and format Tachyon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Tachyon''s web interface is `http://hostname:19999`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3056_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Run the sample program to see whether Tachyon is running fine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it...](img/3056_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can stop Tachyon any time by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Run Spark on Tachyon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf](http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf)
    to learn about the origins of Tachyon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.tachyonnexus.com](http://www.tachyonnexus.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

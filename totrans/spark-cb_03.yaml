- en: Chapter 3. External Data Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the strengths of Spark is that it provides a single runtime that can
    connect with various underlying data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will connect to different data sources. This chapter is
    divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from the local filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from HDFS using a custom InputFormat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from Apache Cassandra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from relational databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides a unified runtime for big data. HDFS, which is Hadoop's filesystem,
    is the most used storage platform for Spark as it provides cost-effective storage
    for unstructured and semi-structured data on commodity hardware. Spark is not
    limited to HDFS and can work with any Hadoop-supported storage.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop supported storage means a storage format that can work with Hadoop's
    `InputFormat` and `OutputFormat` interfaces. `InputFormat` is responsible for
    creating `InputSplits` from input data and dividing it further into records. `OutputFormat`
    is responsible for writing to storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with writing to the local filesystem and then move over to loading
    data from HDFS. In the *Loading data from HDFS* recipe, we will cover the most
    common file format: regular text files. In the next recipe, we will cover how
    to use any `InputFormat` interface to load data in Spark. We will also explore
    loading data stored in Amazon S3, a leading cloud storage platform.'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore loading data from Apache Cassandra, which is a NoSQL database.
    Finally, we will explore loading data from a relational database.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from the local filesystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though the local filesystem is not a good fit to store big data due to disk
    size limitations and lack of distributed nature, technically you can load data
    in distributed systems using the local filesystem. But then the file/directory
    you are accessing has to be available on each node.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that if you are planning to use this feature to load side data,
    it is not a good idea. To load side data, Spark has a broadcast variable feature,
    which will be discussed in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will look at how to load data in Spark from the local filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the example of Shakespeare''s "to be or not to be":'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `words` directory by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Get into the `words` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `sh.txt` text file and enter `"to be or not to be"` in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `words` directory as RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the line (or lines) into multiple words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert `word` to (word,1)—that is, output `1` as the value for each occurrence
    of `word` as a key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `reduceByKey` method to add the number of occurrences for each word
    as a key (this function works on two consecutive values at a time, represented
    by `a` and `b`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Doing all of the preceding operations in one step is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B03056_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Loading data from HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HDFS is the most widely used big data storage system. One of the reasons for
    the wide adoption of HDFS is schema-on-read. What this means is that HDFS does
    not put any restriction on data when data is being written. Any and all kinds
    of data are welcome and can be stored in a raw format. This feature makes it ideal
    storage for raw unstructured data and semi-structured data.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to reading data, even unstructured data needs to be given some
    structure to make sense. Hadoop uses `InputFormat` to determine how to read the
    data. Spark provides complete support for Hadoop's `InputFormat` so anything that
    can be read by Hadoop can be read by Spark as well.
  prefs: []
  type: TYPE_NORMAL
- en: The default `InputFormat` is `TextInputFormat`. `TextInputFormat` takes the
    byte offset of a line as a key and the content of a line as a value. Spark uses
    the `sc.textFile` method to read using `TextInputFormat`. It ignores the byte
    offset and creates an RDD of strings.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the filename itself contains useful information, for example, time-series
    data. In that case, you may want to read each file separately. The `sc.wholeTextFiles`
    method allows you to do that. It creates an RDD with the filename and path (for
    example, `hdfs://localhost:9000/user/hduser/words`) as a key and the content of
    the whole file as the value.
  prefs: []
  type: TYPE_NORMAL
- en: Spark also supports reading various serialization and compression-friendly formats
    such as Avro, Parquet, and JSON using DataFrames. These formats will be covered
    in coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will look at how to load data in the Spark shell from HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s do the word count, which counts the number of occurrences of each word.
    In this recipe, we will load data from HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `words` directory by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the directory to `words`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `sh.txt text` file and enter `"to be or not to be"` in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `words` directory as the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `sc.textFile` method also supports passing an additional argument for the
    number of partitions. By default, Spark creates one partition for each `InputSplit`
    class, which roughly corresponds to one block.
  prefs: []
  type: TYPE_NORMAL
- en: You can ask for a higher number of partitions. It works really well for compute-intensive
    jobs such as in machine learning. As one partition cannot contain more than one
    block, having fewer partitions than blocks is not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Count the number of lines (the result will be `1`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the line (or lines) into multiple words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert word to (word,1)—that is, output `1` as a value for each occurrence
    of `word` as a key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `reduceByKey` method to add the number of occurrences of each word
    as a key (this function works on two consecutive values at a time, represented
    by `a` and `b`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Doing all of the preceding operations in one step is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B03056_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we need to access the whole file at once. Sometimes the filename contains
    useful data like in the case of time-series. Sometimes you need to process more
    than one line as a record. `sparkContext.wholeTextFiles` comes to the rescue here.
    We will look at weather dataset from [ftp://ftp.ncdc.noaa.gov/pub/data/noaa/](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what a top-level directory looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B03056_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking into a particular year directory—for example, 1901 resembles the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B03056_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data here is divided in such a way that each filename contains useful information,
    that is, USAF-WBAN-year, where USAF is the US air force station number and WBAN
    is the weather bureau army navy location number.
  prefs: []
  type: TYPE_NORMAL
- en: You will also notice that all files are compressed as gzip with a `.gz` extension.
    Compression is handled automatically so all you need to do is to upload data in
    HDFS. We will come back to this dataset in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the whole dataset is not large, it can be uploaded in HDFS in the pseudo-distributed
    mode also:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the weather data in HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Load weather data for 1901 in the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Cache weather in the RDD so that it is not recomputed every time it''s accessed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Spark, there are various StorageLevels at which the RDD can be persisted.
    `rdd.cache` is a shorthand for the `rdd.persist(MEMORY_ONLY)` StorageLevel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Count the number of elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the whole contents of a file are loaded as an element, we need to manually
    interpret the data, so let''s load the first element:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the value of the first RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `firstElement` contains tuples in the form (string, string). Tuples can
    be accessed in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a positional function starting with `_1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `productElement` method, for example, `tuple.productElement(0)`. Indexes
    here start with `0` like most other methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Split `firstValue` by lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of elements in `firstVals`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The schema of weather data is very rich with the position of the text working
    as a delimiter. You can get more information about schemas at the national weather
    service website. Let''s get wind speed, which is from section 66-69 (in meter/sec):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Loading data from HDFS using a custom InputFormat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes you need to load data in a specific format and `TextInputFormat`
    is not a good fit for that. Spark provides two methods for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sparkContext.hadoopFile`: This supports the old MapReduce API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparkContext.newAPIHadoopFile`: This supports the new MapReduce API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two methods provide support for all of Hadoop's built-in InputFormats
    interfaces as well as any custom `InputFormat`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to load text data in key-value format and load it in Spark using
    `KeyValueTextInputFormat`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `currency` directory by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the current directory to `currency`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `na.txt` text file and enter currency values in key-value format
    delimited by tab (key: country, value: currency):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You can create more files for each continent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upload the `currency` folder to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Import statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `currency` directory as the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert it from tuple of (Text,Text) to tuple of (String,String):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of elements in the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it...](img/B03056_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use this approach to load data in any Hadoop-supported `InputFormat`
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon **Simple Storage Service** (**S3**) provides developers and IT teams
    with a secure, durable, and scalable storage platform. The biggest advantage of
    Amazon S3 is that there is no up-front IT investment and companies can build capacity
    (just by clicking a button a button) as they need.
  prefs: []
  type: TYPE_NORMAL
- en: Though Amazon S3 can be used with any compute platform, it integrates really
    well with Amazon's cloud services such as Amazon **Elastic Compute Cloud** (**EC2**)
    and Amazon **Elastic Block Storage** (**EBS**). For this reason, companies who
    use **Amazon Web Services** (**AWS**) are likely to have significant data is already
    stored on Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: This makes a good case for loading data in Spark from Amazon S3 and that is
    exactly what this recipe is about.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the AWS portal:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [http://aws.amazon.com](http://aws.amazon.com) and log in with your username
    and password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once logged in, navigate to **Storage & Content Delivery** | **S3** | **Create
    Bucket**:![How to do it...](img/B03056_03_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the bucket name—for example, `com.infoobjects.wordcount`. Please make
    sure you enter a unique bucket name (no two S3 buckets can have the same name
    globally).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Region**, click on **Create**, and then on the bucket name you created
    and you will see the following screen:![How to do it...](img/B03056_03_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create Folder** and enter `words` as the folder name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `sh.txt` text file on the local filesystem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to **Words** | **Upload** | **Add Files** and choose `sh.txt` from
    the dialog box, as shown in the following screenshot:![How to do it...](img/B03056_03_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Start Upload**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **sh.txt** and click on **Properties** and it will show you details of
    the file:![How to do it...](img/B03056_03_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `AWS_ACCESS_KEY` and `AWS_SECRET_ACCESS_KEY` as environment variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the Spark shell and load the `words` directory from `s3` in the `words`
    RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now the RDD is loaded and you can continue doing regular transformations and
    actions on the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes there is confusion between `s3://` and `s3n://`. `s3n://` means a
    regular file sitting in the S3 bucket but readable and writable by the outside
    world. This filesystem puts a 5 GB limit on the file size.
  prefs: []
  type: TYPE_NORMAL
- en: '`s3://` means an HDFS file sitting in the S3 bucket. It is a block-based filesystem.
    The filesystem requires you to dedicate a bucket for this filesystem. There is
    no limit on file size in this system.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from Apache Cassandra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Cassandra is a NoSQL database with a masterless ring cluster structure.
    While HDFS is a good fit for streaming data access, it does not work well with
    random access. For example, HDFS will work well when your average file size is
    100 MB and you want to read the whole file. If you frequently access the *n*th
    line in a file or some other part as a record, HDFS would be too slow.
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases have traditionally provided a solution to that, providing
    low latency, random access, but they do not work well with big data. NoSQL databases
    such as Cassandra fill the gap by providing relational database type access but
    in a distributed architecture on commodity servers.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will load data from Cassandra as a Spark RDD. To make that
    happen Datastax, the company behind Cassandra, has contributed `spark-cassandra-connector`.
    This connector lets you load Cassandra tables as Spark RDDs, write Spark RDDs
    back to Cassandra, and execute CQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to load data from Cassandra:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a keyspace named `people` in Cassandra using the CQL shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a column family (from CQL 3.0 onwards, it can also be called a **table**)
    `person` in newer versions of Cassandra:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert a few records in the column family:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Add Cassandra connector dependency to SBT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also add the Cassandra dependency to Maven:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can also download the `spark-cassandra-connector` JAR to
    use directly with the Spark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to build the `uber` JAR with all dependencies, refer to the
    *There's more…* section.
  prefs: []
  type: TYPE_NORMAL
- en: Now start the Spark shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the `spark.cassandra.connection.host` property in the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Import Cassandra-specific libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `person` column family as an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of records in the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Print data in the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the first row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Cassandra can also be accessed through Spark SQL. It has a wrapper around `SQLContext`
    called `CassandraSQLContext`; let''s load it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `person` data as `SchemaRDD`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the `person` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark Cassandra's connector library has a lot of dependencies. The connector
    itself and several of its dependencies are third-party to Spark and are not available
    as part of the Spark installation.
  prefs: []
  type: TYPE_NORMAL
- en: These dependencies need to be made available to the driver as well as executors
    at runtime. One way to do this is to bundle all transitive dependencies, but that
    is a laborious and error-prone process. The recommended approach is to bundle
    all the dependencies along with the connector library. This will result in a fat
    JAR, popularly known as the `uber` JAR.
  prefs: []
  type: TYPE_NORMAL
- en: 'SBT provides the `sbt-assembly` plugin, which makes creating `uber` JARs very
    easy. The following are the steps to create an `uber` JAR for `spark-cassandra-connector`.
    These steps are general enough so that you can use them to create any `uber` JAR:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a folder named `uber`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the directory to `uber`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the SBT prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Give this project a name `sc-uber`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create `build.sbt`, `project`, and `target` folders in the `uber`
    folder as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/B03056_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Add the `spark-cassandra-driver` dependency to `build.sbt` at the end after
    leaving a blank line as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/B03056_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will use `MergeStrategy.first` as the default. Besides that, there are some
    files, such as `manifest.mf`, that every JAR bundles for metadata, and we can
    simply discard them. We are going to use `MergeStrategy.discard` for that. The
    following is the screenshot of `build.sbt` with `assemblyMergeStrategy` added:![There's
    more...](img/B03056_03_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now create `plugins.sbt` in the `project` folder and type the following for
    the `sbt-assembly` plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to build (`assembly`) a JAR now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `uber` JAR is now created in `target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy it to a suitable location where you keep all third-party JARs—for example,
    `/home/hduser/thirdparty`—and rename it to an easier name (unless you like longer
    names):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the Spark shell with the `uber` JAR using `--jars`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'To submit the Scala code to a cluster, you can call `spark-submit` with the
    same JARS option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Merge strategies in sbt-assembly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If multiple JARs have files with the same name and the same relative path, the
    default merge strategy for the `sbt-assembly` plugin is to verify that content
    is same for all the files and error out otherwise. This strategy is called `MergeStrategy.deduplicate`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the available merge strategies in the `sbt-assembly plugin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategy name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.deduplicate` | The default strategy |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.first` | Picks first file according to classpath |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.last` | Picks last file according to classpath |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.singleOrError` | Errors out (merge conflict not expected)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.concat` | Concatenates all matching files together |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.filterDistinctLines` | Concatenates leaving out duplicates
    |'
  prefs: []
  type: TYPE_TB
- en: '| `MergeStrategy.rename` | Renames files |'
  prefs: []
  type: TYPE_TB
- en: Loading data from relational databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of important data lies in relational databases that Spark needs to query.
    JdbcRDD is a Spark feature that allows relational tables to be loaded as RDDs.
    This recipe will explain how to use JdbcRDD.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL to be introduced in the next chapter includes a data source for JDBC.
    This should be preferred over the current recipe as results are returned as DataFrames
    (to be introduced in the next chapter), which can be easily processed by Spark
    SQL and also joined with other data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please make sure that the JDBC driver JAR is visible on the client node and
    all slaves nodes on which executor will run.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to load data from relational databases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a table named `person` in MySQL using the following DDL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert some data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Download `mysql-connector-java-x.x.xx-bin.jar` from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make the MySQL driver available to the Spark shell and launch it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that `path-to-mysql-jar` is not the actual path name. You should
    use the actual path name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create variables for the username, password, and JDBC URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Import JdbcRDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Import JDBC-related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an instance of the JDBC driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Load JdbcRDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now query the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the RDD to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JdbcRDD is an RDD that executes a SQL query on a JDBC connection and retrieves
    the results. The following is a JdbcRDD constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: The two ?'s are bind variables for a prepared statement inside JdbcRDD. The
    first ? is for the offset (lower bound), that is, which row should we start computing
    with, the second ? is for the limit (upper bound), that is, how many rows should
    we read.
  prefs: []
  type: TYPE_NORMAL
- en: JdbcRDD is a great way to load data in Spark directly from relational databases
    on an ad-hoc basis. If you would like to load data in bulk from RDBMS, there are
    other approaches that would work better, for example, Apache Sqoop is a powerful
    tool that imports and exports data from relational databases to HDFS.
  prefs: []
  type: TYPE_NORMAL

- en: Chapter 5. Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming adds the holy grail of big data processing—that is, real-time
    analytics—to Apache Spark. It enables Spark to ingest live data streams and provides
    real-time intelligence at a very low latency of a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Word count using Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Twitter data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming using Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming is the process of dividing continuously flowing input data into discreet
    units so that it can be processed easily. Familiar examples in real life are streaming
    video and audio content (though a user can download the full movie before he/she
    can watch it, a faster solution is to stream data in small chunks that start playing
    for the user while the rest of the data is being downloaded in the background).
  prefs: []
  type: TYPE_NORMAL
- en: Real-world examples of streaming, besides multimedia, are the processing of
    market feeds, weather data, electronic stock trading data, and so on. All of these
    applications produce large volumes of data at very fast rates and require special
    handling of the data so that insights can be derived from data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming has a few basic concepts, which are better to understand before we
    focus on Spark Streaming. The rate at which a streaming application receives data
    is called **data rate** and is expressed in the form of **kilobytes per second**
    (**kbps**) or **megabytes per second** (**mbps**).
  prefs: []
  type: TYPE_NORMAL
- en: One important use case of streaming is **complex event processing** (**CEP**).
    In CEP, it is important to control the scope of the data being processed. This
    scope is called window, which can be either based on time or size. An example
    of a time-based window is to analyze data that has come in last one minute. An
    example of a size-based window can be the average ask price of the last 100 trades
    of a given stock.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming is Spark's library that provides support to process live data.
    This stream can come from any source, such as Twitter, Kafka, or Flume.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming has a few fundamental building blocks that we need to understand
    well before diving into the recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming has a context wrapper called `StreamingContext`, which wraps
    around `SparkContext` and is the entry point to the Spark Streaming functionality.
    Streaming data, by definition, is continuous and needs to be time-sliced to process.
    This slice of time is called the **batch interval**, which is specified when `StreamingContext`
    is created. There is one-to-one mapping of RDD and batch, that is, each batch
    results in one RDD. As you can see in the following image, Spark Streaming takes
    continuous data, break it into batches and feed to Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Batch interval is important to optimize your streaming application. Ideally,
    you want to process data at least as fast as it is getting ingested; otherwise,
    your application will develop a backlog. Spark Streaming collects data for the
    duration of a batch interval, say, 2 seconds. The moment this 2 second interval
    is over, data collected in that interval will be given to Spark for processing
    and Streaming will focus on collecting data for the next batch interval. Now,
    this 2 second batch interval is all Spark has to process data, as it should be
    free to receive data from the next batch. If Spark can process the data faster,
    you can reduce the batch interval to, say, 1 second. If Spark is not able to keep
    up with this speed, you have to increase the batch interval.
  prefs: []
  type: TYPE_NORMAL
- en: The continuous stream of RDDs in Spark Streaming needs to be represented in
    the form of an abstraction through which it can be processed. This abstraction
    is called **Discretized Stream** (**DStream**). Any operation applied on DStream
    results in an operation on underlying RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every input DStream is associated with a receiver (except for file stream).
    A receiver receives data from the input source and stores it in Spark''s memory.
    There are two types of streaming sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic sources, such as file and socket connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced sources, such as Kafka and Flume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark Streaming also provides windowed computations in which you can apply
    the transformation over a sliding window of data. A sliding window operation is
    based on two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Window length**: This is the duration of the window. For example, if you
    want to get analytics of the last 1 minute of data, the window length will be
    1 minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sliding interval**: This depicts how frequently you want to perform an operation.
    Say you want to perform the operation every 10 seconds; this means that every
    10 seconds, 1 minute of window will have 50 seconds of data common with the last
    window and 10 seconds of the new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these parameters work on underlying RDDs that, obviously, cannot be broken
    apart; therefore, both of these should be a multiple of the batch interval. The
    window length has to be a multiple of the sliding interval as well.
  prefs: []
  type: TYPE_NORMAL
- en: DStream also has output operations, which allow data to be pushed to external
    systems. They are similar to actions on RDDs (one higher level of abstraction
    of what you do at DStream happens to RDDs).
  prefs: []
  type: TYPE_NORMAL
- en: Besides print to print content of DStream, standard RDD actions, such as `saveAsTextFile`,
    `saveAsObjectFile`, and `saveAsHadoopFile`, are supported by similar counterparts,
    such as `saveAsTextFiles`, `saveAsObjectFiles`, and `saveAsHadoopFiles`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: One very useful output operation is `foreachRDD(func)`, which applies any arbitrary
    function to all the RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Word count using Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with a simple example of Streaming in which in one terminal, we
    will type some text and the Streaming application will capture it in another window.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell and give it some extra memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Stream specific imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Import for an implicit conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `StreamingContext` with a 2 second batch interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SocketTextStream` Dstream on localhost with port `8585` with the
    `MEMORY_ONLY` caching:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the lines into multiple words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert word to (word,1), that is, output `1` as the value for each occurrence
    of a word as the key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `reduceByKey` method to add a number of occurrences for each word as
    the key (the function works on two consecutive values at a time, represented by
    `a` and `b`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Print `wordCount`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Start `StreamingContext`; remember, nothing happens until `StreamingContext`
    is started:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in a separate window, start the netcat server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter different lines, such as `to be or not to be`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Check the Spark shell, and you will see word count results like the following
    screenshot:![How to do it...](img/3056_05_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Streaming Twitter data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter is a famous microblogging platform. It produces a massive amount of
    data with around 500 million tweets sent each day. Twitter allows its data to
    be accessed by APIs and that makes it the poster child of testing any big data
    streaming application.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how we can live stream data in Spark using Twitter
    streaming libraries. Twitter is just one source of providing the streaming data
    to Spark and has no special status. Therefore, there are no built-in libraries
    for Twitter. Spark does provide some APIs to facilitate integration with Twitter
    libraries, though.
  prefs: []
  type: TYPE_NORMAL
- en: An example use of live Twitter data feed can be to find trending tweets in the
    last 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a Twitter account if you do not already have one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to [http://apps.twitter.com](http://apps.twitter.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create New App**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter **Name**, **Description**, **Website**, and **Callback URL**, and then
    click on **Create your Twitter Application**.![How to do it...](img/3056_05_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will reach **Application Management** screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Keys and Access Tokens** | **Create my access Token**.![How to
    do it...](img/3056_05_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note down the four values in this screen that we will use in step 14:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consumer Key (API Key)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consumer Secret (API Secret)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access Token**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access Token Secret**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to provide the values in this screen in some time, but, for now,
    let''s download the third-party libraries needed from Maven central:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the Spark shell, supplying the preceding three JARS as the dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform imports that are Twitter-specific:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Stream specific imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Import for an implicit conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `StreamingContext` with a 10 second batch interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `StreamingContext` with a 2 second batch interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are sample values and you should put your own values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create Twitter DStream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter out English tweets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Get text out of the tweets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the checkpoint directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Start `StreamingContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can put all these commands together using `:paste`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Streaming using Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is a distributed, partitioned, and replicated commit log service. In simple
    words, it is a distributed messaging server. Kafka maintains the message feed
    in categories called **topics**. An example of the topic can be a ticker symbol
    of a company you would like to get news about, for example, CSCO for Cisco.
  prefs: []
  type: TYPE_NORMAL
- en: 'Processes that produce messages are called **producers** and those that consume
    messages are called **consumers**. In traditional messaging, the messaging service
    has one central messaging server, also called **broker**. Since Kafka is a distributed
    messaging service, it has a cluster of brokers, which functionally act as one
    Kafka broker, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Streaming using Kafka](img/B03056_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For each topic, Kafka maintains the partitioned log. This partitioned log consists
    of one or more partitions spread across the cluster, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Streaming using Kafka](img/B03056_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kafka borrows a lot of concepts from Hadoop and other big data frameworks. The
    concept of partition is very similar to the concept of `InputSplit` in Hadoop.
    In the simplest form, while using `TextInputFormat`, an `InputSplit` is same as
    a block. A block is read in the form of a key-value pair in `TextInputFormat`,
    where the key is the byte offset of a line and the value is content of the line
    itself. In a similar way, in a Kafka partition, records are stored and retrieved
    in the form of key-value pairs, where the key is a sequential ID number called
    the offset and the value is the actual message.
  prefs: []
  type: TYPE_NORMAL
- en: In Kafka, message retention does not depend on the consumption by a consumer.
    Messages are retained for a configurable period of time. Each consumer is free
    to read messages in any order they like. All it needs to retain is an offset.
    Another analogy can be reading a book in which the page number is analogous to
    the offset, while the page content is analogous to the message. The reader is
    free to read whichever way he/she wants as long as they remember the bookmark
    (the current offset).
  prefs: []
  type: TYPE_NORMAL
- en: To provide functionality similar to pub/sub and PTP (queues) in traditional
    messaging systems, Kafka has the concept of consumer groups. A consumer group
    is a group of consumers, which the Kafka cluster treats as a single unit. In a
    consumer group, only one consumer needs to receive a message. If consumer C1,
    in the following diagram, receives the first message for topic T1, all the following
    messages on that topic will also be delivered to this consumer. Using this strategy,
    Kafka guarantees the order of message delivery for a given topic.
  prefs: []
  type: TYPE_NORMAL
- en: In extreme cases, when all consumers are in one consumer group, the Kafka cluster
    acts like PTP/queue. In another extreme case, if every consumer belongs to a different
    group, it acts like pub/sub. In practice, each consumer group has a limited number
    of consumers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Streaming using Kafka](img/B03056_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This recipe will show you how to perform a word count using data coming from
    Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe assumes Kafka is already installed. Kafka comes with ZooKeeper
    bundled. We are assuming Kafka''s home is in `/opt/infoobjects/kafka`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start ZooKeeper:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Kafka server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `test` topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download the `spark-streaming-kafka` library and its dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell and provide the `spark-streaming-kafka` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Stream specific imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Import for implicit conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `StreamingContext` with a 2 second batch interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Set Kafka-specific variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `topicMap`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Kafka DStream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the value out of lineMap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `flatMap` of values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the key-value pair of (word,occurrence):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the word count for a sliding window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the `checkpoint` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Start `StreamingContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Publish a message on the `test` topic in Kafka in another window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, publish messages on Kafka by pressing *Enter* at step 15 and after every
    message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, as you publish messages on Kafka, you will see them in the Spark shell:![How
    to do it...](img/3056_05_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's say you want to maintain a running count of the occurrence of each word.
    Spark Streaming has a feature for this called `updateStateByKey` operation. The
    `updateStateByKey` operation allows you to maintain any arbitrary state while
    updating it with the new information supplied.
  prefs: []
  type: TYPE_NORMAL
- en: 'This arbitrary state can be an aggregation value, or just a change in state
    (like the mood of a user on twitter). Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call `updateStateByKey` on pairs RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `updateStateByKey` operation returns a new "state" DStream where the state
    for each key is updated by applying the given function on the previous state of
    the key and the new values for the key. This can be used to maintain arbitrary
    state data for each key.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two steps involved in making this operation work:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the state `update` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `updateStateByKey` operation is called once for each key, values represent
    the sequence of values associated with that key, very much like MapReduce and
    the state can be any arbitrary state, which we chose to make `Option[Int]`. With
    every call in step 18, the previous state gets updated by adding the sum of current
    values to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are all the steps combined to maintain the arbitrary state using
    the `updateStateByKey` operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Run it by pressing *Ctrl* + *D* (which executes the code pasted using `:paste`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

- en: Chapter 7. Supervised Learning with MLlib – Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the cost function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing linear regression with lasso
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing ridge regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is Wikipedia''s definition of supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Supervised learning is the machine learning task of inferring a function
    from labeled training data."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Supervised learning has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the algorithm with training dataset; it is like giving questions and their
    answers first
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use test dataset to ask another set of questions to the trained algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two types of supervised learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**: This predicts continuous value output, such as house price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: This predicts discreet valued output (0 or 1) called label,
    such as whether an e-mail is a spam or not. Classification is not limited to two
    values; it can have multiple values such as marking an e-mail important, not important,
    urgent, and so on (0, 1, 2…).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are going to cover regression in this chapter and classification in the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example dataset for regression, we will use the recently sold house data
    of the City of Saratoga, CA, as a training set to train the algorithm. Once the
    algorithm is trained, we will ask it to predict the house price by the given size
    of that house. The following figure illustrates the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hypothesis, for what it does, may sound like a misnomer here, and you may think
    that prediction function may be a better name, but the word hypothesis is used
    for historic reasons.
  prefs: []
  type: TYPE_NORMAL
- en: If we use only one feature to predict the outcome, it is called **bivariate
    analysis**. When we have multiple features, it is called **multivariate analysis**.
    In fact, we can have as many features as we like. One such algorithm, **support
    vector machines** (**SVM**), which we will cover in the next chapter, in fact,
    allows you to have an infinite number of features.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover how we can do supervised learning using MLlib, Spark's
    machine learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mathematical explanations have been provided in as simple a way as possible,
    but you can feel free to skip math and directly go to *How to do it...* section.
  prefs: []
  type: TYPE_NORMAL
- en: Using linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is the approach to model the value of a response variable
    *y*, based on one or more predictor variables or feature *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use some housing data to predict the price of a house based on its size.
    The following are the sizes and prices of houses in the City of Saratoga, CA,
    in early 2014:'
  prefs: []
  type: TYPE_NORMAL
- en: '| House size (sq ft) | Price |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2100 | $ 1,620,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2300 | $ 1,690,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2046 | $ 1,400,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4314 | $ 2,000,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1244 | $ 1,060,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4608 | $ 3,830,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2173 | $ 1,230,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2750 | $ 2,400,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4010 | $ 3,380,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1959 | $ 1,480,000 |'
  prefs: []
  type: TYPE_TB
- en: 'Here''s a graphical representation of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an RDD of the preceding data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model using this data using 100 iterations. Here, step size has been
    kept small to account for very large values of response variables, that is, the
    house price. The fourth parameter is a fraction of the dataset to use per iteration,
    and the last parameter is the initial set of weights to be used (weights of different
    features):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the price for a house with 2,500 sq ft:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: House size is just one predictor variable. House price depends upon other variables,
    such as lot size, age of the house, and so on. The more variables you have, the
    better your prediction will be.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cost function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cost function or loss function is a very important function in machine learning
    algorithms. Most algorithms have some form of cost function and the goal is to
    minimize that. Parameters, which affect cost function, such as `stepSize` in the
    last recipe, need to be set by hand. Therefore, understanding the whole concept
    of cost function is very important.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to analyze cost function for linear regression.
    Linear regression is a simple algorithm to understand and it will help readers
    understand the role of cost functions for even complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to linear regression. The goal is to find the best-fitting line
    so that the mean square of error is minimum. Here, we are referring error as the
    difference between the value as per the best-fitting line and the actual value
    of the response variable for the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a simple case of single predicate variable, the best-fit line can be written
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This function is also called **hypothesis function**, and can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of the linear regression is to find the best-fit line. On this line,
    θ[0] represents intercept on the *y* axis and θ[1] represents the slope of the
    line as obvious from the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have to choose θ[0] and θ[1] in a way that *h(x)* is closest to *y* for
    the training dataset. So, for the *i* ^(th) data point, the square of distance
    between the line and data point is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To put it in words, this is the square of the difference between the predicted
    house price and the actual price the house got sold for. Now, let''s take average
    of this value across the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation is called the cost function *J* for linear regression.
    The goal is to minimize this cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This cost function is also called **squared error function**. Both θ[0] and
    theta θ[1] follow convex curve independently if they are plotted against *J*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a very simple example of dataset of three values, (1,1), (2,2),
    and (3,3), to make the calculation easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s assume θ[1] is 0, that is, the best-fit line parallel to the *x* axis.
    In the first case, assume that the best-fit line is the *x* axis, that is, *y=0*.
    The following will be the value of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s move this line slightly up to *y=1*. The following will be the
    value of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s move this line further up to *y=2*. Then, the following will be
    the value of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, when we move this line further up to *y=3*, the following will be the
    value of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s move this line further up to *y=4*. The following will be the value
    of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, you saw that the cost function value first decreased, and then increased
    again like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's repeat the exercise by making θ[0] 0 and using different values of
    θ[1].
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, assume the best-fit line is the *x* axis, that is, *y=0*.
    The following will be the value of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s use a slope of 0.5\. The following will be the value of the cost
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s use a slope of 1\. The following will be the value of the cost
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, when we use a slope of 1.5, the following will be the value of the cost
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s use a slope of 2.0\. The following will be the value of the cost
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in both the graphs, the minimum value of *J* is when slope or
    gradient of curve is 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cost function](img/3056_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When both θ[0] and θ[1] are mapped to a 3D space, it becomes like the shape
    of a bowl with the minimum value of the cost function being at the bottom of it.
  prefs: []
  type: TYPE_NORMAL
- en: This approach to arrive at this minimum is called **gradient descent**. In Spark,
    the implementation is stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Doing linear regression with lasso
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lasso is a shrinkage and selection method for linear regression. It minimizes
    the usual sum of squared errors, with a bound on the sum of the absolute values
    of the coefficients. It is based on the original lasso paper found at [http://statweb.stanford.edu/~tibs/lasso/lasso.pdf](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The least square method we used in the last recipe is also called **ordinary
    least squares** (**OLS**). OLS has two challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction accuracy**: Predictions made using OLS usually have low forecast
    bias and high variance. Prediction accuracy can be improved by shrinking some
    coefficients (or even making them zero). There will be some increase in bias,
    but overall prediction accuracy will improve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: With a large number of predictors, it is desirable to find
    a subset of them that exhibits the strongest effect (correlation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bias versus variance
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two primary reasons behind prediction error: bias and variance. The
    best way to understand bias and variance is to look at a case where we are doing
    predictions on the same dataset multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: Bias is an estimate of how far the predicted results are from the actual values,
    and variance is an estimate of the difference in predicted values among different
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, adding more features helps to reduce bias, as can easily be understood.
    If, in building a prediction model, we have left out some features with significant
    correlation, it would lead to significant error.
  prefs: []
  type: TYPE_NORMAL
- en: If your model has high variance, you can remove features to reduce it. A bigger
    dataset also helps to reduce variance.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are going to use a simple dataset, which is ill-posed. An ill-posed
    dataset is a dataset where the sample data size is smaller than the number of
    predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '| y | x0 | x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | 3 | 1 | 2 | 1 | 3 | 2 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 9 | 8 | 8 | 9 | 7 | 9 | 8 | 7 | 9 |'
  prefs: []
  type: TYPE_TB
- en: You can easily guess that, here, out of nine predictors, only two have a strong
    correlation with *y*, that is, *x0* and *x1*. We will use this dataset with the
    lasso algorithm to see its validity.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an RDD of the preceding data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model using this data using 100 iterations. Here, the step size and
    regularization parameter have been set by hand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Check how many predictors have their coefficients set to zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, six out of nine predictors have got their coefficients set
    to zero. This is the primary feature of lasso: any predictor it thinks is not
    useful, it literally moves them out of equation by setting their coefficients
    to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Doing ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternate way to lasso to improve prediction quality is ridge regression.
    While in lasso, a lot of features get their coefficients set to zero and, therefore,
    eliminated from an equation, in ridge, predictors or features are penalized, but
    are never set to zero.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an RDD of the preceding data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model using this data using 100 iterations. Here, the step size and
    regularization parameter have been set by hand :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Check how many predictors have their coefficients set to zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, unlike lasso, ridge regression does not assign any predictor
    coefficients zero, but it does make some very close to zero.
  prefs: []
  type: TYPE_NORMAL

["```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.regression.LabeledPoint\n    scala> import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n\n    ```", "```scala\n    scala> val points = Array(\n    LabeledPoint(0.0,Vectors.dense(0.245)),\n    LabeledPoint(0.0,Vectors.dense(0.247)),\n    LabeledPoint(1.0,Vectors.dense(0.285)),\n    LabeledPoint(1.0,Vectors.dense(0.299)),\n    LabeledPoint(1.0,Vectors.dense(0.327)),\n    LabeledPoint(1.0,Vectors.dense(0.347)),\n    LabeledPoint(0.0,Vectors.dense(0.356)),\n    LabeledPoint(1.0,Vectors.dense(0.36)),\n    LabeledPoint(0.0,Vectors.dense(0.363)),\n    LabeledPoint(1.0,Vectors.dense(0.364)),\n    LabeledPoint(0.0,Vectors.dense(0.398)),\n    LabeledPoint(1.0,Vectors.dense(0.4)),\n    LabeledPoint(0.0,Vectors.dense(0.409)),\n    LabeledPoint(1.0,Vectors.dense(0.421)),\n    LabeledPoint(0.0,Vectors.dense(0.432)),\n    LabeledPoint(1.0,Vectors.dense(0.473)),\n    LabeledPoint(1.0,Vectors.dense(0.509)),\n    LabeledPoint(1.0,Vectors.dense(0.529)),\n    LabeledPoint(0.0,Vectors.dense(0.561)),\n    LabeledPoint(0.0,Vectors.dense(0.569)),\n    LabeledPoint(1.0,Vectors.dense(0.594)),\n    LabeledPoint(1.0,Vectors.dense(0.638)),\n    LabeledPoint(1.0,Vectors.dense(0.656)),\n    LabeledPoint(1.0,Vectors.dense(0.816)),\n    LabeledPoint(1.0,Vectors.dense(0.853)),\n    LabeledPoint(1.0,Vectors.dense(0.938)),\n    LabeledPoint(1.0,Vectors.dense(1.036)),\n    LabeledPoint(1.0,Vectors.dense(1.045)))\n\n    ```", "```scala\n    scala> val spiderRDD = sc.parallelize(points)\n\n    ```", "```scala\n    scala> val lr = new LogisticRegressionWithLBFGS().setIntercept(true)\n    scala> val model = lr.run(spiderRDD)\n\n    ```", "```scala\n    scala> val predict = model.predict(Vectors.dense(0.938))\n\n    ```", "```scala\n    $ hdfs dfs -put /opt/infoobjects/spark/data/mllib/sample_libsvm_data.txt /user/hduser/sample_libsvm_data.txt\n\n    ```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.classification.SVMWithSGD\n    scala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n    scala> import org.apache.spark.mllib.regression.LabeledPoint\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.util.MLUtils\n\n    ```", "```scala\n    scala> val svmData = MLUtils.loadLibSVMFile(sc,\"sample_libsvm_data.txt\")\n\n    ```", "```scala\n    scala> svmData.count\n\n    ```", "```scala\n    scala> val trainingAndTest = svmData.randomSplit(Array(0.5,0.5))\n\n    ```", "```scala\n    scala> val trainingData = trainingAndTest(0)\n    scala> val testData = trainingAndTest(1)\n\n    ```", "```scala\n    scala> val model = SVMWithSGD.train(trainingData,100)\n\n    ```", "```scala\n    scala> val label = model.predict(testData.first.features)\n\n    ```", "```scala\n    scala> val predictionsAndLabels = testData.map( r => (model.predict(r.features),r.label))\n\n    ```", "```scala\n    scala> predictionsAndLabels.filter(p => p._1 != p._2).count\n\n    ```", "```scala\n$vi tennis.csv\n0.0,1.0,1.0,2.0\n0.0,1.0,1.0,1.0\n0.0,1.0,1.0,0.0\n0.0,0.0,1.0,2.0\n0.0,0.0,1.0,0.0\n1.0,0.0,0.0,2.0\n1.0,0.0,0.0,1.0\n0.0,0.0,0.0,0.0\n\n```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.tree.DecisionTree\n    scala> import org.apache.spark.mllib.regression.LabeledPoint\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.tree.configuration.Algo._\n    scala> import org.apache.spark.mllib.tree.impurity.Entropy\n\n    ```", "```scala\n    scala> val data = sc.textFile(\"tennis.csv\")\n    ```", "```scala\n    scala> val parsedData = data.map {\n    line =>  val parts = line.split(',').map(_.toDouble)\n     LabeledPoint(parts(0), Vectors.dense(parts.tail)) }\n\n    ```", "```scala\n    scala> val model = DecisionTree.train(parsedData, Classification, Entropy, 3)\n\n    ```", "```scala\n    scala> val v=Vectors.dense(0.0,1.0,0.0)\n\n    ```", "```scala\n    scala> model.predict(v)\n\n    ```", "```scala\nscala> model.depth\nInt = 2\n\n```", "```scala\nscala> model.toDebugString\nString =  \"DecisionTreeModel classifier of depth 2 with 5 nodes\nIf (feature 1 <= 0.0)\n If (feature 2 <= 0.0)\n Predict: 0.0\n Else (feature 2 > 0.0)\n Predict: 1.0\nElse (feature 1 > 0.0)\n Predict: 0.0\n\n```", "```scala\nrf_libsvm_data.txt\n0 5:1 6:1\n1 1:1 2:1 3:1 4:1\n0 3:1 5:1 6:1\n1 1:1 2:1 4:1\n0 5:1 6:1\n1 1:1 2:1 3:1 4:1\n0 1:1 5:1 6:1\n1 2:1 3:1 4:1\n0 1:1 5:1 6:1\n\n```", "```scala\n$ hdfs dfs -put rf_libsvm_data.txt\n\n```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.tree.RandomForest\n    scala> import org.apache.spark.mllib.tree.configuration.Strategy\n    scala> import org.apache.spark.mllib.util.MLUtils\n\n    ```", "```scala\n    scala> val data =\n     MLUtils.loadLibSVMFile(sc, \"rf_libsvm_data.txt\")\n\n    ```", "```scala\n    scala> val splits = data.randomSplit(Array(0.7, 0.3))\n    scala> val (trainingData, testData) = (splits(0), splits(1))\n\n    ```", "```scala\n    scala> val treeStrategy = Strategy.defaultStrategy(\"Classification\")\n\n    ```", "```scala\n    scala> val model = RandomForest.trainClassifier(trainingData,\n     treeStrategy, numTrees=3, featureSubsetStrategy=\"auto\", seed = 12345)\n\n    ```", "```scala\n    scala> val testErr = testData.map { point =>\n     val prediction = model.predict(point.features)\n     if (point.label == prediction) 1.0 else 0.0\n    }.mean()\n    scala> println(\"Test Error = \" + testErr)\n\n    ```", "```scala\n    scala> println(\"Learned Random Forest:n\" + model.toDebugString)\n    Learned Random Forest:nTreeEnsembleModel classifier with 3 trees\n     Tree 0:\n     If (feature 5 <= 0.0)\n     Predict: 1.0\n     Else (feature 5 > 0.0)\n     Predict: 0.0\n     Tree 1:\n     If (feature 3 <= 0.0)\n     Predict: 0.0\n     Else (feature 3 > 0.0)\n     Predict: 1.0\n     Tree 2:\n     If (feature 0 <= 0.0)\n     Predict: 0.0\n     Else (feature 0 > 0.0)\n     Predict: 1.0\n\n    ```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.tree.GradientBoostedTrees\n    scala> import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n    scala> import org.apache.spark.mllib.util.MLUtils\n\n    ```", "```scala\n    scala> val data =\n     MLUtils.loadLibSVMFile(sc, \"rf_libsvm_data.txt\")\n\n    ```", "```scala\n    scala> val splits = data.randomSplit(Array(0.7, 0.3))\n    scala> val (trainingData, testData) = (splits(0), splits(1))\n\n    ```", "```scala\n    scala> val boostingStrategy =\n     BoostingStrategy.defaultParams(\"Classification\")\n    scala> boostingStrategy.numIterations = 3\n\n    ```", "```scala\n    scala> val model = GradientBoostedTrees.train(trainingData, boostingStrategy)\n\n    ```", "```scala\n    scala> val testErr = testData.map { point =>\n     val prediction = model.predict(point.features)\n     if (point.label == prediction) 1.0 else 0.0\n    }.mean()\n    scala> println(\"Test Error = \" + testErr)\n\n    ```", "```scala\n    scala> println(\"Learned Random Forest:n\" + model.toDebugString)\n\n    ```", "```scala\n$ hdfs dfs -put /opt/infoobjects/spark/data/mllib/sample_naive_bayes_data.txt\n sample_naive_bayes_data.txt\n\n```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.mllib.classification.NaiveBayes\n    scala> import org.apache.spark.mllib.linalg.Vectors\n    scala> import org.apache.spark.mllib.regression.LabeledPoint\n\n    ```", "```scala\n    scala> val data = sc.textFile(\"sample_naive_bayes_data.txt\")\n\n    ```", "```scala\n    scala> val parsedData = data.map { line =>\n     val parts = line.split(',')\n     LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))\n    }\n\n    ```", "```scala\n    scala> val splits = parsedData.randomSplit(Array(0.5, 0.5), seed = 11L)\n    scala> val training = splits(0)\n    scala> val test = splits(1)\n\n    ```", "```scala\n    val model = NaiveBayes.train(training, lambda = 1.0)\n\n    ```", "```scala\n    val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))\n\n    ```"]
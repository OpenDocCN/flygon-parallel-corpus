- en: Chapter 8. Supervised Learning with MLlib – Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is divided into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using logistic regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing binary classification using SVM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using Random Forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using Gradient Boosted Trees
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification with Naïve Bayes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classification problem is like the regression problem discussed in the
    previous chapter except that the outcome variable *y* takes only a few discrete
    values. In binary classification, *y* takes only two values: 0 or 1\. You can
    also think of values that the response variable can take in classification as
    representing categories.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using logistic regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In classification, the response variable *y* has discreet values as opposed
    to continuous values. Some examples are e-mail (spam/non-spam), transactions (safe/fraudulent),
    and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The *y* variable in the following equation can take on two values, 0 or 1:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Here, 0 is referred to as a negative class and 1 means a positive class. Though
    we are calling them a positive or negative class, it is only for convenience's
    sake. Algorithms are neutral about this assignment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression, though it works well for regression tasks, hits a few limitations
    for classification tasks. These include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The fitting process is very susceptible to outliers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no guarantee that the hypothesis function *h(x)* will fit in the range
    0 (negative class) to 1 (positive class)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logistic regression guarantees that *h(x)* will fit between 0 and 1\. Though
    logistic regression has the word regression in it, it is more of a misnomer and
    it is very much a classification algorithm:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'In linear regression, the hypothesis function is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'In logistic regression, we slightly modify the hypothesis equation like this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_04.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'The *g* function is called the **sigmoid function** or **logistic function**
    and is defined as follows for a real number *t*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_05.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'This is what the sigmoid function looks like as a graph:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_06.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: As you can see, as *t* approaches negative infinity, *g(t)* approaches 0 and,
    as *t* approaches infinity, *g(t)* approaches 1\. So, this guarantees that the
    hypothesis function output will never fall out of the 0 to 1 range.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the hypothesis function can be rewritten as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_07.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: '*h(x)* is the estimated probability that *y = 1* for a given predictor *x*,
    so *h(x)* can also be rewritten as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_08.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: In other words, the hypothesis function is showing the probability of *y* being
    1 given feature matrix *x*, parameterized by ![Doing classification using logistic
    regression](img/3056_08_09.jpg). This probability can be any real number between
    0 and 1 but our goal of classification does not allow us to have continuous values;
    we can only have two values 0 or 1 indicating the negative or positive class.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we predict *y = 1* if
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_10.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'and *y = 0* otherwise. If we look at the sigmoid function graph again, we realize
    that, when the ![Doing classification using logistic regression](img/3056_08_11.jpg)
    sigmoid function is ![Doing classification using logistic regression](img/3056_08_12.jpg),
    that is, for positive values of *t*, it will predict the positive class:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Since ![Doing classification using logistic regression](img/3056_08_13.jpg),
    this means for ![Doing classification using logistic regression](img/3056_08_14.jpg)
    the positive class will be predicted. To better illustrate this, let''s expand
    it to a non-matrix form for a bivariate case:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_15.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: The plane represented by the equation ![Doing classification using logistic
    regression](img/3056_08_16.jpg) will decide whether a given vector belongs to
    the positive class or negative class. This line is called the decision boundary.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'This boundary does not have to be linear depending on the training set. If
    training data does not separate across a linear boundary, higher-level polynomial
    features can be added to facilitate it. An example can be to add two new features
    by squaring x1 and x2 as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_17.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Please note that, to the learning algorithm, this enhancement is exactly the
    same as the following equation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_18.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: The learning algorithm will treat the introduction of polynomials just as another
    feature. This gives you great power in the fitting process. It means any complex
    decision boundary can be created with the right choice of polynomials and parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s spend some time trying to understand how we choose the right value for
    parameters like we did in the case of linear regression. The cost function *J*
    in the case of linear regression was:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_19.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'As you know, we are averaging the cost in this cost function. Let''s represent
    this in terms of cost term:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_20.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'In other words, the cost term is the cost the algorithm has to pay if it predicts
    *h(x)* for the real response variable value *y*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_21.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: This cost works fine for linear regression but, for logistic regression, this
    cost function is non-convex (that is, it leads to multiple local minimums) and
    we need to find a better convex way to estimate the cost.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost functions that work well for logistic regression are the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_22.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put these two cost functions into one by combining the two:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_23.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put back this cost function to *J*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_24.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'The goal would be to minimize the cost, that is, minimize the value of ![Doing
    classification using logistic regression](img/3056_08_25.jpg). This is done using
    the gradient descent algorithm. Spark has two classes that support logistic regression:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithSGD`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithLBFGS`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LogisticRegressionWithLBFGS` class is preferred as it eliminates the step
    of optimizing the step size.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2006, Suzuki, Tsurusaki, and Kodama did some research on the distribution
    of an endangered burrowing spider on different beaches in Japan ([https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf](https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see some data about grain size and the presence of spiders:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '| Grain size (mm) | Spider present |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| 0.245 | Absent |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| 0.247 | Absent |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| 0.285 | Present |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| 0.299 | Present |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 0.327 | Present |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| 0.347 | Present |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| 0.356 | Absent |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| 0.36 | Present |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| 0.363 | Absent |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| 0.364 | Present |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| 0.398 | Absent |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | Present |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| 0.409 | Absent |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| 0.421 | Present |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| 0.432 | Absent |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| 0.473 | Present |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| 0.509 | Present |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| 0.529 | Present |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| 0.561 | Absent |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| 0.569 | Absent |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| 0.594 | Present |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| 0.638 | Present |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| 0.656 | Present |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| 0.816 | Present |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| 0.853 | Present |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| 0.938 | Present |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| 1.036 | Present |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| 1.045 | Present |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: We will use this data to train the algorithm. Absent will be denoted as 0 and
    present will be denoted as 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些数据来训练算法。缺席将表示为0，存在将表示为1。
- en: How to do it…
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import statistics and related classes:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a `LabeledPoint` array with the presence or absence of spiders being
    the label:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个带有蜘蛛存在或不存在的`LabeledPoint`数组作为标签：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建前述数据的RDD：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Train a model using this data (intercept is the value when all predictors are
    zero):'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些数据训练模型（当所有预测因子为零时，截距是该值）：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Predict the presence of spiders for grain size `0.938`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测粒度为`0.938`的蜘蛛的存在：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Doing binary classification using SVM
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM进行二元分类
- en: Classification is a technique to put data into different classes based on its
    utility. For example, an e-commerce company can apply two labels "will buy" or
    "will not buy" to potential visitors.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是一种根据其效用将数据分为不同类别的技术。例如，电子商务公司可以对潜在访客应用两个标签“会购买”或“不会购买”。
- en: 'This classification is done by providing some already labeled data to machine
    learning algorithms called **training data**. The challenge is how to mark the
    boundary between two classes. Let''s take a simple example as shown in the following
    figure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类是通过向机器学习算法提供一些已经标记的数据来完成的，称为**训练数据**。挑战在于如何标记两个类之间的边界。让我们以下图所示的简单示例为例：
- en: '![Doing binary classification using SVM](img/3056_08_26.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![使用SVM进行二元分类](img/3056_08_26.jpg)'
- en: 'In the preceding case, we designated gray and black to the "will not buy" and
    "will buy" labels. Here, drawing a line between the two classes is as easy as
    follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的案例中，我们将灰色和黑色指定为“不会购买”和“会购买”标签。在这里，画一条线将两个类别分开就像下面这样简单：
- en: '![Doing binary classification using SVM](img/3056_08_27.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![使用SVM进行二元分类](img/3056_08_27.jpg)'
- en: 'Is this the best we can do? Not really, let''s try to do a better job. The
    black classifier is not really equidistant from the "will buy" and "will not buy"
    carts. Let''s make a better attempt like the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们能做到的最好吗？实际上并不是，让我们试着做得更好。黑色分类器与“会购买”和“不会购买”车辆并不是真正等距的。让我们尝试做得更好，就像下面这样：
- en: '![Doing binary classification using SVM](img/3056_08_28.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![使用SVM进行二元分类](img/3056_08_28.jpg)'
- en: 'Now this is looking good. This in fact is what the SVM algorithm does. You
    can see in the preceding diagram that in fact there are only three carts that
    decide the slope of the line: two black carts above the line, and one gray cart
    below the line. These carts are called **support vectors** and the rest of the
    carts, that is, the vectors, are irrelevant.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来不错。实际上，这正是SVM算法所做的。您可以在前面的图中看到，实际上只有三辆车决定了线的斜率：线上方的两辆黑色车和线下方的一辆灰色车。这些车被称为**支持向量**，而其余的车，即向量，是无关紧要的。
- en: 'Sometimes it''s not easy to draw a line and a curve may be needed to separate
    two classes like the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候画一条线并不容易，可能需要一条曲线来分开两个类别，就像下面这样：
- en: '![Doing binary classification using SVM](img/3056_08_29.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![使用SVM进行二元分类](img/3056_08_29.jpg)'
- en: 'Sometimes even that is not enough. In that case, we need more than two dimensions
    to resolve the problem. Rather than a classified line, what we need is a hyperplane.
    In fact, whenever data is too cluttered, adding extra dimensions help to find
    a hyperplane to separate classes. The following diagram illustrates this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有时甚至这还不够。在这种情况下，我们需要超过两个维度来解决问题。我们需要的不是分类线，而是一个超平面。实际上，每当数据过于混乱时，增加额外的维度有助于找到一个分离类别的超平面。下图说明了这一点：
- en: '![Doing binary classification using SVM](img/3056_08_30.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![使用SVM进行二元分类](img/3056_08_30.jpg)'
- en: This does not mean that adding extra dimensions is always a good idea. Most
    of the time, our goal is to reduce dimensions and keep only the relevant dimensions/features.
    A whole set of algorithms is dedicated to dimensionality reduction; we will cover
    these in later chapters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着增加额外的维度总是一个好主意。大多数情况下，我们的目标是减少维度，只保留相关的维度/特征。有一整套算法专门用于降维；我们将在后面的章节中介绍这些算法。
- en: How to do it…
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'The Spark library comes loaded with sample `libsvm` data. We will use this
    and load the data into HDFS:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark库中加载了示例`libsvm`数据。我们将使用这些数据并将其加载到HDFS中：
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Start the Spark shell:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Perform the required imports:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行所需的导入：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load the data as the RDD:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载为RDD：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Count the number of records:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录的数量：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s divide the dataset into half training data and half testing data:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将数据集分成一半训练数据和一半测试数据：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Assign the `training` and `test` data:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配`training`和`test`数据：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the algorithm and build the model for 100 iterations (you can try different
    iterations but you will see that, at a certain point, the results start to converge
    and that is a good number to choose):'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练算法并构建模型进行100次迭代（您可以尝试不同的迭代次数，但您会发现，在某个时候，结果开始收敛，这是一个不错的选择）：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can use this model to predict a label for any dataset. Let''s predict
    the label for the first point in the test data:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个模型来预测任何数据集的标签。让我们预测测试数据中第一个点的标签：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s create a tuple that has the first value as a prediction for test data
    and a second value actual label, which will help us compute the accuracy of our
    algorithm:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个元组，第一个值是测试数据的预测值，第二个值是实际标签，这将帮助我们计算算法的准确性：
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can count how many records have prediction and actual label mismatches:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以计算有多少记录预测和实际标签不匹配：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Doing classification using decision trees
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行分类
- en: Decision trees are the most intuitive among machine learning algorithms. We
    use decision trees in daily life all the time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是机器学习算法中最直观的。我们经常在日常生活中使用决策树。
- en: 'Decision tree algorithms have a lot of useful features:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法有很多有用的特性：
- en: Easy to understand and interpret
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于理解和解释
- en: Work with both categorical and continuous features
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类和连续特征
- en: Work with missing features
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失的特征
- en: Do not require feature scaling
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要特征缩放
- en: 'Decision tree algorithms work in an upside-down order in which an expression
    containing a feature is evaluated at every level and that splits the dataset into
    two categories. We''ll help you understand this with the simple example of a dumb
    charade, which most of us played in college. I guessed an animal and asked my
    coworker ask me questions to work out my choice. Here''s how her questioning went:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Is it a big animal?'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does this animal live more than 40 years?'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Is this animal an elephant?'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an obviously oversimplified case in which she knew I had postulated
    an elephant (what else would you guess in a Big Data world?). Let''s expand this
    example to include some more animals as in the following figure (grayed boxes
    are classes):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using decision trees](img/3056_08_31.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: The preceding example is a case of multiclass classification. In this recipe,
    we are going to focus on binary classification.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever our son has to take tennis lessons in the morning, the night before
    the instructor checks the weather reports and decides whether the next morning
    would be good to play tennis. This recipe will use this example to build a decision
    tree.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s decide on the features of weather that affect the decision whether to
    play tennis in the morning or not:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Rain
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wind speed
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s build a table of the different combinations:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '| Rain | Windy | Temperature | Play tennis? |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Hot | No |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Normal | No |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Cool | No |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| No | Yes | Hot | No |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| No | Yes | Cool | No |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| No | No | Hot | Yes |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| No | No | Normal | Yes |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| No | No | Cool | No |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: 'Now how do we build a decision tree? We can start with one of three features:
    rain, windy, or temperature. The rule is to start with a feature so that the maximum
    information gain is possible.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: On a rainy day, as you can see in the table, other features do not matter and
    there is no play. The same is true for high wind velocity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees, like most other algorithms, take feature values only as double
    values. So, let''s do the mapping:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_08_32.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'The positive class is 1.0 and the negative class is 0.0\. Let''s load the data
    using the CSV format using the first value as a label:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How to do it…
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Perform the required imports:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Load the file:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Parse the data and load it into `LabeledPoint`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Train the algorithm with this data:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a vector for no rain, high wind, and a cool temperature:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Predict whether tennis should be played:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works…
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s draw the decision tree for tennis that we created in this recipe:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_33.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: This model has a depth of three levels. Which attribute to select depends upon
    how we can maximize information gain. The way it is measured is by measuring the
    purity of the split. Purity means that, whether or not certainty is increasing,
    then that given dataset will be considered as positive or negative. In this example,
    this equates to whether the chances of play are increasing or the chances of not
    playing are increasing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Purity is measured using entropy. Entropy is a measure of disorder in a system.
    In this context, it is easier to understand it as a measure of uncertainty:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_34.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: The highest level of purity is 0 and the lowest is 1\. Let's try to determine
    the purity using the formula.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'When rain is yes, the probability of playing tennis is *p+* is 0/3 = 0\. The
    probability of not playing tennis *p_* is 3/3 = 1:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_35.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: This is a pure set.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'When rain is a no, the probability of playing tennis is *p+* is 2/5 = 0.4\.
    The probability of not playing tennis *p_* is 3/5 = 0.6:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_36.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: This is almost an impure set. The most impure would be the case where the probability
    is 0.5.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark uses three measures to determine impurity:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Gini impurity (classification)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy (classification)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance (regression)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information gain is the difference between the parent node impurity and the
    weighted sum of two child node impurities. Let''s look at the first split, which
    partitions data of size eight to two datasets of size three (left) and five (right).
    Let''s call the first split *s1*, the parent node *rain*, the left child *no rain*,
    and the right child *wind*. So the information gain would be:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益是父节点杂质与两个子节点杂质的加权和之差。让我们看一下第一个分裂，将大小为8的数据分成大小为3（左）和5（右）的两个数据集。让我们称第一个分裂为*s1*，父节点为*rain*，左子节点为*no
    rain*，右子节点为*wind*。所以信息增益将是：
- en: '![How it works…](img/3056_08_37.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/3056_08_37.jpg)'
- en: 'As we calculated impurity for *no rain* and *wind* already for the entropy,
    let''s calculate the entropy for *rain*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经为*no rain*和*wind*计算了熵的杂质，现在让我们计算*rain*的熵：
- en: '![How it works…](img/3056_08_38.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/3056_08_38.jpg)'
- en: 'Let''s calculate the information gain now:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算信息增益：
- en: '![How it works…](img/3056_08_39.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/3056_08_39.jpg)'
- en: 'So the information gain is 0.2 in the first split. Is this the best we can
    achieve? Let''s see what our algorithm comes up with. First, let''s find out the
    depth of the tree:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在第一个分裂中，信息增益为0.2。这是我们能达到的最好效果吗？让我们看看我们的算法得出了什么。首先，让我们找出树的深度：
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here, the depth is `2` compared to the `3` we intuitively built, so this model
    seems to be better optimized. Let''s look at the structure of the tree:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，深度是`2`，而我们直观地构建的是`3`，所以这个模型似乎更优化。让我们看看树的结构：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s build it visually to get a better understanding:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以可视化的方式构建它，以便更好地理解：
- en: '![How it works…](img/3056_08_40.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/3056_08_40.jpg)'
- en: 'We will not go into detail here as we already did this with the previous model.
    We will straightaway calculate the information gain: 0.44'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里详细介绍，因为我们已经在之前的模型中做过了。我们将直接计算信息增益：0.44
- en: As you can see in this case, the information gain is 0.44, which is more than
    double the first model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这种情况下所看到的，信息增益为0.44，是第一个模型的两倍多。
- en: If you look at the second level nodes, the impurity is zero. In this case, it
    is great as we got it at a depth of 2\. Image a situation in which the depth is
    50\. In that case, the decision tree would work well for training data and would
    do badly for test data. This situation is called **overfitting**.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看第二级节点，杂质为零。在这种情况下，这是很好的，因为我们在深度为2的情况下得到了它。想象一种情况，深度为50。在那种情况下，决策树对训练数据效果很好，但对测试数据效果很差。这种情况被称为**过拟合**。
- en: 'One solution to avoid overfitting is pruning. You divide your training data
    into two sets: the training set and validation set. You train the model using
    the training set. Now you test with the model against the validation set by slowly
    removing the left nodes. If removing the leaf node (which is mostly a singleton—that
    is, it contains only one data point) improves the performance of the model, this
    leaf node is pruned from the model.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合的一个解决方案是修剪。你将训练数据分成两组：训练集和验证集。你使用训练集训练模型。现在你用模型对验证集进行测试，逐渐移除左节点。如果移除叶节点（通常是单例节点，即只包含一个数据点）改善了模型的性能，那么这个叶节点就从模型中被修剪掉。
- en: Doing classification using Random Forests
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行分类
- en: Sometimes one decision tree is not enough, so a set of decision trees is used
    to produce more powerful models. These are called **ensemble learning algorithms**.
    Ensemble learning algorithms are not limited to using decision trees as base models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有时一个决策树是不够的，所以会使用一组决策树来产生更强大的模型。这些被称为**集成学习算法**。集成学习算法不仅限于使用决策树作为基本模型。
- en: The most popular among the ensemble learning algorithms is Random Forest. In
    Random Forest, rather than growing one single tree, *K* trees are grown. Every
    tree is given a random subset *S* of training data. To add a twist to it, every
    tree only uses a subset of features. When it comes to making predictions, a majority
    vote is done on the trees and that becomes the prediction.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习算法中最受欢迎的是随机森林。在随机森林中，不是生长单一树，而是生长*K*棵树。每棵树都被赋予训练数据的一个随机子集*S*。更有趣的是，每棵树只使用特征的一个子集。在进行预测时，对树进行多数投票，这就成为了预测。
- en: Let's explain this with an example. The goal is to make a prediction for a given
    person about whether he/she has good credit or bad credit.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来解释这一点。目标是对一个给定的人做出预测，判断他/她的信用是好还是坏。
- en: To do this, we will provide labeled training data—that is, in this case, a person
    with features and labels whether he/she has good credit or bad credit. Now we
    do not want to create feature bias so we will provide a randomly selected set
    of features. There is another reason to provide a randomly selected subset of
    features and that is because most real-world data has hundreds if not thousands
    of features. Text classification algorithms, for example, typically have 50k-100k
    features.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将提供带有标签的训练数据，也就是说，在这种情况下，一个带有特征和标签的人。现在我们不想创建特征偏差，所以我们将提供一个随机选择的特征集。提供一个随机选择的特征子集的另一个原因是，大多数真实世界的数据具有数百甚至数千个特征。例如，文本分类算法通常具有50k-100k个特征。
- en: In this case, to add flavor to the story we are not going to provide features,
    but we will ask different people why they think a person has good or bad credit.
    Now by definition, different people are exposed to different features (sometimes
    overlapping) of a person, which gives us the same functionality as randomly selected
    features.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，为了给故事增添趣味，我们不会提供特征，而是会问不同的人为什么他们认为一个人信用好或坏。现在根据定义，不同的人暴露于一个人的不同特征（有时是重叠的），这给了我们与随机选择特征相同的功能。
- en: 'Our first example is Jack who carries a label "bad credit." We will start with
    Joey who works at Jack''s favorite bar, the Elephant Bar. The only way a person
    can deduce why a given label was given is by asking yes/no questions. Let''s see
    what Joey says:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个例子是Jack，他被贴上了“坏信用”的标签。我们将从Jack最喜欢的酒吧——大象酒吧的Joey开始。一个人能够推断为什么给定一个标签的唯一方法是通过问是/否的问题。让我们看看Joey说了什么：
- en: 'Q1: Does Jack tip well? (Feature: generosity)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q1: Jack是否慷慨地给小费？（特征：慷慨）'
- en: 'A: No'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 不'
- en: 'Q2: Does Jack spend at least $60 per visit? (Feature: spendthrift)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：杰克每次至少花60美元吗？（特征：挥霍）
- en: 'A: Yes'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q3: Does he tend to get into bar fights even at the smallest provocation? (Feature:
    volatile)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Q3：他是否倾向于在最小的挑衅下卷入酒吧斗殴？（特征：易怒）
- en: 'A: Yes'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: That explains why Jack has bad credit.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这就解释了为什么杰克信用不好。
- en: 'We now ask Jack''s girlfriend, Stacey:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们问杰克的女朋友斯泰西：
- en: 'Q1: When we hangout, does Jack always cover the bill? (Feature: generosity)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Q1：我们一起出去玩时，杰克是否总是买单？（特征：慷慨）
- en: 'A: No'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: A：不
- en: 'Q2: Has Jack paid me back the $500 he owes me? (Feature: responsibility)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：杰克是否还我500美元？（特征：责任）
- en: 'A: No'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: A：不
- en: 'Q3: Does he overspend sometimes just to show off? (Feature: spendthrift)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Q3：他是否有时为了炫耀而过度花钱？（特征：挥霍）
- en: 'A: Yes'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: That explains why Jack has bad credit.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这就解释了为什么杰克信用不好。
- en: 'We now ask Jack''s best friend George:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们问杰克的好朋友乔治：
- en: 'Q1: When Jack and I hang out at my apartment, does he clean up after himself?
    (Feature: organized)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Q1：当杰克和我在我的公寓里玩时，他会自己清理吗？（特征：有组织）
- en: 'A: No'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: A：不
- en: 'Q2: Did Jack arrive empty-handed during my Super Bowl potluck? (Feature: care)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：杰克在我超级碗聚餐时是空手而来吗？（特征：关心）
- en: 'A: Yes'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q3: Has he used the "I forgot my wallet at home" excuse for me to cover his
    tab at restaurants? (Feature: responsibility)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Q3：他是否曾经用“我忘了在家里带钱包”这个借口让我付他在餐馆的账单？（特征：责任）
- en: 'A: Yes'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: That explains why Jack has bad credit.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这就解释了为什么杰克信用不好。
- en: 'Now we talk about Jessica who has good credit. Let''s ask Stacey who happens
    to be Jessica''s sister:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们谈谈信用良好的杰西卡。让我们问杰西卡的姐姐斯泰西：
- en: 'Q1: Whenever I run short of money, does Jessica offer to help? (Feature: generosity)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Q1：每当我钱不够时，杰西卡是否会主动帮忙？（特征：慷慨）
- en: 'A: Yes'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q2: Does Jessica pay her bills on time? (Feature: responsibility)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：杰西卡是否按时支付账单？（特征：责任）
- en: 'A: Yes'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q3: Does Jessica offer to babysit my child? (Feature: care)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Q3：杰西卡是否愿意帮我照顾孩子？（特征：关心）
- en: 'A: Yes'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: That explains why Jessica has good credit.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这就解释了为什么杰西卡信用良好。
- en: 'Now we ask George who happens to be her husband:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们问乔治，他碰巧是她的丈夫：
- en: 'Q1: Does Jessica keep the house tidy? (Feature: organized)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Q1：杰西卡是否保持房子整洁？（特征：有组织）
- en: 'A: Yes'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q2: Does she expect expensive gifts? (Feature: spendthrift)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：她是否期望昂贵的礼物？（特征：挥霍）
- en: 'A: No'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: A：不
- en: 'Q3: Does she get upset when you forget to mow the lawn? (Feature: volatile)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Q3：当你忘记割草时，她会生气吗？（特征：易怒）
- en: 'A: No'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: A：不
- en: That explains why Jessica has good credit.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这就解释了为什么杰西卡信用良好。
- en: 'Now let''s ask Joey, the bartender at the Elephant Bar:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们问大象酒吧的调酒师乔伊：
- en: 'Q1: Whenever she comes to the bar with friends, is she mostly the designated
    driver? (Feature: responsible)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Q1：每当她和朋友一起来酒吧时，她是否大多是指定司机？（特征：负责）
- en: 'A: Yes'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q2: Does she always take leftovers home? (Feature: spendthrift)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：她是否总是带剩菜回家？（特征：挥霍）
- en: 'A: Yes'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'Q3: Does she tip well? (Feature: generosity)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Q3：她是否慷慨地给小费？（特征：慷慨）
- en: 'A: Yes'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: A：是的
- en: 'The way Random Forest works is that it does random selection on two levels:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的工作方式是在两个级别上进行随机选择：
- en: A subset of the data
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的一个子集
- en: A subset of features to split that data
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些特征的子集来分割数据
- en: Both these subsets can overlap.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个子集可能会重叠。
- en: In our example, we have six features and we are going to assign three features
    to each tree. This way, there is a good chance we will have an overlap.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们有六个特征，我们将为每棵树分配三个特征。这样，我们有很大的机会会有重叠。
- en: 'Let''s add eight more people to our training dataset:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将另外八个人添加到我们的训练数据集中：
- en: '| Names | Label | Generosity | Responsibility | Care | Organization | Spendthrift
    | Volatile |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 名字 | 标签 | 慷慨 | 责任 | 关心 | 组织 | 挥霍 | 易怒 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Jack | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 杰克 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
- en: '| Jessica | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 杰西卡 | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
- en: '| Jenny | 0 | 0 | 0 | 1 | 0 | 1 | 1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 珍妮 | 0 | 0 | 0 | 1 | 0 | 1 | 1 |'
- en: '| Rick | 1 | 1 | 1 | 0 | 1 | 0 | 0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 瑞克 | 1 | 1 | 1 | 0 | 1 | 0 | 0 |'
- en: '| Pat | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 帕特 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
- en: '| Jeb | 1 | 1 | 1 | 1 | 0 | 0 | 0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: 杰布：1 | 1 | 1 | 1 | 0 | 0 | 0
- en: '| Jay | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 杰伊 | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
- en: '| Nat | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 纳特 | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
- en: '| Ron | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 罗恩 | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
- en: '| Mat | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 马特 | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
- en: Getting ready
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好了
- en: 'Let''s put the data we created into the `libsvm` format in the following file:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将创建的数据放入以下文件的`libsvm`格式中：
- en: '[PRE27]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now upload it to HDFS:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将其上传到HDFS：
- en: '[PRE28]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How to do it…
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE29]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Perform the required imports:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行所需的导入：
- en: '[PRE30]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Load and parse the data:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载和解析数据：
- en: '[PRE31]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Split the data into the `training` and `test` datasets:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割成“训练”和“测试”数据集：
- en: '[PRE32]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a classification as a tree strategy (Random Forest also supports regression):'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建分类作为树策略（随机森林也支持回归）：
- en: '[PRE33]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Train the model:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE34]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Evaluate the model on test instances and compute the test error:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试实例上评估模型并计算测试错误：
- en: '[PRE35]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Check the model:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型：
- en: '[PRE36]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works…
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As you can see in such a small example, three trees are using different features.
    In real-world use cases with thousands of features and training data, this would
    not happen, but most of the trees would differ in how they look at features and
    the vote of the majority will win. Please remember that, in the case of regression,
    averaging is done over trees to get a final value.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这个小例子中所看到的，三棵树使用了不同的特征。在具有数千个特征和训练数据的实际用例中，这种情况不会发生，但大多数树在如何看待特征和多数票的情况下会有所不同。请记住，在回归的情况下，树的平均值会得到最终值。
- en: Doing classification using Gradient Boosted Trees
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度提升树进行分类
- en: Another ensemble learning algorithm is **Gradient Boosted Trees** (**GBTs**).
    GBTs train one tree at a time, where each new tree improves upon the shortcomings
    of previously trained trees.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个集成学习算法是**梯度提升树**（**GBTs**）。GBTs一次训练一棵树，每棵新树都改进了先前训练树的缺点。
- en: As GBTs train one tree at a time, they can take longer than Random Forest.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GBTs一次训练一棵树，所以它们可能比随机森林需要更长的时间。
- en: Getting ready
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好了
- en: We are going to use the same data we used in the previous recipe.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前一个配方中使用的相同数据。
- en: How to do it…
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE37]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perform the required imports:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行所需的导入操作：
- en: '[PRE38]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load and parse the data:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并解析数据：
- en: '[PRE39]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Split the data into `training` and `test` datasets:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成“训练”和“测试”数据集：
- en: '[PRE40]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a classification as a boosting strategy and set the number of iterations
    to `3`:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个分类作为增强策略，并将迭代次数设置为`3`：
- en: '[PRE41]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Train the model:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE42]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Evaluate the model on the test instances and compute the test error:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试实例上评估模型并计算测试误差：
- en: '[PRE43]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Check the model:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型：
- en: '[PRE44]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In this case, the accuracy of the model is 0.9, which is less than what we got
    in the case of Random Forest.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型的准确率为0.9，低于我们在随机森林情况下得到的准确率。
- en: Doing classification with Naïve Bayes
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: 'Let''s consider building an e-mail spam filter using machine learning. Here
    we are interested in two classes: spam for unsolicited messages and non-spam for
    regular emails:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑使用机器学习构建电子邮件垃圾邮件过滤器。在这里，我们对两类感兴趣：垃圾邮件表示未经请求的消息，非垃圾邮件表示常规电子邮件：
- en: '![Doing classification with Naïve Bayes](img/3056_08_42.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/3056_08_42.jpg)'
- en: 'The first challenge is that, when given an e-mail, how do we represent it as
    feature vector *x*. An e-mail is just bunch of text or a collection of words (therefore,
    this problem domain falls into a broader category called **text classification**).
    Let''s represent an e-mail with a feature vector with the length equal to the
    size of the dictionary. If a given word in a dictionary appears in an e-mail,
    the value will be 1; otherwise 0\. Let''s build a vector representing e-mail with
    the content *online pharmacy sale*:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个挑战是，当给定一封电子邮件时，我们如何将其表示为特征向量*x*。一封电子邮件只是一堆文本或一组单词（因此，这个问题领域属于更广泛的**文本分类**类别）。让我们用一个长度等于字典大小的特征向量来表示一封电子邮件。如果字典中的给定单词出现在电子邮件中，则值为1；否则为0。让我们构建一个表示内容为*在线药店销售*的电子邮件的向量：
- en: '![Doing classification with Naïve Bayes](img/3056_08_43.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/3056_08_43.jpg)'
- en: The dictionary of words in this feature vector is called *vocabulary* and the
    dimensions of the vector are the same as the size of vocabulary. If the vocabulary
    size is 10,000, the possible values in this feature vector will be 210,000.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 该特征向量中的单词字典称为*词汇表*，向量的维度与词汇表的大小相同。如果词汇表大小为10,000，则该特征向量中的可能值将为210,000。
- en: Our goal is to model the probability of *x* given *y*. To model *P(x|y)*, we
    will make a strong assumption, and that assumption is that *x*'s are conditionally
    independent. This assumption is called the **Naïve Bayes assumption** and the
    algorithm based on this assumption is called the **Naïve Bayes classifier**.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是对*y*给定*x*的概率进行建模。为了对*P(x|y)*进行建模，我们将做出一个强烈的假设，即*x*是有条件独立的。这个假设被称为**朴素贝叶斯假设**，基于这个假设的算法被称为**朴素贝叶斯分类器**。
- en: For example, for *y =1*, which means spam, the probability of "online" appearing
    and "pharmacy appearing" are independent. This is a strong assumption that has
    nothing to do with reality but works out really well when it comes to getting
    good predictions.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于*y=1*，表示垃圾邮件，出现“在线”和“药店”这两个词的概率是独立的。这是一个与现实无关的强烈假设，但在获得良好预测时效果非常好。
- en: Getting ready
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Spark comes bundled with a sample dataset to use with Naïve Bayes. Let''s load
    this dataset to HDFS:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Spark自带一个用于朴素贝叶斯的示例数据集。让我们将这个数据集加载到HDFS中：
- en: '[PRE45]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How to do it…
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Start the Spark shell:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE46]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Perform the required imports:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行所需的导入操作：
- en: '[PRE47]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Load the data into RDD:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载到RDD中：
- en: '[PRE48]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Parse the data into `LabeledPoint`:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据解析为“LabeledPoint”：
- en: '[PRE49]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Split the data half and half into the `training` and `test` datasets:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据一分为二，分别放入“训练”和“测试”数据集中：
- en: '[PRE50]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Train the model with the `training` dataset:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用“训练”数据集训练模型：
- en: '[PRE51]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Predict the label of the `test` dataset:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测“测试”数据集的标签：
- en: '[PRE52]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'

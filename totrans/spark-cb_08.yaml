- en: Chapter 8. Supervised Learning with MLlib – Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing binary classification using SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using Random Forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using Gradient Boosted Trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification with Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classification problem is like the regression problem discussed in the
    previous chapter except that the outcome variable *y* takes only a few discrete
    values. In binary classification, *y* takes only two values: 0 or 1\. You can
    also think of values that the response variable can take in classification as
    representing categories.'
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In classification, the response variable *y* has discreet values as opposed
    to continuous values. Some examples are e-mail (spam/non-spam), transactions (safe/fraudulent),
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *y* variable in the following equation can take on two values, 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, 0 is referred to as a negative class and 1 means a positive class. Though
    we are calling them a positive or negative class, it is only for convenience's
    sake. Algorithms are neutral about this assignment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression, though it works well for regression tasks, hits a few limitations
    for classification tasks. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: The fitting process is very susceptible to outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no guarantee that the hypothesis function *h(x)* will fit in the range
    0 (negative class) to 1 (positive class)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logistic regression guarantees that *h(x)* will fit between 0 and 1\. Though
    logistic regression has the word regression in it, it is more of a misnomer and
    it is very much a classification algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In linear regression, the hypothesis function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In logistic regression, we slightly modify the hypothesis equation like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The *g* function is called the **sigmoid function** or **logistic function**
    and is defined as follows for a real number *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is what the sigmoid function looks like as a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, as *t* approaches negative infinity, *g(t)* approaches 0 and,
    as *t* approaches infinity, *g(t)* approaches 1\. So, this guarantees that the
    hypothesis function output will never fall out of the 0 to 1 range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the hypothesis function can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*h(x)* is the estimated probability that *y = 1* for a given predictor *x*,
    so *h(x)* can also be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the hypothesis function is showing the probability of *y* being
    1 given feature matrix *x*, parameterized by ![Doing classification using logistic
    regression](img/3056_08_09.jpg). This probability can be any real number between
    0 and 1 but our goal of classification does not allow us to have continuous values;
    we can only have two values 0 or 1 indicating the negative or positive class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we predict *y = 1* if
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'and *y = 0* otherwise. If we look at the sigmoid function graph again, we realize
    that, when the ![Doing classification using logistic regression](img/3056_08_11.jpg)
    sigmoid function is ![Doing classification using logistic regression](img/3056_08_12.jpg),
    that is, for positive values of *t*, it will predict the positive class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since ![Doing classification using logistic regression](img/3056_08_13.jpg),
    this means for ![Doing classification using logistic regression](img/3056_08_14.jpg)
    the positive class will be predicted. To better illustrate this, let''s expand
    it to a non-matrix form for a bivariate case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The plane represented by the equation ![Doing classification using logistic
    regression](img/3056_08_16.jpg) will decide whether a given vector belongs to
    the positive class or negative class. This line is called the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'This boundary does not have to be linear depending on the training set. If
    training data does not separate across a linear boundary, higher-level polynomial
    features can be added to facilitate it. An example can be to add two new features
    by squaring x1 and x2 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that, to the learning algorithm, this enhancement is exactly the
    same as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The learning algorithm will treat the introduction of polynomials just as another
    feature. This gives you great power in the fitting process. It means any complex
    decision boundary can be created with the right choice of polynomials and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s spend some time trying to understand how we choose the right value for
    parameters like we did in the case of linear regression. The cost function *J*
    in the case of linear regression was:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you know, we are averaging the cost in this cost function. Let''s represent
    this in terms of cost term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, the cost term is the cost the algorithm has to pay if it predicts
    *h(x)* for the real response variable value *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This cost works fine for linear regression but, for logistic regression, this
    cost function is non-convex (that is, it leads to multiple local minimums) and
    we need to find a better convex way to estimate the cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost functions that work well for logistic regression are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put these two cost functions into one by combining the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put back this cost function to *J*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal would be to minimize the cost, that is, minimize the value of ![Doing
    classification using logistic regression](img/3056_08_25.jpg). This is done using
    the gradient descent algorithm. Spark has two classes that support logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithSGD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithLBFGS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LogisticRegressionWithLBFGS` class is preferred as it eliminates the step
    of optimizing the step size.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2006, Suzuki, Tsurusaki, and Kodama did some research on the distribution
    of an endangered burrowing spider on different beaches in Japan ([https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf](https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see some data about grain size and the presence of spiders:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Grain size (mm) | Spider present |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.245 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.247 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.285 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.299 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.327 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.347 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.356 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.36 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.363 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.364 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.398 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.409 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.421 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.432 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.473 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.509 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.529 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.561 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.569 | Absent |'
  prefs: []
  type: TYPE_TB
- en: '| 0.594 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.638 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.656 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.816 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.853 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 0.938 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 1.036 | Present |'
  prefs: []
  type: TYPE_TB
- en: '| 1.045 | Present |'
  prefs: []
  type: TYPE_TB
- en: We will use this data to train the algorithm. Absent will be denoted as 0 and
    present will be denoted as 1.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `LabeledPoint` array with the presence or absence of spiders being
    the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an RDD of the preceding data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model using this data (intercept is the value when all predictors are
    zero):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the presence of spiders for grain size `0.938`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Doing binary classification using SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is a technique to put data into different classes based on its
    utility. For example, an e-commerce company can apply two labels "will buy" or
    "will not buy" to potential visitors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This classification is done by providing some already labeled data to machine
    learning algorithms called **training data**. The challenge is how to mark the
    boundary between two classes. Let''s take a simple example as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding case, we designated gray and black to the "will not buy" and
    "will buy" labels. Here, drawing a line between the two classes is as easy as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Is this the best we can do? Not really, let''s try to do a better job. The
    black classifier is not really equidistant from the "will buy" and "will not buy"
    carts. Let''s make a better attempt like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now this is looking good. This in fact is what the SVM algorithm does. You
    can see in the preceding diagram that in fact there are only three carts that
    decide the slope of the line: two black carts above the line, and one gray cart
    below the line. These carts are called **support vectors** and the rest of the
    carts, that is, the vectors, are irrelevant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes it''s not easy to draw a line and a curve may be needed to separate
    two classes like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Sometimes even that is not enough. In that case, we need more than two dimensions
    to resolve the problem. Rather than a classified line, what we need is a hyperplane.
    In fact, whenever data is too cluttered, adding extra dimensions help to find
    a hyperplane to separate classes. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This does not mean that adding extra dimensions is always a good idea. Most
    of the time, our goal is to reduce dimensions and keep only the relevant dimensions/features.
    A whole set of algorithms is dedicated to dimensionality reduction; we will cover
    these in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Spark library comes loaded with sample `libsvm` data. We will use this
    and load the data into HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data as the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s divide the dataset into half training data and half testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the `training` and `test` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the algorithm and build the model for 100 iterations (you can try different
    iterations but you will see that, at a certain point, the results start to converge
    and that is a good number to choose):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use this model to predict a label for any dataset. Let''s predict
    the label for the first point in the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a tuple that has the first value as a prediction for test data
    and a second value actual label, which will help us compute the accuracy of our
    algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can count how many records have prediction and actual label mismatches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Doing classification using decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are the most intuitive among machine learning algorithms. We
    use decision trees in daily life all the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision tree algorithms have a lot of useful features:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to understand and interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with both categorical and continuous features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with missing features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not require feature scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision tree algorithms work in an upside-down order in which an expression
    containing a feature is evaluated at every level and that splits the dataset into
    two categories. We''ll help you understand this with the simple example of a dumb
    charade, which most of us played in college. I guessed an animal and asked my
    coworker ask me questions to work out my choice. Here''s how her questioning went:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Is it a big animal?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does this animal live more than 40 years?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Is this animal an elephant?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an obviously oversimplified case in which she knew I had postulated
    an elephant (what else would you guess in a Big Data world?). Let''s expand this
    example to include some more animals as in the following figure (grayed boxes
    are classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using decision trees](img/3056_08_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding example is a case of multiclass classification. In this recipe,
    we are going to focus on binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever our son has to take tennis lessons in the morning, the night before
    the instructor checks the weather reports and decides whether the next morning
    would be good to play tennis. This recipe will use this example to build a decision
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s decide on the features of weather that affect the decision whether to
    play tennis in the morning or not:'
  prefs: []
  type: TYPE_NORMAL
- en: Rain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wind speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s build a table of the different combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rain | Windy | Temperature | Play tennis? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Hot | No |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Normal | No |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Cool | No |'
  prefs: []
  type: TYPE_TB
- en: '| No | Yes | Hot | No |'
  prefs: []
  type: TYPE_TB
- en: '| No | Yes | Cool | No |'
  prefs: []
  type: TYPE_TB
- en: '| No | No | Hot | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| No | No | Normal | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| No | No | Cool | No |'
  prefs: []
  type: TYPE_TB
- en: 'Now how do we build a decision tree? We can start with one of three features:
    rain, windy, or temperature. The rule is to start with a feature so that the maximum
    information gain is possible.'
  prefs: []
  type: TYPE_NORMAL
- en: On a rainy day, as you can see in the table, other features do not matter and
    there is no play. The same is true for high wind velocity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees, like most other algorithms, take feature values only as double
    values. So, let''s do the mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The positive class is 1.0 and the negative class is 0.0\. Let''s load the data
    using the CSV format using the first value as a label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the data and load it into `LabeledPoint`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the algorithm with this data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a vector for no rain, high wind, and a cool temperature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict whether tennis should be played:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s draw the decision tree for tennis that we created in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This model has a depth of three levels. Which attribute to select depends upon
    how we can maximize information gain. The way it is measured is by measuring the
    purity of the split. Purity means that, whether or not certainty is increasing,
    then that given dataset will be considered as positive or negative. In this example,
    this equates to whether the chances of play are increasing or the chances of not
    playing are increasing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Purity is measured using entropy. Entropy is a measure of disorder in a system.
    In this context, it is easier to understand it as a measure of uncertainty:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The highest level of purity is 0 and the lowest is 1\. Let's try to determine
    the purity using the formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'When rain is yes, the probability of playing tennis is *p+* is 0/3 = 0\. The
    probability of not playing tennis *p_* is 3/3 = 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a pure set.
  prefs: []
  type: TYPE_NORMAL
- en: 'When rain is a no, the probability of playing tennis is *p+* is 2/5 = 0.4\.
    The probability of not playing tennis *p_* is 3/5 = 0.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is almost an impure set. The most impure would be the case where the probability
    is 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark uses three measures to determine impurity:'
  prefs: []
  type: TYPE_NORMAL
- en: Gini impurity (classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy (classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance (regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information gain is the difference between the parent node impurity and the
    weighted sum of two child node impurities. Let''s look at the first split, which
    partitions data of size eight to two datasets of size three (left) and five (right).
    Let''s call the first split *s1*, the parent node *rain*, the left child *no rain*,
    and the right child *wind*. So the information gain would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we calculated impurity for *no rain* and *wind* already for the entropy,
    let''s calculate the entropy for *rain*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s calculate the information gain now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So the information gain is 0.2 in the first split. Is this the best we can
    achieve? Let''s see what our algorithm comes up with. First, let''s find out the
    depth of the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the depth is `2` compared to the `3` we intuitively built, so this model
    seems to be better optimized. Let''s look at the structure of the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build it visually to get a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will not go into detail here as we already did this with the previous model.
    We will straightaway calculate the information gain: 0.44'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in this case, the information gain is 0.44, which is more than
    double the first model.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the second level nodes, the impurity is zero. In this case, it
    is great as we got it at a depth of 2\. Image a situation in which the depth is
    50\. In that case, the decision tree would work well for training data and would
    do badly for test data. This situation is called **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution to avoid overfitting is pruning. You divide your training data
    into two sets: the training set and validation set. You train the model using
    the training set. Now you test with the model against the validation set by slowly
    removing the left nodes. If removing the leaf node (which is mostly a singleton—that
    is, it contains only one data point) improves the performance of the model, this
    leaf node is pruned from the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes one decision tree is not enough, so a set of decision trees is used
    to produce more powerful models. These are called **ensemble learning algorithms**.
    Ensemble learning algorithms are not limited to using decision trees as base models.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular among the ensemble learning algorithms is Random Forest. In
    Random Forest, rather than growing one single tree, *K* trees are grown. Every
    tree is given a random subset *S* of training data. To add a twist to it, every
    tree only uses a subset of features. When it comes to making predictions, a majority
    vote is done on the trees and that becomes the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explain this with an example. The goal is to make a prediction for a given
    person about whether he/she has good credit or bad credit.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we will provide labeled training data—that is, in this case, a person
    with features and labels whether he/she has good credit or bad credit. Now we
    do not want to create feature bias so we will provide a randomly selected set
    of features. There is another reason to provide a randomly selected subset of
    features and that is because most real-world data has hundreds if not thousands
    of features. Text classification algorithms, for example, typically have 50k-100k
    features.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, to add flavor to the story we are not going to provide features,
    but we will ask different people why they think a person has good or bad credit.
    Now by definition, different people are exposed to different features (sometimes
    overlapping) of a person, which gives us the same functionality as randomly selected
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first example is Jack who carries a label "bad credit." We will start with
    Joey who works at Jack''s favorite bar, the Elephant Bar. The only way a person
    can deduce why a given label was given is by asking yes/no questions. Let''s see
    what Joey says:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Does Jack tip well? (Feature: generosity)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does Jack spend at least $60 per visit? (Feature: spendthrift)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does he tend to get into bar fights even at the smallest provocation? (Feature:
    volatile)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jack has bad credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now ask Jack''s girlfriend, Stacey:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: When we hangout, does Jack always cover the bill? (Feature: generosity)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Has Jack paid me back the $500 he owes me? (Feature: responsibility)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does he overspend sometimes just to show off? (Feature: spendthrift)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jack has bad credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now ask Jack''s best friend George:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: When Jack and I hang out at my apartment, does he clean up after himself?
    (Feature: organized)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Did Jack arrive empty-handed during my Super Bowl potluck? (Feature: care)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Has he used the "I forgot my wallet at home" excuse for me to cover his
    tab at restaurants? (Feature: responsibility)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jack has bad credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we talk about Jessica who has good credit. Let''s ask Stacey who happens
    to be Jessica''s sister:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Whenever I run short of money, does Jessica offer to help? (Feature: generosity)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does Jessica pay her bills on time? (Feature: responsibility)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does Jessica offer to babysit my child? (Feature: care)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jessica has good credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we ask George who happens to be her husband:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Does Jessica keep the house tidy? (Feature: organized)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does she expect expensive gifts? (Feature: spendthrift)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does she get upset when you forget to mow the lawn? (Feature: volatile)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jessica has good credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s ask Joey, the bartender at the Elephant Bar:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Whenever she comes to the bar with friends, is she mostly the designated
    driver? (Feature: responsible)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does she always take leftovers home? (Feature: spendthrift)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does she tip well? (Feature: generosity)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'The way Random Forest works is that it does random selection on two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: A subset of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A subset of features to split that data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these subsets can overlap.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we have six features and we are going to assign three features
    to each tree. This way, there is a good chance we will have an overlap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add eight more people to our training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Names | Label | Generosity | Responsibility | Care | Organization | Spendthrift
    | Volatile |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Jack | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jessica | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Jenny | 0 | 0 | 0 | 1 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Rick | 1 | 1 | 1 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Pat | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jeb | 1 | 1 | 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Jay | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Nat | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Ron | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mat | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s put the data we created into the `libsvm` format in the following file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now upload it to HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Load and parse the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into the `training` and `test` datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a classification as a tree strategy (Random Forest also supports regression):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model on test instances and compute the test error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see in such a small example, three trees are using different features.
    In real-world use cases with thousands of features and training data, this would
    not happen, but most of the trees would differ in how they look at features and
    the vote of the majority will win. Please remember that, in the case of regression,
    averaging is done over trees to get a final value.
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using Gradient Boosted Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another ensemble learning algorithm is **Gradient Boosted Trees** (**GBTs**).
    GBTs train one tree at a time, where each new tree improves upon the shortcomings
    of previously trained trees.
  prefs: []
  type: TYPE_NORMAL
- en: As GBTs train one tree at a time, they can take longer than Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to use the same data we used in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Load and parse the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into `training` and `test` datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a classification as a boosting strategy and set the number of iterations
    to `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model on the test instances and compute the test error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the accuracy of the model is 0.9, which is less than what we got
    in the case of Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification with Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider building an e-mail spam filter using machine learning. Here
    we are interested in two classes: spam for unsolicited messages and non-spam for
    regular emails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification with Naïve Bayes](img/3056_08_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first challenge is that, when given an e-mail, how do we represent it as
    feature vector *x*. An e-mail is just bunch of text or a collection of words (therefore,
    this problem domain falls into a broader category called **text classification**).
    Let''s represent an e-mail with a feature vector with the length equal to the
    size of the dictionary. If a given word in a dictionary appears in an e-mail,
    the value will be 1; otherwise 0\. Let''s build a vector representing e-mail with
    the content *online pharmacy sale*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification with Naïve Bayes](img/3056_08_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The dictionary of words in this feature vector is called *vocabulary* and the
    dimensions of the vector are the same as the size of vocabulary. If the vocabulary
    size is 10,000, the possible values in this feature vector will be 210,000.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to model the probability of *x* given *y*. To model *P(x|y)*, we
    will make a strong assumption, and that assumption is that *x*'s are conditionally
    independent. This assumption is called the **Naïve Bayes assumption** and the
    algorithm based on this assumption is called the **Naïve Bayes classifier**.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for *y =1*, which means spam, the probability of "online" appearing
    and "pharmacy appearing" are independent. This is a strong assumption that has
    nothing to do with reality but works out really well when it comes to getting
    good predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark comes bundled with a sample dataset to use with Naïve Bayes. Let''s load
    this dataset to HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data into RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the data into `LabeledPoint`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data half and half into the `training` and `test` datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model with the `training` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the label of the `test` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE

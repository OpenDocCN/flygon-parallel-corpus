- en: Chapter 12. Optimizations and Performance Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers various optimizations and performance-tuning best practices
    when working with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter is divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using compression to improve performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using serialization to improve performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing garbage collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the level of parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the future of optimization – project Tungsten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before looking into various ways to optimize Spark, it is a good idea to look
    at the Spark internals. So far, we have looked at Spark at higher level, where
    focus was the functionality provided by the various libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with redefining an RDD. Externally, an RDD is a distributed immutable
    collection of objects. Internally, it consists of the following five parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Set of partitions (`rdd.getPartitions`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of dependencies on parent RDDs (`rdd.dependencies`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function to compute a partition, given its parents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioner (optional) (`rdd.partitioner`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preferred location of each partition (optional) (`rdd.preferredLocations`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first three are needed for an RDD to be recomputed, in case the data is
    lost. When combined, it is called **lineage**. The last two parts are optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: A set of partitions is how data is divided into nodes. In case of HDFS, it means
    `InputSplits`, which are mostly the same as block (except when a record crosses
    block boundaries; in that case, it will be slightly bigger than a block).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit our `wordCount` example to understand these five parts. This
    is how the RDD graph looks for `wordCount` at dataset level view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Basically, this is how the flow goes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `words` folder as an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the five parts of `words` RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Partitions** | One partition per hdfs inputsplit/block (`org.apache.spark.rdd.HadoopPartition`)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Dependencies** | None |'
  prefs: []
  type: TYPE_TB
- en: '| **Compute function** | Read the block |'
  prefs: []
  type: TYPE_TB
- en: '| **Preferred location** | The hdfs block location |'
  prefs: []
  type: TYPE_TB
- en: '| **Partitioner** | None |'
  prefs: []
  type: TYPE_TB
- en: 'Tokenize the words from `words` RDD with each word on a separate line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the five parts of `wordsFlatMap` RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Partitions** | Same as parent RDD, that is, `words` (`org.apache.spark.rdd.HadoopPartition`)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Dependencies** | Same as parent RDD, that is, `words` (`org.apache.spark.OneToOneDependency`)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Compute function** | Compute parent and split each element and flattens
    the results |'
  prefs: []
  type: TYPE_TB
- en: '| **Preferred location** | Ask parent |'
  prefs: []
  type: TYPE_TB
- en: '| **Partitioner** | None |'
  prefs: []
  type: TYPE_TB
- en: 'Transform each word in `wordsFlatMap` RDD to (word,1) tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the five parts of `wordsMap` RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Partitions** | Same as parent RDD, that is, wordsFlatMap (org.apache.spark.rdd.HadoopPartition)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Dependencies** | Same as parent RDD, that is, wordsFlatMap (org.apache.spark.OneToOneDependency)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Compute function** | Compute parent and map it to PairRDD |'
  prefs: []
  type: TYPE_TB
- en: '| **Preferred Location** | Ask parent |'
  prefs: []
  type: TYPE_TB
- en: '| **Partitioner** | None |'
  prefs: []
  type: TYPE_TB
- en: 'Reduce all the values for a given key and sum them up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the five parts of `wordCount` RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Partitions** | One per reduce task (`org.apache.spark.rdd.ShuffledRDDPartition`)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Dependencies** | Shuffle dependency on each parent (`org.apache.spark.ShuffleDependency`)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Compute function** | Do addition on shuffled data |'
  prefs: []
  type: TYPE_TB
- en: '| **Preferred location** | None |'
  prefs: []
  type: TYPE_TB
- en: '| **Partitioner** | HashPartitioner (`org.apache.spark.HashPartitioner`) |'
  prefs: []
  type: TYPE_TB
- en: 'This is how an RDD graph for `wordCount` looks at the partition level view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Optimizing memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a complex distributed computing framework, and has many moving parts.
    Various cluster resources, such as memory, CPU, and network bandwidth, can become
    bottlenecks at various points. As Spark is an in-memory compute framework, the
    impact of the memory is the biggest.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is that it is common for Spark applications to use a huge amount
    of memory, sometimes more than 100 GB. This amount of memory usage is not common
    in traditional Java applications.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, there are two places where memory optimization is needed, and that
    is at the driver and at the executor level.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following commands to set the driver memory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark shell:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark submit:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the following commands to set the executor memory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark shell:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark submit:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To understand memory optimization, it is a good idea to understand how memory
    management works in Java. Objects reside in Heap in Java. Heap is created when
    JVM starts, and it can resize itself when needed (based on minimum and maximum
    size, that is, `-Xms` and `-Xmx`, respectively assigned in configuration).
  prefs: []
  type: TYPE_NORMAL
- en: 'Heap is divided into two spaces or generations: young space and old space.
    The young space is reserved for the allocation of new objects. Young space consists
    of an area called **Eden** and two smaller survivor spaces. When the nursery becomes
    full, garbage is collected by running a special process called **young collection**,
    where all the objects, which have lived long enough, are promoted to old space.
    When the old space becomes full, the garbage is collected there by running a process
    called **old collection**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimizing memory](img/3056_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The logic behind nursery is that most objects have a very short life span. A
    young collection is designed to be fast at finding newly allocated objects and
    moving them to the old space.
  prefs: []
  type: TYPE_NORMAL
- en: The JVM uses mark and sweep algorithm for garbage collection. Mark and sweep
    collection consists of two phases.
  prefs: []
  type: TYPE_NORMAL
- en: During the mark phase, all the objects, which have live references, are marked
    alive, the rest are presumed candidates for garbage collection. During the sweep
    phase, the space occupied by garbage collectable candidates is added to the free
    list, that is, they are available to be allocated to new objects.
  prefs: []
  type: TYPE_NORMAL
- en: There are two improvements to mark and sweep. One is **concurrent mark and sweep**
    (**CMS**) and the other is parallel mark and sweep. CMS focuses on lower latency,
    while the latter focuses on higher throughput. Both strategies have performance
    trade-offs. CMS does not do compaction, while parallel **garbage collector** (**GC**)
    performs whole-heap only compaction, which results in pause times. As a thumb
    rule, for real-time streaming, CMS should be used, and parallel GC otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to have both low latency and high throughput, Java 1.7 update
    4 onwards has another option called **garbage-first GC** (**G1**). G1 is a server-style
    garbage collector, primarily meant for multicore machines with large memories.
    It is planned as a long-term replacement for CMS. So, to modify our thumb rule,
    if you are using Java 7 onwards, simply use G1.
  prefs: []
  type: TYPE_NORMAL
- en: G1 partitions the heap into a set of equal-sized regions, where each set is
    a contiguous range of virtual memory. Each region is assigned a role like Eden,
    Survivor, and Old. G1 performs a concurrent global marking phase to determine
    the live references of objects throughout the heap. After the mark phase is over,
    G1 knows which regions are mostly empty. It collects in these regions first and
    this frees the larger amount of memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimizing memory](img/3056_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The regions selected by G1 as candidates for garbage collection are garbage
    collected using evacuation. G1 copies objects from one or more regions of the
    heap to a single region on the heap, and it both compacts and frees up memory.
    This evacuation is performed in parallel on multiple cores to reduce pause times
    and increase throughput. So, each garbage collection round reduces fragmentation
    while working within user-defined pause times.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three aspects in memory optimization in Java:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory footprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost of accessing objects in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost of garbage collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java objects, in general, are fast to access but consume much more space than
    the actual data inside them.
  prefs: []
  type: TYPE_NORMAL
- en: Using compression to improve performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data compression involves encoding information using fewer bits than the original
    representation. Compression has an important role to play in big data technologies.
    It makes both storage and transport of data more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: When data is compressed, it becomes smaller, so both disk I/O and network I/O
    become faster. It also saves storage space. Every optimization has a cost, and
    the cost of compression comes in the form of added CPU cycles to compress and
    decompress data.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop needs to split data to put them into blocks, irrespective of whether
    the data is compressed or not. Only few compression formats are splittable.
  prefs: []
  type: TYPE_NORMAL
- en: Two most popular compression formats for big data loads are LZO and Snappy.
    Snappy is not splittable, while LZO is. Snappy, on the other hand, is a much faster
    format.
  prefs: []
  type: TYPE_NORMAL
- en: If compression format is splittable like LZO, input file is first split into
    blocks and then compressed. Since compression happened at block level, decompression
    can happen at block level as well as node level.
  prefs: []
  type: TYPE_NORMAL
- en: If compression format is not splittable, compression happens at file level and
    then it is split into blocks. In this case, blocks have to be merged back to file
    before they can be decompressed, so decompression cannot happen at node level.
  prefs: []
  type: TYPE_NORMAL
- en: For supported compression formats, Spark will deploy codecs automatically to
    decompress, and no action is required from the user's side.
  prefs: []
  type: TYPE_NORMAL
- en: Using serialization to improve performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Serialization plays an important part in distributed computing. There are two
    persistence (storage) levels, which support serializing RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_SER`: This stores RDDs as serialized objects. It will create one
    byte array per partition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_SER`: This is similar to the `MEMORY_ONLY_SER`, but it spills
    partitions that do not fit in the memory to disk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the steps to add appropriate persistence levels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `StorageLevel` and implicits associated with it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Persist the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Though serialization reduces the memory footprint substantially, it adds extra
    CPU cycles due to deserialization.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Spark uses Java's serialization. Since the Java serialization is
    slow, the better approach is to use `Kryo` library. `Kryo` is much faster and
    sometimes even 10 times more compact than the default.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use `Kryo` by doing the following settings in your `SparkConf`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Spark shell by setting `Kryo` as serializer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Kryo` automatically registers most of the core Scala classes, but if you would
    like to register your own classes, you can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing garbage collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JVM garbage collection can be a challenge if you have a lot of short lived RDDs.
    JVM needs to go over all the objects to find the ones it needs to garbage collect.
    The cost of the garbage collection is proportional to the number of objects the
    GC needs to go through. Therefore, using fewer objects and the data structures
    that use fewer objects (simpler data structures, such as arrays) helps.
  prefs: []
  type: TYPE_NORMAL
- en: Serialization also shines here as a byte array needs only one object to be garbage
    collected.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Spark uses 60 percent of the executor memory to cache RDDs and the
    rest 40 percent for regular objects. Sometimes, you may not need 60 percent for
    RDDs and can reduce this limit so that more space is available for object creation
    (less need for GC).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can set the memory allocated for RDD cache to 40 percent by starting the
    Spark shell and setting the memory fraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing the level of parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizing the level of parallelism is very important to fully utilize the cluster
    capacity. In the case of HDFS, it means that the number of partitions is the same
    as the number of `InputSplits`, which is mostly the same as the number of blocks.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover different ways to optimize the number of partitions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Specify the number of partitions when loading a file into RDD with the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the RDD with a custom number of partitions as a second parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Another approach is to change the default parallelism by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Spark shell with the new value of default parallelism:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the default value of parallelism:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also reduce the number of partitions using an RDD method called `coalesce(numPartitions)`
    where `numPartitions` is the final number of partitions you would like. If you
    would like the data to be reshuffled over the network, you can call the RDD method
    called `repartition(numPartitions)` where `numPartitions` is the final number
    of partitions you would like.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the future of optimization – project Tungsten
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Project Tungsten, starting with Spark Version 1.4, is the initiative to bring
    Spark closer to bare metal. The goal of this project is to substantially improve
    the memory and CPU efficiency of the Spark applications and push the limits of
    underlying hardware.
  prefs: []
  type: TYPE_NORMAL
- en: In distributed systems, conventional wisdom has been to always optimize network
    I/O as that has been the most scarce and bottlenecked resource. This trend has
    changed in the last few years. Network bandwidth, in the last 5 years, has changed
    from 1 gigabit per second to 10 gigabit per second.
  prefs: []
  type: TYPE_NORMAL
- en: On similar lines, the disk bandwidth has increased from 50 MB/s to 500 MB/s
    and SSDs are being deployed more and more. CPU clock speed, on the other hand,
    was ~3 GHz 5 years back and is still the same. This has unseated the network and
    made CPU the new bottleneck in distributed processing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another trend that has put more load on CPU performance is the new compressed
    data formats such as Parquet. Both compression and serialization, as we have seen
    in the previous recipes in this chapter, lead to more CPU cycles. This trend has
    also pushed the need for CPU optimization to reduce the CPU cycle cost.
  prefs: []
  type: TYPE_NORMAL
- en: On the similar lines, let's look at the memory footprint. In Java, GC does memory
    management. GC has done an amazing job at taking away the memory management from
    the programmer and making it transparent. To do this, Java has to put a lot of
    overhead, and that substantially increases the memory footprint. As an example,
    a simple String "abcd", which should ideally take 4 bytes, takes 48 bytes in Java.
  prefs: []
  type: TYPE_NORMAL
- en: What if we do away with GC and manage memory manually like in lower-level programming
    languages such as C? Java does provide a way to do that since 1.7 version and
    it is called `sun.misc.Unsafe`. Unsafe essentially means that you can build long
    regions of memory without any safety checks. This is the first feature of project
    Tungsten.
  prefs: []
  type: TYPE_NORMAL
- en: Manual memory management by leverage application semantics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manual memory management by leverage application semantics, which can be very
    risky if you do not know what you are doing, is a blessing with Spark. We used
    knowledge of data schema (DataFrames) to directly layout the memory ourselves.
    It not only gets rid of GC overheads, but lets you minimize the memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: The second point is storing data in CPU cache versus memory. Everyone knows
    CPU cache is great as it takes three cycles to get data from the main memory versus
    one cycle in cache. This is the second feature of project Tungsten.
  prefs: []
  type: TYPE_NORMAL
- en: Using algorithms and data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithms and data structures are used to exploit memory hierarchy and enable
    more cache-aware computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'CPU caches are small pools of memory that store the data the CPU is going to
    need next. CPUs have two types of caches: instruction cache and data cache. Data
    caches are arranged in hierarchy of L1, L2, and L3:'
  prefs: []
  type: TYPE_NORMAL
- en: L1 cache is the fastest and most expensive cache in a computer. It stores the
    most critical data and is the first place the CPU looks for information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 cache is slightly slower than L1, but still located on the same processor
    chip. It is the second place the CPU looks for information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L3 cache is still slower, but is shared by all cores, such as DRAM (memory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using algorithms and data structures](img/3056_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The third point is that Java is not very good at bytecode generation for things
    like expression evaluation. If this code generation is done manually, it is much
    more efficient. Code generation is the third feature of project Tungsten.
  prefs: []
  type: TYPE_NORMAL
- en: Code generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This involves exploiting modern compliers and CPUs to allow efficient operations
    directly on binary data. Project Tungsten is in its infancy at present and will
    have much wider support in version 1.5.
  prefs: []
  type: TYPE_NORMAL

["```scala\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, lSTM, Dropout, Embedding\nimport numpy as np\nfrom pickle import dump\nimport string\n```", "```scala\ndef load_document(name):\n    file = open(name, 'r')\n    text = file.read()\n    file.close()\n    return text\n```", "```scala\ninput_filename = 'junglebook.txt'\ndoc = load_document(input_filename)\nprint(doc[:2000])\n```", "```scala\n import string\n def clean_document(doc):\n     doc = doc.replace('--', ' ')\n     tokens = doc.split()\n     table = str.maketrans('', '', string.punctuation)\n     tokens = [w.translate(table) for w in tokens]\n     tokens = [word for word in tokens if word.isalpha()]\n     tokens = [word.lower() for word in tokens]\n     return tokens\n```", "```scala\ntokens = clean_document(doc)\nprint(tokens[:200])\nprint('Total Tokens: %d' % len(tokens))\nprint('Total Unique Tokens: %d' % len(set(tokens)))\n```", "```scala\n length = 50 + 1\n sequences = list()\n for i in range(length, len(tokens)):\n     seq = tokens[i-sequence_length:i]\n     line = ' '.join(seq)\n     sequences.append(line)\n print('Total Sequences: %d' % len(sequences))\n```", "```scala\ndef save_document(lines, name):\n    data = '\\n'.join(lines)\n    file = open(name, 'w')\n    file.write(data)\n    file.close()\n```", "```scala\n output_filename = 'junglebook_sequences.txt'\n save_document(sequences, output_filename)\n```", "```scala\ndef load_document(name):\n    file = open(name, 'r')\n    text = file.read()\n    file.close()\n    return text\n\n# function to load document and split based on lines\ninput_filename = 'junglebook_sequences.txt'\ndoc = load_document(input_filename)\nlines = doc.split('\\n')\n```", "```scala\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nsequences = tokenizer.texts_to_sequences(lines)\n```", "```scala\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size : %d' % vocab_size)\n```", "```scala\nsequences = array(sequences)\nInput, Output = sequences[:,:-1], sequences[:,-1]\nOutput = to_categorical(Output, num_classes=vocab_size)\nsequence_length = Input.shape[1]\n```", "```scala\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=sequence_length))\nmodel.add(LSTM(200, return_sequences=True))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())\n```", "```scala\n model.compile(loss='categorical_crossentropy', optimizer='adam', \n        metrics=['accuracy'])\n\n model.fit(Input, Output, batch_size=250, epochs=75)\n```", "```scala\nmodel.save('junglebook_trained.h5')\n\ndump(tokenizer, open('tokenizer.pkl', 'wb'))\n```", "```scala\ndef load_document(name):\n    file = open(name, 'r')\n    text = file.read()\n    file.close()\n    return text\n\n# load sequences of cleaned text\ninput_filename = 'junglebook_sequences.txt'\ndoc = load_document(input_filename)\nlines = doc.split('\\n')\n```", "```scala\n from keras.models import load_model\n model = load_model('junglebook.h5')\n```", "```scala\nfrom random import randint\nseed_text = lines[randint(0,len(lines))]\nprint(seed_text + '\\n')\n```", "```scala\n prediction = model.predict_classes(encoded, verbose=0)\n\n```", "```scala\n out_word = ''\n for word, index in tokenizer.word_index.items():\n         if index == prediction:\n                 out_word = word\n                 break\n```", "```scala\n encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n\n```", "```scala\n from random import randint\n from pickle import load\n from keras.models import load_model\n from keras.preprocessing.sequence import pad_sequences\n\n def load_document(filename):\n     file = open(filename, 'r')\n     text = file.read()\n     file.close()\n     return text\n\n def generate_sequence(model, tokenizer, sequence_length, seed_text, n_words):\n     result = list()\n     input_text = seed_text\n     for _ in range(n_words):\n         encoded = tokenizer.texts_to_sequences([input_text])[0]\n         encoded = pad_sequences([encoded], maxlen=seq_length,                 truncating='pre')\n         prediction = model.predict_classes(encoded, verbose=0)\n         out_word = ''\n             for word, index in tokenizer.word_index.items():\n                 if index == prediction:\n                     out_word = word\n                     break\n      input_text += ' ' + out_word\n      result.append(out_word)\n    return ' '.join(result)\n\n input_filename = 'junglebook_sequences.txt'\n doc = load_document(input_filename)\n lines = doc.split('\\n')\n seq_length = len(lines[0].split()) - 1\n```", "```scala\n model = load_model('junglebook.h5')\n```", "```scala\n tokenizer = load(open('tokenizer.pkl', 'rb'))\n```", "```scala\n seed_text = lines[randint(0,len(lines))]\n print(seed_text + '\\n')\n```", "```scala\n generated = generate_sequence(model, tokenizer, sequence_length,             seed_text, 50)\n print(generated)\n```"]
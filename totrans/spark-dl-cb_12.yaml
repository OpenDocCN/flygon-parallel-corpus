- en: Creating a Movie Recommendation Engine with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading MovieLens datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating and merging the MovieLens datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the MovieLens datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing dataset for the deep learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the deep learning pipeline with Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the recommendation engine's accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2006, a small DVD rental company set out to make their recommendation engine
    10% better. That company was Netflix and The Netflix Prize was worth $1M. This
    competition attracted many engineers and scientists from some of the largest tech
    companies around the world. The recommendation engine for the winning participant
    was built with machine learning. Netflix is now one of the leading tech giants
    when it comes to streaming data and recommending to its customers what they should
    watch next.
  prefs: []
  type: TYPE_NORMAL
- en: Ratings are everywhere these days, no matter what you are doing. If you are
    looking for a recommendation to go out to eat at a new restaurant, to order some
    clothing online, to watch a new movie at your local theater, or to watch a new
    series on television or online, there is most likely a website or a mobile application
    that will give you some type of rating along with feedback on the product or service
    you are looking to purchase. It is because of this immediate increase in feedback
    that recommendation algorithms have become more in demand over the last couple
    of years. This chapter will focus on building a movie recommendation engine for
    users, using the deep learning library Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading MovieLens datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a great research lab center that began in 1992 in Minneapolis, MN called
    **GroupLens**, whichfocuses on recommendation engines and has graciously put together
    millions of rows of data over several years from the MovieLens website. We will
    use its dataset as our data source for training our recommendation engine model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MovieLens dataset is housed and maintained by GroupLens on the following
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that the dataset we will use will come directly from
    their website and not from a third-party intermediary or repository. Additionally,
    there are two different datasets that are available for us to query:'
  prefs: []
  type: TYPE_NORMAL
- en: Recommended for new research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommended for education and development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The purpose of using this dataset is purely for educational purposes, so we
    will download the data from the education and development section of the website.
    The educational data still contains a significant number of rows for our model,
    as it contains 100,000 ratings, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00349.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, this dataset has information regarding over 600 anonymous users
    collected over a period of several years between 1/9/1995 and 3/31/2015\. The
    dataset was last updated in October 2017.
  prefs: []
  type: TYPE_NORMAL
- en: 'F Maxwell Harper and Joseph A Konstan, 2015\. *The MovieLens Datasets: History
    and Context*. ACM **Transactions on Interactive Intelligent Systems** (**TiiS**)
    5, 4, Article 19 (December 2015), 19 pages. DOI: [http://dx.doi.org/10.1145/2827872](http://dx.doi.org/10.1145/2827872)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will cover downloading and unzipping the MovieLens dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the research version of the smaller MovieLens dataset, which is available
    for public download at the following website: [https://grouplens.org/datasets/movielens/latest/](https://grouplens.org/datasets/movielens/latest/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the `ZIP` file called `ml-latest-small.zip` to one of our local folders,
    as seen in in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00350.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'When `ml-latest-small.zip` is downloaded and unzipped, the following four files
    should be extracted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`links.csv`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`movies.csv`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ratings.csv`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '``tags.csv``'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following script to begin our `SparkSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the following six files are available for access by executing the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Load each dataset into a Spark dataframe using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the row counts for each dataset by executing the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will focus on explaining the fields in each of the datasets available
    in the MovieLens 100K dataset. Take a look at these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets are all available in the zipped file, `ml-latest-small.zip`, where
    the `ratings.csv` dataset will serve as the pseudo-fact table of our data, since
    it has transactions for each movie that is rated. The dataset, `ratings`, has
    the four column names shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00351.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset shows the rating selected by each userId over the course of their
    time, from the earliest rating to the latest rating. The range of a rating can
    vary from 0.5 to 5.0 stars, as seen by `userId = 1` in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00352.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `tags` dataset contains a tag column that contains a specific word or phrase
    used by that user to describe a specific movieId at a specific timestamp. As can
    be seen in the following screenshot, userId 15 was not particularly fond of Sandra
    Bulluck in one of her movies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00353.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `movies` dataset is primarily a lookup table for the genre of films that
    have ratings. There are 19 unique genres that can be associated with a film; however,
    it is important to note that a film can be affiliated with more than one genre
    at a time, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00354.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final dataset is the `links` dataset, which also functions as a lookup
    table. It connects movies from MovieLens to data available for those same movies
    on popular film database sites such as [http://www.imdb.com](http://www.imdb.com),
    as well as [https://www.themoviedb.org](https://www.themoviedb.org). Links to
    IMDB are under the column called imdbId, and links to the MovieDB are under the
    column called tmdbId, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00355.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we finish, it is always a good idea to confirm that we are truly experiencing
    the expected row counts from all of the datasets. This helps to ensure that we
    did not encounter any issues with uploading the files to the notebook. We should
    expect to see around 100k rows for the ratings dataset, as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00356.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we are not going to use the 20 million-row dataset version of MovieLens
    for this chapter, you could elect to use it for this recommendation engine. You
    will still have the same four datasets, but with much more data, especially for
    the `ratings` dataset. If you choose to go with this approach, the full zipped
    dataset can be downloaded from the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://files.grouplens.org/datasets/movielens/ml-latest.zip](http://files.grouplens.org/datasets/movielens/ml-latest.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the metadata behind the MovieLens dataset used in this
    chapter, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about the history and context of the MovieLens dataset used in
    this chapter, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/maxharp3r/the-movielens-datasets-history-and-context](https://www.slideshare.net/maxharp3r/the-movielens-datasets-history-and-context)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about *The Netflix Prize*, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.netflixprize.com/](https://www.netflixprize.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating and merging the MovieLens datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We currently have four separate datasets that we are working with, but ultimately
    we would like to get it down to a single dataset. This chapter will focus on pairing
    down our datasets to one.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will not require any import of PySpark libraries but a background
    in SQL joins will come in handy, as we will explore multiple approaches to joining
    dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will walk through the following steps for joining dataframes in
    PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to rename all field names in `ratings`, by appending
    a `_1` to the end of the name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to `inner join` the `movies` dataset to the `ratings`
    dataset, creating a new table called `temp1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to inner join the `temp1` dataset to the `links`
    dataset, creating a new table called `temp2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create our final combined dataset, `mainDF`, by left-joining `temp2` to `tags`
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Select only the columns needed for our final `mainDF` dataset by executing
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will walk through our design process for joining tables together
    as well as which final columns will be kept:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As was mentioned in the previous section, the ratings dataframe will serve
    as our fact table, since it contains all the main transactions of ratings for
    each user over time. The columns in ratings will be used in each subsequent join
    with the other three tables, and to maintain a uniqueness of the columns, we will
    attach a _1 to the end of each column name, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00357.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now join the three lookup tables to the ratings table. The first two
    joins to ratings are inner joins, as the row counts for temp1 and temp2 are still
    100,004 rows. The third join to ratings from tags needs to be an outer join to
    avoid dropping rows. Additionally, the join needs to be applied to both movieId
    as well as userId, as a tag is unique to both a specific user and a specific movie
    at any given time. The row counts for the three tables temp1, temp2, and mainDF
    can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00358.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Often times when working with joins between datasets, we encounter three types
    of joins: inner, left, and right. An inner join will only produce a result set
    when both join keys are available from dataset 1 and dataset 2\. A left join will
    produce all of the rows from dataset 1 and only the rows with matching keys from
    dataset 2\. A right join will produce all of the rows from dataset 2 and only
    the rows from the matching keys from dataset 1\. Later on in this section, we
    will explore SQL joins within Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is interesting to note that our newly created dataset, mainDF, has 100,441
    rows, instead of the 100,004 rows that are in the original dataset for ratings,
    as well as temp1 and temp2\. There are 437 ratings that have more than one tag
    associated with them. Additionally, we can see that the majority of ratings_1 have
    a null tag value affiliated with them, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00359.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have accumulated additional duplicative columns that will no longer be needed.
    There are 14 columns in total, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00360.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, we have determined that the tags field is relatively useless
    as it has over 99k null values. Therefore, we will use the `select()` function
    on the dataframe to pull in only the eight columns that we will use for our recommendation
    engine. We can then confirm that our final new dataframe, mainDF, has the correct
    amount of rows, 100,004, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00361.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we did do our joins using functions within a Spark dataframe using PySpark,
    we could have also done it by registering the dataframes as temporary tables and
    then joining them using `sqlContext.sql()`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we would register each of our datasets as temporary views using `creatorReplaceTempView()`,
    as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we would write our SQL script just as we would do with any other relational
    database using the `sqlContext.sql()` function, as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can profile the new dataframe, mainDF_SQL, and observe that it
    looks the same as our other dataframe, mainDF, while also keeping the exact same
    row count, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00362.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about SQL programming within Spark, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the MovieLens datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before any modeling takes place, it is important to get familiar with the source
    dataset and perform some exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will import the following library to assist with visualizing and exploring
    the MovieLens dataset: `matplotlib`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will walk through the steps to analyze the movie ratings in the
    MovieLens database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve some summary statistics on the `rating_1` column by executing the
    following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a histogram of the distribution of ratings by executing the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to view the values of the histogram in a spreadsheet
    dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A unique count of user selections of ratings can be stored as a dataframe, `userId_frequency`,
    by executing the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot a histogram of `userID_frequency` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will discuss how the ratings and user activities are distributed
    in the MovieLens database. Take a look at these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the average movie rating made by a user is approximately 3.5,
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00363.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Even though the average rating is 3.54, we can see that the histogram shows
    that the median rating is 4, which indicates that the user ratings are heavily
    skewed towards higher ratings, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00364.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another look at the data behind the histogram shows that users select 4.0 most
    frequently, followed by 3.0, and then 5.0\. Additionally, it is interesting to
    note that users are more likely to give ratings that are at the 0.0 level and
    not at the 0.5 level, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00365.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can look at the distribution of user selection of ratings and see that some
    users are very active in expressing their opinions on the films they''ve seen.
    This is the case with anonymous user 547 who has posted 2391 ratings, as seen
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00366.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, when we look at the distribution of users making rating selections,
    we do see that while there are some instances of users making over a thousand
    selections on their own, the overwhelming majority of users have made less than
    250 selections, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00367.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The distribution of the histogram is the previous screenshot is in a long-tail
    format which indicates that the majority of the occurrences are away from the
    center of the histogram. This is an indication that the overwhelming majority
    of ratings are defined by a few users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are features that the `pyspark` dataframe that are similar to those of
    the `pandas` dataframe and can perform some summary statistics on specific columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `pandas`, we perform summary statistics using the following script: `dataframe[''column''].describe()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `pyspark`, we perform summary statistics using the following script: `dataframe.describe(''column'').show()`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the `describe()` function in PySpark, visit the following
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing dataset for the deep learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to prepare our dataset to be fed into the deep learning model
    that we will build in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While preparing the dataset for `Keras` we will import the following libraries
    into our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`import pyspark.sql.functions as F`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from pyspark.ml.feature import StringIndexer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import keras.utils`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section walks through the following steps to prepare the dataset for the
    deep learning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to clean up the column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `rating` column is currently divided into 0.5 increments. Tweak the ratings
    to be rounded to a whole integer using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the `genres` column from a string to an index with a name of `genreCount`
    based on the frequency of the `genres` labels as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Pair down our dataframe using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Split `mainDF` into a training and testing set for model-training purposes,
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert our two Spark dataframes, `trainDF` and `testDF`, into four `numpy`
    arrays for consumption within our deep learning model using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert both `ytrain_array` and `ytest_array` into one-hot encoded labels,
    `ytrain_OHE` and `ytest_OHE`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains how we prepare the dataset for the deep learning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ease of use inside the deep learning pipeline, it is best to clean up the
    column names and the order of the columns before the pipeline receives the data.
    After renaming the column headers, we can view the updated columns, as seen in
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00368.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A bit of manipulation is performed on the `ratings` column to round up values
    of 0.5 increments to the next-highest whole number. This will assist when we are
    doing our multi-class classification within Keras to group `ratings` into six
    categories, instead of 11 categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To consume the movie genre types into the deep learning model within, we need
    to convert the string values of `genres` into a numeric label. The most frequent
    genres type will get a value of 0, and the values increase for the next most frequent-
    type. In the following screenshot, we can see that Good Will Hunting has two genres
    associated with it (Drama | Romance), and that is the fourth most-frequent genreCount,
    with a value of 3.0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00369.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The genres column is no longer needed for the deep model, as it will be replaced
    by the genreCount column, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00370.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our main dataframe, mainDF, is split into a trainDF and testDF for modeling,
    training, and evaluation purposes, using an 80/20 split. The row count for all
    three dataframes can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00371.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Data is passed into a Keras deep learning model, using matrices instead of
    dataframes. Therefore, our training and testing dataframes are converted into
    numpy arrays and split out into *x* and *y*. The features selected for `xtrain_array`
    and `xtest_array` are userid, movieid, and genreCount. These are the only features
    that will we will use to determine what a potential rating will be for a user.
    We are dropping `imdbid` and `tmdbid`, as they are directly tied to the `movieid`
    and therefore will not provide any additional value. `timestamp` will be removed
    to filter out any bias associated with frequency of voting. Finally, `ytest_array`
    and `ytrain_array` will contain the label value for rating. The `shape` of all
    four arrays can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00372.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While `ytrain_array` and `ytest_array` are both labels in a matrix format,
    they are not ideally encoded for deep learning. Since this is technically a classification
    model that we are building we need to encode our labels in a manner for them to
    be understood by the model. This means that our ratings of 0 through 5 should
    be encoded as 0 or 1 values, based on their value elements. Therefore, if a rating
    received the highest value of 5, it should be encoded as [0,0,0,0,0,1]. The first
    position is reserved for 0, and the sixth position is reserved for 1, indicating
    a value of 5\. We can make this conversion using `keras.utils` and convert our
    categorical variables to one-hot encoded variables. In doing this, the shape of
    our training label is converted from (80146,1) to (80146,6) as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00373.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about `keras.utils` visit the following website: [https://keras.io/utils/](https://keras.io/utils/)
  prefs: []
  type: TYPE_NORMAL
- en: Applying the deep learning model with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we are ready to apply Keras to our data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following from Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from keras.models import Sequential`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from keras.layers import Dense, Activation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section walks through the following steps to apply a deep learning model,
    using Keras on our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries to build a `Sequential` model from `keras`,
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the `Sequential` model from `keras`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We `fit` and train the model and store the results to a variable called `accuracy_history`,
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the configuration of the Keras model that is applied to
    the dataset to predict a rating based on the features selected.
  prefs: []
  type: TYPE_NORMAL
- en: In Keras, a `Sequential` model is simply a linear combination of layers, which
    are the following: `Dense` is used to define the layer types to a fully-connected
    layer within a deep neural network. Finally, `Activation` is used to convert the
    inputs from the features into an output that can be used as a prediction. There
    are many types of activation functions that can be used in a neural network; however,
    for this chapter, we will go with `relu` and `softmax`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `Sequential` model is configured to include three `Dense` layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first layer has `input_dim` set to the number of features from `xtrain_array`.
    The `shape` feature pulls in the value of 3, using `xtrain_array.shape[1]`. Additionally,
    the first layer is set to have `32` neurons in the first layer of the neural network.
    Finally, the three input parameters are activated using the `relu` activation
    function. Only the first layer requires an explicit definition of the input dimensions.
    This is not required in subsequent layers, as they will be able to infer the number
    of dimensions from the previous layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second layer in the `Sequential` model has `10` neurons in the neural network
    along with an activation function set to `relu`. Rectified linear units are used
    early on in the neural network process because they are effective during the training
    process. This is due to the simplicity of the equation as any value less than
    0 is thrown out, which is not the case with other activation functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third and final layer of the `Sequential` model requires six outputs based
    on every possible scenario of a rating from 0 to 5\. This requires setting the
    output to the value of `ytrain_OHE.shape[1]`. The output is generated using a
    `softmax` function which is often the case at the end of a neural network, as
    it is very useful for classification purposes. At this point, we are looking to
    classify a value between 0 and 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the layers are specified, we must `compile` the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We optimize the model using `adam`, which stands for **Adaptive Moment Estimation**.
    Optimizers are great for configuring the learning rate of the gradient descent
    that the model uses to tweak and update the weights of the neural network. `adam`
    is a popular optimizer, as it is said to combine some of the best features from
    other common optimizers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our loss function is set to `categorical_crossentroy`, which is often used when
    looking to predict a multi-class classification. The loss function evaluates the
    performance of the model as it is being trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We train the model using the training features, `xtrain_array`, and the training
    labels `ytrain_OHE`. The model is trained over 20 epochs, each time with a batch_size
    set to 32\. The model output for `accuracy` and `loss` over each epoch are captured
    in a variable called `accuracy_history` and can be viewed as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00374.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we can print out the loss and accuracy scores over each epoch, it is
    always better to visualize both outputs over each of the 20 epochs. We can plot
    both by using the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the script can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00375.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It appears that after the second epoch, both the loss and accuracy are stabilized
    in the model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about getting started with the `Sequential` model from `keras`,
    visit the following website: [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the recommendation engine's accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now calculate the accuracy rate of our deep learning model built on Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating a `Sequential` model for accuracy requires using the `model.evaluate()`
    function within Keras.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can simply calculate the accuracy score, `accuracy_rate`, by executing the
    following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our model performance is based on evaluating our test features, `xtest_array`,
    with our test labels, `ytest_OHE`. We can use `model.evaluate()` and set the `batch_size`
    for evaluation at `128` elements. We can see that our accuracy is around 39%,
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00376.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This means that we are able to determine the rating by a user between 0 and
    5 and at nearly a 39% accuracy rate.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about model performance with Keras metrics, visit the following
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://keras.io/metrics/](https://keras.io/metrics/)'
  prefs: []
  type: TYPE_NORMAL

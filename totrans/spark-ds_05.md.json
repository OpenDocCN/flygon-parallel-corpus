["```scala\n// From RDD: Create an RDD and convert to DataFrame\n>>> employees = sc.parallelize([(1, \"John\", 25), (2, \"Ray\", 35), (3, \"Mike\", 24), (4, \"Jane\", 28), (5, \"Kevin\", 26), (6, \"Vincent\", 35), (7, \"James\", 38), (8, \"Shane\", 32), (9, \"Larry\", 29), (10, \"Kimberly\", 29), (11, \"Alex\", 28), (12, \"Garry\", 25), (13, \"Max\", 31)]).toDF([\"emp_id\",\"name\",\"age\"])\n>>>\n\n// From JSON: reading a JSON file\n>>> salary = sqlContext.read.json(\"./salary.json\")\n>>> designation = sqlContext.read.json(\"./designation.json\")\n```", "```scala\n// From RDD: Create an RDD and convert to DataFrame\nscala> val employees = sc.parallelize(List((1, \"John\", 25), (2, \"Ray\", 35), (3, \"Mike\", 24), (4, \"Jane\", 28), (5, \"Kevin\", 26), (6, \"Vincent\", 35), (7, \"James\", 38), (8, \"Shane\", 32), (9, \"Larry\", 29), (10, \"Kimberly\", 29), (11, \"Alex\", 28), (12, \"Garry\", 25), (13, \"Max\", 31))).toDF(\"emp_id\",\"name\",\"age\")\nemployees: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 1 more field]\nscala> // From JSON: reading a JSON file\nscala> val salary = spark.read.json(\"./salary.json\")\nsalary: org.apache.spark.sql.DataFrame = [e_id: bigint, salary: bigint]\nscala> val designation = spark.read.json(\"./designation.json\")\ndesignation: org.apache.spark.sql.DataFrame = [id: bigint, role: string]\n```", "```scala\n// Creating the final data matrix using the join operation\n>>> final_data = employees.join(salary, employees.emp_id == salary.e_id).join(designation, employees.emp_id == designation.id).select(\"emp_id\", \"name\", \"age\", \"role\", \"salary\")\n>>> final_data.show(5)\n+------+-----+---+---------+------+\n|emp_id| name|age|\u00a0\u00a0\u00a0\u00a0 role|salary|\n+------+-----+---+---------+------+\n|\u00a0\u00a0\u00a0\u00a0 1| John| 25|Associate| 10000|\n|\u00a0\u00a0\u00a0\u00a0 2|\u00a0 Ray| 35|\u00a0 Manager| 12000|\n|\u00a0\u00a0\u00a0\u00a0 3| Mike| 24|\u00a0 Manager| 12000|\n|\u00a0\u00a0\u00a0\u00a0 4| Jane| 28|Associate|\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 5|Kevin| 26|\u00a0 Manager|\u00a0\u00a0 120|\n+------+-----+---+---------+------+\nonly showing top 5 rows\n```", "```scala\n// Creating the final data matrix using the join operation\nscala> val final_data = employees.join(salary, $\"emp_id\" === $\"e_id\").join(designation, $\"emp_id\" === $\"id\").select(\"emp_id\", \"name\", \"age\", \"role\", \"salary\")\nfinal_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\n```", "```scala\n// Dropping rows with missing value(s)\n>>> clean_data = final_data.na.drop()\n>>>\u00a0\n// Replacing missing value by mean\n>>> import math\n>>> from pyspark.sql import functions as F\n>>> mean_salary = math.floor(salary.select(F.mean('salary')).collect()[0][0])\n>>> clean_data = final_data.na.fill({'salary' : mean_salary})\n>>>\u00a0\n//Another example for missing value treatment\n>>> authors = [['Thomas','Hardy','June 2, 1840'],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ['Charles','Dickens','7 February 1812'],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ['Mark','Twain',None],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ['Jane','Austen','16 December 1775'],\n\u00a0\u00a0\u00a0\u00a0\u00a0 ['Emily',None,None]]\n>>> df1 = sc.parallelize(authors).toDF(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [\"FirstName\",\"LastName\",\"Dob\"])\n>>> df1.show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens| 7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------------+\n\n// Drop rows with missing values\n>>> df1.na.drop().show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens| 7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n+---------+--------+----------------+\n\n// Drop rows with at least 2 missing values\n>>> df1.na.drop(thresh=2).show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens| 7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n+---------+--------+----------------+\n\n// Fill all missing values with a given string\n>>> df1.na.fill('Unknown').show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens| 7 February 1812|\n|\u00a0 \u00a0\u00a0\u00a0Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0\u00a0 Emily| Unknown|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n+---------+--------+----------------+\n\n// Fill missing values in each column with a given string\n>>> df1.na.fill({'LastName':'--','Dob':'Unknown'}).show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens| 7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0Unknown|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0\u00a0\u00a0 --|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n+---------+--------+----------------+\n```", "```scala\n//Missing value treatment\n// Dropping rows with missing value(s)\nscala> var clean_data = final_data.na.drop() //Note the var declaration instead of val\nclean_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\nscala>\n\n// Replacing missing value by mean\nscal> val mean_salary = final_data.select(floor(avg(\"salary\"))).\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 first()(0).toString.toDouble\nmean_salary: Double = 20843.0\nscal> clean_data = final_data.na.fill(Map(\"salary\" -> mean_salary)) \n\n//Reassigning clean_data\nclean_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\nscala>\n\n//Another example for missing value treatment\nscala> case class Author (FirstName: String, LastName: String, Dob: String)\ndefined class Author\nscala> val authors = Seq(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Thomas\",\"Hardy\",\"June 2, 1840\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Charles\",\"Dickens\",\"7 February 1812\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Mark\",\"Twain\",null),\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0Author(\"Emily\",null,null))\nauthors: Seq[Author] = List(Author(Thomas,Hardy,June 2, 1840),\n\u00a0\u00a0 Author(Charles,Dickens,7 February 1812), Author(Mark,Twain,null),\n\u00a0\u00a0 Author(Emily,null,null))\nscala> val ds1 = sc.parallelize(authors).toDS()\nds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]\nscala> ds1.show()\n+---------+--------+---------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+---------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens|7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+---------------+\nscala>\n\n// Drop rows with missing values\nscala> ds1.na.drop().show()\n+---------+--------+---------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+---------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens|7 February 1812|\n+---------+--------+---------------+\nscala>\n\n//Drop rows with at least 2 missing values\n//Note that there is no direct scala function to drop rows with at least n missing values\n//However, you can drop rows containing under specified non nulls\n//Use that function to achieve the same result\nscala> ds1.na.drop(minNonNulls = df1.columns.length - 1).show()\n//Fill all missing values with a given string\nscala> ds1.na.fill(\"Unknown\").show()\n+---------+--------+---------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+---------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens|7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n|\u00a0\u00a0\u00a0 Emily| Unknown|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n+---------+--------+---------------+\nscala>\n\n//Fill missing values in each column with a given string\nscala> ds1.na.fill(Map(\"LastName\"->\"--\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"Dob\"->\"Unknown\")).show()\n+---------+--------+---------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+---------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens|7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 \u00a0\u00a0--|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unknown|\n+---------+--------+---------------+\n```", "```scala\n// Identify outliers and replace them with mean\n//The following example reuses the clean_data dataset and mean_salary computed in previous examples\n>>> mean_salary\n20843.0\n>>>\u00a0\n//Compute deviation for each row\n>>> devs = final_data.select(((final_data.salary - mean_salary) ** 2).alias(\"deviation\"))\n\n//Compute standard deviation\n>>> stddev = math.floor(math.sqrt(devs.groupBy().\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 avg(\"deviation\").first()[0]))\n\n//check standard deviation value\n>>> round(stddev,2)\n30351.0\n>>>\u00a0\n//Replace outliers beyond 2 standard deviations with the mean salary\n>>> no_outlier = final_data.select(final_data.emp_id, final_data.name, final_data.age, final_data.salary, final_data.role, F.when(final_data.salary.between(mean_salary-(2*stddev), mean_salary+(2*stddev)), final_data.salary).otherwise(mean_salary).alias(\"updated_salary\"))\n>>>\u00a0\n//Observe modified values\n>>> no_outlier.filter(no_outlier.salary != no_outlier.updated_salary).show()\n+------+----+---+------+-------+--------------+\n|emp_id|name|age|salary|\u00a0\u00a0 role|updated_salary|\n+------+----+---+------+-------+--------------+\n|\u00a0\u00a0\u00a0 13| Max| 31|120000|Manager|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 20843.0|\n+------+----+---+------+-------+--------------+\n>>>\n\n```", "```scala\n// Identify outliers and replace them with mean\n//The following example reuses the clean_data dataset and mean_salary computed in previous examples\n//Compute deviation for each row\nscala> val devs = clean_data.select(((clean_data(\"salary\") - mean_salary) *\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (clean_data(\"salary\") - mean_salary)).alias(\"deviation\"))\ndevs: org.apache.spark.sql.DataFrame = [deviation: double]\n\n//Compute standard deviation\nscala> val stddev = devs.select(sqrt(avg(\"deviation\"))).\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 first().getDouble(0)\nstddev: Double = 29160.932595617614\n\n//If you want to round the stddev value, use BigDecimal as shown\nscala> scala.math.BigDecimal(stddev).setScale(2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 BigDecimal.RoundingMode.HALF_UP)\nres14: scala.math.BigDecimal = 29160.93\nscala>\n\n//Replace outliers beyond 2 standard deviations with the mean salary\nscala> val outlierfunc = udf((value: Long, mean: Double) => {if (value > mean+(2*stddev)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 || value < mean-(2*stddev)) mean else value})\n\n//Use the UDF to compute updated_salary\n//Note the usage of lit() to wrap a literal as a column\nscala> val no_outlier = clean_data.withColumn(\"updated_salary\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 outlierfunc(col(\"salary\"),lit(mean_salary)))\n\n//Observe modified values\nscala> no_outlier.filter(no_outlier(\"salary\") =!=\u00a0 //Not !=\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 no_outlier(\"updated_salary\")).show()\n+------+----+---+-------+------+--------------+\n|emp_id|name|age|\u00a0\u00a0 role|salary|updated_salary|\n+------+----+---+-------+------+--------------+\n|\u00a0\u00a0\u00a0 13| Max| 31|Manager|120000|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 20843.0|\n+------+----+---+-------+------+--------------+\n```", "```scala\n// Deleting the duplicate rows\n>>> authors = [['Thomas','Hardy','June 2,1840'],\n\u00a0\u00a0\u00a0 ['Thomas','Hardy','June 2,1840'],\n\u00a0\u00a0\u00a0 ['Thomas','H',None],\n\u00a0\u00a0\u00a0 ['Jane','Austen','16 December 1775'],\n\u00a0\u00a0\u00a0 ['Emily',None,None]]\n>>> df1 = sc.parallelize(authors).toDF(\n\u00a0\u00a0\u00a0\u00a0\u00a0 [\"FirstName\",\"LastName\",\"Dob\"])\n>>> df1.show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 H|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------------+\n\n// Drop duplicated rows\n>>> df1.dropDuplicates().show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 H|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n+---------+--------+----------------+\n\n// Drop duplicates based on a sub set of columns\n>>> df1.dropDuplicates(subset=[\"FirstName\"]).show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n+---------+--------+----------------+\n>>>\u00a0\n```", "```scala\n//Duplicate values treatment\n// Reusing the Author case class\n// Deleting the duplicate rows\nscala> val authors = Seq(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Thomas\",\"Hardy\",\"June 2,1840\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Thomas\",\"Hardy\",\"June 2,1840\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Thomas\",\"H\",null),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Jane\",\"Austen\",\"16 December 1775\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Emily\",null,null))\nauthors: Seq[Author] = List(Author(Thomas,Hardy,June 2,1840), Author(Thomas,Hardy,June 2,1840), Author(Thomas,H,null), Author(Jane,Austen,16 December 1775), Author(Emily,null,null))\nscala> val ds1 = sc.parallelize(authors).toDS()\nds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]\nscala> ds1.show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0\u00a0 June 2,1840|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0\u00a0 June 2,1840|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 H|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------------+\nscala>\n\n// Drop duplicated rows\nscala> ds1.dropDuplicates().show()\n+---------+--------+----------------+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 \u00a0\u00a0June 2,1840|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 H|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------------+\nscala>\n\n// Drop duplicates based on a sub set of columns\nscala> ds1.dropDuplicates(\"FirstName\").show()\n+---------+--------+----------------+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0\u00a0 June 2,1840|\n+---------+--------+----------------+\n```", "```scala\n// Merging columns\n//Create a udf to concatenate two column values\n>>> import pyspark.sql.functions\n>>> concat_func = pyspark.sql.functions.udf(lambda name, age: name + \"_\" + str(age))\n\n//Apply the udf to create merged column\n>>> concat_df = final_data.withColumn(\"name_age\", concat_func(final_data.name, final_data.age))\n>>> concat_df.show(4)\n+------+----+---+---------+------+--------+\n|emp_id|name|age|\u00a0\u00a0 \u00a0\u00a0role|salary|name_age|\n+------+----+---+---------+------+--------+\n|\u00a0\u00a0\u00a0\u00a0 1|John| 25|Associate| 10000| John_25|\n|\u00a0\u00a0\u00a0\u00a0 2| Ray| 35|\u00a0 Manager| 12000|\u00a0 Ray_35|\n|\u00a0\u00a0\u00a0\u00a0 3|Mike| 24|\u00a0 Manager| 12000| Mike_24|\n|\u00a0\u00a0\u00a0\u00a0 4|Jane| 28|Associate|\u00a0 null| Jane_28|\n+------+----+---+---------+------+--------+\nonly showing top 4 rows\n// Adding constant to data\n>>> data_new = concat_df.withColumn(\"age_incremented\",concat_df.age + 10)\n>>> data_new.show(4)\n+------+----+---+---------+------+--------+---------------+\n|emp_id|name|age|\u00a0\u00a0\u00a0\u00a0 role|salary|name_age|age_incremented|\n+------+----+---+---------+------+--------+---------------+\n|\u00a0\u00a0\u00a0\u00a0 1|John| 25|Associate| 10000| John_25|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 35|\n|\u00a0\u00a0\u00a0\u00a0 2| Ray| 35|\u00a0 Manager| 12000|\u00a0 Ray_35|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 45|\n|\u00a0\u00a0\u00a0\u00a0 3|Mike| 24|\u00a0 Manager| 12000| Mike_24|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 34|\n|\u00a0\u00a0\u00a0\u00a0 4|Jane| 28|Associate|\u00a0 null| Jane_28|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 38|\n+------+----+---+---------+------+--------+---------------+\nonly showing top 4 rows\n>>>\u00a0\n\n//Replace values in a column\n>>> df1.replace('Emily','Charlotte','FirstName').show()\n+---------+--------+----------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens| 7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\n|Charlotte|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------------+\n\n// If the column name argument is omitted in replace, then replacement is applicable to all columns\n//Append new columns based on existing values in a column\n//Give 'LastName' instead of 'Initial' if you want to overwrite\n>>> df1.withColumn('Initial',df1.LastName.substr(1,1)).show()\n+---------+--------+----------------+-------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|Initial|\n+---------+--------+----------------+-------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0\u00a0 June 2, 1840|\u00a0\u00a0\u00a0\u00a0\u00a0 H|\n|\u00a0 Charles| Dickens| 7 February 1812|\u00a0\u00a0\u00a0\u00a0\u00a0 D|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0 T|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|16 December 1775|\u00a0\u00a0\u00a0\u00a0\u00a0 A|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0null|\u00a0\u00a0 null|\n+---------+--------+----------------+-------+\n```", "```scala\n// Merging columns\n//Create a udf to concatenate two column values\nscala> val concatfunc = udf((name: String, age: Integer) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 {name + \"_\" + age})\nconcatfunc: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,StringType,Some(List(StringType, IntegerType)))\nscala>\n\n//Apply the udf to create merged column\nscala> val concat_df = final_data.withColumn(\"name_age\",\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0concatfunc($\"name\", $\"age\"))\nconcat_df: org.apache.spark.sql.DataFrame =\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [emp_id: int, name: string ... 4 more fields]\nscala> concat_df.show(4)\n+------+----+---+---------+------+--------+\n|emp_id|name|age|\u00a0\u00a0\u00a0\u00a0 role|salary|name_age|\n+------+----+---+---------+------+--------+\n|\u00a0\u00a0\u00a0\u00a0 1|John| 25|Associate| 10000| John_25|\n|\u00a0\u00a0\u00a0\u00a0 2| Ray| 35|\u00a0 Manager| 12000|\u00a0 Ray_35|\n|\u00a0\u00a0\u00a0\u00a0 3|Mike| 24|\u00a0 Manager| 12000| Mike_24|\n|\u00a0\u00a0\u00a0\u00a0 4|Jane| 28|Associate|\u00a0 null| Jane_28|\n+------+----+---+---------+------+--------+\nonly showing top 4 rows\nscala>\n\n// Adding constant to data\nscala> val addconst = udf((age: Integer) => {age + 10})\naddconst: org.apache.spark.sql.expressions.UserDefinedFunction =\n\u00a0\u00a0\u00a0\u00a0\u00a0 UserDefinedFunction(<function1>,IntegerType,Some(List(IntegerType)))\nscala> val data_new = concat_df.withColumn(\"age_incremented\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 addconst(col(\"age\")))\ndata_new: org.apache.spark.sql.DataFrame =\n\u00a0\u00a0\u00a0\u00a0 [emp_id: int, name: string ... 5 more fields]\nscala> data_new.show(4)\n+------+----+---+---------+------+--------+---------------+\n|emp_id|name|age|\u00a0\u00a0\u00a0\u00a0 role|salary|name_age|age_incremented|\n+------+----+---+---------+------+--------+---------------+\n|\u00a0\u00a0\u00a0\u00a0 1|John| 25|Associate| 10000| John_25|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 35|\n|\u00a0\u00a0\u00a0\u00a0 2| Ray| 35|\u00a0 Manager| 12000|\u00a0 Ray_35|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 45|\n|\u00a0\u00a0\u00a0\u00a0 3|Mike| 24|\u00a0 Manager| 12000| Mike_24|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 34|\n|\u00a0\u00a0\u00a0\u00a0 4|Jane| 28|Associate|\u00a0 null| Jane_28|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 38|\n+------+----+---+---------+------+--------+---------------+\nonly showing top 4 rows\n\n// Replace values in a column\n//Note: As of Spark 2.0.0, there is no replace on DataFrame/ Dataset does not work so .na. is a work around\nscala> ds1.na.replace(\"FirstName\",Map(\"Emily\" -> \"Charlotte\")).show()\n+---------+--------+---------------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+---------------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 June 2, 1840|\n|\u00a0 Charles| Dickens|7 February 1812|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|Charlotte|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+---------------+\nscala>\n\n// If the column name argument is \"*\" in replace, then replacement is applicable to all columns\n//Append new columns based on existing values in a column\n//Give \"LastName\" instead of \"Initial\" if you want to overwrite\nscala> ds1.withColumn(\"Initial\",ds1(\"LastName\").substr(1,1)).show()\n+---------+--------+---------------+-------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|Initial|\n+---------+--------+---------------+-------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|\u00a0\u00a0 June 2, 1840|\u00a0\u00a0\u00a0\u00a0\u00a0 H|\n|\u00a0 Charles| Dickens|7 February 1812|\u00a0\u00a0\u00a0\u00a0\u00a0 D|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0 T|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null|\u00a0\u00a0 null|\n+---------+--------+---------------+-------+\n```", "```scala\n// Date conversions\n//Create udf for date conversion that converts incoming string to YYYY-MM-DD format\n// The function assumes month is full month name and year is always 4 digits\n// Separator is always a space or comma\n// Month, date and year may come in any order\n//Reusing authors data\n>>> authors = [['Thomas','Hardy','June 2, 1840'],\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 ['Charles','Dickens','7 February 1812'],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ['Mark','Twain',None],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ['Jane','Austen','16 December 1775'],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ['Emily',None,None]]\n>>> df1 = sc.parallelize(authors).toDF(\n\u00a0\u00a0\u00a0\u00a0\u00a0 [\"FirstName\",\"LastName\",\"Dob\"])\n>>>\u00a0\n\n// Define udf\n//Note: You may create this in a script file and execute with execfile(filename.py)\n>>> def toDate(s):\n\u00a0import re\n\u00a0year = month = day = \"\"\n\u00a0if not s:\n\u00a0 return None\n\u00a0mn = [0,'January','February','March','April','May',\n\u00a0 'June','July','August','September',\n\u00a0 'October','November','December']\n\n\u00a0//Split the string and remove empty tokens\n\u00a0l = [tok for tok in re.split(\",| \",s) if tok]\n\n//Assign token to year, month or day\n\u00a0for a in l:\n\u00a0 if a in mn:\n\u00a0\u00a0 month = \"{:0>2d}\".format(mn.index(a))\n\u00a0 elif len(a) == 4:\n\u00a0\u00a0 year = a\n\u00a0 elif len(a) == 1:\n\u00a0\u00a0 day = '0' + a\n\u00a0 else:\n\u00a0\u00a0 day = a\n\u00a0return year + '-' + month + '-' + day\n>>>\u00a0\n\n//Register the udf\n>>> from pyspark.sql.functions import udf\n>>> from pyspark.sql.types import StringType\n>>> toDateUDF = udf(toDate, StringType())\n\n//Apply udf\n>>> df1.withColumn(\"Dob\",toDateUDF(\"Dob\")).show()\n+---------+--------+----------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy|1840-06-02|\n|\u00a0 Charles| Dickens|1812-02-07|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|1775-12-16|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------+\n>>>\u00a0\n```", "```scala\n//Date conversions\n//Create udf for date conversion that converts incoming string to YYYY-MM-DD format\n// The function assumes month is full month name and year is always 4 digits\n// Separator is always a space or comma\n// Month, date and year may come in any order\n//Reusing authors case class and data\n>>> val authors = Seq(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Thomas\",\"Hardy\",\"June 2, 1840\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Charles\",\"Dickens\",\"7 February 1812\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Mark\",\"Twain\",null),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Jane\",\"Austen\",\"16 December 1775\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Author(\"Emily\",null,null))\nauthors: Seq[Author] = List(Author(Thomas,Hardy,June 2, 1840), Author(Charles,Dickens,7 February 1812), Author(Mark,Twain,null), Author(Jane,Austen,16 December 1775), Author(Emily,null,null))\nscala> val ds1 = sc.parallelize(authors).toDS()\nds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]\nscala>\n\n// Define udf\n//Note: You can type :paste on REPL to paste\u00a0 multiline code. CTRL + D signals end of paste mode\ndef toDateUDF = udf((s: String) => {\n\u00a0\u00a0\u00a0 var (year, month, day) = (\"\",\"\",\"\")\n\u00a0\u00a0\u00a0 val mn = List(\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"June\",\"July\",\"August\",\"September\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"October\",\"November\",\"December\")\n\u00a0\u00a0\u00a0 //Tokenize the date string and remove trailing comma, if any\n\u00a0\u00a0\u00a0 if(s != null) {\n\u00a0\u00a0\u00a0\u00a0\u00a0 for (x <- s.split(\" \")) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val token = x.stripSuffix(\",\")\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 token match {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 case \"\" =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 case x if (mn.contains(token)) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 month = \"%02d\".format(mn.indexOf(token))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 case x if (token.length() == 4) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 year = token\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 case x =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 day = token\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0 }\u00a0\u00a0 //End of token processing for\n\u00a0\u00a0\u00a0\u00a0 year + \"-\" + month + \"-\" + day=\n\u00a0\u00a0 } else {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null\n\u00a0\u00a0 }\n})\ntoDateUDF: org.apache.spark.sql.expressions.UserDefinedFunction\nscala>\n\n//Apply udf and convert date strings to standard form YYYY-MM-DD\nscala> ds1.withColumn(\"Dob\",toDateUDF(ds1(\"Dob\"))).show()\n+---------+--------+----------+\n|FirstName|LastName|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dob|\n+---------+--------+----------+\n|\u00a0\u00a0 Thomas|\u00a0\u00a0 Hardy| 1840-06-2|\n|\u00a0 Charles| Dickens| 1812-02-7|\n|\u00a0\u00a0\u00a0\u00a0 Mark|\u00a0\u00a0 Twain|\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n|\u00a0\u00a0\u00a0\u00a0 Jane|\u00a0 Austen|1775-12-16|\n|\u00a0\u00a0\u00a0 Emily|\u00a0\u00a0\u00a0 null|\u00a0\u00a0\u00a0\u00a0\u00a0 null|\n+---------+--------+----------+\n```", "```scala\n/* \u201dSample\u201d function is defined for DataFrames (not RDDs) which takes three parameters:\nwithReplacement - Sample with replacement or not (input: True/False)\nfraction - Fraction of rows to generate (input: any number between 0 and 1 as per your requirement of sample size)\nseed - Seed for sampling (input: Any random seed)\n*/\n>>> sample1 = data_new.sample(False, 0.6) //With random seed as no seed value specified\n>>> sample2 = data_new.sample(False, 0.6, 10000) //With specific seed value of 10000\n```", "```scala\nscala> val sample1 = data_new.sample(false, 0.6) //With random seed as no seed value specified\nsample1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 5 more fields]\nscala> val sample2 = data_new.sample(false, 0.6, 10000) //With specific seed value of 10000\nsample2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 5 more fields]\n```", "```scala\n>>> mean_age = data_new.agg({'age': 'mean'}).first()[0]\n>>> age_counts = data_new.groupBy(\"age\").agg({\"age\": \"count\"}).alias(\"freq\")\n>>> mode_age = age_counts.sort(age_counts[\"COUNT(age)\"].desc(), age_counts.age.asc()).first()[0]\n>>> print(mean_age, mode_age)\n(29.615384615384617, 25)\n>>> age_counts.sort(\"count(age)\",ascending=False).show(2)\n+---+----------+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n|age|count(age)|\n+---+----------+\n| 28|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3|\n| 29|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2|\n+---+----------+\nonly showing top 2 rows\n```", "```scala\n//Reusing data_new created \nscala> val mean_age = data_new.select(floor(avg(\"age\"))).first().getLong(0)\nmean_age: Long = 29\nscala> val mode_age = data_new.groupBy($\"age\").agg(count($\"age\")).\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sort($\"count(age)\".desc, $\"age\").first().getInt(0)\nmode_age: Int = 28\nscala> val age_counts = data_new.groupBy(\"age\").agg(count($\"age\") as \"freq\")\nage_counts: org.apache.spark.sql.DataFrame = [age: int, freq: bigint]\nscala> age_counts.sort($\"freq\".desc).show(2)\n+---+----+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n|age|freq|\n+---+----+\n| 35|\u00a0\u00a0 2|\n| 28|\u00a0\u00a0 2|\n+---+----+\n```", "```scala\n//Reusing data_new created before\nimport math\n>>> range_salary = data_new.agg({'salary': 'max'}).first()[0] - data_new.agg({'salary': 'min'}).first()[0]\n>>> mean_salary = data_new.agg({'salary': 'mean'}).first()[0]\n>>> salary_deviations = data_new.select(((data_new.salary - mean_salary) *\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (data_new.salary - mean_salary)).alias(\"deviation\"))\n>>> stddev_salary = math.sqrt(salary_deviations.agg({'deviation' : \n'avg'}).first()[0])\n>>> variance_salary = salary_deviations.groupBy().avg(\"deviation\").first()[0]\n>>> print(round(range_salary,2), round(mean_salary,2),\n\u00a0\u00a0\u00a0\u00a0\u00a0 round(variance_salary,2), round(stddev_salary,2))\n(119880.0, 20843.33, 921223322.22, 30351.66)\n>>>\u00a0\n```", "```scala\n//Reusing data_new created before\nscala> val range_salary = data_new.select(max(\"salary\")).first().\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 getLong(0) - data_new.select(min(\"salary\")).first().getLong(0)\nrange_salary: Long = 119880\nscala> val mean_salary = data_new.select(floor(avg(\"salary\"))).first().getLong(0)\nmean_salary: Long = 20843\nscala> val salary_deviations = data_new.select(((data_new(\"salary\") - mean_salary)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * (data_new(\"salary\") - mean_salary)).alias(\"deviation\"))\nsalary_deviations: org.apache.spark.sql.DataFrame = [deviation: bigint]\nscala> val variance_salary = { salary_deviations.select(avg(\"deviation\"))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .first().getDouble(0) }\nvariance_salary: Double = 9.212233223333334E8\nscala> val stddev_salary = { salary_deviations\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .select(sqrt(avg(\"deviation\")))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .first().getDouble(0) }\nstddev_salary: Double = 30351.660948510435\n```", "```scala\n>>> import numpy\n>>> from pyspark.mllib.stat import Statistics\n// Create an RDD of number vectors\n//This example creates an RDD with 5 rows with 5 elements each\n>>> observations = sc.parallelize(numpy.random.random_integers(0,100,(5,5)))\n// Compute column summary statistics.\n//Note that the results may vary because of random numbers\n>>> summary = Statistics.colStats(observations)\n>>> print(summary.mean())\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 // mean value for each column\n>>> print(summary.variance())\u00a0 // column-wise variance\n>>> print(summary.numNonzeros())// number of nonzeros in each column\n```", "```scala\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nscala> import org.apache.spark.mllib.stat.{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 MultivariateStatisticalSummary, Statistics}\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n// Create an RDD of number vectors\n//This example creates an RDD with 5 rows with 5 elements each\nscala> val observations = sc.parallelize(Seq.fill(5)(Vectors.dense(Array.fill(5)(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 scala.util.Random.nextDouble))))\nobservations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[43] at parallelize at <console>:27\nscala>\n// Compute column summary statistics.\n//Note that the results may vary because of random numbers\nscala> val summary = Statistics.colStats(observations)\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@36836161\nscala> println(summary.mean)\u00a0 // mean value for each column\n[0.5782406967737089,0.5903954680966121,0.4892908815930067,0.45680701799234835,0.6611492334819364]\nscala> println(summary.variance)\u00a0\u00a0\u00a0 // column-wise variance\n[0.11893608153330748,0.07673977181967367,0.023169197889513014,0.08882605965192601,0.08360159585590332]\nscala> println(summary.numNonzeros) // number of nonzeros in each column\n[5.0,5.0,5.0,5.0,5.0]\n```", "```scala\n//Histogram\n>>>from random import randint\n>>> numRDD = sc.parallelize([randint(0,9) for x in xrange(1,1001)])\n// Generate histogram data for given bucket count\n>>> numRDD.histogram(5)\n([0.0, 1.8, 3.6, 5.4, 7.2, 9], [202, 213, 215, 188, 182])\n//Alternatively, specify ranges\n>>> numRDD.histogram([0,3,6,10])\n([0, 3, 6, 10], [319, 311, 370])\n```", "```scala\n//Histogram\nscala> val numRDD = sc.parallelize(Seq.fill(1000)(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 scala.util.Random.nextInt(10)))\nnumRDD: org.apache.spark.rdd.RDD[Int] =\n\u00a0\u00a0\u00a0\u00a0 ParallelCollectionRDD[0] at parallelize at <console>:24\n// Generate histogram data for given bucket count\nscala> numRDD.histogram(5)\nres10: (Array[Double], Array[Long]) = (Array(0.0, 1.8, 3.6, 5.4, 7.2, 9.0),Array(194, 209, 215, 195, 187))\nscala>\n//Alternatively, specify ranges\nscala> numRDD.histogram(Array(0,3.0,6,10))\nres13: Array[Long] = Array(293, 325, 382)\n```", "```scala\n\u00a0//Chi-Square test\n>>> from pyspark.mllib.linalg import Vectors, Matrices\n>>> from pyspark.mllib.stat import Statistics\n>>> import random\n>>>\u00a0\n//Make a vector of frequencies of events\n>>> vec = Vectors.dense( random.sample(xrange(1,101),10))\n>>> vec\nDenseVector([45.0, 40.0, 93.0, 66.0, 56.0, 82.0, 36.0, 30.0, 85.0, 15.0])\n// Get Goodnesss of fit test results\n>>> GFT_Result = Statistics.chiSqTest(vec)\n// Here the \u2018goodness of fit test\u2019 is conducted because your input is a vector\n//Make a contingency matrix\n>>> mat = Matrices.dense(5,6,random.sample(xrange(1,101),30))\\\n//Get independense test results\\\\\n>>> IT_Result = Statistics.chiSqTest(mat)\n// Here the \u2018independence test\u2019 is conducted because your input is a vector\n//Examine the independence test results\n>>> print(IT_Result)\nChi squared test summary:\nmethod: pearson\ndegrees of freedom = 20\nstatistic = 285.9423808343265\npValue = 0.0\nVery strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n```", "```scala\nscala> import org.apache.spark.mllib.linalg.{Vectors, Matrices}\nimport org.apache.spark.mllib.linalg.{Vectors, Matrices}\u00a0\n\nscala> import org.apache.spark.mllib.stat.Statistics\u00a0\n\nscala> val vec = Vectors.dense( Array.fill(10)(\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 scala.util.Random.nextDouble))vec: org.apache.spark.mllib.linalg.Vector = [0.4925741159101148,....]\u00a0\n\nscala> val GFT_Result = Statistics.chiSqTest(vec)GFT_Result: org.apache.spark.mllib.stat.test.ChiSqTestResult =Chi squared test summary:\nmethod: pearson\ndegrees of freedom = 9\nstatistic = 1.9350768763253192\npValue = 0.9924531181394086\nNo presumption against null hypothesis: observed follows the same distribution as expected..\n// Here the \u2018goodness of fit test\u2019 is conducted because your input is a vector\nscala> val mat = Matrices.dense(5,6, Array.fill(30)(scala.util.Random.nextDouble)) // a contingency matrix\nmat: org.apache.spark.mllib.linalg.Matrix =.....\u00a0\nscala> val IT_Result = Statistics.chiSqTest(mat)\nIT_Result: org.apache.spark.mllib.stat.test.ChiSqTestResult =Chi squared test summary:\nmethod: pearson\ndegrees of freedom = 20\nstatistic = 2.5401190679900663\npValue = 0.9999990459111089\nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n// Here the \u2018independence test\u2019 is conducted because your input is a vector\n\n```", "```scala\n>>> from pyspark.mllib.stat import Statistics\n>>> import random \n// Define two series\n//Number of partitions and cardinality of both Ser_1 and Ser_2 should be the same\n>>> Ser_1 = sc.parallelize(random.sample(xrange(1,101),10))\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\n// Define Series_1>>> Ser_2 = sc.parallelize(random.sample(xrange(1,101),10))\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n// Define Series_2\u00a0\n>>> correlation = Statistics.corr(Ser_1, Ser_2, method = \"pearson\") \n//if you are interested in Spearman method, use \u201cspearman\u201d switch instead\n>>> round(correlation,2)-0.14\n>>> correlation = Statistics.corr(Ser_1, Ser_2, method =\"spearman\")\n>>> round(correlation,2)-0.19//Check on matrix//The following statement creates 100 rows of 5 elements each\n>>> data = sc.parallelize([random.sample(xrange(1,51),5) for x in range(100)])\n>>> correlMatrix = Statistics.corr(data, method = \"pearson\") \n//method may be spearman as per you requirement\n>>> correlMatrix\narray([[ 1.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,\u00a0 0.09889342, -0.14634881,\u00a0 0.00178334,\u00a0 0.08389984],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [ 0.09889342,\u00a0 1.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , -0.07068631, -0.02212963, -0.1058252 ],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [-0.14634881, -0.07068631,\u00a0 1.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , -0.22425991,\u00a0 0.11063062],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [ 0.00178334, -0.02212963, -0.22425991,\u00a0 1.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , -0.04864668],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [ 0.08389984, -0.1058252 ,\u00a0 0.11063062, -0.04864668,\u00a0 1.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n]])\n>>> \n\n```", "```scala\nscala> val correlation = Statistics.corr(Ser_1, Ser_2, \"pearson\")correlation: Double = 0.43217145308272087\u00a0\n//if you are interested in Spearman method, use \u201cspearman\u201d switch instead\nscala> val correlation = Statistics.corr(Ser_1, Ser_2, \"spearman\")correlation: Double = 0.4181818181818179\u00a0\nscala>\n//Check on matrix\n//The following statement creates 100 rows of 5 element Vectors\nscala> val data = sc.parallelize(Seq.fill(100)(Vectors.dense(Array.fill(5)(\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 scala.util.Random.nextDouble))))\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[37] at parallelize at <console>:27\u00a0\nscala> val correlMatrix = Statistics.corr(data, method=\"pearson\") \n//method may be spearman as per you requirement\ncorrelMatrix: org.apache.spark.mllib.linalg.Matrix =1.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -0.05478051936343809\u00a0 ... (5 total)-0.05478051936343809\u00a0\u00a0 1.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ..........\n```"]
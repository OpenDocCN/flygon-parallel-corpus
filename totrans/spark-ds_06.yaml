- en: Chapter 6.  Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are the consumers of machine learning every day, whether we notice or not.
    E-mail providers such as Google automatically push some incoming mails into the `Spam`
    folder and online shopping sites such as Amazon or social networking sites such
    as Facebook jump in with unsolicited recommendations that are surprisingly useful.
    So, what enables these software products to reconnect long lost friends? These
    are just a few examples of machine learning in action.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, machine learning is a part of **Artificial Intelligence** (**AI**)
    which deals with a class of algorithms that can learn from data and make predictions.
    The techniques and underlying concepts are drawn from the field of statistics.
    Machine learning exists at the intersection of computer science and statistics
    and is considered one of the most important components of data science. It has
    been around for quite some time now, but its complexity has only increased with
    increase in data and scalability requirements. Machine learning algorithms tend
    to be resource intensive and iterative in nature, which render them a poor fit
    for MapReduce paradigm. MapReduce works very well for single pass algorithms but
    does not cater so well for multi-pass counterparts. The Spark research program
    was started precisely to address this challenge. Apache Spark is equipped with
    efficient algorithms in its MLlib library that are designed to perform well even
    in iterative computational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter outlined the data analytics' life cycle and its various
    components such as data cleaning, data transformation, sampling techniques, and
    graphical techniques to visualize the data, along with concepts covering descriptive
    statistics and inferential statistics. We also looked at some of the statistical
    testing that could be performed on the Spark platform. Further to the basics we
    built up in the previous chapter, we are going to cover in this chapter most of
    the machine learning algorithms and how to use them to build models on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite for this chapter, basic understanding of machine learning
    algorithms and computer science fundamentals are nice to have. However, we have
    covered some theoretical basics of the algorithms with right set of practical
    examples to make those more comprehendible and easy to implement. The topics covered
    in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLlib and the Pipeline API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parametric methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-parametric methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization on regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Support Vector Machines (SVMs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impurity measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split canditate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of decision tress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosted trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer perceptron classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is all about learning by example data; examples that produce
    a particular output for a given input. There are various business use cases for
    machine learning. Let us look at a few examples to get an idea of what exactly
    it is:'
  prefs: []
  type: TYPE_NORMAL
- en: A recommendation engine that recommends users what they might be interested
    in buying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer segmentation (grouping customers who share similar characteristics)
    for marketing campaigns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disease classification for cancer - malignant/benign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive modeling, for example, sales forecasting, weather forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing business inferences, for example, understanding what effect will change
    the price of a product have on sales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of statistical learning was existent even before the first computer
    system was introduced. In the nineteenth century, the least squares technique
    (now called linear regression) had already been developed. For classification
    problems, Fisher came up with **Linear Discriminant Analysis** (**LDA**). Around
    the 1940s, an alternative to LDA, known as **logistic regression**, was proposed
    and all these approaches not only improved with time, but also inspired the development
    of other new algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: During those times, computation was a big problem as it was done using pen and
    paper. So fitting non-linear equations was not quite feasible as it required a
    lot of computations. After the 1980s, with improvements in technology and the
    introduction of computer systems, classification/regression trees were introduced.
    Slowly, with further advancements in technology and computing systems, statistical
    learning in a way converged with what is now known as machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in the previous section, machine learning is all about learning
    by example data. Based on how the algorithms understand data and get trained on
    it, they are broadly divided into two categories: **supervised learning** and
    **unsupervised learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised statistical learning involves building a model based on one or more
    inputs for a particular output. This means that the output that we get can supervise
    our analysis based on the inputs we supply. In other words, for each observation of
    the predictor variables (for example, age, education, and expense variables),
    there is an associated response measurement of the outcome variable (for example,
    salary). Refer to the following table to get an idea of the example dataset where
    we are trying to predict the **Salary** based on the **Age**, **Education,** and
    **Expense** variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised learning](img/image_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Supervised algorithms can be used for predicting, estimating, classifying, and
    other similar requirements which we will cover in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised statistical learning involves building a model based on one or
    more inputs but with no intention to produce a specific output. This means that
    there is no response/output variable to predict explicitly; but the output is
    usually the groups of data points that share some similar characteristics. Unlike
    supervised learning, you are not aware of the groups/labels to classify the data
    points into, per say, and you leave it to the algorithm to decide by itself.
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is no concept of a `training` dataset that is used to `relate` the
    outcome variable with the `predictor` variables by building a model and then validate
    the model using the `test` dataset. The output of unsupervised algorithm cannot
    supervise your analysis based on the inputs you supply. Such algorithms can learn
    relationships and structure from data. *Clustering* and *Association rule learning*
    are examples of unsupervised learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image depicts how clustering is used to group the data items
    that share some similar characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/image_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MLlib and the Pipeline API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us first learn some Spark fundamentals to be able to perform the machine
    learning operations on it. We will discuss the MLlib and the pipeline API in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLlib is the machine learning library built on top of Apache Spark which homes
    most of the algorithms that can be implemented at scale. The seamless integration
    of MLlib with other components such as GraphX, SQL, and Streaming provides developers
    with an opportunity to assemble complex, scalable, and efficient workflows relatively
    easily. The MLlib library consists of common learning algorithms and utilities
    including classification, regression, clustering, collaborative filtering, and
    dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib works in conjunction with the `spark.ml` package which provides a high
    level Pipeline API. The fundamental difference between these two packages is that
    MLlib (`spark.mllib`) works on top of RDDs whereas the ML (`spark.ml`) package
    works on top of DataFrames and supports ML Pipeline. Currently, both packages
    are supported by Spark but it is recommended to use the `spark.ml` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamental data types in this library are vectors and matrices. Vectors are
    local, and may be dense or sparse. Dense vectors are stored as an array of values.
    Sparse vectors are stored as two arrays; the first array stores the non-zero value
    indices and the second array stores the actual values. All element values are
    stored as doubles and indices are stored as integers starting from zero. Understanding
    the fundamental structures goes a long way in effective use of the libraries and
    it should help code up any new algorithm from scratch. Let us see some example
    code for a better understanding of these two vector representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Matrices may be local or distributed, dense or sparse. A local matrix is stored
    on a single machine as a single dimensional array. A dense local matrix is stored
    in column major order (column members are contiguous) whereas a sparse matrix
    values are stored in **Compressed Sparse Column** (**CSC**) format in column major
    order. In this format, the matrix is stored in the form of three arrays. The first
    array contains row indices of non-zero values, the second array has the beginning
    value index for each column, and the third one is an array of all the non-zero
    values. Indices are of type integer starting from zero. The first array contains
    values from zero to the number of rows minus one. The third array has elements
    of type double. The second array requires some explanation. Every entry in this
    array corresponds to the index of the first non-zero element in each column. For
    example, assume that there is only one non-zero element in each column in a 3
    by 3 matrix. Then the second array would contain 0,1,2 as its elements. The first
    array contains row positions and the third array contains three values. If none
    of the elements in a column are non-zero, you will note the same index repeating
    in the second array. Let us examine some example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Distributed matrices are the most sophisticated ones and choosing the right
    type of distributed matrix is very important. A distributed matrix is backed by
    one or more RDDs. The row and column indices are of the type `long` to support
    very large matrices. The basic type of distributed matrix is a `RowMatrix`, which
    is simply backed by an RDD of its rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each row in turn is a local vector. This is suitable when the number of columns
    is very low. Remember, we need to pass RDDs to create distributed matrices, unlike
    the local ones. Let us look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: An `IndexedRowMatrix` stores a row index prefixed to the row entry. This is
    useful in executing joins. You need to pass `IndexedRow` objects to create an
    `IndexedRowMatrix`. An `IndexedRow` object is a wrapper with a long `Index` and
    a `Vector` of row elements.
  prefs: []
  type: TYPE_NORMAL
- en: A `CoordinatedMatrix` stores data as tuples of row, column indexes, and element
    value. A `BlockMatrix` represents a distributed matrix in blocks of local matrices.
    Methods to convert matrices from one type to another are provided but these are
    expensive operations and should be used with caution.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A real life machine learning workflow is an iterative cycle of data extraction,
    data cleansing, pre-processing, exploration, feature extraction, model fitting,
    and evaluation. ML Pipeline on Spark is a simple API for users to set up complex
    ML workflows. It was designed to address some of the pain areas such as parameter
    tuning, or training many models based on different splits of data (cross-validation),
    or different sets of parameters. Writing scripts to automate this whole thing
    is no more a requirement and can be taken care of within the Pipeline API itself.
  prefs: []
  type: TYPE_NORMAL
- en: The Pipeline API consists of a series of pipeline stages (implemented as abstractions
    such as *transformers* and *estimators*) to get executed in a desired order.
  prefs: []
  type: TYPE_NORMAL
- en: In the ML Pipeline, you can invoke the data cleaning/transformation functions
    as discussed in the previous chapter and call the machine learning algorithms
    that are available in the MLlib. This can be done in an iterative fashion till
    you get the desired performance of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '![ML pipeline](img/image_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A transformer is an abstraction which implements the `transform()` method to
    convert one DataFrame into another. If the method is a feature transformer, the
    resulting DataFrame might contain some additional transformed columns based on
    the operation you performed. However, if the method is a learning model, then
    the resulting DataFrame would contain an extra column with predicted outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An Estimator is an abstraction that can be any learning algorithm which implements
    the `fit()` method to get trained on a DataFrame to produce a model. Technically,
    this model is a transformer for the given DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Logistic regression is a learning algorithm, hence an estimator. Calling
    `fit()` trains a logistic regression model, which is a resultant model, and hence
    a transformer which can produce a DataFrame containing a predicted column.'
  prefs: []
  type: TYPE_NORMAL
- en: The following example demonstrates a simple, single stage pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: "The above example showed pipeline creation and execution although with a single\
    \ stage, a Tokenizer in this context. Spark provides several \"\x80\x9Cfeature\
    \ transformers\x80\" out of the box. These feature transformers are quite handy\
    \ during data cleaning and data preparation phases."
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows a real world example of converting raw text into
     feature vectors. If you are not familiar with TF-IDF, read this short tutorial
    from [http://www.tfidf.com](http://www.tfidf.com).
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This example has created and executed a multi-stage pipeline that has converted
    text to a feature vector that can be processed by machine learning algorithms.
    Let us see a few more features before we move on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections of the book, we learnt how the response/outcome variable
    is related to the predictor variables, typically in a supervised learning context.
    There are various different names for both of those types of variables that people
    use these days. Let us see some of the synonymous terms for them and we will use
    them interchangeably in the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input variables (X)**: Features, predictors, explanatory variables, independent
    variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output variables (Y)**: Response variable, dependent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there is a relation between *Y* and *X* where *X=X[1], X[2], X[3],..., X[n]*
    (n different predictors) then it can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![Introduction to machine learning](img/image_06_005.jpg)is a function
    that represents how *X* describes *Y* and is unknown! This is what we figure out
    using the observed data points at hand. The term
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is a random error term with mean zero and is independent of *X*.
  prefs: []
  type: TYPE_NORMAL
- en: There are basically two types of errors associated with such an equation - reducible
    errors and irreducible errors. As the name suggests, a reducible error is associated
    with the function and can be minimized by improving the accuracy of
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: by using a better learning algorithm or by tuning the same algorithm. Since
    *Y* is also a function of
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', which is independent of *X*, there would still be some error associated that
    cannot be addressed. This is called an irreducible error ('
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ). There are always some factors which influence the outcome variable but are
    not considered in building the model (as they are unknown most of the time), and
    contribute to the irreducible error term. So, our approaches discussed throughout
    this book will only be focused on minimizing the reducible error.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the machine learning models that we build can be used for either prediction
    or for inference, or a combination of both. For some of the algorithms, the function
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: can be represented as an equation which tells us how the dependent variable
    *Y* is related to the independent variables (*X1*, *X2*,..., *Xn*). In such cases,
    we can do both inference and prediction. However, some of the algorithms are black
    box, where we can only predict and no inference is possible, because how *Y* is
    related to *X* is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the linear machine learning models can be more apt for an inference
    setting because they are more interpretable to business users. However, on a prediction
    setting, there can be better algorithms providing more accurate predictions but
    they are less interpretable. When inference is the target, we should prefer the
    restrictive models such as linear regression for better interpretability, and
    when only prediction is the goal, we may choose to use highly flexible models
    such as **Support Vector Machines** (**SVM**) that are less interpretable and
    more accurate (this may not hold true in all cases, however). You need to be careful
    in choosing an algorithm based on the business requirement, by accounting for
    the trade-off between interpretability and accuracy. Let us dive deeper into understanding
    the fundamentals behind these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, we need a set of data points (training data) to build a model to
    estimate
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*(X)* so that *Y =*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to machine learning](img/image_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*(X)*. Broadly, such learning methods can be either parametric or non-parametric.'
  prefs: []
  type: TYPE_NORMAL
- en: Parametric methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parametric methods follow a two-step process. In the first step, you assume
    the shape of
  prefs: []
  type: TYPE_NORMAL
- en: '![Parametric methods](img/image_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*()*. For example, *X* is linearly related to *Y*, so the function of *X,*
    which is'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parametric methods](img/image_06_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*(X),* can be represented with a linear equation as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parametric methods](img/Beta1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the model is selected, the second step is to estimate the parameters
    *Î²0*, *Î²1*,..., *Î²n* by using the data points at hand to train the model, so
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parametric methods](img/Beta-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The one disadvantage to this parametric approach is that our assumption of linearity
    for ![Parametric methods](img/image_06_016.jpg) *()* might not hold true in real
    life situations.
  prefs: []
  type: TYPE_NORMAL
- en: Non-parametric methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We do not make any assumptions about the linear relation between *Y* and *X*
    as well as data distributions of variables, and hence the form of
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-parametric methods](img/image_06_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*()* in non-parametric. Since it does not assume any form of'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-parametric methods](img/image_06_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*()*, it can produce better results by fitting well with data points, which
    could be an advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the non-parametric methods require more data points compared to parametric
    methods to estimate
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-parametric methods](img/image_06_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*()* accurately. Note however, it can lead to overfitting problems if not handled
    properly. We will discuss more on this as we move further.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression methods are a type of supervised learning. If the response variable
    is quantitative/continuous (takes on numeric values such as age, salary, height,
    and so on), then the problem can be called a regression problem regardless of
    the explanatory variables' type. There are various kinds of modeling techniques
    to address the regression problems. In this section, our focus will be on linear
    regression techniques and some different variations of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression methods can be used to predict any real valued outcomes. Following
    are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the salary of an employee based on his educational level, location,
    type of job, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict stock prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict buying potential of a customer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the time a machine would take before failing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Further to what we discussed in the previous section *Parametric methods*, after
    the assumption of linearity is made for
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*(X)*, we need the training data to fit a model that would describe the relation
    between explanatory variables (denoted as *X*) and the response variable (denoted
    as *Y*). When there is only one explanatory variable present, it is called simple
    linear regression and when there are multiple explanatory variables present, it
    is called multiple linear regression. The simple linear regression is all about
    fitting a straight line in a 2-D setting, and when there are say two predictor
    variables, it would fit a plane in a 3-D setting, and so on for higher dimensional
    settings when there are more than two variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The usual form of a linear regression equation can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: Y' =
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (X) +
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *Y'* represents the predicted outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear regression equation with only one predictor variable can be given
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/Beta11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A linear regression equation with multiple predictor variables can be given
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/Beta22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![Linear regression](img/image_06_025.jpg) is the irreducible error term
    independent of *X* and has a mean of zero. We do not have any control over it,
    but we can work towards optimizing
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*(X)*. Since none of the models can achieve a 100 percent accuracy, there would
    always be some error associated with it because of the irreducible error component
    ('
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common approach of fitting a linear regression is called **least squares**,
    also known as, the **Ordinary Least Squares** (**OLS**) approach. This method
    finds the regression line that best fits the observed data points by minimizing
    the sum of squares of the vertical deviations from each data point to the regression
    line. To get a better understanding on how the linear regression works, let us
    look at a simple linear regression of the following form for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/Beta33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where, *Î²0* is the Y-intercept of the regression line and *Î²1* defines the
    slope of the line. What it means is that *Î²1* is the average change in *Y* for
    every one unit change in *X*. Let us take an example with *X* and *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **X** | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 38 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | 27 |'
  prefs: []
  type: TYPE_TB
- en: 'If we fit a linear regression line through the data points as shown in the
    preceding table, then it would appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The red vertical lines in the preceding figure indicate the error of prediction
    which can be defined as the difference between the actual *Y* value and the predicted
    *Y''* value. If you square these differences and sum them up, it is called the
    **Sum of Squared Error** (**SSE**), which is the most common measure that is used
    to find the best fitting line. The following table shows how to calculate the
    SSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **X** | **Y** | **Y''** | **Y-Y''** | **(Y-Y'') 2** |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 12 | 12.4 | 0.4 | 0.16 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 20 | 17.2 | 2.8 | 7.84 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 13 | 22 | -9 | 81 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 38 | 26.8 | 11.2 | 125.44 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | 27 | 31.6 | -4.6 | 21.16 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | SUM | 235.6 |'
  prefs: []
  type: TYPE_TB
- en: 'In the above table, the term **(Y-Y'')** is called the residual. The **Residual
    Sum of Squares** (**RSS**) can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*RSS = residual[1]² + residual[2]² + residual[3]² + ......+ residual[n]²*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that regression is highly susceptible to outliers and can introduce huge
    RSS error if not handled prior to applying regression.
  prefs: []
  type: TYPE_NORMAL
- en: After a regression line is fit into the observed data points, you should examine
    the residuals by plotting them on the Y-Axis against explanatory the variable
    on the X-Axis. If the plot is nearly a straight line, then your assumption about
    linear relationship is valid, or else it may indicate the presence of some kind
    of non-linear relationship. In case of the presence of nonlinear relationships,
    you may have to account for the non-linearity. One of the techniques is by adding
    higher order polynomials to the equation.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that RSS was an important characteristic in fitting the regression line
    (while building the model). Now, to assess how good your regression fit is (once
    the model is built), you need two other statistics - **Residual Standard Error**
    (**RSE**) and **R²** statistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed the irreducible error component *Îµ*, because of which there would
    always be some level of error with your regression (even if your equation exactly
    fits your data points and you have estimated the coefficients properly). RSE is
    an estimate of standard deviation of *Îµ* which can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that the actual values would deviate from the true regression line
    by a factor of RSE on an average.
  prefs: []
  type: TYPE_NORMAL
- en: Since RSE is actually measured in the units of *Y* (refer to how we calculated
    RSS in the previous section), it is difficult to say that it is the only best
    statistic for the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, an alternative approach was introduced, called the R² statistic (also known
    as the coefficient of determination). The formula to calculate R² is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Total Sum of Squares** (**TSS**) can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image_06_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note here that TSS measures the total variance inherent in *Y* even before performing
    the regression to predict *Y*. Observe that there is no *Y'* in it. On the contrary,
    RSS represents the variability in *Y* that is unexplained after regression. This
    means that (*TSS - RSS*) is able to explain the variability in response after
    regression is performed.
  prefs: []
  type: TYPE_NORMAL
- en: The *R²* statistic usually ranges from 0 to 1, but can be negative if the fit
    is worse than fitting just a horizontal line, but that is rarely the case. A value
    close to 1 indicates that the regression equation could explain a large proportion
    of the variability in the response variable and is a good fit. On the contrary,
    a value close to 0 indicates that the regression did not explain much of the variance
    in the response variable and is not a good fit. As an example, an *R²* of 0.25
    means that 25 percent of the variance in *Y* is explained by *X* and is indicating
    to tune the model for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now discuss how to address the non-linearity in the dataset through regression.
    As discussed earlier, when you find nonlinear relations, it needs to be handled
    properly. To model a non-linear equation using the same linear regression technique,
    you have to create the higher order features, which will be treated as just another
    variable by the regression technique. For example, if *salary* is a feature/variable
    that is predicting the *buying potential*, and we find that there is a non-linear
    relationship between them, then we might create a feature called (*salary3*) depending
    on how much of the non-linearity needs to be addressed. Note that while you create
    such higher order features, you also have to keep the base features. In this example,
    you have to use both (*salary*) and (*salary3*) in the regression equation.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have kind of assumed that all the predictor variables are continuous.
    What if there are categorical predictors? In such cases, we have to dummy-code
    those variables (say 1 for male and 0 for female) so that the regression technique
    generates two equations, one for gender = male (the equation will have the gender
    variable) and the other for gender = female (the equation will not have the gender
    variable as it will be dropped as coded 0). At times, with very few categorical
    variables, it may be a good idea to divide the dataset based on the levels of
    categorical variables and build separate models for them.
  prefs: []
  type: TYPE_NORMAL
- en: One major advantage of the least squares linear regression is that it explains
    how the outcome variable is related to the predictor variables. This makes it
    very interpretable and can be used to draw inferences as well as to do predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many machine learning problems can be formulated as a convex optimization problem.
    The objective of this problem is to find the values of the coefficients for which
    the squared loss is minimum. This objective function has basically two components
    - regularizer and the loss function. The regularizer is there to control the complexity
    of the model (so it does not overfit) and the loss function is there to estimate
    the coefficients of the regression function for which squared loss (RSS) is minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function used for least squares is called **squared loss**, as shown
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/image_06_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *Y* is the response variable (real valued), *W* is the weight vector (value
    of the coefficients), and *X* is the feature vector. So
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/Capture-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: gives the predicted values which we equate with the actual values *Y* to find
    the squared loss that needs to be minimized.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm used to estimate the coefficients is called **gradient descent**.
    There are different types of loss functions and optimization algorithms for different
    kinds of machine learning algorithms which we will cover as and when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultimately, the linear methods have to optimize the loss function. Under the
    hood, linear methods use convex optimization methods to optimize the objective
    functions. MLlib has **Stochastic Gradient Descent** (**SGD**) and **Limited Memory
    - Broyden-Fletcher-Goldfarb-Shanno** (**L-BFGS**) supported out of the box. Currently,
    most algorithm APIs support SGD and a few support L-BFGS.
  prefs: []
  type: TYPE_NORMAL
- en: SGD is a first-order optimization technique that works best for large scale
    data and distributed computing environment. Optimization problems whose objective
    function (loss function) is written as a sum are best suited to be solved using
    SGD.
  prefs: []
  type: TYPE_NORMAL
- en: L-BFGS is an optimization algorithm in the family of quasi-Newton methods to
    solve the optimization problems. L-BFGS often achieves a rapider convergence compared
    with other first-order optimization techniques such as SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the linear methods available in MLlib support both SGD and L-BFGS. You
    should choose one over the other depending on the objective function under consideration.
    In general, L-BFGS is recommended over SGD as it converges faster but you need
    to evaluate carefully based on the requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizations on regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With large weights (coefficient values), it is easier to overfit the model.
    Regularization is a technique used mainly to eliminate the overfitting problem
    by controlling the complexity of the model. This is usually done when you see
    a difference between the model performance on training data and test data. If
    the training performance is more than that of the test data, it could be a case
    of overfitting (high variance case).
  prefs: []
  type: TYPE_NORMAL
- en: To address this, a regularization technique was introduced that would penalize
    the loss function. It is always recommended to use any of the regularizations
    techniques, especially when the training data has a small number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: Before we discuss further on the regularization techniques, we have to understand
    what *bias* and *variance* mean in a supervised learning setting and why there
    is always a trade-off associated. While both are related to errors, a *biased*
    model means that it is biased towards some erroneous assumption and may miss the
    relation between the predictor variables and the response variable to some extent.
    This is a case of underfitting! On the other hand, a *high variance* model means
    that it tries to touch every data point and ends up modelling the random noise
    present in the dataset. It represents the case of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with the L2 penalty (L2 regularization) is called **ridge
    regression** and with the L1 penalty (L1 regularization) is called **lasso regression**.
    When both L1 and L2 penalties are used together, it is called **elastic net regression**.
    We will discuss them one by one in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularized problems are usually easy to solve compared to L1 regularized
    problems due to smoothness, but the L1 regularized problems can cause sparsity
    in weights leading to smaller and more interpretable models. Because of this,
    lasso is at times used for feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we add the L2 penalty (also known as the **shrinkage penalty**) to the
    loss function of least squares, it becomes the ridge regression, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ridge regression](img/image_06_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *λ* (greater than 0) is a tuning parameter which is determined separately.
    The second term in the preceding equation is called the shrinkage penalty and
    can be small only if the coefficients (*Î²0*, *Î²1*...and so on) are small and
    close to 0\. When *λ = 0*, the ridge regression becomes least squares. As lambda
    approaches infinity, the regression coefficients approach zero (but are never
    zero).
  prefs: []
  type: TYPE_NORMAL
- en: The ridge regression generates different sets of coefficient values for each
    value of *λ*. So, the lambda value needs to be carefully selected using cross-validation.
    As we increase the lambda value, the flexibility of the regression line decreases,
    thereby decreasing variance and increasing bias.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the shrinkage penalty is applied to all the explanatory variables
    except the intercept term *Î²0*.
  prefs: []
  type: TYPE_NORMAL
- en: The ridge regression works really well when the training data is less or even
    in the case where the number of predictors or features are more than the number
    of observations. Also, the computation needed for ridge is almost the same as
    that of least squares.
  prefs: []
  type: TYPE_NORMAL
- en: Since ridge does not reduce any coefficient value to zero, all the variables
    will be present in the model which can make it less interpretable if the number
    of variables is high.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lasso was introduced after ridge. When we add the L1 penalty to the loss function
    of least squares, it becomes lasso regression, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lasso regression](img/image_06_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The difference here is that instead of taking the squared coefficients, it takes
    the mod of the coefficient. Unlike ridge, it can force some of its coefficients
    to be exactly zero which can result in elimination of some of the variables. So,
    lasso can be used for variable selection as well!
  prefs: []
  type: TYPE_NORMAL
- en: Lasso generates different sets of coefficient values for each value of lambda.
    So lambda value needs to be carefully selected using cross-validation. Like ridge,
    as you increase lambda, variance decreases and bias increases.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso produces better interpretable models compared to ridge because it usually
    has a subset of the total number of variables. When there are many categorical
    variables, it is advisable to choose lasso over ridge.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, neither ridge nor lasso is always better over the other. Lasso usually
    performs well with a small number of predictor variables that have substantial
    coefficients and the rest have very small coefficients. Ridge usually performs
    better when there are many predictors and almost all have substantial yet similar
    coefficient sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge is good for grouped selection and can also address multicollinearity problems.
    Lasso, on the other hand, cannot do grouped selection and tends to pick only one
    of the predictors. Also, if a group of predictors are highly correlated amongst
    themselves, Lasso tends to pick only one of them and shrink the others to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we add both L1 and L2 penalties to the loss function of least squares,
    it becomes elastic net regression, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Elastic net regression](img/image_06_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are the advantages of elastic net regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Enforces sparsity and helps remove least effective variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encourages grouping effect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combines the strengths of both ridge and lasso
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Naive version of elastic net regression incurs a double shrinkage problem
    which leads to increased bias and poorer prediction accuracy. To address this,
    one approach could be rescaling the estimated coefficients by multiplying (*1+
    λ2*) with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Classification methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the response variable is qualitative/categorical (takes on categorical values
    such as gender, loan default, marital status, and such), then the problem can
    be called a classification problem regardless of the explanatory variables' type.
    There are various types of classification methods, but we will focus on logistic
    regression and Support Vector Machines in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are a few examples of some implications of classification methods:'
  prefs: []
  type: TYPE_NORMAL
- en: A customer buys a product or does not buy it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A person is diabetic or not diabetic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An individual applying for a loan would default or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An e-mail receiver would read the e-mail or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression measures the relation between the explanatory variables
    and the categorical response variable. We do not use linear regression for the
    categorical response variable because the response variable is not on a continuous
    scale and hence the error terms are not normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'So logistic regression is a classification algorithm. Instead of modelling
    the response variable *Y* directly, logistic regression models the probability
    distribution of *P(Y*|*X)* that *Y* belongs to a particular category. The conditional
    distribution of (*Y*|*X*) is a Bernoulli distribution rather than a Gaussian distribution.
    The logistic regression equation can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image_06_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a two class classification, the output of the model should be restricted
    to only one of the two classes (say either 0 or 1). Since logistic regression
    predicts probabilities and not classes directly, we use a logistic function (also
    known as the, *sigmoid function*) to restrict the output to a single class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image_06_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Solving for the preceding equation gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/Capture-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be further simplified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image_06_040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The quantity on the left *P(X)/1-P(X)* is called the *odds*. The value of odds
    ranges from 0 to infinity. The values close to 0 indicate very less probability
    and the ones bigger in numbers indicate high probability. At times odds are used
    directly instead of probabilities, depending on the situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take the log of the odds, it becomes log-odd or logit and can be shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image_06_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see from the previous equation that logit is linearly related to *X*.
  prefs: []
  type: TYPE_NORMAL
- en: In the situation where there are two classes, 1 and 0, then we predict *Y =
    1* if *p >= 0.5* and *Y = 0* when *p < 0.5*. So logistic regression is actually
    a linear classifier with decision boundary at *p = 0.5*. There could be business
    cases where *p* is just not set to 0.5 by default and you may have to figure out
    the right value using some mathematical techniques.
  prefs: []
  type: TYPE_NORMAL
- en: A method known as maximum likelihood is used to fit the model by computing the
    regression coefficients, and the algorithm can be a gradient descent like in a
    linear regression setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In logistic regression, the loss function should address the misclassification
    rate. So, the loss function used for logistic regression is called *logistic loss*,
    as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image_06_042.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that logistic regression is also prone to overfitting when you use higher
    order polynomial to better fit a model. To solve this, you can use regularization
    terms like you did in linear regression. As of this writing, Spark does not support
    regularized logistic regression so we will skip this part for now.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Support Vector Machines (SVM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVM**) is a type of supervised machine learning
    algorithm and can be used for both classification and regression. However, it
    is more popular in addressing the classification problems, and since Spark offers
    it as an SVM classifier, we will limit our discussion to the classification setting
    only. When used as a classifier, unlike logistic regression, it is a non-probabilistic
    classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: The SVM has evolved from a simple classifier called the **maximal margin classifier**.
    Since the maximal margin classifier required that the classes be separable by
    a linear boundary, it could not be applied to many datasets. So it was extended
    to an improved version called the **support vector classifier** that could address
    the cases where the classes overlapped and there were no clear separation between
    the classes. The support vector classifier was further extended to what we call
    an SVM to accommodate the non-linear class boundaries. Let us discuss the evolution
    of the SVM step by step so we get a clear understanding of how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there are *p* dimensions (features) in a dataset, then we fit a hyperplane
    in that p-dimensional space whose equation can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear Support Vector Machines (SVM)](img/image_06_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This hyperplane is called the separating hyperplane that forms the decision
    boundary. The result will be classified based on the result; if greater than 0,
    then on one side and if less than 0, then on the other side, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear Support Vector Machines (SVM)](img/image_06_044.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Observe in the preceding figure that there can be multiple hyperplanes (they
    can be infinite). There should be a reasonable way to choose the best hyperplane.
    This is where we select the maximal margin hyperplane. If you compute the perpendicular
    distance of all data points to the separating hyperplane, then the smallest distance
    would be called as the margin. So, for the maximal margin classifier, the hyperplane
    should have the highest margin.
  prefs: []
  type: TYPE_NORMAL
- en: The training observations that are close yet equidistant from the separating
    hyperplane are known as support vectors. For any slight change in the support
    vectors, the hyperplane would also get reoriented. These support vectors actually
    define the margin. Now, what if the two classes under consideration are not separable?
    We would probably want a classifier that does not perfectly separate the two classes
    and has a softer boundary that allows some level of misclassification as well.
    This requirement led to the introduction of the support vector classifier (also
    known as the soft margin classifier).
  prefs: []
  type: TYPE_NORMAL
- en: "Mathematically, it is the slack variable in the equation that allows for misclassification.\
    \ Also, there is a tuning parameter in the support vector classifier which should\
    \ be selected using cross-\x80\x93validation. This tuning parameter is the one\
    \ that trades off between bias and variance and should be handled with care. When\
    \ it is large, the margin is wider and includes many support vectors, and has\
    \ low variance and high bias. If it is small, then the margin will have fewer\
    \ support vectors and the classifier will have low bias but high variance."
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function for the SVM can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear Support Vector Machines (SVM)](img/image_06_045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As of this writing, Spark supports only linear SVMs. By default, linear SVMs
    are trained with an L2 regularization. Spark also supports alternative L1 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far so good! But how would the support vector classifier work when there
    is a non-linear boundary between the classes, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear Support Vector Machines (SVM)](img/image_06_046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Any linear classifier, such as a support vector classifier, would perform very
    poorly in the preceding situation. If it draws a straight line through the data
    points, then the classes would not be separated properly. This is a case of non-linear
    class boundaries. A solution to this problem is the SVM. In other words, when
    a support vector classifier is fused with a non-linear kernel, it becomes an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the way we introduced higher order polynomial terms in the regression
    equation to account for the non-linearity, something can also be done in the SVM
    context. The SVM uses something called kernels to take care of different kinds
    of non-linearity in the dataset; different kernels for different kinds of non-linearity.
    Kernel methods map the data into higher dimensional space as the data might get
    well separated if it does so. Also, it makes distinguishing different classes
    easier. Let us discuss a few of the important kernels so as to be able to select
    the right one.
  prefs: []
  type: TYPE_NORMAL
- en: Linear kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the most basic type of kernels that allows us to pick out only
    lines or hyperplanes. It is equivalent to a support vector classifier. It cannot
    address the non-linearity if present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This allows us to address some level of non-linearity to the extent of the order
    of polynomials. This works well when the training data is normalized. This kernel
    usually has more hyperparameters and therefore increases the complexity of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Radial Basis Function kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are not really sure of which kernel to use, **Radial Basis Function**
    (**RBF**) can be a good default choice. It allows you to pick out even circles
    or hyperspheres. Though this usually performs better than linear or polynomial
    kernel, it does not perform well when the number of features is huge.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sigmoid kernel has its roots in neural networks. So, an SVM with a sigmoid
    kernel is equivalent to a neural network with a two layered perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Training an SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While training an SVM, the modeler has to take a number of decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: How to pre-process the data (transformation and scaling). The categorical variables
    should be converted to numeric ones by dummifying them. Also, scaling the numeric
    values is needed (either 0 to 1 or -1 to +1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "Which kernel to use (check using cross-\x80\x93validation if you are unable\
    \ to visualize the data and/ or conclude on it)."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "What parameters to set for the SVM: penalty parameter and the kernel parameter\
    \ (find using cross-\x80\x93validation or grid search)"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If needed, you can use an entropy based feature selection to include only the
    important features in your model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`mllib` has already entered maintenance mode and SVM is still not available
    under ml so only Scala code is provided for illustration.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a non-parametric supervised learning algorithm which can
    be used for both classification and regression. Decision trees are like inverted
    trees with the root node at the top and leaf nodes forming downwards. There are
    different algorithms to split the dataset into branch-like segments. Each leaf
    node is assigned to a class that represents the most appropriate target values.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees do not require any scaling or transformations of the dataset
    and work as the data is. They can handle both categorical and continuous features,
    and also address non-linearity in the dataset. At its core, a decision tree is
    a greedy algorithm (it considers the best split at the moment and does not take
    into consideration the future scenarios) that performs a recursive binary partitioning
    of the feature space. Splitting is done based on information gain at each node
    because information gain measures how well a given attribute separates the training
    examples as per the target class or value. The first split happens for the feature
    that generates maximum information gain and becomes the root node.
  prefs: []
  type: TYPE_NORMAL
- en: The information gain at a node is the difference between the parent node impurity
    and the weighted sum of two child node impurities. To estimate information gain,
    Spark currently has two impurity measures for classification problems and one
    impurity measure for regression, as explained next.
  prefs: []
  type: TYPE_NORMAL
- en: Impurity measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Impurity is a measure of homogeneity and the best criteria for recursive partitioning.
    By calculating the impurity, the best split candidate is decided. Most of the
    impurity measures are probability based:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Probability of a class = number of observations of that class / total number
    of observations*'
  prefs: []
  type: TYPE_NORMAL
- en: Let us spend some time on different types of important impurity measures that
    are supported by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Gini Index is mainly intended for the continuous attributes or features
    in a dataset. If not, it would assume that all the attributes and features are
    continuous. The split makes the child nodes more *purer* than the parent node.
    Gini tends to find the largest class - the class of response variable that has
    got the maximum observations. It can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gini Index](img/image_06_047.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If all observations of a response belong to a single class, then probability
    *P* of that class *j*, that is (*Pj*), will be 1 as there is only one class, and
    *(Pj)2* would also be 1\. This makes the Gini Index to be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Entropy is mainly intended for the categorical attributes or features in a
    dataset. It can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Entropy](img/image_06_048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If all observations of a response belong to a single class, then the probability
    of that class (*Pj*) will be 1, and *log(P)* would be zero. This makes the entropy
    to be zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph depicts the probability of a fair coin toss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Entropy](img/Capture-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Just to explain the preceding graph, if you toss a fair coin, the probability
    of a head or a tail would be 0.5, so there will be maximum observations at a probability
    of 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: If the data sample is completely homogeneous then the entropy will be zero,
    and if the sample can be equally divided into two, then the entropy will be one.
  prefs: []
  type: TYPE_NORMAL
- en: It is a little slower to compute than Gini because it has to compute the log
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the Gini Index and entropy, variance is used for calculating information
    gain for regression problems. Variance can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance](img/image_06_050.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stopping rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recursive tree construction is stopped at a node when one of the following
    conditions is met:'
  prefs: []
  type: TYPE_NORMAL
- en: The node depth is equal to the `maxDepth` training parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No split candidate leads to an information gain greater than `minInfoGain`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No split candidate produces child nodes, each of which have at least a `minInstancesPerNode`
    training instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split candidates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A dataset typically has a mixture of categorical and continuous features. How
    the features get split further into split candidates is something we should understand
    because we at times need some level of control over them to build a better model.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: "For a categorical feature with *M* possible values (categories), one could\
    \ come up with *2(M-\x88\x921)-\x88\x921* split candidates. Whether for binary\
    \ classification or regression, the number of split candidates can be reduced\
    \ to *M-\x88\x921* by ordering the categorical feature values by the average label."
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a binary classification (0/1) problem with one categorical
    feature that has three categories A, B, and C, and their corresponding proportions
    of label-1 response variables are 0.2, 0.6, and 0.4 respectively. In this case,
    the categorical features can be ordered as A, C, B. So, the two split candidates
    (*M-1* = *3-1* = *2*) can be *A | (C, B)* and *A, (C | B)* where '*|'* denotes
    the split.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a continuous feature variable, there can be a chance that no two values
    are the same (at least we can assume so). If there are *n* observations, then
    *n* split candidates might not be a good idea, especially in a big data setting.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, it is done by performing a quantile calculation on a sample of data,
    and binning the data accordingly. You can still have control over the maximum
    bins that you would like to allow, using the `maxBins` parameter. The maximum
    default value for `maxBins` is `32`.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: They are simple to understand and interpret, so easy to explain to business
    users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They works for both classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both qualitative and quantitative data can be accommodated in constructing the
    decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information gains in decision trees are biased in favor of the attributes with
    more levels.
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages of decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: They do not work that greatly for effectively continuous outcome variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance is poor when there are many classes and the dataset is small
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axis parallel split reduces the accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They suffer from high variance as they try to fit almost all data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: "Implementation -\x80\x93 wise there are no major differences between classification\
    \ and regression trees. Let us have a look at the practical implementation of\
    \ it on Spark."
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, ensemble methods use multiple learning algorithms to obtain
    a more accurate model in terms of prediction accuracy. Usually these techniques
    require more computing power and make the model more complex, which makes it difficult
    to interpret. Let us discuss the various types of ensemble techniques available
    on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A random forest is an ensemble technique for the decision trees. Before we get
    to random forests, let us see how it has evolved. We know that decision trees
    usually have high variance issues and tend to overfit the model. To address this,
    a concept called *bagging* (also known as bootstrap aggregating) was introduced.
    For the decision trees, the idea was to take multiple training sets (bootstrapped
    training sets) from the dataset and create separate decision trees out of those,
    and then average them out for regression trees. For the classification trees,
    we can take the majority vote or the most commonly occurring class from all the
    trees. These trees grew deep and were not pruned at all. This definitely reduced
    the variance though the individual trees might have high variance.
  prefs: []
  type: TYPE_NORMAL
- en: One problem with the plain bagging approach was that for most of the bootstrapped
    training sets, the strong predictors took their positions at the top split which
    almost made the bagged trees look similar. This meant that the prediction also
    looked similar and if you averaged them out, then it did not reduce the variance
    to the extent expected. To address this, a technique was needed which would take
    a similar approach as that of bagged trees but eliminate the correlation amongst
    the trees, hence the *random forest*.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, you build bootstrapped training samples to create decision
    trees, but the only difference is that every time a split happens, a random sample
    of P predictors are chosen from a total of say K predictors. This is how a random
    forest injects randomness to this approach. As a thumb rule, we can take P as
    the square root of Q.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like in the case of bagging, in this approach you also average the predictions
    if your goal is regression and take the majority vote if the goal is classification.
    Spark provides some tuning parameters to tune this model, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numTrees`: You can specify the number of trees to consider in the random forest.
    If the numbers are high then the variance in prediction would be less, but the
    time required would be more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxDepth`: You can specify the maximum depth of each tree. An increased depth
    makes the trees more powerful in terms of prediction accuracy. Though they tend
    to overfit the individual trees, the overall output is still good because we average
    the results anyway, which reduces the variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsamplingRate`: This parameter is mainly used to speed up training. It is
    used to set the bootstrapped training sample size. A value less than 1.0 speeds
    up the performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`featureSubsetStrategy`: This parameter can also help speed up the execution.
    It is used to set the number of features to use as split candidates for every
    node. It should be set carefully as too low or too high a value can impact the
    accuracy of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: They run faster as the execution happens in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are less prone to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are easy to tune
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction accuracy is more compared to trees or bagged trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They work well even when the predictor variables are a mixture of categorical
    and continuous features, and do not require scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-Boosted Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like random forests, **Gradient-Boosted Trees** (**GBTs**) are also an ensemble
    of trees. They can be applied to both classification and regression problems.
    Unlike bagged trees or random forests, where trees are built in parallel on independent
    datasets and are independent of each other, GBTs are built sequentially. Each
    tree is grown using the result of the previously grown tree. Note that GBTs do
    not work on bootstrapped samples.
  prefs: []
  type: TYPE_NORMAL
- en: On each iteration, GBTs use the current ensemble at hand to predict the labels
    for the training instances and compares them with true labels and estimates the
    error. The training instances with poor prediction accuracy get relabeled so that
    the decision trees get corrected in the next iteration based on the error rate
    for the previous mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mechanism behind finding the error rate and relabeling the instances is
    based on the loss function. GBTs are designed to reduce this loss function for
    every iteration. The following types of loss functions are supported by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log loss**: This is used for classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Squared error (L2 loss)**: This is used for regression problems and is set
    by default. It is the summation of the squared differences between the actual
    and predicted output for all the observations. Outliers should be treated well
    for this loss function to perform well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Absolute error (L1 loss)**: This is also used for regression problems. It
    is the summation of the absolute differences between the actual and predicted
    output for all the observations. It is more robust to outliers compared to squared
    error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark provides some tuning parameters to tune this model, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss`: You can pass a loss function as discussed in the previous section,
    depending on the dataset you are dealing with and whether you intend to do classification
    or regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numIterations`: Each iteration produces only one tree! If you set this very
    high, then the time needed for execution will also be high as the operation would
    be sequential and can also lead to overfitting. It should be carefully set for
    better performance and accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learningRate`: This is not really a tuning parameter. If the algorithm''s
    behavior is unstable then reducing this can help stabilize the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`algo`: *Classification* or *regression* is set based on what you want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GBTs can overfit the models with a greater number of trees, so Spark provides
    the `runWithValidation` method to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of this writing, GBTs on Spark do not yet support multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: "Let us look at an example to illustrate GBTs in action. The example dataset\
    \ contains average marks and attendance of twenty students. The data also contains\
    \ result as Pass or Fail, which follow a set of criteria. However, a couple of\
    \ students (ids 1009 and 1020) were \"\x80\x9Cgranted\" Pass status event though\
    \ they did not really qualify. Now our task is to check if the models pick up\
    \ these two students are not."
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pass criteria are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "Marks should be at least 40 and Attendance should be at least \"\x80\x9CEnough\""
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "If Marks are between 40 and 60, then attendance should be \"\x80\x9CFull\"\
    \ to pass"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following example also emphasizes on reuse of pipeline stages across multiple
    models. So, we build a DecisionTree classifier first and then a GBT. We build
    two different pipelines that share stages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Multilayer perceptron classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **multilayer perceptron classifier** (**MLPC**) is a feedforward artificial
    neural network with multiple layers of nodes connected to each other in a directed
    fashion. It uses a supervised learning technique called *backpropagation* for
    training the network.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes in the intermediary layer use the sigmoid function to restrict the output
    between 0 and 1, and the nodes in the output layer use the `softmax` function,
    which is a generalized version of the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised learning technique where there is no response
    variable to supervise the model. The idea is to cluster the data points that have
    some level of similarity. Apart from exploratory data analysis, it is also used
    as a part of a supervised pipeline where classifiers or regressors can be built
    on the distinct clusters. There are a bunch of clustering techniques available.
    Let us look into a few important ones that are supported by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-means is one of the most common clustering techniques. The k-means problem
    is to find cluster centers that minimize the intra-class variance, that is, the
    sum of squared distances from each data point being clustered to its cluster center
    (the center that is closest to it). You have to specify in advance the number
    of clusters you want in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since it uses the Euclidian distance measure to find the differences between
    the data points, the features need to be scaled to a comparable unit prior to
    using k-means. The Euclidian distance can be better explained in a graphical way
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/image_06_051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Given a set of data points (*x1*, *x2*, ..., *xn*) with as many dimensions
    as the number of variables, k-means clustering aims to partition the n observations
    into k (less than *n*) sets where *S = {S1, S2, ..., Sk}*, so as to minimize the
    **within-cluster sum of squares** (**WCSS**). In other words, its objective is
    to find:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/image_06_052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Spark requires the following parameters to be passed to this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '`k`: This is the number of desired clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIterations`: This is the maximum number of iterations to run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializationMode`: This specifies either random initialization or initialization
    via k-means||.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runs`: This is the number of times to run the k-means algorithm (k-means is
    not guaranteed to find a globally optimal solution, and when run multiple times
    on a given dataset, the algorithm returns the best clustering result).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializationSteps`: This determines the number of steps in the k-means||
    algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon`: This determines the distance threshold within which we consider
    k-means to have converged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initialModel`: This is an optional set of cluster centers used for initialization.
    If this parameter is supplied, only one run is performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of k-means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It works only on the numeric features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It requires scaling before implementing the algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is susceptible to local optima (the solution to this is k-means++)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us run k-means clustering on the same students data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explained various machine learning algorithms, how they
    are implemented in the MLlib library and how they can be used with the pipeline API
    for a streamlined execution. The concepts were covered with Python and Scala code
    examples for a ready reference.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how Spark supports R programming language
    focusing on some of the algorithms and their executions similar to what we covered
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supported algorithms in MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/mllib-decision-tree.html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark ML Programming Guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advanced datascience on spark.pdf from June 2015 summit slides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

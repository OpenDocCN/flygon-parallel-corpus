- en: Chapter 9. Visualizing Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proper data visualization has solved many business problems in the past without
    much statistics or machine learning being involved. Even today, with so many technological
    advancements, applied statistics, and machine learning, proper visuals are the
    end deliverables for business users to consume information or the output of some
    analyses. Conveying the right information in the right format is something that
    data scientists yearn for, and an effective visual is worth a million words. Also,
    representing the models and the insights generated in a way that is easily consumable
    by the business is extremely important. Nonetheless, exploring big data visually
    is very cumbersome and challenging. Since Spark is designed for big data processing,
    it also supports big data visualization along with it. There are many tools and
    techniques that have been built on Spark for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapters outlined how to model structured and unstructured data
    and generate insights from it. In this chapter, we will look at data visualization
    from two broad perspectives-one is from a data scientist's perspective—where visualization
    is the basic need to explore and understand the data effectively, and the other
    is from a business user's perspective, where the visuals are end deliverables
    to the business and must be easily comprehendible. We will explore various data
    visualization tools such as *IPythonNotebook* and *Zeppelin* that can be used
    on Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite for this chapter, a basic understanding of SQL and programming
    in Python, Scala, or other such frameworks, is nice to have. The topics covered
    in this chapter are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Why visualize data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data engineer's perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data scientist's perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A business user's perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IPython notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing and visualizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsetting and visualizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling and visualizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling and visualizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why visualize data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data visualization deals with representing data in a visual form so as to enable
    people to understand the underlying patterns and trends. Geographical maps, the
    bar and line charts of the seventeenth century, are some examples of early data
    visualizations. Excel is perhaps a familiar data visualization tool that most
    of us have already used. All data analytics tools have been equipped with sophisticated,
    interactive data visualization dashboards. However, the recent surge in big data,
    streaming, and real-time analytics has been pushing the boundaries of these tools
    and they seem to be bursting at the seams. The idea is to make the visualizations
    look simple, accurate, and relevant while hiding away all the complexity. As per
    the business needs, any visualization solution should ideally have the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Interactivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control over the details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from these, if the solution allows users to collaborate over the visuals
    or reports and share with each other, then that would make up an end-to-end visualization
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Big data visualization in particular poses its own challenges because we may
    end up with more data than pixels on the screen. Manipulating large data usually
    requires memory- and CPU-intensive processing and may have long latency. Add real-time
    or streaming data to the mix and the problem becomes even more challenging. Apache
    Spark is designed from the ground up just to tackle this latency by parallelizing
    CPU and memory usage. Before exploring the tools and techniques to visualize and
    work with big data, let's first understand the visualization needs of data engineers,
    data scientists, and business users.
  prefs: []
  type: TYPE_NORMAL
- en: A data engineer's perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data engineers play a crucial role in almost every data-driven requirement:
    sourcing data from different data sources, consolidating them, cleaning and preprocessing
    them, analyzing them, and then the final reporting with visuals and dashboards.
    Their activities can be broadly stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the data from different sources to be able to integrate and consolidate
    it to form a single data matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize and find various anomalies in the data, such as missing values, outliers
    and so on (this could be while scraping, sourcing, ETLing, and so on) and get
    those fixed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advise the data scientists on the properties and characteristics of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore various possible ways of visualizing the data and finalize the ones
    that are more informative and intuitive as per the business requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe here that the data engineers not only play a key role in sourcing and
    preparing the data, but also take a call on the most suitable visualization outputs
    for the business users. They usually work very closely to the business as well
    to have a very clear understanding on the business requirement and the specific
    problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: A data scientist's perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data scientist's need for visualizing data is different from that of data
    engineers. Please note that in some businesses, there are professionals who play
    a dual role of data engineers and data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data scientists need to visualize data to be able to take the right decisions
    in performing statistical analysis and ensure proper execution of the analytics
    projects. They would like to slice and dice data in various possible ways to find
    hidden insights. Let''s take a look at some example requirements that a data scientist
    might have to visualize the data:'
  prefs: []
  type: TYPE_NORMAL
- en: See the data distribution of the individual variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize outliers in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the percentage of missing data in a dataset for all variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the correlation matrix to find the correlated variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the behavior of residuals after a regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After a data cleaning or transformation activity, plot the variable again and
    see how it behaves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that some of the things just mentioned are quite similar to the
    case of data engineers. However, data scientists could have a more scientific/statistical
    intention behind such analyses. For example, data scientists may see an outlier
    from a different perspective and treat it statistically, but a data engineer might
    think of the various options that could have triggered this.
  prefs: []
  type: TYPE_NORMAL
- en: A business user's perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A business user''s perspective is completely different from that of data engineers
    or data scientists. Business users are usually the consumers of information! They
    would like to extract more and more information from the data, and for that, the
    correct visuals play a key role. Also, most business questions are more complex
    and causal these days. The old-school reports are no longer enough. Let''s look
    at some example queries that business users would like to extract from reports,
    visuals, and dashboards:'
  prefs: []
  type: TYPE_NORMAL
- en: Who are the high-value customers in so-and-so region?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the common characteristics of these customers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict whether a new customer would be high-value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advertising in which media would give maximum ROI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if I do not advertise in a newspaper?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the factors influencing a customer's buying behavior?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Out of the many different visualization options, choosing the right visual depends
    on specific requirements. Similarly, selecting a visualization tool depends on
    both the target audience and the business requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists or data engineers would prefer a more interactive console for
    quick and dirty analysis. The visuals they use are usually not intended for business
    users. They would like to dissect the data in every possible way to get more meaningful
    insights. So, they usually prefer a notebook-type interface that supports these
    activities. A notebook is an interactive computational environment where they
    can combine code chunks and plot data for explorations. There are notebooks such
    as **IPython**/**Jupyter** or **DataBricks**, to name a few available options.
  prefs: []
  type: TYPE_NORMAL
- en: Business users would prefer a more intuitive and informative visual that they
    can share with each other or use to generate reports. They expect to receive the
    end result through visuals. There are hundreds and thousands of tools, including
    some popular ones such as **Tableau**, that businesses use; but quite often, developers
    have to custom-build specific types for some unique requirements and expose them
    through web applications. Microsoft's **PowerBI** and open source solutions such
    as **Zeppelin** are a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: IPython notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IPython/Jupyter notebook on top of Spark's **PySpark** API is an excellent
    combination for data scientists to explore and visualize the data. The notebook
    internally spins up a new instance of the PySpark kernel. There are other kernels
    available; for example, the Apache **Toree** kernel can be used to support Scala
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: For many data scientists, it is the default choice because of its capability
    of integrating text, code, formula, and graphics in one JSON document file. The
    IPython notebook supports `matplotlib`, which is a 2D visualization library that
    can produce production-quality visuals. Generating plots, histograms, scatterplots,
    charts, and so on becomes easy and simple. It also supports the `seaborn` library,
    which is actually built upon matplotlib but is easy to use as it provides higher
    level abstraction and hides the underlying complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Zeppelin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Zeppelin is built upon JVM and integrates well with Apache Spark. It
    is a browser-based or frontend-based open source tool that has its own notebook.
    It supports Scala, Python, R, SQL, and other graphical modules to serve as a visualization
    solution not only to business users but also to data scientists. In the following
    section on visualization techniques, we will take a look at how Zeppelin supports
    Apache Spark code to generate interesting visuals. You need to download Zeppelin
    ([https://zeppelin.apache.org/](https://zeppelin.apache.org/)) in order to try
    out the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many products that support Apache Spark as the underlying data processing
    engine and are built to fit in the organizational big data ecosystem. While leveraging
    the processing power of Spark, they provide the visualization interface that supports
    a variety of interactive visuals, and they also support collaboration. Tableau
    is one such example of a tool that leverages Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data visualization is at the center of every stage in the data analytics life
    cycle. It is especially important for exploratory analysis and for communicating
    results. In either case, the goal is to transform data into a format that's efficient
    for human consumption. The approach of delegating the transformation to client-side
    libraries does not scale to large datasets. The transformation has to happen on
    the server side, sending only the relevant data to the client for rendering. Most
    of the common transformations are available in Apache Spark out of the box. Let's
    have a closer look at these transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing and visualizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Summarizing and visualizing** is a technique used by many **Business Intelligence**
    (**BI**) tools. Since summarization will be a concise dataset regardless of the
    size of the underlying dataset, the graphs look simple enough and easy to render.
    There are various ways to summarize the data such as aggregating, pivoting, and
    so on. If the rendering tool supports interactivity and has drill-down capabilities,
    the user gets to explore subsets of interest from the complete data. We will show
    how to do the summarization rapidly and interactively with Spark through the Zeppelin
    notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: The following image shows the Zeppelin notebook with source code and a grouped
    bar chart. The dataset contains 24 observations with sales information of two
    products, **P1** and **P2**, for 12 months. The first cell contains code to read
    a text file and register data as a temporary table. This cell uses the default
    Spark interpreter using Scala. The second cell uses the SQL interpreter which
    is supported by out-of-the-box visualization options. You can switch the chart
    types by clicking on the right icon. Note that the visualization is similar for
    either Scala or Python or R interpreters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarization examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code to read data and register as a SQL View:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scala (default)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing and visualizing](img/image_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**PySpark**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing and visualizing](img/image_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**R**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing and visualizing](img/image_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All three are reading the data file and registering as a temporary SQL view.
    Note that minor differences exist in the preceding three scripts. For example,
    we need to remove the header row for R and set the column names. The next step
    is to produce the visualization, which works from the `%sql` interpreter. The
    following first picture shows the script to produce the quarterly sales for each
    product. It also shows the chart types available out of the box, followed by the
    settings and their selection. You can collapse the settings after making selections.
    You can even make use of Zeppelin's in-built dynamic forms, say to accept a product
    during runtime. The second picture shows the actual output.
  prefs: []
  type: TYPE_NORMAL
- en: The script to produce quarterly sales for two products:![Summarizing and visualizing](img/image_09_004.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output produced:![Summarizing and visualizing](img/image_09_005.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have seen Zeppelin's inbuilt visualization in the preceding example. But
    we can use other plotting libraries as well. Our next example utilizes the PySpark
    interpreter with matplotlib in Zeppelin to draw a histogram. This example code
    computes bin intervals and bin counts using RDD's histogram function and brings
    in just this summarized data to the driver node. Frequency is provided as weights
    while plotting the bins to give the same visual understanding as a normal histogram
    but with very low data transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The histogram examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing and visualizing](img/image_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the generated output (it may come as a separate window):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing and visualizing](img/image_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example of preparing histograms, note that the bucket counts
    could be parameterized using the inbuilt dynamic forms support.
  prefs: []
  type: TYPE_NORMAL
- en: Subsetting and visualizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we may have a large dataset but we may be interested only in a subset
    of it. Divide and conquer is one approach where we explore a small portion of
    data at a time. Spark allows data subsetting using SQL-like filters and aggregates
    on row-column datasets as well as graph data. Let us perform SQL subsetting first,
    followed by a GraphX example.
  prefs: []
  type: TYPE_NORMAL
- en: The following example takes bank data available with Zeppelin and extracts a
    few relevant columns of data related to managers only. It uses the `google visualization
    library` to plot a bubble chart. The data was read using PySpark. Data subsetting
    and visualization are carried out using R. Note that we can choose any of the
    interpreters to these tasks and the choice here was just arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data subsetting example using SQL is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read data and register the SQL view:![Subsetting and visualizing](img/image_09_008.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subset managers' data and show a bubble plot:![Subsetting and visualizing](img/image_09_009.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next example demonstrates some GraphX processing that uses data provided
    by the **Stanford Network Analysis Project** (**SNAP**). The script extracts a
    subgraph covering a given set of nodes. Here, each node represents a Facebook
    ID and an edge represents a connection between the two nodes (or people). Further,
    the script identifies direct connections for a given node (id: 144). These are
    the level 1 nodes. Then it identifies the direct connections to these *level 1
    nodes*, which form *level 2 nodes* to the given node. Even though a second-level
    contact may be connected to more than one first-level contact, it is shown only
    once thereby forming a connection tree without crisscrossing edges. Since the
    connection tree may have too many nodes, the script limits up to three connections
    at level 1 as well as level 2, thereby showing only 12 nodes under the given root
    node (one root + three level 1 nodes + three of each level 2 nodes).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note the use of `z.put` and `z.get`. This is a mechanism to exchange data between
    cells/interpreters in Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created a data frame with level 1 contacts and their direct
    contacts, we are all set to draw the tree. The following script uses the graph
    visualization library igraph and Spark R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract nodes and edges. Plot the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subsetting and visualizing](img/image_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding script gets parent nodes from the nodes table, which are the
    parents of level 2 nodes as well as direct connections to the given head node.
    Ordered pairs of head nodes and level 1 nodes are created and assigned to `edges1`.
    The next step explodes the array of level 2 nodes to form one row per each array
    element. The data frame thus obtained is transposed and pasted to form edge pairs.
    Since paste converts data into strings, they are reconverted to numeric. These
    are the level 2 edges. The level 1 and level 2 edges are concatenated to form
    a single list of edges. These are fed to form the graph as shown next. Note that
    the smudge in `headNode` is 144, though not visible in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Subsetting and visualizing](img/image_09_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Connection tree for the given node
  prefs: []
  type: TYPE_NORMAL
- en: Sampling and visualizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sampling and visualizing has been used by statisticians for a long time. Through
    sampling techniques, we take a portion of the dataset and work on it. We will
    show how Spark supports different sampling techniques such as **random sampling**,
    **stratified sampling**, and **sampleByKey**, and so on. The following example
    is created using the Jupyter notebook, PySpark kernel, and `seaborn` library.
    The data file is the bank dataset provided by Zeppelin. The first plot shows the
    balance for each education category. The colors indicate marital status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read data and take a random sample of 5%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling and visualizing](img/image_09_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Render data using `stripplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling and visualizing](img/image_09_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding example showed a random sample of available data, which is much
    better than completely plotting the population. But if the levels in the categorical
    variable of interest (in this case, `education`) are too many, then this plot
    becomes hard to read. For example, if we want to plot the balance for job instead
    of `education`, there will be too many strips, making the picture look cluttered.
    Instead, we can take desired sample of desired categorical levels only and then
    examine the data. Note that this is different from subsetting because we will
    not be able to specify the sample ratio in normal subsetting using SQL `WHERE`
    clauses. We need to use `sampleByKey` for that, as shown next. The following example
    takes only two jobs and with specific sampling ratios:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling and visualizing](img/image_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stratified sampling
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and visualizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modeling and visualizing are possible with Spark''s **MLLib** and **ML** modules.
    Spark''s unified programming model and diverse programming interfaces enable combining
    these techniques into a single environment to get insights from the data. We have
    already covered most of the modeling techniques in the previous chapters. However,
    here are a few examples for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: K-means, Gaussian Mixture Modeling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification and regression**: Linear model, Decision tree, Naïve Bayes,
    SVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: Singular value decomposition, Principal component
    analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative Filtering**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical testing**: Correlations, Hypothesis testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example takes a model from the [Chapter 7](ch07.xhtml "Chapter 7. 
    Extending Spark with SparkR"), *Extending Spark with SparkR*, which tries to predict
    the students'' pass or fail results using a Naïve Bayes model. The idea is to
    make use of the out-of-the-box functionality provided by Zeppelin and inspect
    the model behavior. So, we load the data, perform data preparation, build the
    model, and run the predictions. Then we register the predictions as an SQL view
    so as to harness inbuilt visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Modeling and visualizing](img/image_09_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next step is to write the desired SQL query and define the appropriate settings.
    Note the use of the UNION operator in SQL and the way the match column is defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define SQL to view model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling and visualizing](img/image_09_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following picture helps us understand where the model prediction deviates
    from the actual data. Such visualizations are helpful in taking business users''
    inputs since they do not require any prior knowledge of data science to comprehend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling and visualizing](img/image_09_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Visualize model performance
  prefs: []
  type: TYPE_NORMAL
- en: We usually evaluate statistical models with error metrics, but visualizing them
    graphically instead of seeing the numbers makes them more intuitive because it
    is usually easier to understand a diagram than numbers in a table. For example,
    the preceding visualization can be easily understood by people outside the data
    science community as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored most of the widely used visualization tools and
    techniques supported on Spark in a big data setup. We explained some of the techniques
    with code snippets for better understanding of visualization needs at different
    stages of the data analytics life cycle. We also saw how business requirements
    are satisfied with proper visualization techniques by addressing the challenges
    of big data.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is the culmination of all the concepts explained till now .
    We will walk through the Complete Data Analysis Life Cycle through an example
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '21 Essential Data Visualization Tools: [http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html](http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Zeppelin notebook home page: [https://zeppelin.apache.org/](https://zeppelin.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jupyter notebook home page: [https://jupyter.org/](https://jupyter.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using IPython Notebook with Apache Spark: [http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/](http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Toree, which enables interactive workloads between applications and
    Spark cluster. Can be used with jupyter to run Scala code: [https://toree.incubator.apache.org/](https://toree.incubator.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GoogleVis package using R: [https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html](https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphX Programming Guide: [http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Going viral with R''s igraph package: [https://www.r-bloggers.com/going-viral-with-rs-igraph-package/](https://www.r-bloggers.com/going-viral-with-rs-igraph-package/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plotting with categorical data: [https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial](https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data source citations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bank data source (citation)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for
    Bank Direct Marketing: An Application of the CRISP-DM Methodology'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling
    Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011\. EUROSIS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available at [pdf] [http://hdl.handle.net/1822/14838](http://hdl.handle.net/1822/14838)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facebook data Source (citation)**'
  prefs: []
  type: TYPE_NORMAL
- en: J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks.
    NIPS, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

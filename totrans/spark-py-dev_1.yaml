- en: Chapter 1. Setting Up a Spark Virtual Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will build an isolated virtual environment for development
    purposes. The environment will be powered by Spark and the PyData libraries provided
    by the Python Anaconda distribution. These libraries include Pandas, Scikit-Learn,
    Blaze, Matplotlib, Seaborn, and Bokeh. We will perform the following activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the development environment using the Anaconda Python distribution.
    This will include enabling the IPython Notebook environment powered by PySpark
    for our data exploration tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and enabling Spark, and the PyData libraries such as Pandas, Scikit-
    Learn, Blaze, Matplotlib, and Bokeh.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a `word count` example app to ensure that everything is working fine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last decade has seen the rise and dominance of data-driven behemoths such
    as Amazon, Google, Twitter, LinkedIn, and Facebook. These corporations, by seeding,
    sharing, or disclosing their infrastructure concepts, software practices, and
    data processing frameworks, have fostered a vibrant open source software community.
    This has transformed the enterprise technology, systems, and software architecture.
  prefs: []
  type: TYPE_NORMAL
- en: This includes new infrastructure and DevOps (short for development and operations),
    concepts leveraging virtualization, cloud technology, and software-defined networks.
  prefs: []
  type: TYPE_NORMAL
- en: To process petabytes of data, Hadoop was developed and open sourced, taking
    its inspiration from the **Google File System** (**GFS**) and the adjoining distributed
    computing framework, MapReduce. Overcoming the complexities of scaling while keeping
    costs under control has also led to a proliferation of new data stores. Examples
    of recent database technology include Cassandra, a columnar database; MongoDB,
    a document database; and Neo4J, a graph database.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop, thanks to its ability to process huge datasets, has fostered a vast
    ecosystem to query data more iteratively and interactively with Pig, Hive, Impala,
    and Tez. Hadoop is cumbersome as it operates only in batch mode using MapReduce.
    Spark is creating a revolution in the analytics and data processing realm by targeting
    the shortcomings of disk input-output and bandwidth-intensive MapReduce jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is written in Scala, and therefore integrates natively with the **Java
    Virtual Machine** (**JVM**) powered ecosystem. Spark had early on provided Python
    API and bindings by enabling PySpark. The Spark architecture and ecosystem is
    inherently polyglot, with an obvious strong presence of Java-led systems.
  prefs: []
  type: TYPE_NORMAL
- en: This book will focus on PySpark and the PyData ecosystem. Python is one of the
    preferred languages in the academic and scientific community for data-intensive
    processing. Python has developed a rich ecosystem of libraries and tools in data
    manipulation with Pandas and Blaze, in Machine Learning with Scikit-Learn, and
    in data visualization with Matplotlib, Seaborn, and Bokeh. Hence, the aim of this
    book is to build an end-to-end architecture for data-intensive applications powered
    by Spark and Python. In order to put these concepts in to practice, we will analyze
    social networks such as Twitter, GitHub, and Meetup. We will focus on the activities
    and social interactions of Spark and the Open Source Software community by tapping
    into GitHub, Twitter, and Meetup.
  prefs: []
  type: TYPE_NORMAL
- en: Building data-intensive applications requires highly scalable infrastructure,
    polyglot storage, seamless data integration, multiparadigm analytics processing,
    and efficient visualization. The following paragraph describes the data-intensive
    app architecture blueprint that we will adopt throughout the book. It is the backbone
    of the book. We will discover Spark in the context of the broader PyData ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of data-intensive applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand the architecture of data-intensive applications, the
    following conceptual framework is used. The is architecture is designed on the
    following five layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytics layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engagement layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the five layers of the **Data Intensive App
    Framework**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the architecture of data-intensive applications](img/B03968_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the bottom up, let's go through the layers and their main purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The infrastructure layer is primarily concerned with virtualization, scalability,
    and continuous integration. In practical terms, and in terms of virtualization,
    we will go through building our own development environment in a VirtualBox and
    virtual machine powered by Spark and the Anaconda distribution of Python. If we
    wish to scale from there, we can create a similar environment in the cloud. The
    practice of creating a segregated development environment and moving into test
    and production deployment can be automated and can be part of a continuous integration
    cycle powered by DevOps tools such as **Vagrant**, **Chef**, **Puppet**, and **Docker**.
    Docker is a very popular open source project that eases the installation and deployment
    of new environments. The book will be limited to building the virtual machine
    using VirtualBox. From a data-intensive app architecture point of view, we are
    describing the essential steps of the infrastructure layer by mentioning scalability
    and continuous integration beyond just virtualization.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The persistence layer manages the various repositories in accordance with data
    needs and shapes. It ensures the set up and management of the polyglot data stores.
    It includes relational database management systems such as **MySQL** and **PostgreSQL**;
    key-value data stores such as **Hadoop**, **Riak**, and **Redis**; columnar databases
    such as **HBase** and **Cassandra**; document databases such as **MongoDB** and
    **Couchbase**; and graph databases such as **Neo4j**. The persistence layer manages
    various filesystems such as Hadoop's HDFS. It interacts with various storage systems
    from native hard drives to Amazon S3\. It manages various file storage formats
    such as `csv`, `json`, and `parquet`, which is a column-oriented format.
  prefs: []
  type: TYPE_NORMAL
- en: Integration layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The integration layer focuses on data acquisition, transformation, quality,
    persistence, consumption, and governance. It is essentially driven by the following
    five Cs: *connect*, *collect*, *correct*, *compose*, and *consume*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The five steps describe the lifecycle of data. They are focused on how to acquire
    the dataset of interest, explore it, iteratively refine and enrich the collected
    information, and get it ready for consumption. So, the steps perform the following
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Connect**: Targets the best way to acquire data from the various data sources,
    APIs offered by these sources, the input format, input schemas if they exist,
    the rate of data collection, and limitations from providers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correct**: Focuses on transforming data for further processing and also ensures
    that the quality and consistency of the data received are maintained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collect**: Looks at which data to store where and in what format, to ease
    data composition and consumption at later stages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compose**: Concentrates its attention on how to mash up the various data
    sets collected, and enrich the information in order to build a compelling data-driven
    product'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consume**: Takes care of data provisioning and rendering and how the right
    data reaches the right individual at the right time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control**: This sixth *additional* step will sooner or later be required
    as the data, the organization, and the participants grow and it is about ensuring
    data governance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the iterative process of data acquisition and
    refinement for consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integration layer](img/B03968_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Analytics layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analytics layer is where Spark processes data with the various models, algorithms,
    and machine learning pipelines in order to derive insights. For our purpose, in
    this book, the analytics layer is powered by Spark. We will delve deeper in subsequent
    chapters into the merits of Spark. In a nutshell, what makes it so powerful is
    that it allows multiple paradigms of analytics processing in a single unified
    platform. It allows batch, streaming, and interactive analytics. Batch processing
    on large datasets with longer latency periods allows us to extract patterns and
    insights that can feed into real-time events in streaming mode. Interactive and
    iterative analytics are more suited for data exploration. Spark offers bindings
    and APIs in Python and R. With its **SparkSQL** module and the Spark Dataframe,
    it offers a very familiar analytics interface.
  prefs: []
  type: TYPE_NORMAL
- en: Engagement layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The engagement layer interacts with the end user and provides dashboards, interactive
    visualizations, and alerts. We will focus here on the tools provided by the PyData
    ecosystem such as Matplotlib, Seaborn, and Bokeh.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop scales horizontally as the data grows. Hadoop runs on commodity hardware,
    so it is cost-effective. Intensive data applications are enabled by scalable,
    distributed processing frameworks that allow organizations to analyze petabytes
    of data on large commodity clusters. Hadoop is the first open source implementation
    of map-reduce. Hadoop relies on a distributed framework for storage called **HDFS**
    (**Hadoop Distributed File System**). Hadoop runs map-reduce tasks in batch jobs.
    Hadoop requires persisting the data to disk at each map, shuffle, and reduce process
    step. The overhead and the latency of such batch jobs adversely impact the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a fast, distributed general analytics computing engine for large-scale
    data processing. The major breakthrough from Hadoop is that Spark allows data
    sharing between processing steps through in-memory processing of data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark is unique in that it allows four different styles of data analysis and
    processing. Spark can be used in:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch**: This mode is used for manipulating large datasets, typically performing
    large map-reduce jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming**: This mode is used to process incoming information in near real
    time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative**: This mode is for machine learning algorithms such as a gradient
    descent where the data is accessed repetitively in order to reach convergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive**: This mode is used for data exploration as large chunks of
    data are in memory and due to the very quick response time of Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure highlights the preceding four processing styles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spark](img/B03968_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Spark operates in three modes: one single mode, standalone on a single machine
    and two distributed modes on a cluster of machines—on Yarn, the Hadoop distributed
    resource manager, or on Mesos, the open source cluster manager developed at Berkeley
    concurrently with Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spark](img/B03968_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark offers a polyglot interface in Scala, Java, Python, and R.
  prefs: []
  type: TYPE_NORMAL
- en: Spark libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark comes with batteries included, with some powerful libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SparkSQL**: This provides the SQL-like ability to interrogate structured
    data and interactively explore large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SparkMLLIB**: This provides major algorithms and a pipeline framework for
    machine learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Streaming**: This is for near real-time analysis of data using micro
    batches and sliding widows on incoming streams of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark GraphX**: This is for graph processing and computation on complex connected
    entities and relationships'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PySpark in action
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark is written in Scala. The whole Spark ecosystem naturally leverages the
    JVM environment and capitalizes on HDFS natively. Hadoop HDFS is one of the many
    data stores supported by Spark. Spark is agnostic and from the beginning interacted
    with multiple data sources, types, and formats.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark is not a transcribed version of Spark on a Java-enabled dialect of Python
    such as Jython. PySpark provides integrated API bindings around Spark and enables
    full usage of the Python ecosystem within all the nodes of the cluster with the
    pickle Python serialization and, more importantly, supplies access to the rich
    ecosystem of Python's machine learning libraries such as Scikit-Learn or data
    processing such as Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: When we initialize a Spark program, the first thing a Spark program must do
    is to create a `SparkContext` object. It tells Spark how to access the cluster.
    The Python program creates a `PySparkContext`. Py4J is the gateway that binds
    the Python program to the Spark JVM `SparkContext`. The JVM `SparkContextserializes`
    the application codes and the closures and sends them to the cluster for execution.
    The cluster manager allocates resources and schedules, and ships the closures
    to the Spark workers in the cluster who activate Python virtual machines as required.
    In each machine, the Spark Worker is managed by an executor that controls computation,
    storage, and cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of how the Spark driver manages both the PySpark context
    and the Spark context with its local filesystems and its interactions with the
    Spark worker through the cluster manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark in action](img/B03968_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Resilient Distributed Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark applications consist of a driver program that runs the user's main function,
    creates distributed datasets on the cluster, and executes various parallel operations
    (transformations and actions) on those datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Spark applications are run as an independent set of processes, coordinated by
    a `SparkContext` in a driver program.
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkContext` will be allocated system resources (machines, memory, CPU)
    from the **Cluster manager**.
  prefs: []
  type: TYPE_NORMAL
- en: The `SparkContext` manages executors who manage workers in the cluster. The
    driver program has Spark jobs that need to run. The jobs are split into tasks
    submitted to the executor for completion. The executor takes care of computation,
    storage, and caching in each machine.
  prefs: []
  type: TYPE_NORMAL
- en: The key building block in Spark is the **RDD** (**Resilient Distributed Dataset**).
    A dataset is a collection of elements. Distributed means the dataset can be on
    any node in the cluster. Resilient means that the dataset could get lost or partially
    lost without major harm to the computation in progress as Spark will re-compute
    from the data lineage in memory, also known as the **DAG** (short for **Directed
    Acyclic Graph**) of operations. Basically, Spark will snapshot in memory a state
    of the RDD in the cache. If one of the computing machines crashes during operation,
    Spark rebuilds the RDDs from the cached RDD and the DAG of operations. RDDs recover
    from node failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of operation on RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformations**: A transformation takes an existing RDD and leads to a
    pointer of a new transformed RDD. An RDD is immutable. Once created, it cannot
    be changed. Each transformation creates a new RDD. Transformations are lazily
    evaluated. Transformations are executed only when an action occurs. In the case
    of failure, the data lineage of transformations rebuilds the RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: An action on an RDD triggers a Spark job and yields a value. An
    action operation causes Spark to execute the (lazy) transformation operations
    that are required to compute the RDD returned by the action. The action results
    in a DAG of operations. The DAG is compiled into stages where each stage is executed
    as a series of tasks. A task is a fundamental unit of work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s some useful information on RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RDDs are created from a data source such as an HDFS file or a DB query. There
    are three ways to create an RDD:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading from a datastore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming an existing RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an in-memory collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDDs are transformed with functions such as `map` or `filter`, which yield new
    RDDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An action such as first, take, collect, or count on an RDD will deliver the
    results into the Spark driver. The Spark driver is the client through which the
    user interacts with the Spark cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the RDD transformation and action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Resilient Distributed Dataset](img/B03968_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Anaconda is a widely used free Python distribution maintained by **Continuum**
    ([https://www.continuum.io/](https://www.continuum.io/)). We will use the prevailing
    software stack provided by Anaconda to generate our apps. In this book, we will
    use PySpark and the PyData ecosystem. The PyData ecosystem is promoted, supported,
    and maintained by **Continuum** and powered by the **Anaconda** Python distribution.
    The Anaconda Python distribution essentially saves time and aggravation in the
    installation of the Python environment; we will use it in conjunction with Spark.
    Anaconda has its own package management that supplements the traditional `pip`
    `install` and `easy-install`. Anaconda comes with batteries included, namely some
    of the most important packages such as Pandas, Scikit-Learn, Blaze, Matplotlib,
    and Bokeh. An upgrade to any of the installed library is a simple command at the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A list of installed libraries in our environment can be obtained with command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The key components of the stack are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anaconda**: This is a free Python distribution with almost 200 Python packages
    for science, math, engineering, and data analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conda**: This is a package manager that takes care of all the dependencies
    of installing a complex software stack. This is not restricted to Python and manages
    the install process for R and other languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numba**: This provides the power to speed up code in Python with high-performance
    functions and just-in-time compilation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blaze**: This enables large scale data analytics by offering a uniform and
    adaptable interface to access a variety of data providers, which include streaming
    Python, Pandas, SQLAlchemy, and Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh**: This provides interactive data visualizations for large and streaming
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wakari**: This allows us to share and deploy IPython Notebooks and other
    apps on a hosted environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the components of the Anaconda stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Anaconda](img/B03968_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up the Spark powered environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn to set up Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a segregated development environment in a virtual machine running on
    Ubuntu 14.04, so it does not interfere with any existing system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Spark 1.3.0 with its dependencies, namely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the Anaconda Python 2.7 environment with all the required libraries
    such as Pandas, Scikit-Learn, Blaze, and Bokeh, and enable PySpark, so it can
    be accessed through IPython Notebooks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up the backend or data stores of our environment. We will use MySQL as the
    relational database, MongoDB as the document store, and Cassandra as the columnar
    database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each storage backend serves a specific purpose depending on the nature of the
    data to be handled. The MySQL RDBMs is used for standard tabular processed information
    that can be easily queried using SQL. As we will be processing a lot of JSON-type
    data from various APIs, the easiest way to store them is in a document. For real-time
    and time-series-related information, Cassandra is best suited as a columnar database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives a view of the environment we will build and use
    throughout the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up the Spark powered environment](img/B03968_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up an Oracle VirtualBox with Ubuntu
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up a clean new VirtualBox environment on Ubuntu 14.04 is the safest
    way to create a development environment that does not conflict with existing libraries
    and can be later replicated in the cloud using a similar list of commands.
  prefs: []
  type: TYPE_NORMAL
- en: In order to set up an environment with Anaconda and Spark, we will create a
    VirtualBox virtual machine running Ubuntu 14.04.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the steps of using VirtualBox with Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: Oracle VirtualBox VM is free and can be downloaded from [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads).
    The installation is pretty straightforward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After installing VirtualBox, let's open the Oracle VM VirtualBox Manager and
    click the **New** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll give the new VM a name, and select Type **Linux** and Version **Ubuntu
    (64 bit)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You need to download the ISO from the Ubuntu website and allocate sufficient
    RAM (4 GB recommended) and disk space (20 GB recommended). We will use the Ubuntu
    14.04.1 LTS release, which is found here: [http://www.ubuntu.com/download/desktop](http://www.ubuntu.com/download/desktop).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation completed, it is advisable to install the VirtualBox Guest
    Additions by going to (from the VirtualBox menu, with the new VM running) **Devices**
    | **Insert Guest Additions CD image**. Failing to provide the guest additions
    in a Windows host gives a very limited user interface with reduced window sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the additional installation completes, reboot the VM, and it will be ready
    to use. It is helpful to enable the shared clipboard by selecting the VM and clicking
    **Settings**, then go to **General** | **Advanced** | **Shared Clipboard** and
    click on **Bidirectional**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Anaconda with Python 2.7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PySpark currently runs only on Python 2.7\. (There are requests from the community
    to upgrade to Python 3.3.) To install Anaconda, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Anaconda Installer for Linux 64-bit Python 2.7 from [http://continuum.io/downloads#all](http://continuum.io/downloads#all).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After downloading the Anaconda installer, open a terminal and navigate to the
    directory or folder where the installer has been saved. From here, run the following
    command, replacing the `2.x.x` in the command with the version number of the downloaded
    installer file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After accepting the license terms, you will be asked to specify the install
    location (which `defaults to ~/anaconda`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the self-extraction is finished, you should add the anaconda binary directory
    to your PATH environment variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Installing Java 8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark runs on the JVM and requires the Java **SDK** (short for **Software Development
    Kit**) and not the **JRE** (short for **Java Runtime Environment**), as we will
    build apps with Spark. The recommended version is Java Version 7 or higher. Java
    8 is the most suitable, as it includes many of the functional programming techniques
    available with Scala and Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Java 8, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Oracle Java 8 using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Set the `JAVA_HOME` environment variable and ensure that the Java program is
    on your PATH.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check that `JAVA_HOME` is properly installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Installing Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Head over to the Spark download page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
  prefs: []
  type: TYPE_NORMAL
- en: The Spark download page offers the possibility to download earlier versions
    of Spark and different package and download types. We will select the latest release,
    pre-built for Hadoop 2.6 and later. The easiest way to install Spark is to use
    a Spark package prebuilt for Hadoop 2.6 and later, rather than build it from source.
    Move the file to the directory `~/spark` under the root directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the latest release of Spark—Spark 1.5.2, released on November 9, 2015:'
  prefs: []
  type: TYPE_NORMAL
- en: Select Spark release **1.5.2 (Nov 09 2015),**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chose the package type **Prebuilt for Hadoop 2.6 and later**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chose the download type **Direct Download**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download Spark: **spark-1.5.2-bin-hadoop2.6.tgz**,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify this release using the 1.3.0 signatures and checksums,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can also be accomplished by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll extract the files and clean up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run the Spark Python interpreter with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The interpreter will have already provided us with a Spark context object,
    `sc`, which we can see by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Enabling IPython Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with IPython Notebook for a friendlier user experience than the
    console.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can launch IPython Notebook by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch PySpark with `IPYNB` in the directory `examples/AN_Spark` where Jupyter
    or IPython Notebooks are stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Building our first app with PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are ready to check now that everything is working fine. The obligatory word
    count will be put to the test in processing a word count on the first chapter
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we will be running is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this program, we are first reading the file from the directory `/home/an/Documents/A00_Documents/Spark4Py
    20150315` into `file_in`.
  prefs: []
  type: TYPE_NORMAL
- en: We are then introspecting the file by counting the number of lines and the number
    of characters per line.
  prefs: []
  type: TYPE_NORMAL
- en: We are splitting the input file in to words and getting them in lower case.
    For our word count purpose, we are choosing words longer than three characters
    in order to avoid shorter and much more frequent words such as *the*, *and*, *for*
    to skew the count in their favor. Generally, they are considered stop words and
    should be filtered out in any language processing task.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we are getting ready for the MapReduce steps. To each word, we
    map a value of `1` and reduce it by summing all the unique words.
  prefs: []
  type: TYPE_NORMAL
- en: Here are illustrations of the code in the IPython Notebook. The first 10 cells
    are preprocessing the word count on the dataset, which is retrieved from the local
    file directory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building our first app with PySpark](img/B03968_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Swap the word count tuples in the format `(count, word)` in order to sort by
    `count`, which is now the primary key of the tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to display our result, we are creating the tuple `(count, word)` and
    displaying the top 20 most frequently used words in descending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building our first app with PySpark](img/B03968_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create a histogram function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we visualize the most frequent words by plotting them in a bar chart.
    We have to first swap the tuple from the original `(count, word)` to `(word, count)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building our first app with PySpark](img/B03968_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So here you have it: the most frequent words used in the first chapter are
    **Spark**, followed by **Data** and **Anaconda**.'
  prefs: []
  type: TYPE_NORMAL
- en: Virtualizing the environment with Vagrant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to create a portable Python and Spark environment that can be easily
    shared and cloned, the development environment can be built with a `vagrantfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will point to the **Massive Open Online Courses** (**MOOCs**) delivered
    by *Berkeley University and Databricks*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to Big Data with Apache Spark, Professor Anthony D. Joseph* can
    be found at [https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalable Machine Learning, Professor* *Ameet Talwalkar* can be found at [https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The course labs were executed on IPython Notebooks powered by PySpark. They
    can be found in the following GitHub repository: [https://github.com/spark-mooc/mooc-setup/](https://github.com/spark-mooc/mooc-setup/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have set up Vagrant on your machine, follow these instructions to
    get started: [https://docs.vagrantup.com/v2/getting-started/index.html](https://docs.vagrantup.com/v2/getting-started/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the `spark-mooc/mooc-setup/ github` repository in your work directory
    and launch the command `$ vagrant up`, within the cloned directory:'
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the version of Spark may be outdated as the `vagrantfile` may
    not be up-to-date.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see an output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will launch the IPython Notebooks powered by PySpark on `localhost:8001`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Virtualizing the environment with Vagrant](img/B03968_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Moving to the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are dealing with distributed systems, an environment on a virtual machine
    running on a single laptop is limited for exploration and learning. We can move
    to the cloud in order to experience the power and scalability of the Spark distributed
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying apps in Amazon Web Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we are ready to scale our apps, we can migrate our development environment
    to **Amazon** **Web Services** (**AWS**).
  prefs: []
  type: TYPE_NORMAL
- en: 'How to run Spark on EC2 is clearly described in the following page: [https://spark.apache.org/docs/latest/ec2-scripts.html](https://spark.apache.org/docs/latest/ec2-scripts.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We emphasize five key steps in setting up the AWS Spark environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an AWS EC2 key pair via the AWS console [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Export your key pair to your environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'SSH into a cluster to run Spark jobs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Destroy your cluster after usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Virtualizing the environment with Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to create a portable Python and Spark environment that can be easily
    shared and cloned, the development environment can be built in Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We wish capitalize on Docker''s two main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating isolated containers that can be easily deployed on different operating
    systems or in the cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing easy sharing of the development environment image with all its dependencies
    using The DockerHub. The DockerHub is similar to GitHub. It allows easy cloning
    and version control. The snapshot image of the configured environment can be the
    baseline for further enhancements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram illustrates a Docker-enabled environment with Spark, Anaconda,
    and the database server and their respective data volumes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Virtualizing the environment with Docker](img/B03968_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Docker offers the ability to clone and deploy an environment from the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find an example Dockerfile with a PySpark and Anaconda setup at the
    following address: [https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/](https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Docker as per the instructions provided at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.docker.com/mac/started/](http://docs.docker.com/mac/started/)
    if you are on Mac OS X'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.docker.com/linux/started/](http://docs.docker.com/linux/started/)
    if you are on Linux'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.docker.com/windows/started/](http://docs.docker.com/windows/started/)
    if you are on Windows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install the docker container with the Dockerfile provided earlier with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Other great sources of information on how to *dockerize* your environment can
    be seen at Lab41\. The GitHub repository contains the necessary code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Lab41/ipython-spark-docker](https://github.com/Lab41/ipython-spark-docker)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The supporting blog post is rich in information on thought processes involved
    in building the docker environment: [http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/](http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We set the context of building data-intensive apps by describing the overall
    architecture structured around the infrastructure, persistence, integration, analytics,
    and engagement layers. We also discussed Spark and Anaconda with their respective
    building blocks. We set up an environment in a VirtualBox with Anaconda and Spark
    and demonstrated a word count app using the text content of the first chapter
    as input.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve more deeply into the architecture blueprint
    for data-intensive apps and tap into the Twitter, GitHub, and Meetup APIs to get
    a feel of the data we will be mining with Spark.
  prefs: []
  type: TYPE_NORMAL

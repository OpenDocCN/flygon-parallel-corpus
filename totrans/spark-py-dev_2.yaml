- en: Chapter 2. Building Batch and Streaming Apps with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of the book is to teach you about PySpark and the PyData libraries
    by building an app that analyzes the Spark community's interactions on social
    networks. We will gather information on Apache Spark from GitHub, check the relevant
    tweets on Twitter, and get a feel for the buzz around Spark in the broader open
    source software communities using **Meetup**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will outline the various sources of data and information.
    We will get an understanding of their structure. We will outline the data processing
    pipeline, from collection to batch and streaming processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Outline data processing pipelines from collection to batch and stream processing,
    effectively depicting the architecture of the app we are planning to build.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the various data sources (GitHub, Twitter, and Meetup), their data
    structure (JSON, structured information, unstructured text, geo-location, time
    series data, and so on), and their complexities. We also discuss the tools to
    connect to three different APIs, so you can build your own data mashups. The book
    will focus on Twitter in the following chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting data-intensive apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We defined the data-intensive app framework architecture blueprint in the previous
    chapter. Let''s put back in context the various software components we are going
    to use throughout the book in our original framework. Here''s an illustration
    of the various components of software mapped in the data-intensive architecture
    framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecting data-intensive apps](img/B03986_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark is an extremely efficient, distributed computing framework. In order to
    exploit its full power, we need to architect our solution accordingly. For performance
    reasons, the overall solution needs to also be aware of its usage in terms of
    CPU, storage, and network.
  prefs: []
  type: TYPE_NORMAL
- en: 'These imperatives drive the architecture of our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: This architecture combines slow and fast processing. Slow processing
    is done on historical data in batch mode. This is also called data at rest. This
    phase builds precomputed models and data patterns that will be used by the fast
    processing arm once live continuous data is fed into the system. Fast processing
    of data or real-time analysis of streaming data refers to data in motion. Data
    at rest is essentially processing data in batch mode with a longer latency. Data
    in motion refers to the streaming computation of data ingested in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Spark is natively linearly scalable through its distributed
    in-memory computing framework. Databases and data stores interacting with Spark
    need to be also able to scale linearly as data volume grows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: When a failure occurs due to hardware, software, or network
    reasons, the architecture should be resilient enough and provide availability
    at all times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: The data pipelines put in place in this architecture can be
    adapted and retrofitted very quickly depending on the use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is unique as it allows batch processing and streaming analytics on the
    same unified platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will consider two data processing pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one handles data at rest and is focused on putting together the pipeline
    for batch analysis of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one, data in motion, targets real-time data ingestion and delivering
    insights based on precomputed models and data patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data at rest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's get an understanding of the data at rest or batch processing pipeline.
    The objective in this pipeline is to ingest the various datasets from Twitter,
    GitHub, and Meetup; prepare the data for Spark MLlib, the machine learning engine;
    and derive the base models that will be applied for insight generation in batch
    mode or in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the data pipeline in order to enable processing
    data at rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Processing data at rest](img/B03986_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Processing data in motion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Processing data in motion introduces a new level of complexity, as we are introducing
    a new possibility of failure. If we want to scale, we need to consider bringing
    in distributed message queue systems such as Kafka. We will dedicate a subsequent
    chapter to understanding streaming analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts a data pipeline for processing data in motion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Processing data in motion](img/B03986_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Exploring data interactively
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a data-intensive app is not as straightforward as exposing a database
    to a web interface. During the setup of both the data at rest and data in motion
    processing, we will capitalize on Spark's ability to analyse data interactively
    and refine the data richness and quality required for the machine learning and
    streaming activities. Here, we will go through an iterative cycle of data collection,
    refinement, and investigation in order to get to the dataset of interest for our
    apps.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to social networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s delve into the first steps of the data-intensive app architecture''s
    integration layer. We are going to focus on harvesting the data, ensuring its
    integrity and preparing for batch and streaming data processing by Spark at the
    next stage. This phase is described in the five process steps: *connect*, *correct*,
    *collect*, *compose*, and *consume*. These are iterative steps of data exploration
    that will get us acquainted with the data and help us refine the data structure
    for further processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the iterative process of data acquisition and
    refinement for consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Connecting to social networks](img/B03986_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We connect to the social networks of interest: Twitter, GitHub, and Meetup.
    We will discuss the mode of access to the **APIs** (short for **Application Programming
    Interface**) and how to create a RESTful connection with those services while
    respecting the rate limitation imposed by the social networks. **REST** (short
    for **Representation State Transfer**) is the most widely adopted architectural
    style on the Internet in order to enable scalable web services. It relies on exchanging
    messages predominantly in **JSON** (short for **JavaScript Object Notation**).
    RESTful APIs and web services implement the four most prevalent verbs `GET`, `PUT`,
    `POST`, and `DELETE`. `GET` is used to retrieve an element or a collection from
    a given `URI`. `PUT` updates a collection with a new one. `POST` allows the creation
    of a new entry, while `DELETE` eliminates a collection.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Twitter data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Twitter allows access to registered users to its search and streaming tweet
    services under an authorization protocol called OAuth that allows API applications
    to securely act on a user's behalf. In order to create the connection, the first
    step is to create an application with Twitter at [https://apps.twitter.com/app/new](https://apps.twitter.com/app/new).
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting Twitter data](img/B03986_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the application has been created, Twitter will issue the four codes that
    will allow it to tap into the Twitter hose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wish to get a feel for the various RESTful queries offered, you can
    explore the Twitter API on the dev console at [https://dev.twitter.com/rest/tools/console](https://dev.twitter.com/rest/tools/console):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting Twitter data](img/B03986_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will make a programmatic connection on Twitter using the following code,
    which will activate our OAuth access and allows us to tap into the Twitter API
    under the rate limitation. In the streaming mode, the limitation is for a GET
    request.
  prefs: []
  type: TYPE_NORMAL
- en: Getting GitHub data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GitHub uses a similar authentication process to Twitter. Head to the developer
    site and retrieve your credentials after duly registering with GitHub at [https://developer.github.com/v3/](https://developer.github.com/v3/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting GitHub data](img/B03986_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting Meetup data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Meetup can be accessed using the token issued in the developer resources to
    members of Meetup.com. The necessary token or OAuth credential for Meetup API
    access can be obtained on their developer''s website at [https://secure.meetup.com/meetup_api](https://secure.meetup.com/meetup_api):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting Meetup data](img/B03986_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get a first feel for the data extracted from each of the social networks
    and get an understanding of the data structure from each these sources.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the anatomy of tweets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are going to establish connection with the Twitter API.
    Twitter offers two connection modes: the REST API, which allows us to search historical
    tweets for a given search term or hashtag, and the streaming API, which delivers
    real-time tweets under the rate limit in place.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get a better understanding of how to operate with the Twitter API,
    we will go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the Twitter Python library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish a connection programmatically via OAuth, the authentication required
    for Twitter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for recent tweets for the query *Apache Spark* and explore the results
    obtained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide on the key attributes of interest and retrieve the information from the
    JSON output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s go through it step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Python Twitter library. In order to install it, you need to write
    `pip install twitter` from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Python Twitter API class and its base methods for authentication,
    searching, and parsing the results. `self.auth` gets the credentials from Twitter.
    It then creates a registered API as `self.api`. We have implemented two methods:
    the first one to search Twitter with a given query and the second one to parse
    the output to retrieve relevant information such as the tweet ID, the tweet text,
    and the tweet author. The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the class with the required authentication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a search on the query term *Apache Spark*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Analyze the JSON output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the Twitter output to retrieve key information of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the GitHub world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to get a better understanding on how to operate with the GitHub API,
    we will go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the GitHub Python library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the API by using the token provided when we registered in the developer
    website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve some key facts on the Apache foundation that is hosting the spark repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s go through the process step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Python PyGithub library. In order to install it, you need to `pip
    install PyGithub` from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Programmatically create a client to instantiate the GitHub API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve key facts from the Apache User. There are 640 active Apache repositories
    in GitHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve key facts from the Spark repository, The programing languages used
    in the Spark repo are given here under:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve a few key participants of the wide Spark GitHub repository network.
    There are 3,738 stargazers in the Apache Spark repository at the time of writing.
    The network is immense. The first stargazer is *Matei Zaharia*, the cofounder
    of the Spark project when he was doing his PhD in Berkeley.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the community through Meetup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to get a better understanding of how to operate with the Meetup API,
    we will go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Python program to call the Meetup API using an authentication token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve information of past events for meetup groups such as *London Data Science*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the profile of the meetup members in order to analyze their participation
    in similar meetup groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s go through the process step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As there is no reliable Meetup API Python library, we will programmatically
    create a client to instantiate the Meetup API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will retrieve past events from a given Meetup group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Get information about the Meetup members:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Previewing our app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our challenge is to make sense of the data retrieved from these social networks,
    finding the key relationships and deriving insights. Some of the elements of interest
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualizing the top influencers: Discover the top influencers in the community:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heavy Twitter users on *Apache Spark*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Committers in GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leading Meetup presentations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding the Network: Network graph of GitHub committers, watchers, and
    stargazers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identifying the Hot Locations: Locating the most active location for Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot provides a preview of our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Previewing our app](img/B03986_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we laid out the overall architecture of our app. We explained
    the two main paradigms of processing data: batch processing, also called data
    at rest, and streaming analytics, referred to as data in motion. We proceeded
    to establish connections to three social networks of interest: Twitter, GitHub,
    and Meetup. We sampled the data and provided a preview of what we are aiming to
    build. The remainder of the book will focus on the Twitter dataset. We provided
    here the tools and API to access three social networks, so you can at a later
    stage create your own data mashups. We are now ready to investigate the data collected,
    which will be the topic of the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve deeper into data analysis, extracting the
    key attributes of interest for our purposes and managing the storage of the information
    for batch and stream processing.
  prefs: []
  type: TYPE_NORMAL

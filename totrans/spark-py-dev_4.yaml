- en: Chapter 4. Learning from Data Using Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have laid the foundation for data to be harvested in the previous chapter,
    we are now ready to learn from the data. Machine learning is about drawing insights
    from data. Our objective is to give an overview of the Spark **MLlib** (short
    for **Machine Learning library**) and apply the appropriate algorithms to our
    dataset in order to derive insights. From the Twitter dataset, we will be applying
    an unsupervised clustering algorithm in order to distinguish between Apache Spark-relevant
    tweets versus the rest. We have as initial input a mixed bag of tweets. We first
    need to preprocess the data in order to extract the relevant features, then apply
    the machine learning algorithm to our dataset, and finally evaluate the results
    and the performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing an overview of the Spark MLlib module with its algorithms and the
    typical machine learning workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the Twitter harvested dataset to extract the relevant features,
    applying an unsupervised clustering algorithm to identify *Apache Spark*-relevant
    tweets. Then, evaluating the model and the results obtained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing the Spark machine learning pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextualizing Spark MLlib in the app architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first contextualize the focus of this chapter on data-intensive app architecture.
    We will concentrate our attention on the analytics layer and more precisely machine
    learning. This will serve as a foundation for streaming apps as we want to apply
    the learning from the batch processing of data as inference rules for the streaming
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram sets the context of the chapter's focus, highlighting
    the machine learning module within the analytics layer while using tools for exploratory
    data analysis, Spark SQL, and Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '![Contextualizing Spark MLlib in the app architecture](img/B03986_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Classifying Spark MLlib algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark MLlib is a rapidly evolving module of Spark with new algorithms added
    with each release of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram provides a high-level overview of Spark MLlib algorithms
    grouped in the traditional broad machine learning techniques and following the
    categorical or continuous nature of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classifying Spark MLlib algorithms](img/B03986_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We categorize the Spark MLlib algorithms in two columns, categorical or continuous,
    depending on the type of data. We distinguish between data that is categorical
    or more qualitative in nature versus continuous data, which is quantitative in
    nature. An example of qualitative data is predicting the weather; given the atmospheric
    pressure, the temperature, and the presence and type of clouds, the weather will
    be sunny, dry, rainy, or overcast. These are discrete values. On the other hand,
    let's say we want to predict house prices, given the location, square meterage,
    and the number of beds; the real estate value can be predicted using linear regression.
    In this case, we are talking about continuous or quantitative values.
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal grouping reflects the types of machine learning method used.
    Unsupervised versus supervised machine learning techniques are dependent on whether
    the training data is labeled. In an unsupervised learning challenge, no labels
    are given to the learning algorithm. The goal is to find the hidden structure
    in its input. In the case of supervised learning, the data is labeled. The focus
    is on making predictions using regression if the data is continuous or classification
    if the data is categorical.
  prefs: []
  type: TYPE_NORMAL
- en: An important category of machine learning is recommender systems, which leverage
    collaborative filtering techniques. The Amazon web store and Netflix have very
    powerful recommender systems powering their recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** is one of the machine learning optimization
    techniques that is well suited for Spark distributed computation.'
  prefs: []
  type: TYPE_NORMAL
- en: For processing large amounts of text, Spark offers crucial libraries for feature
    extraction and transformation such as **TF-IDF** (short for **Term Frequency –
    Inverse Document Frequency**), Word2Vec, standard scaler, and normalizer.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We delve more deeply here in to the traditional machine learning algorithms
    offered by Spark MLlib. We distinguish between supervised and unsupervised learning
    depending on whether the data is labeled. We distinguish between categorical or
    continuous depending on whether the data is discrete or continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram explains the Spark MLlib supervised and unsupervised
    machine learning algorithms and preprocessing techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised and unsupervised learning](img/B03986_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following supervised and unsupervised MLlib algorithms and preprocessing
    techniques are currently available in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: This is an unsupervised machine learning technique where the
    data is not labeled. The aim is to extract structure from the data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Means**: This partitions the data in K distinct clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian Mixture**: Clusters are assigned based on the maximum posterior
    probability of the component'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Iteration Clustering (PIC)**: This groups vertices of a graph based
    on pairwise edge similarities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**): This is used to group collections
    of text documents into topics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming K-Means**: This means clusters dynamically streaming data using
    a windowing function on the incoming data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction**: This aims to reduce the number of features under
    consideration. Essentially, this reduces noise in the data and focuses on the
    key features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**): This breaks the matrix that contains
    the data into simpler meaningful pieces. It factorizes the initial matrix into
    three matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**): This approximates a high dimensional
    dataset with a low dimensional sub space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression and Classification**: Regression predicts output values using
    labeled training data, while Classification groups the results into classes. Classification
    has dependent variables that are categorical or unordered whilst Regression has
    dependent variables that are continuous and ordered:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear Regression Models** (linear regression, logistic regression, and support
    vector machines): Linear regression algorithms can be expressed as convex optimization
    problems that aim to minimize an objective function based on a vector of weight
    variables. The objective function controls the complexity of the model through
    the regularized part of the function and the error of the model through the loss
    part of the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naive Bayes**: This makes predictions based on the conditional probability
    distribution of a label given an observation. It assumes that features are mutually
    independent of each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Trees**: This performs recursive binary partitioning of the feature
    space. The information gain at the tree node level is maximized in order to determine
    the best split for the partition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensembles of trees** (Random Forests and Gradient-Boosted Trees): Tree ensemble
    algorithms combine base decision tree models in order to build a performant model.
    They are intuitive and very successful for classification and regression tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isotonic Regression**: This minimizes the mean squared error between given
    data and observed responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark MLlib offers more algorithms than the supervised and unsupervised learning
    ones. We have broadly three more additional types of machine learning methods:
    recommender systems, optimization algorithms, and feature extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Additional learning algorithms](img/B03986_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following additional MLlib algorithms are currently available in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaborative filtering**: This is the basis for recommender systems. It
    creates a user-item association matrix and aims to fill the gaps. Based on other
    users and items along with their ratings, it recommends an item that the target
    user has no ratings for. In distributed computing, one of the most successful
    algorithms is **ALS** (short for **Alternating Least Square**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternating Least Squares**: This matrix factorization technique incorporates
    implicit feedback, temporal effects, and confidence levels. It decomposes the
    large user item matrix into a lower dimensional user and item factors. It minimizes
    a quadratic loss function by fixing alternatively its factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction and transformation**: These are essential techniques for
    large text document processing. It includes the following techniques:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Term Frequency**: Search engines use TF-IDF to score and rank document relevance
    in a vast corpus. It is also used in machine learning to determine the importance
    of a word in a document or corpus. Term frequency statistically determines the
    weight of a term relative to its frequency in the corpus. Term frequency on its
    own can be misleading as it overemphasizes words such as *the*, *of*, or *and*
    that give little information. Inverse Document Frequency provides the specificity
    or the measure of the amount of information, whether the term is rare or common
    across all documents in the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word2Vec**: This includes two models, **Skip-Gram** and **Continuous Bag
    of Word**. The Skip-Gram predicts neighboring words given a word, based on sliding
    windows of words, while Continuous Bag of Words predicts the current word given
    the neighboring words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard Scaler**: As part of preprocessing, the dataset must often be standardized
    by mean removal and variance scaling. We compute the mean and standard deviation
    on the training data and apply the same transformation to the test data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalizer**: We scale the samples to have unit norm. It is useful for quadratic
    forms such as the dot product or kernel methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: This reduces the dimensionality of the vector space
    by selecting the most relevant features for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chi-Square Selector**: This is a statistical method to measure the independence
    of two events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: These specific Spark MLlib optimization algorithms focus
    on various techniques of gradient descent. Spark provides very efficient implementation
    of gradient descent on a distributed cluster of machines. It looks for the local
    minima by iteratively going down the steepest descent. It is compute-intensive
    as it iterates through all the data available:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent**: We minimize an objective function that is
    the sum of differentiable functions. Stochastic Gradient Descent uses only a sample
    of the training data in order to update a parameter in a particular iteration.
    It is used for large-scale and sparse machine learning problems such as text classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited-memory BFGS** (**L-BFGS**): As the name says, L-BFGS uses limited
    memory and suits the distributed optimization algorithm implementation of Spark
    MLlib.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark MLlib data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLlib supports four essential data types: **local vector**, **labeled point**,
    **local matrix**, and **distributed matrix**. These data types are widely used
    in Spark MLlib algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local vector**: This resides in a single machine. It can be dense or sparse:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense vector is a traditional array of doubles. An example of dense vector is
    `[5.0, 0.0, 1.0, 7.0]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse vector uses integer indices and double values. So the sparse representation
    of the vector `[5.0, 0.0, 1.0, 7.0]` would be `(4, [0, 2, 3], [5.0, 1.0, 7.0])`,
    where represent the dimension of the vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s an example of local vector in PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Labeled point**. A labeled point is a dense or sparse vector with a label
    used in supervised learning. In the case of binary labels, 0.0 represents the
    negative label whilst 1.0 represents the positive value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s an example of a labeled point in PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Local Matrix**: This local matrix resides in a single machine with integer-type
    indices and values of type double.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s an example of a local matrix in PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Distributed Matrix**: Leveraging the distributed mature of the RDD, distributed
    matrices can be shared in a cluster of machines. We distinguish four distributed
    matrix types: `RowMatrix`, `IndexedRowMatrix`, `CoordinateMatrix`, and `BlockMatrix`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RowMatrix`: This takes an RDD of vectors and creates a distributed matrix
    of rows with meaningless indices, called `RowMatrix`, from the RDD of vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndexedRowMatrix`: In this case, row indices are meaningful. First, we create
    an RDD of indexed rows using the class `IndexedRow` and then create an `IndexedRowMatrix`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CoordinateMatrix`: This is useful to represent very large and very sparse
    matrices. `CoordinateMatrix` is created from RDDs of the `MatrixEntry` points,
    represented by a tuple of type (long, long, or float)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BlockMatrix`: These are created from RDDs of sub-matrix blocks, where a sub-matrix
    block is `((blockRowIndex, blockColIndex), sub-matrix)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning workflows and data flows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beyond algorithms, machine learning is also about processes. We will discuss
    the typical workflows and data flows of supervised and unsupervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In supervised machine learning, the input training dataset is labeled. One of
    the key data practices is to split input data into training and test sets, and
    validate the mode accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We typically go through a six-step process flow in supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect the data**: This step essentially ties in with the previous chapter
    and ensures we collect the right data with the right volume and granularity in
    order to enable the machine learning algorithm to provide reliable answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocess the data**: This step is about checking the data quality by sampling,
    filling in the missing values if any, scaling and normalizing the data. We also
    define the feature extraction process. Typically, in the case of large text-based
    datasets, we apply tokenization, stop words removal, stemming, and TF-IDF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of supervised learning, we separate the input data into a training
    and test set. We can also implement various strategies of sampling and splitting
    the dataset for cross-validation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ready the data**: In this step, we get the data in the format or data type
    expected by the algorithms. In the case of Spark MLlib, this includes local vector,
    dense or sparse vectors, labeled points, local matrix, distributed matrix with
    row matrix, indexed row matrix, coordinate matrix, and block matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: In this step, we apply the algorithms that are suitable for the
    problem at hand and get the results for evaluation of the most suitable algorithm
    in the evaluate step. We might have multiple algorithms suitable for the problem;
    their respective performance will be scored in the evaluate step to select the
    best preforming ones. We can implement an ensemble or combination of models in
    order to reach the best results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize**: We may need to run a grid search for the optimal parameters of
    certain algorithms. These parameters are determined during training, and fine-tuned
    during the testing and production phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate**: We ultimately score the models and select the best one in terms
    of accuracy, performance, reliability, and scalability. We move the best performing
    model to test with the held out test data in order to ascertain the prediction
    accuracy of our model. Once satisfied with the fine-tuned model, we move it to
    production to process live data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The supervised machine learning workflow and dataflow are represented in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised machine learning workflows](img/B03986_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unsupervised machine learning workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As opposed to supervised learning, our initial data is not labeled in the case
    of unsupervised learning, which is most often the case in real life. We will extract
    the structure from the data by using clustering or dimensionality reduction algorithms.
    In the unsupervised learning case, we do not split the data into training and
    test, as we cannot make any prediction because the data is not labeled. We will
    train the data along six steps similar to those in supervised learning. Once the
    model is trained, we will evaluate the results and fine-tune the model and then
    release it for production.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning can be a preliminary step to supervised learning. Namely,
    we look at reducing the dimensionality of the data prior to attacking the learning
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unsupervised machine learning workflows and dataflow are represented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised machine learning workflows](img/B03986_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clustering the Twitter dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first get a feel for the data extracted from Twitter and get an understanding
    of the data structure in order to prepare and run it through the K-Means clustering
    algorithms. Our plan of attack uses the process and dataflow depicted earlier
    for unsupervised learning. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Combine all tweet files into a single dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the tweets, remove stop words, extract emoticons, extract URL, and finally
    normalize the words (for example, mapping them to lowercase and removing punctuation
    and numbers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Feature extraction includes the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tokenization**: This breaks down the parsed tweet text into individual words
    or tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TF-IDF**: This applies the TF-IDF algorithm to create feature vectors from
    the tokenized tweet texts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hash TF-IDF**: This applies a hashing function to the token vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the K-Means clustering algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate the results of the K-Means clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify tweet membership to clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform dimensionality reduction to two dimensions with the Multi-Dimensional
    Scaling or the Principal Component Analysis algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune the number of relevant clusters K
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure the model cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the optimal model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying Scikit-Learn on the Twitter dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python's own Scikit-Learn machine learning library is one of the most reliable,
    intuitive, and robust tools around. Let's run through a preprocessing and unsupervised
    learning using Pandas and Scikit-Learn. It is often beneficial to explore a sample
    of the data using Scikit-Learn before spinning off clusters with Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a mixed bag of 7,540 tweets. It contains tweets related to Apache Spark,
    Python, the upcoming presidential election with Hillary Clinton and Donald Trump
    as protagonists, and some tweets related to fashion and music with Lady Gaga and
    Justin Bieber. We are running the K-Means clustering algorithm using Python Scikit-Learn
    on the Twitter dataset harvested. We first load the sample data into a Pandas
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We first perform a feature extraction from the tweets'' text. We apply a sparse
    vectorizer to the dataset using a TF-IDF vectorizer with 10,000 features and English
    stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As the dataset is now broken into a 7540 sample with vectors of 6,638 features,
    we are ready to feed this sparse matrix to the K-Means clustering algorithm. We
    will choose seven clusters and 100 maximum iterations initially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The K-Means clustering algorithm converged after 18 iterations. We see in the
    following results the seven clusters with their respective key words. Clusters
    `0` and `6` are about music and fashion with Justin Bieber and Lady Gaga-related
    tweets. Clusters `1` and `5` are related to the U.S.A. presidential elections
    with Donald Trump-and Hilary Clinton-related tweets. Clusters `2` and `3` are
    the ones of interest to us as they are about Apache Spark and Python. Cluster
    `4` contains Thailand-related tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will visualize the results by plotting the cluster. We have 7,540 samples
    with 6,638 features. It will be impossible to visualize that many dimensions.
    We will use the **Multi-Dimensional Scaling** (**MDS**) algorithm to bring down
    the multidimensional features of the clusters into two tractable dimensions to
    be able to picture them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s a plot of Cluster `2`, *Big Data* and *Spark*, represented by blue
    dots along with Cluster `3`, *Spark* and *Python*, represented by red dots, and
    some sample tweets related to the respective clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying Scikit-Learn on the Twitter dataset](img/B03986_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have gained some good insights into the data with the exploration and processing
    done with Scikit-Learn. We will now focus our attention on Spark MLlib and take
    it for a ride on the Twitter dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will focus on feature extraction and engineering in order to ready
    the data for the clustering algorithm run. We instantiate the Spark Context and
    read the Twitter dataset into a Spark dataframe. We will then successively tokenize
    the tweet text data, apply a hashing Term frequency algorithm to the tokens, and
    finally apply the Inverse Document Frequency algorithm and rescale the data. The
    code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Running the clustering algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the K-Means algorithm against the Twitter dataset. As an unlabeled
    and shuffled bag of tweets, we want to see if the *Apache Spark* tweets are grouped
    in a single cluster. From the previous steps, the TF-IDF sparse vector of features
    is converted into an RDD that will be the input to the Spark MLlib program. We
    initialize the K-Means model with 5 clusters, 10 iterations of 10 runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model and the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to fine-tune the clustering algorithm is by varying the number of clusters
    and verifying the output. Let''s check the clusters and get a feel for the clustering
    results so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We map the `5` clusters with some sample tweets. Cluster `0` is about Spark.
    Cluster `1` is about Python. Cluster `2` is about Lady Gaga. Cluster `3` is about
    Thailand's Phuket News. Cluster `4` is about Donald Trump.
  prefs: []
  type: TYPE_NORMAL
- en: Building machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to compose the feature extraction, preparatory activities, training,
    testing, and prediction activities while optimizing the best tuning parameter
    to get the best performing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tweet captures perfectly in five lines of code a powerful machine
    learning Pipeline implemented in Spark MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building machine learning pipelines](img/B03986_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Spark ML pipeline is inspired by Python's Scikit-Learn and creates a succinct,
    declarative statement of the successive transformations to the data in order to
    quickly deliver a tunable model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got an overview of Spark MLlib's ever-expanding library
    of algorithms Spark MLlib. We discussed supervised and unsupervised learning,
    recommender systems, optimization, and feature extraction algorithms. We then
    put the harvested data from Twitter into the machine learning process, algorithms,
    and evaluation to derive insights from the data. We put the Twitter-harvested
    dataset through a Python Scikit-Learn and Spark MLlib K-means clustering in order
    to segregate the tweets relevant to *Apache Spark*. We also evaluated the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: This gets us ready for the next chapter, which will cover Streaming Analytics
    using Spark. Let's jump right in.
  prefs: []
  type: TYPE_NORMAL

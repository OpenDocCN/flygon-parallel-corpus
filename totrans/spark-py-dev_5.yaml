- en: Chapter 5. Streaming Live Data with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on live streaming data flowing into Spark and
    processing it. So far, we have discussed machine learning and data mining with
    batch processing. We are now looking at processing continuously flowing data and
    detecting facts and patterns on the fly. We are navigating from a lake to a river.
  prefs: []
  type: TYPE_NORMAL
- en: We will first investigate the challenges arising from such a dynamic and ever
    changing environment. After laying the grounds on the prerequisite of a streaming
    application, we will investigate various implementations using live sources of
    data such as TCP sockets to the Twitter firehose and put in place a low latency,
    high throughput, and scalable data pipeline combining Spark, Kafka and Flume.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing a streaming application's architectural challenges, constraints, and
    requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing live data from a TCP socket with Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to the Twitter firehose directly to parse tweets in quasi real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing a reliable, fault tolerant, scalable, high throughput, low latency
    integrated application using Spark, Kafka, and Flume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closing remarks on Lambda and Kappa architecture paradigms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying the foundations of streaming architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As customary, let's first go back to our original drawing of the data-intensive
    apps architecture blueprint and highlight the Spark Streaming module that will
    be the topic of interest.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram sets the context by highlighting the Spark Streaming module
    and interactions with Spark SQL and Spark MLlib within the overall data-intensive
    apps framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![Laying the foundations of streaming architecture](img/B03968_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data flows from stock market time series, enterprise transactions, interactions,
    events, web traffic, click streams, and sensors. All events are time-stamped data
    and urgent. This is the case for fraud detection and prevention, mobile cross-sell
    and upsell, or traffic alerts. Those streams of data require immediate processing
    for monitoring purposes, such as detecting anomalies, outliers, spam, fraud, and
    intrusion; and also for providing basic statistics, insights, trends, and recommendations.
    In some cases, the summarized aggregated information is sufficient to be stored
    for later usage. From an architecture paradigm perspective, we are moving from
    a service-oriented architecture to an event-driven architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two models emerge for processing streams of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing one record at a time as they come in. We do not buffer the incoming
    records in a container before processing them. This is the case of Twitter's Storm,
    Yahoo's S4, and Google's MillWheel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro-batching or batch computations on small intervals as performed by Spark
    Streaming and Storm Trident. In this case, we buffer the incoming records in a
    container according to the time window prescribed in the micro-batching settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming has often been compared against Storm. They are two different
    models of streaming data. Spark Streaming is based on micro-batching. Storm is
    based on processing records as they come in. Storm also offers a micro-batching
    option, with its Storm Trident option.
  prefs: []
  type: TYPE_NORMAL
- en: The driving factor in a streaming application is latency. Latency varies from
    the milliseconds range in the case of **RPC** (short for **Remote Procedure Call**)
    to several seconds or minutes for micro batching solution such as Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: RPC allows synchronous operations between the requesting programs waiting for
    the results from the remote server's procedure. Threads allow concurrency of multiple
    RPC calls to the server.
  prefs: []
  type: TYPE_NORMAL
- en: An example of software implementing a distributed RPC model is Apache Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Storm implements stateless sub millisecond latency processing of unbounded tuples
    using topologies or directed acyclic graphs combining spouts as source of data
    streams and bolts for operations such as filter, join, aggregation, and transformation.
    Storm also implements a higher level abstraction called **Trident** which, similarly
    to Spark, processes data streams in micro batches.
  prefs: []
  type: TYPE_NORMAL
- en: So, looking at the latency continuum, from sub millisecond to second, Storm
    is a good candidate. For seconds to minutes scale, Spark Streaming and Storm Trident
    are excellent fits. For several minutes onward, Spark and a NoSQL database such
    as Cassandra or HBase are adequate solutions. For ranges beyond the hour and with
    high volume of data, Hadoop is the ideal contender.
  prefs: []
  type: TYPE_NORMAL
- en: Although throughput is correlated to latency, it is not a simple inversely linear
    relationship. If processing a message takes 2 ms, which determines the latency,
    then one would assume the throughput is limited to 500 messages per sec. Batching
    messages allows for higher throughput if we allow our messages to be buffered
    for 8 ms more. With a latency of 10 ms, the system can buffer up to 10,000 messages.
    For a bearable increase in latency, we have substantially increased throughput.
    This is the magic of micro-batching that Spark Streaming exploits.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming inner working
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark Streaming architecture leverages the Spark core architecture. It overlays
    on the **SparkContext** a **StreamingContext** as the entry point to the Stream
    functionality. The Cluster Manager will dedicate at least one worker node as Receiver,
    which will be an executor with a *long task* to process the incoming stream. The
    Executor creates Discretized Streams or DStreams from input data stream and replicates
    by default, the DStream to the cache of another worker. One receiver serves one
    input data stream. Multiple receivers improve parallelism and generate multiple
    DStreams that Spark can unite or join Resilient Distributed Datasets (RDD).
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram gives an overview of the inner working of Spark Streaming.
    The client interacts with the Spark Cluster via the cluster manager, while Spark
    Streaming has a dedicated worker with a long running task ingesting the input
    data stream and transforming it into discretized streams or DStreams. The data
    is collected, buffered and replicated by a receiver and then pushed to a stream
    of RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark Streaming inner working](img/B03968_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark receivers can ingest data from many sources. Core input sources range
    from TCP socket and HDFS/Amazon S3 to Akka Actors. Additional sources include
    Apache Kafka, Apache Flume, Amazon Kinesis, ZeroMQ, Twitter, and custom or user-defined
    receivers.
  prefs: []
  type: TYPE_NORMAL
- en: We distinguish between reliable resources that acknowledges receipt of data
    to the source and replication for possible resend, versus unreliable receivers
    who do not acknowledge receipt of the message. Spark scales out in terms of the
    number of workers, partition and receivers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives an overview of Spark Streaming with the possible
    sources and the persistence options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark Streaming inner working](img/B03968_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Going under the hood of Spark Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark Streaming is composed of Receivers and powered by Discretized Streams
    and Spark Connectors for persistence.
  prefs: []
  type: TYPE_NORMAL
- en: As for Spark Core, the essential data structure is the RDD, the fundamental
    programming abstraction for Spark Streaming is the Discretized Stream or DStream.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram illustrates the Discretized Streams as continuous sequences
    of RDDs. The batch intervals of DStream are configurable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Going under the hood of Spark Streaming](img/B03968_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: DStreams snapshots the incoming data in batch intervals. Those time steps typically
    range from 500 ms to several seconds. The underlying structure of a DStream is
    an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: A DStream is essentially a continuous sequence of RDDs. This is powerful as
    it allows us to leverage from Spark Streaming all the traditional functions, transformations
    and actions available in Spark Core and allows us to dialogue with Spark SQL,
    performing SQL queries on incoming streams of data and Spark MLlib. Transformations
    similar to those on generic and key-value pair RDDs are applicable. The DStreams
    benefit from the inner RDDs lineage and fault tolerance. Additional transformation
    and output operations exist for discretized stream operations. Most generic operations
    on DStream are **transform** and **foreachRDD**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives an overview of the lifecycle of DStreams. From
    creation of the micro-batches of messages materialized to RDDs on which `transformation`
    function and actions that trigger Spark jobs are applied. Breaking down the steps
    illustrated in the diagram, we read the diagram top down:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Input Stream, the incoming messages are buffered in a container according
    to the time window allocated for the micro-batching.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the discretized stream step, the buffered micro-batches are transformed as
    DStream RDDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Mapped DStream step is obtained by applying a transformation function to
    the original DStream. These first three steps constitute the transformation of
    the original data received in predefined time windows. As the underlying data
    structure is the RDD, we conserve the data lineage of the transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final step is an action on the RDD. It triggers the Spark job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Going under the hood of Spark Streaming](img/B03968_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transformation can be stateless or stateful. *Stateless* means that no state
    is maintained by the program, while *stateful* means the program keeps a state,
    in which case previous transactions are remembered and may affect the current
    transaction. A stateful operation modifies or requires some state of the system,
    and a stateless operation does not.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless transformations process each batch in a DStream at a time. Stateful
    transformations process multiple batches to obtain results. Stateful transformations
    require the checkpoint directory to be configured. Check pointing is the main
    mechanism for fault tolerance in Spark Streaming to periodically save data and
    metadata about an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of stateful transformations for Spark Streaming: `updateStateByKey`
    and windowed transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: '`updateStateByKey` are transformations that maintain state for each key in
    a stream of Pair RDDs. It returns a new *state* DStream where the state for each
    key is updated by applying the given function on the previous state of the key
    and the new values of each key. An example would be a running count of given hashtags
    in a stream of tweets.'
  prefs: []
  type: TYPE_NORMAL
- en: Windowed transformations are carried over multiple batches in a sliding window.
    A window has a defined length or duration specified in time units. It must be
    a multiple of a DStream batch interval. It defines how many batches are included
    in a windowed transformation.
  prefs: []
  type: TYPE_NORMAL
- en: A window has a sliding interval or sliding duration specified in time units.
    It must be a multiple of a DStream batch interval. It defines how many batches
    to slide a window or how frequently to compute a windowed transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema depicts the windowing operation on DStreams to derive
    window DStreams with a given length and sliding interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Going under the hood of Spark Streaming](img/B03968_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A sample function is `countByWindow` (`windowLength`, `slideInterval`). It returns
    a new DStream in which each RDD has a single element generated by counting the
    number of elements in a sliding window over this DStream. An illustration in this
    case would be a running count of given hashtags in a stream of tweets every 60
    seconds. The window time frame is specified.
  prefs: []
  type: TYPE_NORMAL
- en: Minute scale window length is reasonable. Hour scale window length is not recommended
    as it is compute and memory intensive. It would be more convenient to aggregate
    the data in a database such as Cassandra or HBase.
  prefs: []
  type: TYPE_NORMAL
- en: Windowed transformations compute results based on window length and window slide
    interval. Spark performance is primarily affected by on window length, window
    slide interval, and persistence.
  prefs: []
  type: TYPE_NORMAL
- en: Building in fault tolerance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-time stream processing systems must be operational 24/7\. They need to
    be resilient to all sorts of failures in the system. Spark and its RDD abstraction
    are designed to seamlessly handle failures of any worker nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Main Spark Streaming fault tolerance mechanisms are check pointing, automatic
    driver restart, and automatic failover. Spark enables recovery from driver failure
    using check pointing, which preserves the application state.
  prefs: []
  type: TYPE_NORMAL
- en: Write ahead logs, reliable receivers, and file streams guarantees zero data
    loss as of Spark Version 1.2\. Write ahead logs represent a fault tolerant storage
    for received data.
  prefs: []
  type: TYPE_NORMAL
- en: Failures require recomputing results. DStream operations have exactly-one semantics.
    Transformations can be recomputed multiple times but will yield the same result.
    DStream output operations have at least once semantics. Output operations may
    be executed multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Processing live data with TCP sockets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a stepping stone to the overall understanding of streaming operations, we
    will first experiment with TCP socket. TCP socket establishes two-way communication
    between client and server, and it can exchange data through the established connection.
    WebSocket connections are long lived, unlike typical HTTP connections. HTTP is
    not meant to keep an open connection from the server to push continuously data
    to the web browsers. Most web applications hence resorted to long polling via
    frequent **Asynchronous JavaScript** (**AJAX**) and XML requests. WebSockets,
    standardized and implemented in HTML5, are moving beyond web browsers and are
    becoming a cross-platform standard for real-time communication between client
    and server.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up TCP sockets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a TCP Socket Server by running `netcat`, a small utility found in
    most Linux systems, as a data server with the command `> nc -lk 9999`, where `9999`
    is the port where we are sending data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once netcat is running, we will open a second console with our Spark Streaming
    client to receive the data and process. As soon as the Spark Streaming client
    console is listening, we start typing the words to be processed, that is, `hello
    world`.
  prefs: []
  type: TYPE_NORMAL
- en: Processing live data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using the example program provided in the Spark bundle for Spark
    Streaming called `network_wordcount.py`. It can be found on the GitHub repository
    under [https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py).
    The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we explain the steps of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code first initializes a Spark Streaming Context with the command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, the streaming computation is set up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One or more DStream objects that receive data are defined to connect to localhost
    or `127.0.0.1` on `port 9999`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The DStream computation is defined: transformations and output operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Computation is started:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Program termination is pending manual or error processing completion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Manual completion is an option when a completion condition is known:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can monitor the Spark Streaming application by visiting the Spark monitoring
    home page at `localhost:4040`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the result of running the program and feeding the words on the `netcat`
    4server console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the Spark Streaming `network_count` program by connecting to the socket
    localhost on `port 9999`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we have established connection through the socket on `port 9999`, streamed
    the data sent by the `netcat` server, and performed a word count on the messages
    sent.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating Twitter data in real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter offers two APIs. One search API that essentially allows us to retrieve
    past tweets based on search terms. This is how we have been collecting our data
    from Twitter in the previous chapters of the book. Interestingly, for our current
    purpose, Twitter offers a live streaming API which allows to ingest tweets as
    they are emitted in the blogosphere.
  prefs: []
  type: TYPE_NORMAL
- en: Processing Tweets in real time from the Twitter firehose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following program connects to the Twitter firehose and processes the incoming
    tweets to exclude deleted or invalid tweets and parses on the fly only the relevant
    ones to extract `screen name`, the actual tweet, or `tweet text`, `retweet` count,
    `geo-location` information. The processed tweets are gathered into an RDD Queue
    by Spark Streaming and then displayed on the console at a one-second interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this program, it delivers the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: So, we got an example of streaming tweets with Spark and processing them on
    the fly.
  prefs: []
  type: TYPE_NORMAL
- en: Building a reliable and scalable streaming app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingesting data is the process of acquiring data from various sources and storing
    it for processing immediately or at a later stage. Data consuming systems are
    dispersed and can be physically and architecturally far from the sources. Data
    ingestion is often implemented manually with scripts and rudimentary automation.
    It actually calls for higher level frameworks like Flume and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of data ingestion arise from the fact that the sources are physically
    spread out and are transient which makes the integration brittle. Data production
    is continuous for weather, traffic, social media, network activity, shop floor
    sensors, security, and surveillance. Ever increasing data volumes and rates coupled
    with ever changing data structure and semantics makes data ingestion ad hoc and
    error prone.
  prefs: []
  type: TYPE_NORMAL
- en: The aim is to become more agile, reliable, and scalable. Agility, reliability,
    and scalability of the data ingestion determine the overall health of the pipeline.
    Agility means integrating new sources as they arise and incorporating changes
    to existing sources as needed. In order to ensure safety and reliability, we need
    to protect the infrastructure against data loss and downstream applications from
    silent data corruption at ingress. Scalability avoids ingest bottlenecks while
    keeping cost tractable.
  prefs: []
  type: TYPE_NORMAL
- en: '| Ingest Mode | Description | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Manual or Scripted | File copy using command line interface or GUI interface
    | HDFS Client, Cloudera Hue |'
  prefs: []
  type: TYPE_TB
- en: '| Batch Data Transport | Bulk data transport using tools | DistCp, Sqoop |'
  prefs: []
  type: TYPE_TB
- en: '| Micro Batch | Transport of small batches of data | Sqoop, Sqoop2Storm |'
  prefs: []
  type: TYPE_TB
- en: '| Pipelining | Flow like transport of event streams | Flume Scribe |'
  prefs: []
  type: TYPE_TB
- en: '| Message Queue | Publish Subscribe message bus of events | Kafka, Kinesis
    |'
  prefs: []
  type: TYPE_TB
- en: In order to enable an event-driven business that is able to ingest multiple
    streams of data, process it in flight, and make sense of it all to get to rapid
    decisions, the key driver is the Unified Log.
  prefs: []
  type: TYPE_NORMAL
- en: A Unified Log is a centralized enterprise structured log available for real-time
    subscription. All the organization's data is put in a central log for subscription.
    Records are numbered beginning with zero in the order that they are written. It
    is also known as a commit log or journal. The concept of the *Unified Log* is
    the central tenet of the Kappa architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The properties of the Unified Log are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unified**: There is a single deployment for the entire organization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Append only**: Events are immutable and are appended'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ordered**: Each event has a unique offset within a shard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed**: For fault tolerance purpose, the Unified Log is distributed
    redundantly on a cluster of computers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast**: The systems ingests thousands of messages per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to isolate downstream particular consumption of data from the vagaries
    of upstream emission of data, we need to decouple the providers of data from the
    receivers or consumers of data. As they are living in two different worlds with
    different cycles and constraints, Kafka decouples the data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka is a distributed publish subscribe messaging system rethought as
    a distributed commit log. The messages are stored by topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Kafka has the following properties. It supports:'
  prefs: []
  type: TYPE_NORMAL
- en: High throughput for high volume of events feeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time processing of new and derived feeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large data backlogs and persistence for offline consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low latency as enterprise wide messaging system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance thanks to its distributed nature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messages are stored in partition with a unique sequential ID called `offset`.
    Consumers track their pointers via tuple of (`offset`, `partition`, `topic`).
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive deeper in the anatomy of Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka has essentially three components: *producers*, *consumers* and *brokers*.
    Producers push and write data to brokers. Consumers pull and read data from brokers.
    Brokers do not push messages to consumers. Consumers pull message from brokers.
    The setup is distributed and coordinated by Apache Zookeeper.'
  prefs: []
  type: TYPE_NORMAL
- en: The brokers manage and store the data in topics. Topics are split in replicated
    partitions. The data is persisted in the broker, but not removed upon consumption,
    but until retention period. If a consumer fails, it can always go back to the
    broker to fetch the data.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka requires Apache ZooKeeper. ZooKeeper is a high-performance coordination
    service for distributed applications. It centrally manages configuration, registry
    or naming service, group membership, lock, and synchronization for coordination
    between servers. It provides a hierarchical namespace with metadata, monitoring
    statistics, and state of the cluster. ZooKeeper can introduce brokers and consumers
    on the fly and then rebalances the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka producers do not need ZooKeeper. Kafka brokers use ZooKeeper to provide
    general state information as well elect leader in case of failure. Kafka consumers
    use ZooKeeper to track message offset. Newer versions of Kafka will save the consumers
    to go through ZooKeeper and can retrieve the Kafka special topics information.
    Kafka provides automatic load balancing for producers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram gives an overview of the Kafka setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up Kafka](img/B03968_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Installing and testing Kafka
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will download the Apache Kafka binaries from the dedicated web page at [http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)
    and install the software in our machine using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the 0.8.2.0 release and `un-tar` it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Start `zooeeper`. Kafka uses ZooKeeper so we need to first start a ZooKeeper
    server. We will use the convenience script packaged with Kafka to get a single-node
    ZooKeeper instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now launch the Kafka server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a topic. Let''s create a topic named test with a single partition and
    only one replica:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see that topic if we run the `list` topic command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the Kafka installation by creating a producer and consumer. We first
    launch a `producer` and type a message in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We then launch a consumer to check that we receive the message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The messages were appropriately received by the consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check Kafka and Spark Streaming consumer. We will be using the Spark Streaming
    Kafka word count example provided in the Spark bundle. A word of caution: we have
    to bind the Kafka packages, `--packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0`,
    when we submit the Spark job. The command is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we launch the Spark Streaming word count program with Kafka, we get the
    following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the Kafka Python driver in order to be able to programmatically develop
    Producers and Consumers and interact with Kafka and Spark using Python. We will
    use the road-tested library from David Arthur, aka, Mumrah on GitHub ([https://github.com/mumrah](https://github.com/mumrah)).
    We can pip install it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Developing producers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following program creates a Simple Kafka Producer that will emit the message
    *this is a message sent from the Kafka producer:* five times, followed by a time
    stamp every second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this program, the following output is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It tells us there were no errors and gives the offset of the messages given
    by the Kafka broker.
  prefs: []
  type: TYPE_NORMAL
- en: Developing consumers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To fetch the messages from the Kafka brokers, we develop a Kafka consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this program, we effectively confirm that the consumer received
    all the messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Developing a Spark Streaming consumer for Kafka
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the example code provided in the Spark Streaming bundle, we will create
    a Spark Streaming consumer for Kafka and perform a word count on the messages
    stored with the brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this program with the following Spark submit command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Exploring flume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flume is a continuous ingestion system. It was originally designed to be a log
    aggregation system, but it evolved to handle any type of streaming event data.
  prefs: []
  type: TYPE_NORMAL
- en: Flume is a distributed, reliable, scalable, and available pipeline system for
    efficient collection, aggregation, and transport of large volumes of data. It
    has built-in support for contextual routing, filtering replication, and multiplexing.
    It is robust and fault tolerant, with tunable reliability mechanisms and many
    failover and recovery mechanisms. It uses a simple extensible data model that
    allows for real time analytic application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flume offers the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteed delivery semantics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low latency reliable data transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative configuration with no coding required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extendable and customizable settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with most commonly used end-points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The anatomy of Flume contains the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event**: An event is the fundamental unit of data that is transported by
    Flume from source to destination. It is like a message with a byte array payload
    opaque to Flume and optional headers used for contextual routing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Client**: A client produces and transmits events. A client decouples Flume
    from the data consumers. It is an entity that generates events and sends them
    to one or more agents. Custom client or Flume log4J append program or embedded
    application agent can be client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: An agent is a container hosting sources, channels, sinks, and other
    elements that enable the transportation of events from one place to the other.
    It provides configuration, life cycle management and monitoring for hosted components.
    An agent is a physical Java virtual machine running Flume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source**: Source is the entity through which Flume receives events. Sources
    require at least one channel to function in order to either actively poll data
    or passively wait for data to be delivered to them. A variety of sources allow
    data to be collected, such as log4j logs and syslogs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sink**: Sink is the entity that drains data from the channel and delivers
    it to the next destination. A variety of sinks allow data to be streamed to a
    range of destinations. Sinks support serialization to user''s format. One example
    is the HDFS sink that writes events to HDFS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Channel**: Channel is the conduit between the source and the sink that buffers
    incoming events until drained by sinks. Sources feed events into the channel and
    the sinks drain the channel. Channels decouple the impedance of upstream and downstream
    systems. Burst of data upstream is damped by the channels. Failures downstream
    are transparently absorbed by the channels. Sizing the channel capacity to cope
    with these events is key to realizing these benefits. Channels offer two levels
    of persistence: either memory channel, which is volatile if the JVM crashes, or
    File channel backed by Write Ahead Log that stores the information to disk. Channels
    are fully transactional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s illustrate all these concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring flume](img/B03968_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Developing data pipelines with Flume, Kafka, and Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building resilient data pipeline leverages the learnings from the previous sections.
    We are plumbing together data ingestion and transport with Flume, data brokerage
    with a reliable and sophisticated publish and subscribe messaging system such
    as Kafka, and finally process computation on the fly using Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the composition of streaming data pipelines
    as sequence of *connect*, *collect*, *conduct*, *compose*, *consume*, *consign*,
    and *control* activities. These activities are configurable based on the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect establishes the binding with the streaming API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect creates collection threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct decouples the data producers from the consumers by creating a buffer
    queue or publish-subscribe mechanism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compose is focused on processing the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consume provisions the processed data for the consuming systems. Consign takes
    care of the data persistence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control caters to governance and monitoring of the systems, data, and applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Developing data pipelines with Flume, Kafka, and Spark](img/B03968_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram illustrates the concepts of the streaming data pipelines
    with its key components: Spark Streaming, Kafka, Flume, and low latency databases.
    In the consuming or controlling applications, we are monitoring our systems in
    real time (depicted by a monitor) or sending real-time alerts (depicted by red
    lights) in case certain thresholds are crossed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing data pipelines with Flume, Kafka, and Spark](img/B03968_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following diagram illustrates Spark's unique ability to process in a single
    platform data in motion and data at rest while seamlessly interfacing with multiple
    persistence data stores as per the use case requirement.
  prefs: []
  type: TYPE_NORMAL
- en: This diagram brings in one unified whole all the concepts discussed up to now.
    The top part describes the streaming processing pipeline. The bottom part describes
    the batch processing pipeline. They both share a common persistence layer in the
    middle of the diagram depicting the various modes of persistence and serialization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing data pipelines with Flume, Kafka, and Spark](img/B03968_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Closing remarks on the Lambda and Kappa architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two architecture paradigms are currently in vogue: the Lambda and Kappa architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: Lambda is the brainchild of the Storm creator and main committer, Nathan Marz.
    It essentially advocates building a functional architecture on all data. The architecture
    has two branches. The first is a batch arm envisioned to be powered by Hadoop,
    where historical, high-latency, high-throughput data are pre-processed and made
    ready for consumption. The real-time arm is envisioned to be powered by Storm,
    and it processes incrementally streaming data, derives insights on the fly, and
    feeds aggregated information back to the batch storage.
  prefs: []
  type: TYPE_NORMAL
- en: Kappa is the brainchild of one the main committer of Kafka, Jay Kreps, and his
    colleagues at Confluent (previously at LinkedIn). It is advocating a full streaming
    pipeline, effectively implementing, at the enterprise level, the unified log enounced
    in the previous pages.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Lambda architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lambda architecture combines batch and streaming data to provide a unified
    query mechanism on all available data. Lambda architecture envisions three layers:
    a batch layer where precomputed information are stored, a speed layer where real-time
    incremental information is processed as data streams, and finally the serving
    layer that merges batch and real-time views for ad hoc queries. The following
    diagram gives an overview of the Lambda architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Lambda architecture](img/B03968_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding Kappa architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kappa architecture proposes to drive the full enterprise in streaming mode.
    The Kappa architecture arose from a critique from Jay Kreps and his colleagues
    at LinkedIn at the time. Since then, they moved and created Confluent with Apache
    Kafka as the main enabler of the Kappa architecture vision. The basic tenet is
    to move in all streaming mode with a Unified Log as the main backbone of the enterprise
    information architecture.
  prefs: []
  type: TYPE_NORMAL
- en: A Unified Log is a centralized enterprise structured log available for real-time
    subscription. All the organization's data is put in a central log for subscription.
    Records are numbered beginning with zero so that they are written. It is also
    known as a commit log or journal. The concept of the Unified Log is the central
    tenet of the Kappa architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The properties of the unified log are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unified**: There is a single deployment for the entire organization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Append only**: Events are immutable and are appended'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ordered**: Each event has a unique offset within a shard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed**: For fault tolerance purpose, the unified log is distributed
    redundantly on a cluster of computers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast**: The systems ingests thousands of messages per second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following screenshot captures the moment Jay Kreps announced his reservations
    about the Lambda architecture. His main reservation about the Lambda architecture
    is implementing the same job in two different systems, Hadoop and Storm, with
    each of their specific idiosyncrasies, and with all the complexities that come
    along with it. Kappa architecture processes the real-time data and reprocesses
    historical data in the same framework powered by Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Kappa architecture](img/B03968_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we laid out the foundations of streaming architecture apps
    and described their challenges, constraints, and benefits. We went under the hood
    and examined the inner working of Spark Streaming and how it fits with Spark Core
    and dialogues with Spark SQL and Spark MLlib. We illustrated the streaming concepts
    with TCP sockets, followed by live tweet ingestion and processing directly from
    the Twitter firehose. We discussed the notions of decoupling upstream data publishing
    from downstream data subscription and consumption using Kafka in order to maximize
    the resilience of the overall streaming architecture. We also discussed Flume—a
    reliable, flexible, and scalable data ingestion and transport pipeline system.
    The combination of Flume, Kafka, and Spark delivers unparalleled robustness, speed,
    and agility in an ever changing landscape. We closed the chapter with some remarks
    and observations on two streaming architectural paradigms, the Lambda and Kappa
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The Lambda architecture combines batch and streaming data in a common query
    front-end. It was envisioned with Hadoop and Storm in mind initially. Spark has
    its own batch and streaming paradigms, and it offers a single environment with
    common code base to effectively bring this architecture paradigm to life.
  prefs: []
  type: TYPE_NORMAL
- en: The Kappa architecture promulgates the concept of the unified log, which creates
    an event-oriented architecture where all events in the enterprise are channeled
    in a centralized commit log that is available to all consuming systems in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready for the visualization of the data collected and processed so
    far.
  prefs: []
  type: TYPE_NORMAL

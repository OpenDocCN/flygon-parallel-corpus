["```py\nIn [16]:\n# Read harvested data stored in csv in a Panda DF\nimport pandas as pd\ncsv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweetstxt.csv'\npddf_in = pd.read_csv(csv_in, index_col=None, header=0, sep=';', encoding='utf-8')\nIn [20]:\nprint('tweets pandas dataframe - count:', pddf_in.count())\nprint('tweets pandas dataframe - shape:', pddf_in.shape)\nprint('tweets pandas dataframe - colns:', pddf_in.columns)\n('tweets pandas dataframe - count:', Unnamed: 0    7540\nid            7540\ncreated_at    7540\nuser_id       7540\nuser_name     7538\ntweet_text    7540\ndtype: int64)\n('tweets pandas dataframe - shape:', (7540, 6))\n('tweets pandas dataframe - colns:', Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object'))\n```", "```py\nIn [21]:\npddf_in.head()\nOut[21]:\n  Unnamed: 0   id   created_at   user_id   user_name   tweet_text\n0   0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...\n1   1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...\n2   2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...\n3   3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...\n4   4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...\n```", "```py\nIn [72]:\nimport re\nimport time\n```", "```py\n    re.compile(r'^RT'),\n    ```", "```py\n    re.compile(r'(@[a-zA-Z0-9_]+)'),\n    ```", "```py\n    re.compile(r'(#[\\w\\d]+)'),\n    ```", "```py\n    re.compile(r'\\s+'), \n    ```", "```py\n    re.compile(r'([https://|http://]?[a-zA-Z\\d\\/]+[\\.]+[a-zA-Z\\d\\/\\.]+)')\n    In [24]:\n    regexp = {\"RT\": \"^RT\", \"ALNUM\": r\"(@[a-zA-Z0-9_]+)\",\n              \"HASHTAG\": r\"(#[\\w\\d]+)\", \"URL\": r\"([https://|http://]?[a-zA-Z\\d\\/]+[\\.]+[a-zA-Z\\d\\/\\.]+)\",\n              \"SPACES\":r\"\\s+\"}\n    regexp = dict((key, re.compile(value)) for key, value in regexp.items())\n    In [25]:\n    regexp\n    Out[25]:\n    {'ALNUM': re.compile(r'(@[a-zA-Z0-9_]+)'),\n     'HASHTAG': re.compile(r'(#[\\w\\d]+)'),\n     'RT': re.compile(r'^RT'),\n     'SPACES': re.compile(r'\\s+'),\n     'URL': re.compile(r'([https://|http://]?[a-zA-Z\\d\\/]+[\\.]+[a-zA-Z\\d\\/\\.]+)')}\n    ```", "```py\nIn [77]:\ndef getAttributeRT(tweet):\n    \"\"\" see if tweet is a RT \"\"\"\n    return re.search(regexp[\"RT\"], tweet.strip()) != None\n```", "```py\ndef getUserHandles(tweet):\n    \"\"\" given a tweet we try and extract all user handles\"\"\"\n    return re.findall(regexp[\"ALNUM\"], tweet)\n```", "```py\ndef getHashtags(tweet):\n    \"\"\" return all hashtags\"\"\"\n    return re.findall(regexp[\"HASHTAG\"], tweet)\n```", "```py\ndef getURLs(tweet):\n    \"\"\" URL : [http://]?[\\w\\.?/]+\"\"\"\n    return re.findall(regexp[\"URL\"], tweet)\n```", "```py\ndef getTextNoURLsUsers(tweet):\n    \"\"\" return parsed text terms stripped of URLs and User Names in tweet text\n        ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()) \"\"\"\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(RT)\",\" \", tweet).lower().split())\n```", "```py\ndef setTag(tweet):\n    \"\"\" set tags to tweet_text based on search terms from tags_list\"\"\"\n    tags_list = ['spark', 'python', 'clinton', 'trump', 'gaga', 'bieber']\n    lower_text = tweet.lower()\n    return filter(lambda x:x.lower() in lower_text,tags_list)\n```", "```py\ndef decode_date(s):\n    \"\"\" parse Twitter date into format yyyy-mm-dd hh:mm:ss\"\"\"\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(s,'%a %b %d %H:%M:%S +0000 %Y'))\n```", "```py\nIn [43]:\npddf_in.columns\nOut[43]:\nIndex([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object')\nIn [45]:\n# df.drop([Column Name or list],inplace=True,axis=1)\npddf_in.drop(['Unnamed: 0'], inplace=True, axis=1)\nIn [46]:\npddf_in.head()\nOut[46]:\n  id   created_at   user_id   user_name   tweet_text\n0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...\n1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...\n2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...\n3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...\n4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...\n```", "```py\nIn [82]:\npddf_in['htag'] = pddf_in.tweet_text.apply(getHashtags)\npddf_in['user_handles'] = pddf_in.tweet_text.apply(getUserHandles)\npddf_in['urls'] = pddf_in.tweet_text.apply(getURLs)\npddf_in['txt_terms'] = pddf_in.tweet_text.apply(getTextNoURLsUsers)\npddf_in['search_grp'] = pddf_in.tweet_text.apply(setTag)\npddf_in['date'] = pddf_in.created_at.apply(decode_date)\n```", "```py\nIn [83]:\npddf_in[2200:2210]\nOut[83]:\n  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp\n2200   638242693374681088   Mon Aug 31 06:51:30 +0000 2015   19525954   CENATIC   El impacto de @ApacheSpark en el procesamiento...   [#sparkSpecial]   [://t.co/4PQmJNuEJB]   el impacto de en el procesamiento de datos y e...   [spark]   2015-08-31 06:51:30   [@ApacheSpark]   el impacto de en el procesamiento de datos y e...   [spark]\n2201   638238014695575552   Mon Aug 31 06:32:55 +0000 2015   51115854   Nawfal   Real Time Streaming with Apache Spark\\nhttp://...   [#IoT, #SmartMelboune, #BigData, #Apachespark]   [://t.co/GW5PaqwVab]   real time streaming with apache spark iot smar...   [spark]   2015-08-31 06:32:55   []   real time streaming with apache spark iot smar...   [spark]\n2202   638236084124516352   Mon Aug 31 06:25:14 +0000 2015   62885987   Mithun Katti   RT @differentsachin: Spark the flame of digita...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   []   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 06:25:14   [@differentsachin, @ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]\n2203   638234734649176064   Mon Aug 31 06:19:53 +0000 2015   140462395   solaimurugan v   Installing @ApacheMahout with @ApacheSpark 1.4...   []   [1.4.1, ://t.co/3c5dGbfaZe.]   installing with 1 4 1 got many more issue whil...   [spark]   2015-08-31 06:19:53   [@ApacheMahout, @ApacheSpark]   installing with 1 4 1 got many more issue whil...   [spark]\n2204   638233517307072512   Mon Aug 31 06:15:02 +0000 2015   2428473836   Ralf Heineke   RT @RomeoKienzler: Join me @velocityconf on #m...   [#machinelearning, #devOps, #Bl]   [://t.co/U5xL7pYEmF]   join me on machinelearning based devops operat...   [spark]   2015-08-31 06:15:02   [@RomeoKienzler, @velocityconf, @ApacheSpark]   join me on machinelearning based devops operat...   [spark]\n2205   638230184848687106   Mon Aug 31 06:01:48 +0000 2015   289355748   Akim Boyko   RT @databricks: Watch live today at 10am PT is...   []   [1.5, ://t.co/16cix6ASti]   watch live today at 10am pt is 1 5 presented b...   [spark]   2015-08-31 06:01:48   [@databricks, @ApacheSpark, @databricks, @pwen...   watch live today at 10am pt is 1 5 presented b...   [spark]\n2206   638227830443110400   Mon Aug 31 05:52:27 +0000 2015   145001241   sachin aggarwal   Spark the flame of digital India @ #IBMHackath...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   [://t.co/C1AO3uNexe]   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 05:52:27   [@ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]\n2207   638227031268810752   Mon Aug 31 05:49:16 +0000 2015   145001241   sachin aggarwal   RT @pravin_gadakh: Imagine, innovate and Igni...   [#IBMHackathon, #ISLconnectIN2015]   []   gadakh imagine innovate and ignite digital ind...   [spark]   2015-08-31 05:49:16   [@pravin_gadakh, @ApacheSpark]   gadakh imagine innovate and ignite digital ind...   [spark]\n2208   638224591920336896   Mon Aug 31 05:39:35 +0000 2015   494725634   IBM Asia Pacific   RT @sachinparmar: Passionate about Spark?? Hav...   [#IBMHackathon, #ISLconnectIN]   [India..]   passionate about spark have dreams of clean sa...   [spark]   2015-08-31 05:39:35   [@sachinparmar]   passionate about spark have dreams of clean sa...   [spark]\n2209   638223327467692032   Mon Aug 31 05:34:33 +0000 2015   3158070968   Open Source India   \"Game Changer\" #ApacheSpark speeds up #bigdata...   [#ApacheSpark, #bigdata]   [://t.co/ieTQ9ocMim]   game changer apachespark speeds up bigdata pro...   [spark]   2015-08-31 05:34:33   []   game changer apachespark speeds up bigdata pro...   [spark]\n```", "```py\nIn [84]:\nf_name = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweets_processed.csv'\npddf_in.to_csv(f_name, sep=';', encoding='utf-8', index=False)\nIn [85]:\npddf_in.shape\nOut[85]:\n(7540, 13)\n```", "```py\nIn [21]:\nimport pandas as pd\ncsv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets.csv'\ntspark_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')\nIn [3]:\ntspark_df.head(3)\nOut[3]:\n  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp\n0   638818911773856000   Tue Sep 01 21:01:11 +0000 2015   2511247075   Noor Din   RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]   [://t.co/3bsaTT7eUs]   r leads rapidminer python catches up big data ...   [spark, python]   2015-09-01 21:01:11   [@kdnuggets]   r leads rapidminer python catches up big data ...   [spark, python]\n1   622142176768737000   Fri Jul 17 20:33:48 +0000 2015   24537879   IBM Cloudant   Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]   be one of the first to sign up for ibm analyti...   [spark]   2015-07-17 20:33:48   []   be one of the first to sign up for ibm analyti...   [spark]\n2   622140453069169000   Fri Jul 17 20:26:57 +0000 2015   515145898   Arno Candel   Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]   nice article on apachespark hadoop and datasci...   [spark]   2015-07-17 20:26:57   [@h2oai]   nice article on apachespark hadoop and datasci...   [spark]\n```", "```py\n#\n# Install PIL (does not work with Python 3.4)\n#\nan@an-VB:~$ conda install pil\n\nFetching package metadata: ....\nSolving package specifications: ..................\nPackage plan for installation in environment /home/an/anaconda:\n```", "```py\n    package                    |            build\n    ---------------------------|-----------------\n    libpng-1.6.17              |                0         214 KB\n    freetype-2.5.5             |                0         2.2 MB\n    conda-env-2.4.4            |           py27_0          24 KB\n    pil-1.1.7                  |           py27_2         650 KB\n    ------------------------------------------------------------\n                                           Total:         3.0 MB\n```", "```py\n    conda-env: 2.4.2-py27_0 --> 2.4.4-py27_0\n    freetype:  2.5.2-0      --> 2.5.5-0     \n    libpng:    1.5.13-1     --> 1.6.17-0    \n    pil:       1.1.7-py27_1 --> 1.1.7-py27_2\n\nProceed ([y]/n)? y\n```", "```py\n#\n# Install wordcloud\n# Andreas Mueller\n# https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py\n#\n\nan@an-VB:~$ pip install wordcloud\nCollecting wordcloud\n  Downloading wordcloud-1.1.3.tar.gz (163kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 548kB/s \nBuilding wheels for collected packages: wordcloud\n  Running setup.py bdist_wheel for wordcloud\n  Stored in directory: /home/an/.cache/pip/wheels/32/a9/74/58e379e5dc614bfd9dd9832d67608faac9b2bc6c194d6f6df5\nSuccessfully built wordcloud\nInstalling collected packages: wordcloud\nSuccessfully installed wordcloud-1.1.3\n```", "```py\nIn [4]:\n%matplotlib inline\nIn [11]:\n```", "```py\nlen(tspark_df['txt_terms'].tolist())\nOut[11]:\n2024\nIn [22]:\ntspark_ls_str = [str(t) for t in tspark_df['txt_terms'].tolist()]\nIn [14]:\nlen(tspark_ls_str)\nOut[14]:\n2024\nIn [15]:\ntspark_ls_str[:4]\nOut[15]:\n['r leads rapidminer python catches up big data tools grow spark ignites kdn',\n 'be one of the first to sign up for ibm analytics for apachespark today sparkinsight',\n 'nice article on apachespark hadoop and datascience',\n 'spark 101 running spark and mapreduce together in production hadoopsummit2015 apachespark altiscale']\n```", "```py\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n```", "```py\n# join tweets to a single string\nwords = ' '.join(tspark_ls_str)\n\n# create wordcloud \nwordcloud = WordCloud(\n                      # remove stopwords\n                      stopwords=STOPWORDS,\n                      background_color='black',\n                      width=1800,\n                      height=1400\n                     ).generate(words)\n\n# render wordcloud image\nplt.imshow(wordcloud)\nplt.axis('off')\n\n# save wordcloud image on disk\nplt.savefig('./spark_tweets_wordcloud_1.png', dpi=300)\n\n# display image in Jupyter notebook\nplt.show()\n```", "```py\nIn [4]:\n#\n# This module exposes geometry data for World Country Boundaries.\n#\nimport csv\nimport codecs\nimport gzip\nimport xml.etree.cElementTree as et\nimport os\nfrom os.path import dirname, join\n\nnan = float('NaN')\n__file__ = os.getcwd()\n\ndata = {}\nwith gzip.open(join(dirname(__file__), 'AN_Spark/data/World_Country_Boundaries.csv.gz')) as f:\n    decoded = codecs.iterdecode(f, \"utf-8\")\n    next(decoded)\n    reader = csv.reader(decoded, delimiter=',', quotechar='\"')\n    for row in reader:\n        geometry, code, name = row\n        xml = et.fromstring(geometry)\n        lats = []\n        lons = []\n        for i, poly in enumerate(xml.findall('.//outerBoundaryIs/LinearRing/coordinates')):\n            if i > 0:\n                lats.append(nan)\n                lons.append(nan)\n            coords = (c.split(',')[:2] for c in poly.text.split())\n            lat, lon = list(zip(*[(float(lat), float(lon)) for lon, lat in\n                coords]))\n            lats.extend(lat)\n            lons.extend(lon)\n        data[code] = {\n            'name'   : name,\n            'lats'   : lats,\n            'lons'   : lons,\n        }\nIn [5]:\nlen(data)\nOut[5]:\n235\n```", "```py\nIn [69]:\n# data\n#\n#\nIn [8]:\nimport pandas as pd\ncsv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets_20.csv'\nt20_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')\nIn [9]:\nt20_df.head(3)\nOut[9]:\n    id  created_at  user_id     user_name   tweet_text  htag    urls    ptxt    tgrp    date    user_handles    txt_terms   search_grp  lat     lon\n0   638818911773856000  Tue Sep 01 21:01:11 +0000 2015  2511247075  Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]  [://t.co/3bsaTT7eUs]    r leads rapidminer python catches up big data ...   [spark, python]     2015-09-01 21:01:11     [@kdnuggets]    r leads rapidminer python catches up big data ...   [spark, python]     37.279518   -121.867905\n1   622142176768737000  Fri Jul 17 20:33:48 +0000 2015  24537879    IBM Cloudant    Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]    be one of the first to sign up for ibm analyti...   [spark]     2015-07-17 20:33:48     []  be one of the first to sign up for ibm analyti...   [spark]     37.774930   -122.419420\n2   622140453069169000  Fri Jul 17 20:26:57 +0000 2015  515145898   Arno Candel     Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]    nice article on apachespark hadoop and datasci...   [spark]     2015-07-17 20:26:57     [@h2oai]    nice article on apachespark hadoop and datasci...   [spark]     51.500130   -0.126305\nIn [98]:\nlen(t20_df.user_id.unique())\nOut[98]:\n19\nIn [17]:\nt20_geo = t20_df[['date', 'lat', 'lon', 'user_name', 'tweet_text']]\nIn [24]:\n# \nt20_geo.rename(columns={'user_name':'user', 'tweet_text':'text' }, inplace=True)\nIn [25]:\nt20_geo.head(4)\nOut[25]:\n    date    lat     lon     user    text\n0   2015-09-01 21:01:11     37.279518   -121.867905     Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...\n1   2015-07-17 20:33:48     37.774930   -122.419420     IBM Cloudant    Be one of the first to sign-up for IBM Analyti...\n2   2015-07-17 20:26:57     51.500130   -0.126305   Arno Candel     Nice article on #apachespark, #hadoop and #dat...\n3   2015-07-17 19:35:31     51.500130   -0.126305   Ira Michael Blonder     Spark 101: Running Spark and #MapReduce togeth...\nIn [22]:\ndf = t20_geo\n#\n```", "```py\nIn [29]:\n#\n# Bokeh Visualization of tweets on world map\n#\nfrom bokeh.plotting import *\nfrom bokeh.models import HoverTool, ColumnDataSource\nfrom collections import OrderedDict\n\n# Output in Jupiter Notebook\noutput_notebook()\n\n# Get the world map\nworld_countries = data.copy()\n\n# Get the tweet data\ntweets_source = ColumnDataSource(df)\n\n# Create world map \ncountries_source = ColumnDataSource(data= dict(\n    countries_xs=[world_countries[code]['lons'] for code in world_countries],\n    countries_ys=[world_countries[code]['lats'] for code in world_countries],\n    country = [world_countries[code]['name'] for code in world_countries],\n))\n\n# Instantiate the bokeh interactive tools \nTOOLS=\"pan,wheel_zoom,box_zoom,reset,resize,hover,save\"\n```", "```py\n# Instantiante the figure object\np = figure(\n    title=\"%s tweets \" %(str(len(df.index))),\n    title_text_font_size=\"20pt\",\n    plot_width=1000,\n    plot_height=600,\n    tools=TOOLS)\n\n# Create world patches background\np.patches(xs=\"countries_xs\", ys=\"countries_ys\", source = countries_source, fill_color=\"#F1EEF6\", fill_alpha=0.3,\n        line_color=\"#999999\", line_width=0.5)\n\n# Scatter plots by longitude and latitude\np.scatter(x=\"lon\", y=\"lat\", source=tweets_source, fill_color=\"#FF0000\", line_color=\"#FF0000\")\n# \n\n# Activate hover tool with user and corresponding tweet information\nhover = p.select(dict(type=HoverTool))\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = OrderedDict([\n    (\"user\", \"@user\"),\n   (\"tweet\", \"@text\"),\n])\n\n# Render the figure on the browser\nshow(p)\nBokehJS successfully loaded.\n\ninspect\n\n#\n#\n```", "```py\nIn [ ]:\n#\n# Bokeh Google Map Visualization of London with hover on specific points\n#\n#\nfrom __future__ import print_function\n\nfrom bokeh.browserlib import view\nfrom bokeh.document import Document\nfrom bokeh.embed import file_html\nfrom bokeh.models.glyphs import Circle\nfrom bokeh.models import (\n    GMapPlot, Range1d, ColumnDataSource,\n    PanTool, WheelZoomTool, BoxSelectTool,\n    HoverTool, ResetTool,\n    BoxSelectionOverlay, GMapOptions)\nfrom bokeh.resources import INLINE\n\nx_range = Range1d()\ny_range = Range1d()\n```", "```py\n# JSON style string taken from: https://snazzymaps.com/style/1/pale-dawn\nmap_options = GMapOptions(lat=51.50013, lng=-0.126305, map_type=\"roadmap\", zoom=13, styles=\"\"\"\n[{\"featureType\":\"administrative\",\"elementType\":\"all\",\"stylers\":[{\"visibility\":\"on\"},{\"lightness\":33}]},\n {\"featureType\":\"landscape\",\"elementType\":\"all\",\"stylers\":[{\"color\":\"#f2e5d4\"}]},\n {\"featureType\":\"poi.park\",\"elementType\":\"geometry\",\"stylers\":[{\"color\":\"#c5dac6\"}]},\n {\"featureType\":\"poi.park\",\"elementType\":\"labels\",\"stylers\":[{\"visibility\":\"on\"},{\"lightness\":20}]},\n {\"featureType\":\"road\",\"elementType\":\"all\",\"stylers\":[{\"lightness\":20}]},\n {\"featureType\":\"road.highway\",\"elementType\":\"geometry\",\"stylers\":[{\"color\":\"#c5c6c6\"}]},\n {\"featureType\":\"road.arterial\",\"elementType\":\"geometry\",\"stylers\":[{\"color\":\"#e4d7c6\"}]},\n {\"featureType\":\"road.local\",\"elementType\":\"geometry\",\"stylers\":[{\"color\":\"#fbfaf7\"}]},\n {\"featureType\":\"water\",\"elementType\":\"all\",\"stylers\":[{\"visibility\":\"on\"},{\"color\":\"#acbcc9\"}]}]\n\"\"\")\n```", "```py\n# Instantiate Google Map Plot\nplot = GMapPlot(\n    x_range=x_range, y_range=y_range,\n    map_options=map_options,\n    title=\"London Meetups\"\n)\n```", "```py\nsource = ColumnDataSource(\n    data=dict(\n        lat=[51.49013, 51.50013, 51.51013],\n        lon=[-0.130305, -0.126305, -0.120305],\n        fill=['orange', 'blue', 'green'],\n        name=['LondonDataScience', 'Spark', 'MachineLearning'],\n        text=['Graph Data & Algorithms','Spark Internals','Deep Learning on Spark']\n    )\n)\n```", "```py\ncircle = Circle(x=\"lon\", y=\"lat\", size=15, fill_color=\"fill\", line_color=None)\nplot.add_glyph(source, circle)\n```", "```py\n# TOOLS=\"pan,wheel_zoom,box_zoom,reset,hover,save\"\npan = PanTool()\nwheel_zoom = WheelZoomTool()\nbox_select = BoxSelectTool()\nreset = ResetTool()\nhover = HoverTool()\n# save = SaveTool()\n\nplot.add_tools(pan, wheel_zoom, box_select, reset, hover)\noverlay = BoxSelectionOverlay(tool=box_select)\nplot.add_layout(overlay)\n```", "```py\nhover = plot.select(dict(type=HoverTool))\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = OrderedDict([\n    (\"Name\", \"@name\"),\n    (\"Text\", \"@text\"),\n    (\"(Long, Lat)\", \"(@lon, @lat)\"),\n])\n\nshow(plot)\n```"]
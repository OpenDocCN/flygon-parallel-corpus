- en: Designing Microservices
  prefs: []
  type: TYPE_NORMAL
- en: With the increasing popularity of microservices, we would like to dedicate to
    them an entire chapter of this book. When discussing architecture, you will probably
    at some point hear, "Should we use microservices for that?" This chapter will
    show you how to migrate an existing application to a microservices architecture
    and how to build a new application that leverages microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Diving into microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observing microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the examples presented in this chapter do not require any specific software.
    For the `redis-cpp` library, check [https://github.com/tdv/redis-cpp](https://github.com/tdv/redis-cpp).
  prefs: []
  type: TYPE_NORMAL
- en: The code present in the chapter has been placed on GitHub at [https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter13](https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter13).
  prefs: []
  type: TYPE_NORMAL
- en: Diving into microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While microservices are not tied to any particular programming language or technology,
    a common choice when implementing microservices has been the Go language. That
    does not mean that other languages are not suitable for microservices development
    – quite the contrary. The low computational and memory overhead of C++ makes it
    an ideal candidate for microservices.
  prefs: []
  type: TYPE_NORMAL
- en: But first, we will start with a detailed view of some of the pros and cons of
    microservices. After that, we'll focus on design patterns that are often associated
    with microservices (as opposed to the general design patterns covered in [Chapter
    4](ce968eab-d44c-46ae-8cfb-96076a13e348.xhtml), *Architectural and System Design*).
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may often hear about microservices in superlatives. It is true that they
    can bring some benefits and here are some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Modularity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the entire application is split into many relatively small modules, it
    is easier to understand what each microservice does. The natural consequence of
    this understanding is that it is also easier to test individual microservices.
    Testing is also aided by the fact that each microservice typically has a limited
    scope. After all, it's easier to test just the calendar application than to test
    the entire **Personal Information Management** (**PIM**) suite.
  prefs: []
  type: TYPE_NORMAL
- en: This modularity, however, comes at some cost. Your teams may have a much better
    understanding of individual microservices, but at the same time they may find
    it harder to grasp how the entire application is composed. While it shouldn't
    be necessary to learn all the internal details of the microservices that form
    an application, the sheer number of relationships between components presents
    a cognitive challenge. It's good practice to use microservices contracts when
    using this architectural approach.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is easier to scale applications that are limited in scope. One reason for
    that is that there are fewer potential bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling smaller pieces of a workflow is also more cost-effective. Imagine a
    monolithic application responsible for managing a trade fair. Once the system
    starts showing performance issues, the only way to scale is to bring in a bigger
    machine for the monolith to run on. This is called vertical scaling.
  prefs: []
  type: TYPE_NORMAL
- en: With microservices, the first advantage is that you can scale horizontally,
    that is, bring in more machines instead of a bigger machine (which is usually
    cheaper). The second advantage comes from the fact that you only need to scale
    those parts of the application that are having performance issues. This also contributes
    to money saved on infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Flexibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microservices, when properly designed, are less susceptible to vendor lock-in.
    When you decide you want to switch one of the third-party components, you don't
    have to do the entire painful migration all at once. Microservices design takes
    into account that you need to use interfaces, so the only part that requires modification
    is the interface between your microservice and the third-party component.
  prefs: []
  type: TYPE_NORMAL
- en: The components may also migrate one by one, some still using the software from
    the old provider. This means you can separate the risk of introducing breaking
    changes in many places at once. What's more, you can combine this with the canary
    deployments pattern to manage risk with even more granularity.
  prefs: []
  type: TYPE_NORMAL
- en: This flexibility is not related just to single services. It may also mean different
    databases, different queueing and messaging solutions, or even entirely different
    cloud platforms. While different cloud platforms typically offer different services
    and APIs to use them, with a microservices architecture, you can start migrating
    your workload piece by piece and test it independently on a new platform.
  prefs: []
  type: TYPE_NORMAL
- en: When rewrites are necessary due to performance issues, scalability, or available
    dependencies, it is much faster to rewrite a microservice than a monolith.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with legacy systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microservices are not necessarily an all-or-nothing approach. If your application
    is well-tested and migration to microservices may create a lot of risks, there's
    no pressure to dismantle the working solution altogether. It is even better to
    split only the parts that require further development and introduce them as microservices
    that the original monolith will use.
  prefs: []
  type: TYPE_NORMAL
- en: By following this approach, you will gain the benefits of the agile release
    cycle associated with microservices, while at the same time avoiding creating
    a new architecture from scratch and basically rebuilding an entire application.
    If something is already working well, it's better to focus on how to add new features
    without breaking the good parts, rather than starting from scratch. Be careful
    here, as starting from scratch is often used as an ego boost!
  prefs: []
  type: TYPE_NORMAL
- en: Distributed development
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The times of development teams being small and colocated are long gone. Remote
    work and distributed development are a fact even in traditional office-based companies.
    Giants such as IBM, Microsoft, and Intel have people from different locations
    working together on a single project.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices allow for smaller and more agile teams, which makes distributed
    development much easier. When it's no longer necessary to facilitate communication
    between a group of 20 or more people, it's also easier to build self-organized
    teams that require less external management.
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages of microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if you think you may need microservices due to their benefits, keep in
    mind that they also have some serious drawbacks. In short, they are definitely
    not for everyone. Larger companies can generally offset these drawbacks, but smaller
    companies often don't have this luxury.
  prefs: []
  type: TYPE_NORMAL
- en: Reliance on a mature DevOps approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building and testing microservices should be much faster than performing similar
    operations on big, monolithic applications. But in order to achieve agile development,
    this building and testing would need to be performed much more often.
  prefs: []
  type: TYPE_NORMAL
- en: While it may be sensible to deploy the application manually when you are dealing
    with a monolith, the same approach will lead to a lot of problems if applied to
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: In order to embrace the microservices in your development, you have to ensure
    that your team has a DevOps mindset and understands the requirements of both building
    and running the microservice. It's not enough to simply hand the code to someone
    else and forget about it.
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps mindset will help your team to automate as much as possible. Developing
    microservices without a continuous integration/continuous delivery pipeline is
    probably one of the worst possible ideas in software architecture. Such an approach
    will bring all the other disadvantages of microservices without enabling most
    of the benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Harder to debug
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microservices require introducing observability. Without it, when something
    breaks, you're never sure where to start looking for the potential root cause.
    Observability is a way to deduce the state of your application without the need
    to run a debugger or log to the machines your workload is running on.
  prefs: []
  type: TYPE_NORMAL
- en: A combination of log aggregation, application metrics, monitoring, and distributed
    tracing is a prerequisite to manage microservices-based architecture. This is
    especially true once you consider that autoscaling and self-healing may even prevent
    you from accessing individual services if they start crashing.
  prefs: []
  type: TYPE_NORMAL
- en: Additional overhead
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microservices should be lean and agile. And that's usually true. However, microservices-based
    architecture usually requires additional overhead. The first layer of overhead
    is related to the additional interfaces used for microservices communication.
    RPC libraries and API providers and consumers have to be multiplied not only by
    the number of microservices but also by the number of their replicas. Then there
    are auxiliary services, such as databases, message queues, and so on. Those services
    also include observability facilities that usually consist of both storage facilities
    and individual collectors that gather data.
  prefs: []
  type: TYPE_NORMAL
- en: The costs that you optimize with better scaling may be outweighed by the costs
    required to run the entire fleet of services that don't bring immediate business
    value. What's more, it may be hard for you to justify these costs (both in terms
    of infrastructure and development overhead) to the stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Design patterns for microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of general design patterns apply to microservices as well. There are also
    some design patterns that are typically associated with microservices. The patterns
    presented here are useful for both greenfield projects as well as migration from
    monolithic applications.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposition patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These patterns relate to the ways in which microservices are decomposed. We
    want to ensure the architecture is stable and the services are loosely coupled.
    We also want to make sure that services are cohesive and testable. Finally, we
    want autonomous teams to fully own one or more services.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposition by business capability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the decomposition patterns requires decomposition by business capability.
    Business capability relates to what a business does in order to produce value.
    Examples of business capabilities are merchant management and customer management.
    Business capabilities are often organized in a hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge when applying this pattern is to correctly identify the business
    capabilities. This requires an understanding of the business itself and may benefit
    from cooperation with a business analyst.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposition by subdomain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A different decomposition pattern is related to the **Domain-Driven Design**
    (**DDD**) approach. To define services, it is necessary to identify DDD subdomains.
    Just like business capability, identifying subdomains requires knowledge of the
    business context.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between the two approaches is that with decomposing by business
    capability, the focus is more on the organization of the business (its structure),
    whereas with decomposing by subdomain, the focus is on the problems that the business
    tries to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Database per service pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Storing and handling data is a complex issue in every software architecture.
    Wrong choices may impact scalability, performance, or maintenance costs. With
    microservices, there's an added complexity coming from the fact that we want the
    microservices to be loosely coupled.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to a design pattern where each microservice connects to its own database
    so it is independent of any changes introduced by the other services. While this
    pattern adds some overhead, its additional benefit is that you can optimize the
    schema and indexes for each microservice individually.
  prefs: []
  type: TYPE_NORMAL
- en: Since databases tend to be pretty huge pieces of infrastructure, this approach
    may not be feasible, so sharing a database between microservices is an understandable
    trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With microservices running on multiple hosts, you will probably wonder which
    is the better way to allocate resources. Let's compare the two possible approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Single service per host
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using this pattern, we allow each host to only serve a particular type of microservice.
    The main benefit is that you can tweak the machine to better fit the desired workload
    and services are well isolated. When you provide extra-large memory or fast storage,
    you'll be sure that it is used only for the microservice that needs it. The service
    is also unable to consume more resources than provisioned.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of this approach is that some of the hosts may be under-utilized.
    One possible workaround is to use the smallest possible machines that still satisfy
    the microservice requirements and scale them when necessary. This workaround,
    however, does not solve the issue of additional overhead on the host itself.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple services per host
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An opposite approach is hosting multiple services per host. This helps to optimize
    the utilization of the machines but it also comes with some drawbacks. First of
    all, different microservices may require different optimizations, so hosting them
    on a single host will still be impossible. What's more, with this approach, you
    lose control of the host allocation, so the problems in one microservice may cause
    outages in a colocated microservice even if the latter would be otherwise unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is the dependency conflict between the microservices. When the
    microservices are not isolated from one another, the deployment has to take into
    account different possible dependencies. This model is also less secure.
  prefs: []
  type: TYPE_NORMAL
- en: Observability patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we mentioned that microservices come at a price. This
    price includes the requirement to introduce observability or risk losing the ability
    to debug your applications. Here are some patterns related to observability.
  prefs: []
  type: TYPE_NORMAL
- en: Log aggregation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Microservices use logging just like monolithic applications. Instead of storing
    the logs locally, the logs are aggregated and forwarded to a central facility.
    This way, the logs are available even if the service itself is down. Storing logs
    in a centralized manner also helps correlate data coming from different microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Application metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To make decisions based on data, you first need some data to act on. Collecting
    application metrics helps to understand the application behavior as used by the
    actual users, and not in synthetic tests. The approaches to collect those metrics
    are push (where an application actively calls the performance monitoring service)
    and pull (where the performance monitoring service regularly checks the configured
    endpoints).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Distributed tracing helps not only to investigate performance issues but also
    to gain better insight into the application behavior under real-world traffic.
    Unlike logging, which collects pieces of information from a single point, tracing
    is concerned with the entire life cycle of a single transaction, starting at the
    point where it originates from a user action.
  prefs: []
  type: TYPE_NORMAL
- en: Health check APIs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since microservices are often targets of automation, they need to have the ability
    to communicate their internal state. Even if the process is present in the system,
    it doesn't mean the application is operational. The same goes for an open network
    port; the application may be listening, but it is not yet able to respond. Health
    check APIs provide a way for external services to determine whether the application
    is ready to process the workload. Self-healing and autoscaling use health checks
    to determine when an intervention is needed. The base premise is that a given
    endpoint (such as `/health`) returns an HTTP code `200` when the application behaves
    as expected and a different code (or does not return at all) if any problem is
    found.
  prefs: []
  type: TYPE_NORMAL
- en: Now that all the pros, cons, and patterns are known to you, we'll show you how
    you can split the monolithic application and turn it into microservices part by
    part. The presented approaches are not limited to just microservices; they may
    be useful in other cases as well, including monolithic applications.
  prefs: []
  type: TYPE_NORMAL
- en: Building microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a lot of opinions concerning monolithic applications. Some architects
    believe that monoliths are inherently evil because they don't scale well, are
    tightly coupled, and are hard to maintain. There are others who claim that the
    performance benefits coming from monoliths counterbalance their shortcomings.
    It's a fact that tightly coupled components require much less overhead in terms
    of networking, processing power, and memory than their loosely coupled counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: As each application has unique business requirements and operates in a unique
    environment when it comes to stakeholders, there is no universal rule regarding
    which approach is better suited. Even more confusing is the fact that after the
    initial migration from monoliths to microservices, some companies started consolidating
    microservices into macroservices. This was because the burden of maintaining thousands
    of separate software instances proved to be too big to handle.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of one architecture over another should always come from the business
    requirements and careful analysis of different alternatives. Putting ideology
    before pragmatism usually results in a lot of waste within an organization. When
    a team tries to adhere to a given approach at all costs, without considering different
    solutions or diverse external opinions, that team is no longer fulfilling its
    obligations to deliver the right tools for the right job.
  prefs: []
  type: TYPE_NORMAL
- en: If you are developing or maintaining a monolith, you may consider improving
    its scalability. The techniques presented in this section aim to solve this problem
    while also making your application easier to migrate to microservices if you decide
    so.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three primary causes of bottlenecks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will show you how to approach each of them to develop scalable solutions
    based on microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Outsourcing memory management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the ways to help microservices scale is to outsource some of their tasks.
    One such task that may hinder scaling efforts is memory management and caching
    data.
  prefs: []
  type: TYPE_NORMAL
- en: For a single monolithic application, storing cached data directly in the process
    memory is not a problem as the process will be the only one accessing the cache
    anyway. But with several replicas of a process, this approach starts to show some
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: What if one replica has already computed a piece of a workload and stored it
    in a local cache? The other replica is unaware of this fact and has to compute
    it again. This way, your application wastes both computational time (as the same
    task has to be performed multiple times) and memory (as the results are also stored
    with each replica separately).
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate such challenges, consider switching to an external in-memory store
    rather than managing the cache internally within an application. Another benefit
    of using an external solution is that the life cycle of your cache is no longer
    tied to the life cycle of your application. You can restart and deploy new versions
    of your application and the values already stored in the cache are preserved.
  prefs: []
  type: TYPE_NORMAL
- en: This may also result in shorter startup times as your application no longer
    needs to perform the computing during startup. Two popular solutions for in-memory
    cache are Memcached and Redis.
  prefs: []
  type: TYPE_NORMAL
- en: Memcached
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memcached, released in 2003, is the older product of the two. It's a general-purpose,
    distributed key-value store. The original goal of the project was to offload databases
    used in web applications by storing the cached values in memory. Memcached is
    distributed by design. Since version 1.5.18, it is possible to restart the Memcached
    server without losing the contents of the cache. This is possible through the
    use of RAM disk as a temporary storage space.
  prefs: []
  type: TYPE_NORMAL
- en: It uses a simple API that can be operated via telnet or netcat or using bindings
    for many popular programming languages. There aren't any bindings specifically
    for C++, but it's possible to use the C/C++ `libmemcached` library.
  prefs: []
  type: TYPE_NORMAL
- en: Redis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Redis is a newer project than Memcached with the initial version released in
    2009\. Since then, Redis has replaced the usage of Memcached in many cases. Just
    like Memcached, it is a distributed, general-purpose, in-memory key-value store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike Memcached, Redis also features optional data durability. While Memcached
    operates on keys and values being simple strings, Redis also supports other data
    types, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lists of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorted sets of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash tables where keys and values are strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geospatial data (since Redis 3.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HyperLogLogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design of Redis makes it a great choice for caching session data, caching
    web pages, and implementing leaderboards. Apart from that, it may also be used
    for message queueing. The popular distributed task queue library for Python, Celery,
    uses Redis as one of the possible brokers, along with RabbitMQ and Apache SQS.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft, Amazon, Google, and Alibaba all offer Redis-based managed services
    as part of their cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: There are many implementations of a Redis client in C++. Two interesting ones
    are the `redis-cpp` library ([https://github.com/tdv/redis-cpp](https://github.com/tdv/redis-cpp))
    written using C++17 and QRedisClient ([https://github.com/uglide/qredisclient](https://github.com/uglide/qredisclient))
    using the Qt toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example of `redis-cpp` usage taken from the official documentation
    illustrates how to use it to set and get some data in the store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the library handles processing different data types. The example
    sets the value to a list of strings.
  prefs: []
  type: TYPE_NORMAL
- en: Which in-memory cache is better?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For most applications, Redis would be a better choice nowadays. It has a better
    user community, a lot of different implementations, and is well-supported. Other
    than that, it features snapshots, replication, transactions, and the pub/sub model.
    It is possible to embed Lua scripts with Redis and the support for geospatial
    data makes it a great choice for geo-enabled web and mobile applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, if your main goal is to cache the results of database queries in web
    applications, Memcached is a simpler solution with much less overhead. This means
    it should use the resources better as it doesn't have to store type metadata or
    perform conversions between different types.
  prefs: []
  type: TYPE_NORMAL
- en: Outsourcing storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another possible limitation when introducing and scaling microservices is storage.
    Traditionally, local block devices have been used for storing objects that don't
    belong to the database (such as static PDF files, documents, or images). Even
    nowadays, block storage is still very popular with both local block devices and
    network filesystems such as NFS or CIFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'While NFS and CIFS are the domain of **Network-Attached Storage** (**NAS**),
    there are also protocols related to a concept operating on a different level:
    **Storage Area Network** (**SAN**). Some of the popular ones are iSCSI, **Network
    Block Device** (**NBD**), ATA over Ethernet, Fibre Channel Protocol, and Fibre
    Channel over Ethernet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A different approach features clustered filesystems designed for distributed
    computing: GlusterFS, CephFS, or Lustre. All of these, however, operate as block
    devices exposing the same POSIX file API to the user.'
  prefs: []
  type: TYPE_NORMAL
- en: A fresh point of view on storage has been proposed as part of Amazon Web Services.
    Amazon **Simple Storage Service** (**S3**) is object storage. An API provides
    access to objects stored in buckets. This is different from the traditional filesystem
    as there is no distinction between files, directories, or inodes. There are buckets
    and keys that point to objects and objects are binary data stored by the service.
  prefs: []
  type: TYPE_NORMAL
- en: Outsourcing computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the principles of microservices is that a process should only be responsible
    for doing a single piece of the workflow. A natural step while migrating from
    monoliths to microservices would be to define possible long-running tasks and
    split them into individual processes.
  prefs: []
  type: TYPE_NORMAL
- en: This is the concept behind task queues. Task queues handle the entire life cycle
    of managing tasks. Instead of implementing threading or multiprocessing on your
    own, with task queues, you delegate the task to be performed, which is then asynchronously
    handled by the task queue. The task may be performed on the same machine as the
    originating process but it may also run on a machine with dedicated requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The tasks and their results are asynchronous, so there is no blocking in the
    main process. Examples of popular task queues in web development are Celery for
    Python, Sidekiq for Ruby, Kue for Node.js, and Machinery for Go. All of them can
    be used with Redis as a broker. Unfortunately, there aren't any similar mature
    solutions available for C++.
  prefs: []
  type: TYPE_NORMAL
- en: If you are seriously considering taking this route, one possible approach would
    be to implement a task queue directly in Redis. Redis and its API provide the
    necessary primitives to support such a behavior. Another possible approach is
    to use one of the existing task queues, such as Celery, and invoke them by directly
    calling Redis. This, however, is not advised, as it depends on the implementation
    details of the task queue rather than the documented public API. Yet another approach
    is to interface the task queue using bindings provided by SWIG or similar methods.
  prefs: []
  type: TYPE_NORMAL
- en: Observing microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each microservice you build needs to follow the general architectural design
    patterns. The main distinction between microservices and traditional applications
    is the need for implementing observability for the former.
  prefs: []
  type: TYPE_NORMAL
- en: This section focuses on some approaches to observability. We describe here several
    open source solutions that you might find useful when designing your system.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logging is a topic that should be familiar to you even if you've never designed
    microservices. Logs (or log files) store the information about the events happening
    in a system. The system may mean your application, the operating system your application
    runs on, or the cloud platform you use for deployment. Each of these components
    may provide logs.
  prefs: []
  type: TYPE_NORMAL
- en: Logs are stored as separate files because they provide a permanent record of
    all the events taking place. When the system becomes unresponsive, we want to
    query the logs and figure out the possible root cause of the outage.
  prefs: []
  type: TYPE_NORMAL
- en: This means that logs also provide an audit trail. Because the events are recorded
    in chronological order, we are able to understand the state of the system by examining
    the recorded historical state.
  prefs: []
  type: TYPE_NORMAL
- en: To help with debugging, logs are usually human-readable. There are binary formats
    for logs, but such formats are rather rare when using files to store the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with microservices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach to logging itself doesn't differ much from the traditional approach.
    Rather than using text files to store the logs locally, microservices usually
    print logs to `stdout`. A unified logging layer is then used to retrieve the logs
    and process them. To implement logging, you need a logging library that you can
    configure to suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Logging in C++ with spdlog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the popular and fast logging libraries for C++ is `spdlog`. It's built
    using C++11 and can be used either as a header-only library or as a static library
    (which reduces compile time).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some interesting features of `spdlog` include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Formatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple sinks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotating files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syslog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom (implemented as a single function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-threaded and single-threaded versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional asynchronous mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One feature that might be missing from `spdlog` is the direct support for Logstash
    or Fluentd. If you want to use one of these aggregators, it is still possible
    to configure `spdlog` with file sink output and use Filebeat or Fluent Bit to
    forward the file contents to the appropriate aggregator.
  prefs: []
  type: TYPE_NORMAL
- en: Unified logging layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the time, we won't be able to control all of the microservices that
    we use. Some of them will use one logging library, while others would use a different
    one. On top of that, the formats will be entirely different and so will their
    rotation policies. To make things worse, there are still operating system events
    that we want to correlate with application events. This is where the unified logging
    layer comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: One of the unified logging layer’s purposes is to collect logs from different
    sources. Such unified logging layer tools provide many integrations and understand
    different logging formats and transports (such as file, HTTP, and TCP).
  prefs: []
  type: TYPE_NORMAL
- en: The unified logging layer is also capable of filtering the logs. We may want
    filtering to satisfy compliance, anonymize the personal details of our customers,
    or protect the implementation details of our services.
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to query the logs at a later time, the unified logging layer
    can also perform translation between formats. Even if the different services that
    you use store the logs in JSON, CSV, and the Apache format, the unified logging
    layer solution is able to translate them all to JSON to give them structure.
  prefs: []
  type: TYPE_NORMAL
- en: The final task of the unified logging layer is forwarding the logs to their
    next destination. Depending on the complexity of the system, the next destination
    may be a storage facility or another filtering, translation, and forwarding facility.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some interesting components that let you build the unified logging
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logstash is one of the most popular unified logging layer solutions. Currently,
    it is owned by Elastic, the company behind Elasticsearch. If you've heard of the
    ELK stack (now known as the Elastic Stack), Logstash is the "L" in the acronym.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash was written in Ruby and then has been ported to JRuby. This unfortunately
    means that it is rather resource-intensive. For this reason, it is not advisable
    to run Logstash on each machine. Rather, it is meant to be used mainly as a log
    forwarder with lightweight Filebeat deployed to each machine and performing just
    the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Filebeat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Filebeat is part of the Beats family of products. Their aim is to provide a
    lightweight alternative to Logstash that may be used directly with the application.
  prefs: []
  type: TYPE_NORMAL
- en: This way, Beats provide low overhead that scales well, whereas a centralized
    Logstash installation performs all the heavy lifting, including translation, filtering,
    and forwarding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from Filebeat, the other products from the Beats family are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Metricbeat for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packetbeat for network data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditbeat for audit data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heartbeat for uptime monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluentd
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fluentd is the main competitor of Logstash. It is also the tool of choice of
    some cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to its modular approach with the use of plugins, you can find plugins
    for data sources (such as Ruby applications, Docker containers, SNMP, or MQTT
    protocols), data outputs (such as Elastic Stack, SQL Database, Sentry, Datadog,
    or Slack), and several other kinds of filters and middleware.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd should be lighter on resources than Logstash, but it is still not a
    perfect solution for running at scale. The counterpart to Filebeat that works
    with Fluentd is called Fluent Bit.
  prefs: []
  type: TYPE_NORMAL
- en: Fluent Bit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fluent Bit is written in C and provides a faster and lighter solution that plugs
    into Fluentd. As a log processor and forwarder, it also features many integrations
    for inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Besides log collection, Fluent Bit can also monitor CPU and memory metrics on
    Linux systems. It might be used together with Fluentd or it can forward directly
    to Elasticsearch or InfluxDB.
  prefs: []
  type: TYPE_NORMAL
- en: Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While Logstash and Fluentd are stable, mature, and tried solutions, there are
    also newer propositions in the unified logging layer space.
  prefs: []
  type: TYPE_NORMAL
- en: One of them is Vector, which aims to handle all of the observability data in
    a single tool. To differentiate from the competition, it focuses on performance
    and correctness. This is also reflected in the choice of technology. Vector uses
    Rust for the engine and Lua for scripting (as opposed to the custom domain-specific
    languages used by Logstash and Fluentd).
  prefs: []
  type: TYPE_NORMAL
- en: At the moment of writing, it hasn't yet reached a stable 1.0 version, so at
    this point, it shouldn't be considered production-ready.
  prefs: []
  type: TYPE_NORMAL
- en: Log aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Log aggregation solves another problem that arises from too much data: how
    to store and access the logs. While the unified logging layer makes logs available
    even in the event of machine outage, it is the task of log aggregation to help
    us quickly find the information that we are looking for.'
  prefs: []
  type: TYPE_NORMAL
- en: The two possible products that allow storing, indexing, and querying huge amounts
    of data are Elasticsearch and Loki.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Elasticsearch is the most popular solution for self-hosted log aggregation.
    This is the "E" in the (former) ELK Stack. It features a great search engine based
    on Apache Lucene.
  prefs: []
  type: TYPE_NORMAL
- en: As the de facto standard in its niche, Elasticsearch has a lot of integrations
    and has great support both from the community and as a commercial service. Some
    cloud providers offer Elasticsearch as a managed service, which makes it easier
    to introduce Elasticsearch in your application. Other than that, Elastic, the
    company that makes Elasticsearch, offers a hosted solution that is not tied to
    any particular cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Loki
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loki aims to address some of the shortcomings found in Elasticsearch. The focus
    area for Loki is horizontal scalability and high availability. It’s built from
    the ground up as a cloud-native solution.
  prefs: []
  type: TYPE_NORMAL
- en: The design choices for Loki are inspired by both Prometheus and Grafana. This
    shouldn't be a surprise since it is developed by the team responsible for Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: While Loki should be a stable solution, it is not as popular as Elasticsearch,
    which means some integrations might be missing and the documentation and community
    support won't be on the same level as for Elasticsearch. Both Fluentd and Vector
    have plugins that support Loki for log aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: Log visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last piece of the logging stack we want to consider is log visualization.
    This helps us to query and analyze the logs. It presents the data in an accessible
    way so it can be inspected by all the interested parties, such as operators, developers,
    QA, or business.
  prefs: []
  type: TYPE_NORMAL
- en: Log visualization tools allow us to create dashboards that make it even easier
    to read the data we are interested in. With that, we are able to explore the events,
    search for correlations, and find outlying data from a simple user interface.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major products dedicated to log visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kibana is the final element of the ELK Stack. It provides a simpler query language
    on top of Elasticsearch. Even though you can query and visualize different types
    of data with Kibana, it is mostly focused on logs.
  prefs: []
  type: TYPE_NORMAL
- en: Like the rest of the ELK Stack, it is currently the de facto standard when it
    comes to visualizing logs.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Grafana is another data visualization tool. Until recently, it was mostly focused
    on time-series data from performance metrics. However, with the introduction of
    Loki, it may now also be used for logs.
  prefs: []
  type: TYPE_NORMAL
- en: One of its strengths is that it’s built with pluggable backends in mind, so
    it’s easy to switch the storage to fit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring is the process of collecting performance-related metrics from the
    system. When paired with alerting, monitoring helps us understand when our system
    behaves as expected and when an incident happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three types of metrics that would interest us the most are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Availability, which lets us know which of our resources are up and running,
    and which of them have crashed or became unresponsive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource utilization gives us insight into how the workload fits into the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance, which shows us where and how to improve service quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two models of monitoring are push and pull. In the former, each monitored
    object (a machine, an application, and a network device) pushes data to the central
    point periodically. In the latter, the objects present the data at the configured
    endpoints and the monitoring agent scrapes the data regularly.
  prefs: []
  type: TYPE_NORMAL
- en: The pull model makes it easier to scale. This way, multiple objects won't be
    clogging the monitoring agent connection. Instead, multiple agents may collect
    the data whenever ready, thus better utilizing the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: Two monitoring solutions that feature C++ client libraries are Prometheus and
    InfluxDB. Prometheus is an example of a pull-based model and it focuses on collecting
    and storing time-series data. InfluxDB by default uses a push model. Besides monitoring,
    it is also popular for the Internet of Things, sensor networks, and home automation.
  prefs: []
  type: TYPE_NORMAL
- en: Both Prometheus and InfluxDB are typically used with Grafana for visualizing
    data and managing dashboards. Both have alerting built-in, but they can also integrate
    with the external alerting system through Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traces provide information that is generally lower-level to that of event logs.
    Another important distinction is that traces store the ID of every single transaction
    so it is easy to visualize the entire workflow. This ID is commonly known as the
    trace ID, transaction ID, or correlation ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike event logs, traces are not meant to be human-readable. They are processed
    by a tracer. When implementing tracing, it is necessary to use a tracing solution
    that integrates with all the possible elements of the system: frontend applications,
    backend applications, and databases. This way, tracing helps to pinpoint the exact
    cause of lagging performance.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenTracing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the standards in distributed tracing is OpenTracing. This standard was
    proposed by the authors of Jaeger, one of the open-source tracers.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenTracing supports many different tracers apart from Jaeger and it supports
    many different programming languages. The most important ones include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C#
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objective-C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruby
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most important feature of OpenTracing is that it is vendor-neutral. This
    means that once we instrument our application, we won’t need to modify the entire
    codebase to switch to a different tracer. This way, it prevents vendor lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jaeger is a tracer that can be used with various backends, including Elasticsearch,
    Cassandra, and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: It is natively compatible with OpenTracing, which shouldn't be a surprise. Since
    it is a Cloud Native Computing Foundation-graduated project, it has great community
    support, which also translates to good integration with other services and frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: OpenZipkin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenZipkin is the main competitor for Jaeger. It has been on the market for
    a longer time. Although this should mean it is a more mature solution, its popularity
    is fading when compared to Jaeger. Particularly, the C++ in OpenZipkin isn't actively
    maintained, which may cause future problems with maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated observability solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you don't want to build the observability layer on your own, there are some
    popular commercial solutions that you might consider. They all operate in a software-as-a-service
    model. We won't go into a detailed comparison here, as their offerings may change
    drastically after the writing of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'These services are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Honeycomb
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you have seen implementing observability in Microservices.
    Next, we'll move on to learn how to connect microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are so useful because they can be connected in many different
    ways with other services, thus creating new value. However, as there is no standard
    for microservices, there is not a single way to connect to them.
  prefs: []
  type: TYPE_NORMAL
- en: This means that most of the time when we want to use a particular microservice,
    we have to learn how to interact with it. The good news is that although it is
    possible to implement any communication method in microservices, there are a few
    popular approaches that most microservices follow.
  prefs: []
  type: TYPE_NORMAL
- en: How to connect microservices is just one of the relevant questions when designing
    architecture around them. The other is what to connect with and where. This is
    where service discovery comes into play. With service discovery, we let the microservices
    use automated means of discovering and connecting to other services within our
    application.
  prefs: []
  type: TYPE_NORMAL
- en: These three questions, how, what, and where, will be our next topic. We will
    introduce some of the most popular methods of communication and discovery used
    by modern microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Application programming interfaces (APIs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like software libraries, microservices often expose APIs. These APIs make
    it possible to communicate with the microservices. Since the typical manner of
    communication utilizes computer networking, the most popular form of an API is
    the web API.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we already covered some possible approaches with web
    services. Nowadays, microservices typically use web services based on **REpresentational
    State Transfer** (**REST**).
  prefs: []
  type: TYPE_NORMAL
- en: Remote procedure calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While web APIs such as REST allow easy debugging and great interoperability,
    there's a lot of overhead related to data translation and using HTTP for transport.
  prefs: []
  type: TYPE_NORMAL
- en: This overhead may be too much for some microservices, which is the reason for
    lightweight **Remote Procedure Calls** (**RPCs**).
  prefs: []
  type: TYPE_NORMAL
- en: Apache Thrift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Thrift is an interface description language and binary communication
    protocol. It is used as an RPC method that allows creating distributed and scalable
    services built in a variety of languages.
  prefs: []
  type: TYPE_NORMAL
- en: It supports several binary protocols and transport methods. Native data types
    are used for each programming language, so it is easy to introduce even in an
    existing codebase.
  prefs: []
  type: TYPE_NORMAL
- en: gRPC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you really care about performance, often you'll find that text-based solutions
    don't work for you. REST, however elegant and easily understandable, may turn
    out to be too slow for your needs. If that's the case, you should try to build
    your API around binary protocols. One of them, which is growing in popularity,
    is gRPC.
  prefs: []
  type: TYPE_NORMAL
- en: gRPC, as its name suggests, is an RPC system that was initially developed by
    Google. It uses HTTP/2 for transport, and Protocol Buffers as an **Interface Description
    Language** (**IDL**) for interoperability between multiple programming languages,
    and for data serialization. It's possible to use alternative technologies for
    this, for example, FlatBuffers. gRPC can be used both synchronously and in an
    asynchronous manner and allows creating both simple services and streaming ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming you''ve decided to use `protobufs`, our Greeter service definition
    can look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `protoc` compiler, you can create data access code from this definition.
    Assuming you want to have a synchronous server for our Greeter, you can create
    the service in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you have to build and run the server for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Simple as that. Let''s now take a look at a client to consume this service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This was a simple, synchronous example. To make it work asynchronously, you'll
    need to add tags and `CompletionQueue`, as described on gRPC's website.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting feature of gRPC is that it is available for mobile applications
    on Android and iOS. This means that if you use gRPC internally, you don't have
    to provide an additional server to translate the traffic from your mobile applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned the most popular methods of communication and discovery
    utilized by microservices. Next, we'll see how microservices can be scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the significant benefits of microservices is that they scale more efficiently
    than monoliths. Given the same hardware infrastructure, you could theoretically
    be able to get more performance out of microservices than monoliths.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the benefits are not that straightforward. Microservices and related
    helpers also provide overhead that for smaller-scale applications may be less
    performant than an optimal monolith.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that even if something looks good "on paper," it doesn't mean it will
    fly. If you want to base your architectural decisions on scalability or performance,
    it is better to prepare calculations and experiments. This way, you'll act based
    on data, not just emotion.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a single service per host deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a single service per host deployment, scaling a microservice requires adding
    or removing additional machines that host the microservice. If your application
    is running on a cloud architecture (public or private), many providers offer a
    concept known as autoscaling groups.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling groups define a base virtual machine image that will run on all
    grouped instances. Whenever a critical threshold is reached (for example, 80%
    CPU use), a new instance is created and added to the group. Since autoscaling
    groups run behind a load balancer, the increasing traffic then gets split between
    both the existing and the new instances, thus reducing the mean load on each one.
    When the spike in traffic subsides, the scaling controller shuts down the excess
    machines to keep the costs low.
  prefs: []
  type: TYPE_NORMAL
- en: Different metrics can act as triggers for the scaling event. The CPU load is
    one of the easiest to use, but it may not be the most accurate one. Other metrics,
    such as the number of messages in a queue, may better fit your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an excerpt from a Terraform configuration for a scaler policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It means that at any given time, there will be at least three instances running
    and at most five instances. The scaler will trigger once the CPU load hits at
    least an 80% average for all the group instances. When that happens, a new instance
    is spun up. The metrics from the new machine will only be collected after it has
    been running for at least 60 seconds (the cooldown period).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling multiple services per host deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This mode of scaling is also suitable for multiple services per host deployment.
    As you can probably imagine, this isn't the most efficient method. Scaling an
    entire set of services based only on a reduced throughput of a single one is similar
    to scaling monoliths.
  prefs: []
  type: TYPE_NORMAL
- en: If you're using this pattern, a better way to scale your microservices is to
    use an orchestrator. If you don't want to use containers, Nomad is a great choice
    that works with a lot of different execution drivers. For containerized workloads,
    either Docker Swarm or Kubernetes will help you. Orchestrators are a topic that
    we'll come back to in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are a great new trend in software architecture. They could be
    a good fit provided you make sure you know about the hazards and prepare for them.
    This chapter explained the common design and migration patterns that help to introduce
    microservices. We've also covered advanced topics such as observability and connectivity
    that are crucial when establishing microservices-based architectures.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should be able to design and decompose applications into individual
    microservices. Each microservice is then capable of processing a single piece
    of workload.
  prefs: []
  type: TYPE_NORMAL
- en: While microservices are valid on their own, they're especially popular in combination
    with containers. Containers are the subject of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do microservices help you better use the system resources?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can microservices and monoliths coexist (in an evolving system)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which types of teams benefit the most from microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it necessary to have a mature DevOps approach when introducing microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a unified logging layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do logging and tracing differ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why may REST not be the best choice for connecting microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the deployment strategies for microservices? What are the benefits
    of each of them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Mastering Distributed Tracing*: [https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464](https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Microservices with Kubernetes*: [https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468](https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Microservices* by Martin Fowler: [https://martinfowler.com/articles/microservices.html](https://martinfowler.com/articles/microservices.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microservice architecture: [https://microservices.io/](https://microservices.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

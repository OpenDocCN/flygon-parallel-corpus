- en: '*Chapter 9*: Observability on Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dives into capabilities that are highly recommended to implement
    when running Kubernetes in production. First, we discuss observability in the
    context of distributed systems such as Kubernetes. Then, we look at the built-in
    Kubernetes observability stack and what functionality it implements. Finally,
    we learn how to supplement the built-in observability tooling with additional
    observability, monitoring, logging, and metrics infrastructure from the ecosystem.
    The skills you learn in this chapter will help you deploy observability tools
    to your Kubernetes cluster and enable you to understand how your cluster (and
    applications running on it) are functioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding observability on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using default observability tooling – metrics, logging, and the dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best of the ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To start, we will learn the out-of-the-box tools and processes that Kubernetes
    provides for observability.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the kubectl tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used in this chapter can be found in the book''s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding observability on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No production system is complete without a way to monitor it. In software, we
    define observability as the ability to, at any point in time, understand how our
    system is performing (and, in the best case, why). Observability grants significant
    benefits in security, performance, and operational capacity. By knowing how your
    system is responding at the VM, container, and application level, you can tune
    it to perform efficiently, react quickly to events, and more easily troubleshoot
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's take a scenario where your application is running extremely
    slowly. In order to find the bottleneck, you may look at the application code
    itself, the resource specifications of the Pod, the number of Pods in the deployment,
    the memory and CPU usage at the Pod level or Node level, and externalities such
    as a MySQL database running outside your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: By adding observability tooling, you would be able to diagnose many of these
    variables and figure out what issues may be contributing to your application slowdown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes, as a production-ready container orchestration system, gives us
    some default tools to monitor our applications. For the purposes of this chapter,
    we will separate observability into four ideas: metrics, logs, traces, and alerts.
    Let''s look at each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics** here represents the ability to see numerical representations of
    the system''s current state, with specific attention paid to CPU, memory, network,
    disk space, and more. These numbers allow us to judge the gap in current state
    with the system''s maximum capacity and ensure that the system remains available
    to users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logs** refers to the practice of collecting text logs from applications and
    systems. Logs will likely be a combination of Kubernetes control plane logs and
    logs from your application Pods themselves. Logs can help us diagnose the availability
    of the Kubernetes system, but they also can help with triaging application bugs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traces** refers to collecting distributed traces. Traces are an observability
    pattern that delivers end-to-end visibility of a chain of requests – which can
    be HTTP requests or otherwise. This topic is especially important in a distributed
    cloud-native setting where microservices are used. If you have many microservices
    and they call each other, it can be difficult to find bottlenecks or issues when
    many services are involved in a single end-to-end request. Traces allow you to
    view requests broken down by each leg of a service-to-service call.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerts** correspond to the practice of setting automated touch points when
    certain events happen. Alerts can be set on both *metrics* and *logs*, and delivered
    through a host of mediums, from text messages to emails to third-party applications
    and everything in between.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Between these four aspects of observability, we should be able to understand
    the health of our cluster. However, it is possible to configure many different
    possible data points for metrics, logs, and even alerting. Therefore, knowing
    what to look for is important. The next section will discuss the most important
    observability areas for Kubernetes cluster and application health.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what matters for Kubernetes cluster and application health
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the vast number of possible metrics and logs that Kubernetes or third-party
    observability solutions for Kubernetes can provide, we can narrow down some of
    the ones that are most likely to cause major issues with your cluster. You should
    keep these pieces front and center in whichever observability solution you end
    up using. First, let's look at the connection between CPU usage and cluster health.
  prefs: []
  type: TYPE_NORMAL
- en: Node CPU usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The state of CPU usage across the Nodes in your Kubernetes cluster is a very
    important metric to keep an eye on across your observability solution. We've discussed
    in previous chapters how Pods can define resource requests and limits for CPU
    usage. However, it is still possible for Nodes to oversubscribe their CPU usage
    when the limits are set higher than the maximum CPU capacity of the cluster. Additionally,
    the master Nodes that run our control plane can also encounter CPU capacity issues.
  prefs: []
  type: TYPE_NORMAL
- en: Worker Nodes with maxed-out CPUs may perform poorly or throttle workloads running
    on Pods. This can easily occur if no limits are set on Pods – or if a Node's total
    Pod resource limits are greater than its max capacity, even if its total resource
    requests are lower. Master Nodes with capped-out CPUs may hurt the performance
    of the scheduler, kube-apiserver, or any of the other control plane components.
  prefs: []
  type: TYPE_NORMAL
- en: In general, CPU usage across worker and master Nodes should be visible in your
    observability solution. This is best done via a combination of metrics (for instance
    on a charting solution such as Grafana, which you'll learn about later in this
    chapter) – and alerts for high CPU usage across the nodes in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage is also an extremely important metric to keep track of, similar
    to with CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Node memory usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with CPU usage, memory usage is an extremely important metric to observe
    across your cluster. Memory usage can be oversubscribed using Pod Resource Limits
    – and many of the same issues as with CPU usage can apply for both the master
    and worker Nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Again, a combination of alerting and metrics is important for visibility into
    cluster memory usage. We will learn some tools for this later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For the next major observability piece, we will look not at metrics but at logs.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The components of the Kubernetes control plane, when running, output logs that
    can be used to get an in-depth view of cluster operations. These logs can also
    significantly help with troubleshooting, as we'll see in [*Chapter 10*](B14790_10_Final_PG_ePub.xhtml#_idTextAnchor230),
    *Troubleshooting Kubernetes*. Logs for the Kubernetes API server, controller manager,
    scheduler, kube proxy, and kubelet can all be very useful for certain troubleshooting
    or observability reasons.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Application logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Application logging can also be incorporated into an observability stack for
    Kubernetes – being able to view application logs along with other metrics can
    be very helpful to operators.
  prefs: []
  type: TYPE_NORMAL
- en: Application performance metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with application logging, application performance metrics and monitoring
    are highly relevant to the performance of your applications on Kubernetes. Memory
    usage and CPU profiling at the application level can be a valuable piece of the
    observability stack.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, Kubernetes provides the data infrastructure for application monitoring
    and logging but stays away from providing higher-level functionality such as charting
    and searching. With this in mind, let's review the tools that Kubernetes gives
    us by default to address these concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Using default observability tooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides observability tooling even without adding any third-party
    solutions. These native Kubernetes tools form the basis of many of the more robust
    solutions, so they are important to discuss. Since observability includes metrics,
    logs, traces, and alerts, we will discuss each in turn, focusing first on the
    Kubernetes-native solutions. First, let's discuss metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of information about your applications can be gained by simply running
    `kubectl describe pod`. We can see information about our Pod's spec, what state
    it is in, and key issues preventing its functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume we are having some trouble with our application. Specifically,
    the Pod is not starting. To investigate, we run `kubectl describe pod`. As a reminder
    on kubectl aliases mentioned in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, `kubectl describe pod` is the same as `kubectl
    describe pods`. Here is an example output from the `describe pod` command – we''ve
    stripped out everything apart from the `Events` information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Describe Pod Events output](image/B14790_09_001_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Describe Pod Events output
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this Pod is not being scheduled because our Nodes are all out
    of memory! That would be a good thing to investigate further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s keep going. By running `kubectl describe nodes`, we can learn a lot
    about our Kubernetes Nodes. Some of this information can be very relevant to how
    our system is performing. Here''s another example output, this time from the `kubectl
    describe nodes` command. Rather than putting the entire output here, which can
    be quite lengthy, let''s zero in on two important sections – `Conditions` and
    `Allocated resources`. First, let''s review the `Conditions` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Describe Node Conditions output](image/B14790_09_002_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Describe Node Conditions output
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have included the `Conditions` block of the `kubectl describe
    nodes` command output. It's a great place to look for any issues. As we can see
    here, our Node is actually experiencing issues. Our `MemoryPressure` condition
    is true, and the `Kubelet` has insufficient memory. No wonder our Pods won't schedule!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, check out the `Allocated resources` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we're seeing some metrics! It looks like our Pods are requesting too much
    memory, leading to our Node and Pod issues. As you can tell from this output,
    Kubernetes is already collecting metrics data about our Nodes, by default. Without
    that data, the scheduler would not be able to do its job properly, since maintaining
    Pod resources requests with Node capacity is one of its most important functions.
  prefs: []
  type: TYPE_NORMAL
- en: However, by default, these metrics are not surfaced to the user. They are in
    fact being collected by each Node's `Kubelet` and delivered to the scheduler for
    it to do its job. Thankfully, we can easily get these metrics by deploying Metrics
    Server to our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Server is an officially supported Kubernetes application that collects
    metrics information and surfaces it on an API endpoint for use. Metrics Server
    is in fact required to make the Horizontal Pod Autoscaler work, but it is not
    always included by default, depending on the Kubernetes distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying Metrics Server is very quick. As of the writing of this book, the
    newest version can be installed using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Full documentation on how to use Metrics Server can be found at [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server).
  prefs: []
  type: TYPE_NORMAL
- en: Once Metrics Server is running, we can use a brand-new Kubernetes command. The
    `kubectl top` command can be used with either Pods or Nodes to see granular information
    about how much memory and CPU capacity is in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some example usage. Run `kubectl top nodes` to see Node-level
    metrics. Here''s the output of the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Node Metrics output](image/B14790_09_003_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Node Metrics output
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we are able to see both absolute and relative CPU and memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: CPU cores are measured in `millcpu` or `millicores`. 1,000 `millicores` is equivalent
    to one virtual CPU. Memory is measured in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at the `kubectl top pods` command. Run it with the `–namespace
    kube-system` flag to see Pods in the `kube-system` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this command uses the same absolute units as `kubectl top nodes`
    – millicores and bytes. There are no relative percentages when looking at Pod-level
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll look at how Kubernetes handles logging.
  prefs: []
  type: TYPE_NORMAL
- en: Logging on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can split up logging on Kubernetes into two areas – *application logs* and
    *control plane logs*. Let's start with control plane logs.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Control plane logs refers to the logs created by the Kubernetes control plane
    components, such as the scheduler, API server, and others. For a vanilla Kubernetes
    install, control plane logs can be found on the Nodes themselves and require direct
    access to the Nodes in order to see. For clusters with components set up to use
    `systemd`, logs are found using the `journalctl` CLI tool (refer to the following
    link for more information: [https://manpages.debian.org/stretch/systemd/journalctl.1.en.html](https://manpages.debian.org/stretch/systemd/journalctl.1.en.html)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On master Nodes, you can find logs in the following locations on the filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: At `/var/log/kube-scheduler.log`, you can find the Kubernetes scheduler logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At `/var/log/kube-controller-manager.log`, you can find the controller manager
    logs (for instance, to see scaling events).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At `/var/log/kube-apiserver.log`, you can find the Kubernetes API server logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On worker Nodes, logs are available in two locations on the filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: At `/var/log/kubelet.log`, you can find the kubelet logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At `/var/log/kube-proxy.log`, you can find the kube proxy logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although, generally, cluster health is influenced by the health of the Kubernetes
    master and worker Node components, it is of course also important to keep track
    of your application's logs.
  prefs: []
  type: TYPE_NORMAL
- en: Application logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's very easy to find application logs on Kubernetes. Before we explain how
    it works, let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check logs for a specific Pod, you can use the `kubectl logs <pod_name>`
    command. The output of the command will display any text written to the container''s
    `stdout` or `stderr`. If a Pod has multiple containers, you must include the container
    name in the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Under the hood, Kubernetes handles Pod logs by using the container engine's
    logging driver. Typically, any logs to `stdout` or `stderr` are persisted to each
    Node's disk in the `/var/logs` folder. Depending on the Kubernetes distribution,
    log rotations may be set up to prevent overuse of Node disk space by logs. In
    addition, Kubernetes components such as the scheduler, kubelet, and kube-apiserver
    also persist logs to Node disk space, usually within the `/var/logs` folder. It
    is important to note how limited this default logging capability is – a robust
    observability stack for Kubernetes would certainly include a third-party solution
    for log forwarding, as we'll see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, for general Kubernetes observability, we can use Kubernetes Dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kubernetes Dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes Dashboard provides all of the functionality of kubectl – including
    viewing logs and editing resources – in a GUI. It's very easy to get the dashboard
    set up – let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard can be installed in a single `kubectl apply` command. For customizations,
    check out the Kubernetes Dashboard GitHub page at [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install a version of Kubernetes Dashboard, run the following `kubectl` command,
    replacing the `<VERSION>` tag with your desired version, based on the version
    of Kubernetes you are using (again, check the Dashboard GitHub page for version
    compatibility):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, as of the writing of this book, we will use v2.0.4 – the final
    command looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once Kubernetes Dashboard has been installed, there are a few methods to access
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is not usually recommended to use Ingress or a public load balancer service,
    because Kubernetes Dashboard allows users to update cluster objects. If for some
    reason your login methods for the dashboard are compromised or easy to figure
    out, you could be looking at a large security risk.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, we can use either `kubectl port-forward` or `kubectl proxy`
    in order to view our dashboard from our local machine.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will use the `kubectl proxy` command, because we haven't
    used it in an example yet.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubectl proxy` command, unlike the `kubectl port-forward` command, requires
    only one command to proxy to every service running on your cluster. It does this
    by proxying the Kubernetes API directly to a port on your local machine, which
    is by default `8081`. For a full discussion of the `Kubectl proxy` command, check
    the docs at [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to access a specific Kubernetes service using `kubectl proxy`, you
    just need to have the right path. The path to access Kubernetes Dashboard after
    running `kubectl proxy` will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `kubectl proxy` path we put in our browser is on localhost
    port `8001`, and mentions the namespace (`kubernetes-dashboard`), the service
    name and selector (`https:kubernetes-dashboard`), and a proxy path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put our Kubernetes Dashboard URL in a browser and see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Kubernetes Dashboard login](image/B14790_09_004_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Kubernetes Dashboard login
  prefs: []
  type: TYPE_NORMAL
- en: When we deploy and access Kubernetes Dashboard, we are met with a login screen.
    We can either create a Service Account (or use our own) to log in to the dashboard,
    or simply link our local `Kubeconfig` file. By logging in to Kubernetes Dashboard
    with a specific Service Account's token, the dashboard user will inherit that
    Service Account's permissions. This allows you to specify what type of actions
    a user will be able to take using Kubernetes Dashboard – for instance, read-only
    permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and create a brand-new Service Account for our Kubernetes Dashboard.
    You could customize this Service Account and limit its permissions, but for now
    we will give it admin permissions. To do this, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a Service Account imperatively using the following Kubectl command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output, confirming the creation of our Service
    Account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to link our Service Account to a ClusterRole. You could also use
    a Role, but we want our dashboard user to be able to access all namespaces. To
    link a Service Account to the `cluster-admin` default ClusterRole using a single
    command, we can run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After this command is run, we should be able to log in to our dashboard! First,
    we just need to find the token that we will use to log in. A Service Account''s
    token is stored as a Kubernetes secret, so let''s see what it looks like. Run
    the following command to see which secret our token is stored in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the output, you should see a secret that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to get our token for signing in to the dashboard, we only need to describe
    the secret contents using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To log in to the dashboard, copy the string next to `token`, copy it into the
    token input on the Kubernetes Dashboard login screen, and click **Sign In**. You
    should be greeted with the Kubernetes Dashboard overview page!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go ahead and click around the dashboard – you should be able to see all the
    same resources you would be able to using kubectl, and you can filter by namespace
    in the left-hand sidebar. For instance, here's a view of the **Namespaces** page:![Figure
    9.5 – Kubernetes Dashboard detail](image/B14790_09_005_new.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.5 – Kubernetes Dashboard detail
  prefs: []
  type: TYPE_NORMAL
- en: You can also click on individual resources, and even edit those resources using
    the dashboard as long as the Service Account you used to log in has the proper
    permissions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s a view of editing a Deployment resource from the deployment detail
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Kubernetes Dashboard edit view](image/B14790_09_006_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Kubernetes Dashboard edit view
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Dashboard also lets you view Pod logs and dive into many other resource
    types in your cluster. To understand the full capabilities of the dashboard, check
    the docs at the previously mentioned GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to round out our discussion of default observability on Kubernetes,
    let's take a look at alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts and traces on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, the last two pieces of the observability puzzle – *alerts* and
    *traces* – are not yet native pieces of functionality on Kubernetes. In order
    to create this type of functionality, let's move on to our next section – incorporating
    open source tooling from the Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing Kubernetes observability using the best of the ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've discussed, though Kubernetes provides the basis for powerful visibility
    functionality, it is generally up to the community and vendor ecosystem to create
    higher-level tooling for metrics, logging, traces, and alerting. For the purposes
    of this book, we will focus on fully open source, self-hosted solutions. Since
    many of these solutions fulfill multiple visibility pillars between metrics, logs,
    traces, and alerting, instead of categorizing solutions into each visibility pillar
    during our review, we will review each solution separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with an often-used combination of technologies for metrics and
    alerts: **Prometheus** and **Grafana**.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Prometheus and Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prometheus and Grafana are a typical combination of visibility technologies
    on Kubernetes. Prometheus is a time series database, query layer, and alerting
    system with many integrations, while Grafana is a sophisticated graphing and visualization
    layer that integrates with Prometheus. We'll walk you through the installation
    and usage of these tools, starting with Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Prometheus and Grafana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many ways to install Prometheus on Kubernetes, but most use Deployments
    in order to scale the service. For our purposes, we will be using the `kube-prometheus`
    project ([https://github.com/coreos/kube-prometheus](https://github.com/coreos/kube-prometheus)).
    This project includes an `operator` as well as several **custom resource definitions**
    (**CRDs**). It will also automatically install Grafana for us!
  prefs: []
  type: TYPE_NORMAL
- en: An operator is essentially an application controller on Kubernetes (deployed
    like other applications in a Pod) that happens to make commands to the Kubernetes
    API in order to correctly run or operate its application.
  prefs: []
  type: TYPE_NORMAL
- en: A CRD, on the other hand, allows us to model custom functionality inside of
    the Kubernetes API. We'll learn a lot more about operators and CRDs in [*Chapter
    13*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289), *Extending Kubernetes with
    CRDs*, but for now just think of operators as a way to create *smart deployments*
    where the application can control itself properly and spin up other Pods and Deployments
    as necessary – and think of CRDs as a way to use Kubernetes to store application-specific
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Prometheus, first we need to download a release, which may be different
    depending on the newest version of Prometheus or your intended version of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, unzip the file using any tool. First, we're going to need to install the
    CRDs. In general, most Kubernetes tooling installation instructions will have
    you create the CRDs on Kubernetes first, since any additional setup that uses
    the CRD will fail if the underlying CRD has not already been created on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install them using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll need to wait a few seconds while the CRDs are created. This command
    will also create a `monitoring` namespace for our resources to live in. Once everything
    is ready, let''s spin up the rest of the Prometheus and Grafana resources using
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s talk about what this command will actually create. The entire stack
    consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus Deployment**: Pods of the Prometheus application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Operator**: Controls and operates the Prometheus app Pods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alertmanager Deployment**: A Prometheus component to specify and trigger
    alerts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grafana**: A powerful visualization dashboard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kube-state-metrics agent**: Generates metrics from the Kubernetes API state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Node Exporter**: Exports Node hardware- and OS-level metrics to
    Prometheus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Adapter for Kubernetes Metrics**: Adapter for Kubernetes Resource
    Metrics API and Custom Metrics API for ingest into Prometheus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, all these components will provide sophisticated visibility into our
    cluster, from the command plane down to the application containers themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Once the stack has been created (check by using the `kubectl get po -n monitoring`
    command), we can start using our components. Let's dive into usage, starting with
    plain Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Using Prometheus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the real power of Prometheus is in its data store, query, and alert layer,
    it does provide a simple UI to developers. As you'll see later, Grafana provides
    many more features and customizations, but it is worth it to get acquainted with
    the Prometheus UI.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `kube-prometheus` will only create ClusterIP services for Prometheus,
    Grafana, and Alertmanager. It's up to us to expose them outside the cluster. For
    the purposes of this tutorial, we're simply going to port forward the service
    to our local machine. For production, you may want to use Ingress to route requests
    to the three services.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to `port-forward` to the Prometheus UI service, use the `port-forward`
    kubectl command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We need to use port `9090` for the Prometheus UI. Access the service on your
    machine at `http://localhost:3000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Prometheus UI](image/B14790_09_007_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Prometheus UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Prometheus UI has a **Graph** page, which is what you can
    see in *Figure 9.4*. It also has its own UI for seeing configured alerts – but
    it doesn't allow you to create alerts via the UI. Grafana and Alertmanager will
    help us for that task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform a query, navigate to the **Graph** page and enter the query command
    into the **Expression** bar, then click **Execute**. Prometheus uses a query language
    called `PromQL` – we won''t present it fully to you in this book, but the Prometheus
    docs are a great way to learn. You can refer to it using the following link: [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To show how this works, let''s enter a basic query, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This query will list the total number of HTTP requests made to the kubelet
    on each Node, for each request category, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – HTTP requests query](image/B14790_09_008_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – HTTP requests query
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also see the requests in graph form by clicking the **Graph** tab next
    to **Table** as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – HTTP requests query – graph view](image/B14790_09_009_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – HTTP requests query – graph view
  prefs: []
  type: TYPE_NORMAL
- en: This provides a time series graph view of the data from the preceding screenshot.
    As you can see, the graphing capability is fairly simple.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus also provides an **Alerts** tab for configuring Prometheus alerts.
    Typically, these alerts are configured via code instead of using the **Alerts**
    tab UI, so we will skip that page in our review. For more information, you can
    check the official Prometheus documentation at [https://prometheus.io/docs/alerting/latest/overview/](https://prometheus.io/docs/alerting/latest/overview/).
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to Grafana, where we can extend Prometheus powerful data tooling
    with visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Using Grafana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grafana provides powerful tools for visualizing metrics, with many supported
    charting types that can update in real time. We can connect Grafana to Prometheus
    in order to see our cluster metrics charted on the Grafana UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with Grafana, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will end our current port forwarding (*CTRL* + *C* will do the trick) and
    set up a new port forward listener to the Grafana UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, navigate to `localhost:3000` to see the Grafana UI. You should be able
    to log in with **Username**: `admin` and **Password**: `admin`, at which point
    you should be able to change the initial password as shown in the following screenshot:![Figure
    9.10 – Grafana Change Password screen](image/B14790_09_010_new.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.10 – Grafana Change Password screen
  prefs: []
  type: TYPE_NORMAL
- en: Upon login, you will see the following screen. Grafana does not come preconfigured
    with any dashboards, but we can add them easily by clicking the **+** sign as
    shown in the following screenshot:![Figure 9.11 – Grafana main page](image/B14790_09_011_new.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.11 – Grafana main page
  prefs: []
  type: TYPE_NORMAL
- en: Each Grafana dashboard includes one or more graphs for different sets of metrics.
    To add a preconfigured dashboard (instead of creating one yourself), click the
    plus sign (**+**) on the left-hand menu bar and click **Import**. You should see
    a page like the following screenshot:![Figure 9.12 – Grafana Dashboard Import](image/B14790_09_012_new.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.12 – Grafana Dashboard Import
  prefs: []
  type: TYPE_NORMAL
- en: We can add a dashboard via this page either using the JSON configuration or
    by pasting in a public dashboard ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find public dashboards and their associated IDs at [https://grafana.com/grafana/dashboards/315](https://grafana.com/grafana/dashboards/315).
    Dashboard #315 is a great starter dashboard for Kubernetes – let''s add it to
    the textbox labeled **Grafana.com Dashboard** and click **Load**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, on the next page, select the **Prometheus** data source from the **Prometheus**
    option dropdown, which is used to pick between multiple data sources if available.
    Click **Import**, and the dashboard should be loaded, which will look like the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Grafana dashboard](image/B14790_09_013_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Grafana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: This particular Grafana dashboard provides a good high-level overview of network,
    memory, CPU, and filesystem utilization across the cluster, and it is broken down
    per Pod and container. It is configured with real-time graphs for **Network I/O
    pressure**, **Cluster memory usage**, **Cluster CPU usage**, and **Cluster filesystem
    usage** – though this last option may not be enabled depending on how you have
    installed Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's look at the Alertmanager UI.
  prefs: []
  type: TYPE_NORMAL
- en: Using Alertmanager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alertmanager is an open source solution for managing alerts generated from
    Prometheus alerts. We installed Alertmanager previously as part of our stack –
    let''s take a look at what it can do:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s `port-forward` the Alertmanager service using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, navigate to `localhost:3000` to see the UI as shown in the following
    screenshot. It looks similar to the Prometheus UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Alertmanager UI](image/B14790_09_014_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Alertmanager UI
  prefs: []
  type: TYPE_NORMAL
- en: Alertmanager works together with Prometheus alerts. You can use the Prometheus
    server to specify alert rules, and then use Alertmanager to group similar alerts
    into single notifications, perform deduplications, and create *silences*, which
    are essentially a way to mute alerts if they match specific rules.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will review a popular logging stack for Kubernetes – Elasticsearch,
    FluentD, and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the EFK stack on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the popular ELK stack (Elasticsearch, Logstash, and Kibana), the
    EFK stack swaps out Logstash for the FluentD log forwarder, which is well supported
    on Kubernetes. Implementing this stack is easy and allows us to get started with
    log aggregation and search functionalities using purely open source tooling on
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the EFK stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many ways to install the EFK Stack on Kubernetes, but the Kubernetes
    GitHub repository itself has some supported YAML, so let''s just use that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, clone or download the Kubernetes repository using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The manifests are located in the `kubernetes/cluster/addons` folder, specifically
    under `fluentd-elasticsearch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: For a production workload, we would likely make some changes to these manifests
    in order to properly customize the configuration for our cluster, but for the
    purposes of this tutorial we will leave everything as default. Let's start the
    process of bootstrapping our EFK stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create the Elasticsearch cluster itself. This runs as a StatefulSet
    on Kubernetes, and also provides a Service. To create the cluster, we need to
    run two `kubectl` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A word of warning for the Elasticsearch StatefulSet – by default, the resource
    request for each Pod is 3 GB of memory, so if none of your Nodes have that available,
    you will not be able to deploy it as configured by default.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's deploy the FluentD logging agents. These will run as a DaemonSet
    – one per Node – and forward logs from the Nodes to Elasticsearch. We also need
    to create the ConfigMap YAML, which contains the base FluentD agent configuration.
    This can be further customized to add things such as log filters and new sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To install the DaemonSet for the agents and their configuration, run the following
    two `kubectl` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve created the ConfigMap and the FluentD DaemonSet, we can create
    our Kibana application, which is a GUI for interacting with Elasticsearch. This
    piece runs as a Deployment, with a Service. To deploy Kibana to our cluster, run
    the final two `kubectl` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once everything has been initiated, which may take several minutes, we can
    access the Kibana UI in the same way that we did Prometheus and Grafana. To check
    the status of the resources we just created, we can run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Once all Pods for FluentD, Elasticsearch, and Kibana are in the **Ready** state,
    we can move on. If any of your Pods are in the **Error** or **CrashLoopBackoff**
    stage, consult the Kubernetes GitHub documentation in the `addons` folder for
    more information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we''ve confirmed that our components are working properly, let''s use
    the `port-forward` command to access the Kibana UI. By the way, our EFK stack
    pieces will live in the `kube-system` namespace – so our command needs to reflect
    that. So, let''s use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This command will start a `port-forward` to your local machine's port `8080`
    from the Kibana UI.
  prefs: []
  type: TYPE_NORMAL
- en: Let's check out the Kibana UI at `localhost:8080`. It should look something
    like the following, depending on your exact version and configuration:![Figure
    9.15 – Basic Kibana UI](image/B14790_09_015_new.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.15 – Basic Kibana UI
  prefs: []
  type: TYPE_NORMAL
- en: Kibana provides several different features for searching and visualizing logs,
    metrics, and more. The most important section of the dashboard for our purposes
    is **Logging**, since we are using Kibana solely as a log search UI in our example.
  prefs: []
  type: TYPE_NORMAL
- en: However, Kibana has many other functions, some of which are comparable to Grafana.
    For instance, it includes a full visualization engine, **application performance
    monitoring** (**APM**) capabilities, and Timelion, an expression engine for time
    series data very similar to what is found in Prometheus's PromQL. Kibana's metrics
    functionality is similar to Prometheus and Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: In order to get Kibana working, we will first need to specify an index pattern.
    To do this, click on the **Visualize** button, then click **Add an Index Pattern**.
    Select an option from the list of patterns and choose the index with the current
    date on it, then create the index pattern.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we''re set up, the **Discover** page will provide you with search
    functionality. This uses the Apache Lucene query syntax ([https://www.elastic.co/guide/en/elasticsearch/reference/6.7/query-dsl-query-string-query.html#query-string-syntax](https://www.elastic.co/guide/en/elasticsearch/reference/6.7/query-dsl-query-string-query.html#query-string-syntax))
    and can handle everything from simple string matching expressions to extremely
    complex queries. In the following screenshot, we are doing a simple string match
    for the letter `h`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Discover UI](image/B14790_09_016_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – Discover UI
  prefs: []
  type: TYPE_NORMAL
- en: When Kibana cannot find any results, it gives you a handy set of possible solutions
    including query examples, as you can see in *Figure 9.13*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know how to create search queries, you can create visualizations
    from queries on the **Visualize** page. These can be chosen from a selection of
    visualization types including graphs, charts, and more, and then customized with
    specific queries as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – New visualization](image/B14790_09_017_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – New visualization
  prefs: []
  type: TYPE_NORMAL
- en: Next, these visualizations can be combined into dashboards. This works similarly
    to Grafana where multiple visualizations can be added to a dashboard, which can
    then be saved and reused.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the search bar to further filter your dashboard visualizations
    – pretty nifty! The following screenshot shows how a dashboard can be tied to
    a specific query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Dashboard UI](image/B14790_09_018_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Dashboard UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, a dashboard can be created for a specific query using the **Add**
    button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Kibana provides a tool called *Timelion*, which is a time series visualization
    synthesis tool. Essentially, it allows you to combine separate data sources into
    a single visualization. Timelion is very powerful, but a full discussion of its
    feature set is outside the scope of this book. The following screenshot shows
    the Timelion UI – you may notice some similarities to Grafana, as these two sets
    of tools offer very similar capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Timelion UI](image/B14790_09_019_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Timelion UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in Timelion a query can be used to drive a real-time updating
    graph, just like in Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, though less relevant to this book, Kibana provides APM functionality,
    which requires some further setup, especially with Kubernetes. In this book we
    lean on Prometheus for this type of information while using the EFK stack to search
    logs from our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered Prometheus and Grafana for metrics and alerting, and
    the EFK stack for logging, only one piece of the observability puzzle is left.
    To solve this, we will use another excellent piece of open source software – Jaeger.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing distributed tracing with Jaeger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jaeger is an open source distributed tracing solution compatible with Kubernetes.
    Jaeger implements the OpenTracing specification, which is a set of standards for
    defining distributed traces.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger exposes a UI for viewing traces and integrates with Prometheus. The official
    Jaeger documentation can be found at [https://www.jaegertracing.io/docs/](https://www.jaegertracing.io/docs/).
    Always check the docs for new information, since things may have changed since
    the publishing of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Jaeger using the Jaeger Operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To install Jaeger, we are going to use the Jaeger Operator, which is the first
    operator that we've come across in this book. An *operator* in Kubernetes is simply
    a pattern for creating custom application controllers that speak Kubernetes's
    language. This means that instead of having to deploy all the various Kubernetes
    resources for an application, you can deploy a single Pod (or usually, single
    Deployment) and that application will talk to Kubernetes and spin up all the other
    required resources for you. It can even go further and self-operate the application,
    making resource changes when necessary. Operators can be highly complex, but they
    make it easier for us as end users to deploy commercial or open source software
    on our Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with the Jaeger Operator, we need to create a few initial resources
    for Jaeger, and then the operator will do the rest. A prerequisite for this installation
    of Jaeger is that the `nginx-ingress` controller is installed on our cluster,
    since that is how we will access the Jaeger UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a namespace for Jaeger to live in. We can get this
    via the `kubectl create namespace` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our namespace is created, we need to create some **CRDs** that Jaeger
    and the operator will use. We will discuss CRDs in depth in our chapter on extending
    Kubernetes, but for now, think of them as a way to co-opt the Kubernetes API to
    build custom functionality for applications. Using the following steps, let''s
    install Jaeger:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the Jaeger CRDs, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With our CRDs created, the operator needs a few Roles and Bindings to be created
    in order to do its work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want Jaeger to have cluster-wide permission in our cluster, so we will create
    some optional ClusterRoles and ClusterRoleBindings as well. To accomplish this,
    we run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we finally have all the pieces necessary for our operator to work. Let''s
    install the operator with one last `kubectl` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, check to see if the operator is running, using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If the operator is running correctly, you will see something similar to the
    following output, with one available Pod for the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Jaeger Operator Pod output](image/B14790_09_020_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Jaeger Operator Pod output
  prefs: []
  type: TYPE_NORMAL
- en: We now have our Jaeger Operator up and running – but Jaeger itself isn't running.
    Why is this the case? Jaeger is a highly complex system and can run in different
    configurations, and the operator makes it easier to deploy these configurations.
  prefs: []
  type: TYPE_NORMAL
- en: The Jaeger Operator uses a CRD called `Jaeger` to read a configuration for your
    Jaeger instance, at which time the operator will deploy all the necessary Pods
    and other resources on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jaeger can run in three main configurations: *AllInOne*, *Production*, and
    *Streaming*. A full discussion of these configurations is outside the scope of
    this book (check the Jaeger docs link shared previously), but we will be using
    the AllInOne configuration. This configuration combines the Jaeger UI, Collector,
    Agent, and Ingestor into a single Pod, without any persistent storage included.
    This is perfect for demo purposes – to see production-ready configurations, check
    the Jaeger docs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create our Jaeger deployment, we need to tell the Jaeger Operator
    about our chosen configuration. We do that using the CRD that we created earlier
    – the Jaeger CRD. Create a new file for this CRD instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger-allinone.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We are just using a small subset of the possible Jaeger type configurations
    – again, check the docs for the full story.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create our Jaeger instance by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This command creates an instance of the Jaeger CRD we installed previously.
    At this point, the Jaeger Operator should realize that the CRD has been created.
    In less than a minute, our actual Jaeger Pod should be running. We can check for
    it by listing all the Pods in the observability namespace, with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As an output, you should see the newly created Jaeger Pod for our all-in-one
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The Jaeger Operator creates an Ingress record when we also have an Ingress controller
    running on our cluster. This means that we can simply list our Ingress entries
    using kubectl to see where to access the Jaeger UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can list ingresses using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should show your new Ingress for the Jaeger UI as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Jaeger UI Service output](image/B14790_09_021_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – Jaeger UI Service output
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can navigate to the address listed in your cluster''s Ingress record
    to see the Jaeger UI. It should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Jaeger UI](image/B14790_09_022_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – Jaeger UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Jaeger UI is pretty simple. There are three tabs at the
    top – **Search**, **Compare**, and **System Architecture**. We will focus on the
    **Search** tab, but for more information about the other two, check the Jaeger
    docs at [https://www.jaegertracing.io](https://www.jaegertracing.io).
  prefs: []
  type: TYPE_NORMAL
- en: The Jaeger **Search** page lets us search for traces based on many inputs. We
    can search based on which Service is included in the trace, or based on tags,
    duration, or more. However, right now there's nothing in our Jaeger system.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that even though we have Jaeger up and running, our apps
    still need to be configured to send traces to Jaeger. This usually needs to be
    done at the code or framework level and is out of the scope of this book. If you
    want to play around with Jaeger's tracing capabilities, a sample app is available
    to install – see the Jaeger docs page at [https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod](https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod).
  prefs: []
  type: TYPE_NORMAL
- en: 'With services sending traces to Jaeger, it is possible to see traces. A trace
    in Jaeger looks like the following. We''ve cropped out some of the later parts
    of the trace for readability, but this should give you a good idea of what a trace
    can look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Trace view in Jaeger](image/B14790_09_023_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – Trace view in Jaeger
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Jaeger UI view for a trace splits up service traces into
    constituent parts. Each service-to-service call, as well as any specific calls
    within the services themselves, have their own line in the trace. The horizontal
    bar chart you see moves from left to right with time, and each individual call
    in the trace has its own line. In this trace, you can see we have HTTP calls,
    SQL calls, as well as some Redis statements.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to see how Jaeger and tracing in general can help developers
    make sense of a web of service-to-service calls and can help find bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: With that review of Jaeger, we have a fully open source solution to every problem
    in the observability bucket. However, that does not mean that there is no use
    case where a commercial solution makes sense – in many cases it does.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party tooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to many open source libraries, there are many commercially available
    products for metrics, logging, and alerting on Kubernetes. Some of these can be
    much more powerful than the open source options.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, most tooling in metrics and logging will require you to provision
    resources on your cluster to forward metrics and logs to your service of choice.
    In the examples we've used in this chapter, these services are running in the
    cluster, though in commercial products these can often be separate SaaS applications
    where you log on to analyze your logs and see your metrics. For instance, with
    the EFK stack we provisioned in this chapter, you can pay Elastic for a hosted
    solution where the Elasticsearch and Kibana pieces of the solution would be hosted
    on Elastic's infrastructure, reducing complexity in the solution. There are also
    many other solutions in this space, from vendors including Sumo Logic, Logz.io,
    New Relic, DataDog, and AppDynamics.
  prefs: []
  type: TYPE_NORMAL
- en: For a production environment, it is common to use separate compute (either a
    separate cluster, service, or SaaS tool) to perform log and metric analytics.
    This ensures that the cluster running your actual software can be dedicated to
    the application alone, and any costly log searching or querying functionality
    can be handled separately. It also means that if our application cluster goes
    down, we can still view logs and metrics up until the point of the failure.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about observability on Kubernetes. We first learned
    about the four major tenets of observability: metrics, logging, traces, and alerts.
    Then we discovered how Kubernetes itself provides tooling for observability, including
    how it manages logs and resource metrics and how to deploy Kubernetes Dashboard.
    Finally, we learned how to implement and use some key open source tools to provide
    visualization, searching, and alerting for the four pillars. This knowledge will
    help you build robust observability infrastructure for your future Kubernetes
    clusters and help you decide what is most important to observe in your cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use what we learned about observability to help
    us troubleshoot applications on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explain the difference between metrics and logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you use Grafana instead of simply using the Prometheus UI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When running an EFK stack in production (so as to keep as much compute off the
    production app cluster as possible), which piece(s) of the stack would run on
    the production app cluster? And which piece(s) would run off the cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In-depth review of Kibana Timelion: [https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html](https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

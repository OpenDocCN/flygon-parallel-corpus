- en: '*Chapter 11*: Template Code Generation and CI/CD on Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discusses some easier ways to template and configure large Kubernetes
    deployments with many resources. It also details a number of methods for implementing
    **Continuous Integration**/**Continuous Deployment** (**CI**/**CD**) on Kubernetes,
    as well as the pros and cons associated with each possible method. Specifically,
    we talk about in-cluster CI/CD, where some or all of the CI/CD steps are performed
    in our Kubernetes cluster, and out-of-cluster CI/CD, where all the steps take
    place outside our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The case study in this chapter will include creating a Helm chart from scratch,
    along with an explanation of each piece of a Helm chart and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we will cover the landscape of Kubernetes resource template generation,
    and the reasons why a template generation tool should be used at all. Then, we
    will cover implementing CI/CD to Kubernetes, first with AWS CodeBuild, and next
    with FluxCD.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding options for template code generation on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing templates on Kubernetes with Helm and Kustomize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding CI/CD paradigms on Kubernetes – in-cluster and out-of-cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing in-cluster and out-of-cluster CI/CD with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. Refer to [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, for several methods for getting up and running
    with Kubernetes quickly, and for instructions on how to install the kubectl tool.
    Additionally, you will need a machine that supports the Helm CLI tool, which typically
    has the same prerequisites as kubectl – for details, check out the Helm documentation
    at [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
  prefs: []
  type: TYPE_NORMAL
- en: The code used in this chapter can be found in the book's GitHub repository at
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter11](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter11).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding options for template code generation on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, one of the greatest strengths of Kubernetes is
    that its API can communicate in terms of declarative resource files. This allows
    us to run commands such as `kubectl apply` and have the control plane ensure that
    whatever resources are running in the cluster match our YAML or JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: However, this capability introduces some unwieldiness. Since we want to have
    all our workloads declared in configuration files, any large or complex applications,
    especially if they include many microservices, could result in a large number
    of configuration files to write and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: This issue is further compounded with multiple environments. Say we want development,
    staging, UAT, and production environments, this would require four separate YAML
    files per Kubernetes resource, assuming we wanted to maintain one resource per
    file for cleanliness.
  prefs: []
  type: TYPE_NORMAL
- en: One way to fix these issues is to work with templating systems that support
    variables, allowing a single template file to work for multiple applications or
    multiple environments by injecting different sets of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several popular community-supported open source options for this
    purpose. In this book, we will focus on two of the most popular ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Helm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kustomize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other options available, including Kapitan, Ksonnet, Jsonnet,
    and more, but a full review of all of them is not within the scope of this book.
    Let's start by reviewing Helm, which is, in many ways, the most popular templating
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: Helm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helm actually plays double duty as a templating/code generation tool and a CI/CD
    tool. It allows you to create YAML-based templates that can be hydrated with variables,
    allowing for code and template reuse across applications and environments. It
    also comes with a Helm CLI tool to roll out changes to applications based on the
    templates themselves.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, you are likely to see Helm all over the Kubernetes ecosystem
    as the default way to install tools or applications. We'll be using Helm for both
    of its purposes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to Kustomize, which is quite different to Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Kustomize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike Helm, Kustomize is officially supported by the Kubernetes project, and
    support is integrated directly into `kubectl`. Unlike Helm, Kustomize operates
    using vanilla YAML without variables, and instead recommends a *fork and patch*
    workflow where sections of YAML are replaced with new YAML depending on the patch
    chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how the tools differ, we can use them
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing templates on Kubernetes with Helm and Kustomize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know our options, we can implement each of them with an example
    application. This will allow us to understand the specifics of how each tool handles
    variables and the process of templating. Let's start with Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Using Helm with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, Helm is an open source project that makes it easy to
    template and deploy applications on Kubernetes. For the purposes of this book,
    we will be focused on the newest version (as of the time of writing), Helm V3\.
    A previous version, Helm V2, had more moving parts, including a controller, called
    *Tiller*, that would run on the cluster. Helm V3 is simplified and only contains
    the Helm CLI tool. It does, however, use custom resource definitions on the cluster
    to track releases, as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by installing Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Helm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to use a specific version of Helm, you can install it by following
    the specific version docs at [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
    For our use case, we will simply use the `get helm` script, which will install
    the newest version.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can fetch and run the script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we should be able to run `helm` commands. By default, Helm will automatically
    use your existing `kubeconfig` cluster and context, so in order to switch clusters
    for Helm, you just need to use `kubectl` to change your `kubeconfig` file, as
    you would normally do.
  prefs: []
  type: TYPE_NORMAL
- en: To install an application using Helm, run the `helm install` command. But how
    does Helm decide what and how to install? We'll need to discuss the concepts of
    Helm charts, Helm repositories, and Helm releases.
  prefs: []
  type: TYPE_NORMAL
- en: Helm charts, repositories, and releases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Helm provides a way to template and deploy applications on Kubernetes with variables.
    In order to do this, we specify workloads via a set of templates, which is called
    a *Helm chart*.
  prefs: []
  type: TYPE_NORMAL
- en: A Helm chart consists of one or more templates, some chart metadata, and a `values`
    file that fills in the template variables with final values. In practice, you
    would then have one `values` file per environment (or app, if you are reusing
    your template for multiple apps), which would hydrate the shared template with
    a new configuration. This combination of template and values would then be used
    to install or deploy an application to your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: So, where can you store Helm charts? You can put them in a Git repository as
    you would with any other Kubernetes YAML (which works for most use cases), but
    Helm also supports the concept of repositories. A Helm repository is represented
    by a URL and can contain multiple Helm charts. For instance, Helm has its own
    official repository at [https://hub.helm.sh/charts](https://hub.helm.sh/charts).
    Again, each Helm chart consists of a folder with a metadata file, a `Chart.yaml`
    file, one or more template files, and optionally a values file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install a local Helm chart with a local values file, you can pass
    a path for each to `helm install`, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: However, for commonly installed charts, you can also install the chart directly
    from a chart repository, and you can optionally add a custom repository to your
    local Helm in order to be able to install charts easily from non-official sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in order to install Drupal via the official Helm chart, you can
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This code installs charts out of the official Helm chart repository. To use
    a custom repository, you just need to add it to Helm first. For instance, to install
    `cert-manager`, which is hosted on the `jetstack` Helm repository, we can do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code adds the `jetstack` Helm repository to your local Helm CLI tool, and
    then installs `cert-manager` via the charts hosted there. We also name the release
    as `cert-manager`. A Helm release is a concept implemented using Kubernetes secrets
    in Helm V3\. When we create a Release in Helm, it will be stored as a secret in
    the same namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, we can create a Helm release using the preceding `install`
    command. Let''s do it now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This command should result in the following output, which may be slightly different
    depending on the current version of Cert Manager. We'll split the output into
    two sections for readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the output of the command gives us a status of the Helm release:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this section contains a timestamp for the deployment, namespace
    information, a revision, and a status. Next, we''ll see the notes section of the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, our Helm `install` command has resulted in a success message,
    which also gives us some information from `cert-manager` about how to use it.
    This output can be helpful to look at when installing Helm packages, as they sometimes
    include documentation such as the previous snippet. Now, to see how our release
    object looks in Kubernetes, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Secrets List output from kubectl](image/B14790_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Secrets List output from kubectl
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, one of the secrets has its type as `helm.sh/release.v1`. This
    is the secret that Helm is using to track the Cert Manager release.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to see the release listed in the Helm CLI, we can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will list Helm releases in all namespaces (just like `kubectl
    get pods -A` would list pods in all namespaces). The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Helm Release List output](image/B14790_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Helm Release List output
  prefs: []
  type: TYPE_NORMAL
- en: Now, Helm has more moving parts, including `upgrades`, `rollbacks` and more,
    and we'll review these in the next section. In order to show off what Helm can
    do, we will create and install a chart from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Helm chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, we want to create a Helm chart for our application. Let's set the stage.
    Our goal is to deploy a simple Node.js application easily to multiple environments.
    To this end, we will create a chart with the component pieces of our application,
    and then combine it with three separate values files (`dev`, `staging`, and `production`)
    in order to deploy our application to our three environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the folder structure of our Helm chart. As we mentioned previously,
    a Helm chart consists of templates, a metadata file, and optional values. We''re
    going to inject the values when we actually install our chart, but we can structure
    our folder like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: One thing we haven't yet mentioned is that you can actually have a folder of
    Helm charts inside an existing chart! These subcharts can make it easy to split
    up complex applications into components. For the purpose of this book, we will
    not be using subcharts, but if your application is getting too complex or modular
    for a singular chart, this is a valuable feature.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you can see that we have a different environment file for each environment,
    which we will use during our installation command.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what does a `Chart.yaml` file look like? This file will contain some basic
    metadata about your chart, and typically looks something like this as a minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `Chart.yaml` file supports many optional fields, which you can see at [https://helm.sh/docs/topics/charts/](https://helm.sh/docs/topics/charts/),
    but for the purposes of this tutorial, we will keep it simple. The mandatory fields
    are `apiVersion`, `name`, and `version`.
  prefs: []
  type: TYPE_NORMAL
- en: In our `Chart.yaml` file, `apiVersion` corresponds to the version of Helm that
    the chart corresponds to. Somewhat confusingly, the current release of Helm, Helm
    V3, uses `apiVersion` `v2`, while older versions of Helm, including Helm V2, also
    use `apiVersion` `v2`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the `name` field corresponds to the name of our chart. This is pretty
    self-explanatory, although remember that we have the ability to name a specific
    release of a chart – something that comes in handy for multiple environments.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `version` field, which corresponds to the version of the
    chart. This field supports **SemVer** (semantic versioning).
  prefs: []
  type: TYPE_NORMAL
- en: So, what do our templates actually look like? Helm charts use the Go templates
    library under the hood (see [https://golang.org/pkg/text/template/](https://golang.org/pkg/text/template/)
    for more information) and support all sorts of powerful manipulations, helper
    functions, and much, much more. For now, we will keep things extremely simple
    to give you an idea of the basics. A full discussion of Helm chart creation could
    be a book on its own!
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we can use a Helm CLI command to autogenerate our `Chart` folder,
    with all the previous files and folders, minus subcharts and values files, generated
    for you. Let''s try it – first create a new Helm chart with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will create an autogenerated chart in a folder named `myfakenodeapp`.
    Let''s check the contents of our `templates` folder with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This autogenerated chart can help a lot as a starting point, but for the purposes
    of this tutorial, we will make these from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new folder called `mynodeapp` and put the `Chart.yaml` file we showed
    you earlier in it. Then, create a folder inside called `templates`.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to keep in mind: a Kubernetes resource YAML is, by itself, a valid
    Helm template. There is no requirement to use any variables in your templates.
    You can just write regular YAML, and Helm installs will still work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To show this, let''s get started by adding a single template file to our templates
    folder. Call it `deployment.yaml` and include the following non-variable YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: 'deployment.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this YAML is just a regular Kubernetes resource YAML. We aren't
    using any variables in our template.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have enough to actually install our chart. Let's do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and uninstalling a Helm chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install a chart with Helm V3, you run a `helm install` command from the
    `root` directory of the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This installation command creates a Helm release called `frontend-app` and
    installs our chart. Right now, our chart only consists of a single deployment
    with two pods, and we should be able to see it running in our cluster with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the output, our Helm `install` command has successfully
    created a deployment object in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Uninstalling our chart is just as easy. We can install all the Kubernetes resources
    installed via our chart by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This `uninstall` command (`delete` in Helm V2) just takes the name of our Helm
    release.
  prefs: []
  type: TYPE_NORMAL
- en: Now, so far, we have not used any of the real power of Helm – we've been using
    it as a `kubectl` alternative without any added features. Let's change this by
    implementing some variables in our chart.
  prefs: []
  type: TYPE_NORMAL
- en: Using template variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adding variables to our Helm chart templates is as simple as using double bracket
    – `{{ }}` – syntax. What we put in the double brackets will be taken directly
    from the values that we use when installing our chart using dot notation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a quick example. So far, we have our app name (and container image
    name/version) hardcoded into our YAML file. This constrains us significantly if
    we want to use our Helm chart to deploy different applications or different application
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to address this, we''re going to add template variables to our chart.
    Take a look at this resulting template:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Templated-deployment.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Let's go over this YAML file and review our variables. We're using a few different
    types of variables in this file, but they all use the same dot notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Helm actually supports a few different top-level objects. These are the main
    objects you can reference in your templates:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.Chart`: Used to reference metadata values in the `Chart.yaml` file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.Values`: Used to reference values passed into the chart from a `values` file
    at install time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.Template`: Used to reference some info about the current template file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.Release`: Used to reference information about the Helm release'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.Files`: Used to reference files in the chart that are not YAML templates
    (for instance, `config` files)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.Capabilities`: Used to reference information about the target Kubernetes
    cluster (in other words, version)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our YAML file, we're using several of these. Firstly, we're referencing the
    `name` of our release (contained within the `.Release` object) in several places.
    Next, we are leveraging the `Chart` object to inject metadata into the `chartVersion`
    key. Finally, we are using the `Values` object to reference both the container
    image `name` and `tag`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the last thing we're missing is the actual values that we will inject via
    `values.yaml`, or in the CLI command. Everything else will be created using `Chart.yaml`,
    or values that we will inject at runtime via the `helm` command itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, let''s create our values file from our template that we
    will be passing in our image `name` and `tag`. So, let''s include those in the
    proper format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can install our app via our Helm chart! Do this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are passing in our values with the `-f` key (you can also
    use `--values`). This command will install the release of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a release, we can upgrade to a new version or roll back to an old
    one using the Helm CLI – we'll cover this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrades and rollbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have an active Helm release, we can upgrade it. Let''s make a small
    change to our `values.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To make this a new version of our release, we also need to change our chart
    YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can upgrade our release using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If, for any reason, we wanted to roll back to an earlier version, we can do
    so with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Helm allows for seamless templating, releases, upgrades, and
    rollbacks of applications. As we mentioned previously, Kustomize hits many of
    the same points but does it in a much different way – let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kustomize with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Helm charts can get quite complex, Kustomize uses YAML without any variables,
    and instead uses a patch and override-based method of applying different configurations
    to a base set of Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kustomize is extremely simple, and as we mentioned earlier in the chapter,
    there's no prerequisite CLI tool. Everything works by using the `kubectl apply
    -k /path/kustomize.yaml` command without installing anything new. However, we
    will also demonstrate the flow using the Kustomize CLI tool.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In order to install the Kustomize CLI tool, you can check the installation instructions
    at [https://kubernetes-sigs.github.io/kustomize/installation](https://kubernetes-sigs.github.io/kustomize/installation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the installation uses the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have Kustomize installed, let''s apply Kustomize to our existing
    use case. We''re going to start from our plain Kubernetes YAML (before we started
    adding Helm variables):'
  prefs: []
  type: TYPE_NORMAL
- en: 'plain-deployment.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: With our initial `deployment.yaml` created, we can now create a Kustomization
    file, which we call `kustomize.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: When we later call a `kubectl` command with the `-k` parameter, `kubectl` will
    look for this `kustomize` YAML file and use it to determine which patches to apply
    to all the other YAML files passed to the `kubectl` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kustomize lets us patch individual values or set common values to be automatically
    set. In general, Kustomize will create new lines, or update old lines if the key
    already exists in the YAML. There are three ways to apply these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify changes directly in a Kustomization file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `PatchStrategicMerge` strategy with a `patch.yaml` file along with a
    Kustomization file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `JSONPatch` strategy with a `patch.yaml` file along with a Kustomization
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with using a Kustomization file specifically to patch the YAML.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying changes directly in a Kustomization file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we want to directly specify changes within the Kustomization file, we can
    do so, but our options are somewhat limited. The types of keys we can use for
    a Kustomization file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`resources` – Specifies which files are to be customized when patches are applied'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers` – Ways to directly apply patches from within the Kustomization
    file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generators` – Ways to create new resources from the Kustomization file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`meta` – Sets metadata fields that can influence generators, transformers,
    and resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to specify direct patches in our Kustomization file, we need to use
    transformers. The aforementioned `PatchStrategicMerge` and `JSONPatch` merge strategies
    are two types of transformers. However, to directly apply changes to the Kustomization
    file, we can use one of several transformers, which include `commonLabels`, `images`,
    `namePrefix`, and `nameSuffix`.
  prefs: []
  type: TYPE_NORMAL
- en: In the following Kustomization file, we are applying changes to our initial
    deployment `YAML` using both `commonLabels` and `images` transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment-kustomization-1.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This particular `Kustomization.yaml` file updates the image tag from `1.0.0`
    to `2.0.0`, updates the name of the app from `frontend-myapp` to `frontend-app`,
    and updates the name of the container from `frontend-myapp` to `frontend-app-1`.
  prefs: []
  type: TYPE_NORMAL
- en: For a full rundown of the specifics of each of these transformers, you can check
    the Kustomize docs at [https://kubernetes-sigs.github.io/kustomize/](https://kubernetes-sigs.github.io/kustomize/).
    The Kustomize file assumes that `deployment.yaml` is in the same folder as itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the result when our Kustomize file is applied to our deployment, we
    can use the Kustomize CLI tool. We will use the following command to generate
    the kustomized output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the customizations from our Kustomization file have been applied.
    Because a `kustomize build` command outputs Kubernetes YAML, we can easily deploy
    the output to Kubernetes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's see how we can patch our deployment using a YAML file with `PatchStrategicMerge`.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying changes using PatchStrategicMerge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To illustrate a `PatchStrategicMerge` strategy, we once again start with our
    same `deployment.yaml` file. This time, we will issue our changes via a combination
    of the `kustomization.yaml` file and a `patch.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our `kustomization.yaml` file, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment-kustomization-2.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our Kustomization file references a new file, `deployment-patch-1.yaml`,
    in the `patchesStrategicMerge` section. Any number of patch YAML files can be
    added here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, our `deployment-patch-1.yaml` file is a simple file that mirrors our
    deployment with the changes we intend to make. Here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment-patch-1.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This patch file is a subset of the fields in the original deployment. In this
    case, it simply updates the `replicas` from `2` to `4`. Once again, to apply the
    changes, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we can also use the `-k` flag in a `kubectl` command! This is how
    it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This command is the equivalent of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Similar to `PatchStrategicMerge`, we can also specify JSON-based patches in
    our Kustomization – let's look at that now.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying changes using JSONPatch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To specify changes with a JSON patch file, the process is very similar to that
    involving a YAML patch.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need our Kustomization file. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment-kustomization-3.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our Kustomize file has a section, `patches`, which references
    a JSON patch file along with a target. You can reference as many JSON patches
    as you want in this section. `target` is used to determine which Kubernetes resource
    specified in the resources section will receive the patch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need our patch JSON itself, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment-patch-2.json:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This patch, when applied will perform the `replace` operation on the name of
    our first container. You can follow the path along with our original `deployment.yaml`
    file to see that it references the name of that first container. It will replace
    this name with the new value, `frontend-myreplacedapp`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a solid foundation in Kubernetes resource templating and releases
    with Kustomize and Helm, we can move on to the automation of deployments to Kubernetes.
    In the next section, we'll look at two patterns to accomplishing CI/CD with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CI/CD paradigms on Kubernetes – in-cluster and out-of-cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous integration and deployment to Kubernetes can take many forms.
  prefs: []
  type: TYPE_NORMAL
- en: Most DevOps engineers will be familiar with tools such as Jenkins, TravisCI,
    and others. These tools are fairly similar in that they provide an execution environment
    to build applications, perform tests, and call arbitrary Bash scripts in a controlled
    environment. Some of these tools run commands inside containers, while others
    don't.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to Kubernetes, there are multiple schools of thought in how and
    where to use these tools. There is also a newer breed of CI/CD platforms that
    are much more tightly coupled to Kubernetes primitives, and many that are architected
    to run on the cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To thoroughly discuss how tooling can pertain to Kubernetes, we will split
    our pipelines into two logical steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build**: Compiling, testing applications, building container images, and
    sending to image repositories'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploy**: Updating Kubernetes resources via kubectl, Helm, or a different
    tool'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the purposes of this book, we are going to focus mostly on the second deploy-focused
    step. Though many of the options available handle both build and deploy steps,
    the build step can happen just about anywhere, and is not worth our focus in a
    book relating to the specifics of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, to discuss our tooling options, we will split our set of
    tools into two categories as far as the Deploy part of our pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-cluster CI/CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-cluster CI/CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out-of-cluster CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first pattern, our CI/CD tool runs outside of our target Kubernetes cluster.
    We call this out-of-cluster CI/CD. There is a gray area where the tool may run
    in a separate Kubernetes cluster that is focused on CI/CD, but we will ignore
    that option for now as the difference between the two categories is still mostly
    valid.
  prefs: []
  type: TYPE_NORMAL
- en: You'll often find industry standard tooling such as Jenkins used with this pattern,
    but any CI tool that has the ability to run scripts and retain secret keys in
    a secure way can work here. A few examples are **GitLab CI**`,` **CircleCI**,
    **TravisCI**, **GitHub Actions**, and **AWS CodeBuild**. Helm is also a big part
    of this pattern, as out-of-cluster CI scripts can call Helm commands in lieu of
    kubectl.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the strengths of this pattern are to be found in its simplicity and
    extensibility. This is a `push`-based pattern where changes to code synchronously
    trigger changes in Kubernetes workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the weaknesses of out-of-cluster CI/CD are scalability when pushing
    to many clusters, and the need to keep cluster credentials in the CI/CD pipeline
    so it has the ability to call kubectl or Helm commands.
  prefs: []
  type: TYPE_NORMAL
- en: In-cluster CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the second pattern, our tool runs on the same cluster that our applications
    run on, which means that CI/CD happens within the same Kubernetes context as our
    applications, as pods. We call this in-cluster CI/CD. This in-cluster pattern
    can still have the "build" steps occur outside the cluster, but the deploy step
    happens from within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These types of tools have been gaining popularity since Kubernetes was released,
    and many use custom resource definitions and custom controllers to do their jobs.
    Some examples are **FluxCD**, **Argo CD**, **JenkinsX**, and **Tekton Pipelines**.
    The **GitOps** pattern, where a Git repository is used as the source of truth
    for what applications should be running on a cluster, is popular in these tools.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the strengths of the in-cluster CI/CD pattern are scalability and security.
    By having the cluster "pull" changes from GitHub via a GitOps operating model,
    the solution can be scaled to many clusters. Additionally, it removes the need
    to keep powerful cluster credentials in the CI/CD system, instead having GitHub
    credentials on the cluster itself, which can be much better from a security standpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The weaknesses of the in-cluster CI/CD pattern include complexity, since this
    pull-based operation is slightly asynchronous (as `git pull` usually occurs on
    a loop, not always occurring exactly when changes are pushed).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing in-cluster and out-of-cluster CI/CD with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since there are so many options for CI/CD with Kubernetes, we will choose two
    options and implement them one by one so you can compare their feature sets. First,
    we'll implement CI/CD to Kubernetes on AWS CodeBuild, which is a great example
    implementation that can be reused with any external CI system that can run Bash
    scripts, including Bitbucket Pipelines, Jenkins, and others. Then, we'll move
    on to FluxCD, an in-cluster GitOps-based CI option that is Kubernetes-native.
    Let's start with the external option.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Kubernetes CI with AWS Codebuild
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, our AWS CodeBuild CI implementation will be easy to duplicate
    in any script- based CI system. In many cases, the pipeline YAML definition we'll
    use is near identical. Also, as we discussed earlier, we are going to skip the
    actual building of the container image. We will instead focus on the actual deployment
    piece.
  prefs: []
  type: TYPE_NORMAL
- en: To quickly introduce AWS CodeBuild, it is a script-based CI tool that runs Bash
    scripts, like many other similar tools. In the context of AWS CodePipeline, a
    higher-level tool, multiple separate AWS CodeBuild steps can be combined into
    larger pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will be using both AWS CodeBuild and AWS CodePipeline. We
    will not be discussing in depth how to use these two tools, but instead will keep
    our discussion tied specifically to how to use them for deployment to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We highly recommend that you read and review the documentation for both CodePipeline
    and CodeBuild, since we will not be covering all of the basics in this chapter.
    You can find the documentation at [https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html](https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html)
    for CodeBuild, and [https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html](https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html)
    for CodePipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you would have two CodePipelines, each with one or more CodeBuild
    steps. The first CodePipeline is triggered on a code change in either AWS CodeCommit
    or another Git repository (such as GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: The first CodeBuild step for this pipeline runs tests and builds the container
    image, pushing the image to AWS **Elastic Container Repository** (**ECR**). The
    second CodeBuild step for the first pipeline deploys the new image to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The second CodePipeline is triggered anytime we commit a change to our secondary
    Git repository with Kubernetes resource files (infrastructure repository). It
    will update the Kubernetes resources using the same process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the first CodePipeline. As mentioned earlier, it contains
    two CodeBuild steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, to test and build the container image and push it to the ECR
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, to deploy the updated container to Kubernetes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we mentioned earlier in this section, we will not be spending much time
    on the code-to-container-image pipeline, but here is an example (not production
    ready) `codebuild` YAML for implementing this first step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline-1-codebuild-1.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This CodeBuild pipeline consists of four phases. CodeBuild pipeline specs are
    written in YAML, and contain a `version` tag that corresponds to the version of
    the CodeBuild spec. Then, we have a `phases` section, which is executed in order.
    This CodeBuild first runs a `build` command, and then runs a `test` command in
    the test phase. Finally, the `containerbuild` phase creates the container image,
    and the `push` phase pushes the image to our container repository.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that every value with a `$` in front of it in CodeBuild
    is an environment variable. These can be customized via the AWS Console or the
    AWS CLI, and some can come directly from the Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the YAML for the second CodeBuild step of our first
    CodePipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline-1-codebuild-2.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break this file down. Our CodeBuild setup is broken down into three
    phases: `install`, `pre_deploy`, and `deploy`. In the `install` phase, we install
    the kubectl CLI tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the `pre_deploy` phase, we use an AWS CLI command and a couple of environment
    variables to update our `kubeconfig` file for communicating with our EKS cluster.
    In any other CI tool (or when not using EKS) you could use a different method
    for giving cluster credentials to your CI tool. It is important to use a safe
    option here, as including the `kubeconfig` file directly in your Git repository
    is not secure. Typically, some combination of environment variables would be great
    here. Jenkins, CodeBuild, CircleCI, and more have their own systems for this.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the `deploy` phase, we use `kubectl` to update our deployment (also
    contained in an environment variable) with the new image tag specified in the
    first CodeBuild step. This `kubectl rollout restart` command will ensure that
    new pods are started for our deployment. In combination with using the `imagePullPolicy`
    of `Always`, this will result in our new application version being deployed.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we are patching our deployment with a specific image tag name
    in the ECR. The `$IMAGE_TAG` environment variable will be auto populated with
    the newest tag from GitHub so we can use that to automatically roll out the new
    container image to our deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at our second CodePipeline. This one contains only one
    step – it listens to changes from a separate GitHub repository, our "infrastructure
    repository". This repository does not contain code for our applications themselves,
    but instead Kubernetes resource YAMLs. Thus, we can change a Kubernetes resource
    YAML value – for instance, the number of replicas in a deployment, and see it
    updated in Kubernetes after the CodePipeline runs. This pattern can be extended
    to use Helm or Kustomize very easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the first, and only, step of our second CodePipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline-2-codebuild-1.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this CodeBuild spec is quite similar to our previous one. As
    before, we install kubectl and prep it for use with our Kubernetes cluster. Since
    we are running on AWS, we do it using the AWS CLI, but this could be done any
    number of ways, including by just adding a `Kubeconfig` file to our CodeBuild
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: The difference here is that instead of patching a specific deployment with a
    new version of an application, we are running an across-the-board `kubectl apply`
    command while piping in our entire infrastructure folder. This could then make
    any changes performed in Git be applied to the resources in our cluster. For instance,
    if we scaled our deployment from 2 replicas to 20 replicas by changing the value
    in the `deployment.yaml` file, it would be deployed to Kubernetes in this CodePipeline
    step and the deployment would scale up.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered the basics of using an out-of-cluster CI/CD environment
    to make changes to Kubernetes resources, let's take a look at a completely different
    CI paradigm, where the pipeline runs on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Kubernetes CI with FluxCD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our in-cluster CI tool, we will be using **FluxCD**. There are several options
    for in-cluster CI, including **ArgoCD** and **JenkinsX**, but we like **FluxCD**
    for its relative simplicity, and for the fact that it automatically updates pods
    with new container versions without any additional configuration. As an added
    twist, we will use FluxCD's Helm integration for managing deployments. Let's start
    with the installation of FluxCD (we'll assume you already have Helm installed
    from the previous parts of the chapter). These installations follow the official
    FluxCD installation instructions for Helm compatibility, as of the time of writing
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The official FluxCD docs can be found at [https://docs.fluxcd.io/](https://docs.fluxcd.io/),
    and we highly recommend you give them a look! FluxCD is a very complex tool, and
    we are only scratching the surface in this book. A full review is not in scope
    – we are simply trying to introduce you to the in-cluster CI/CD pattern and relevant
    tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start our review by installing FluxCD on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing FluxCD (H3)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FluxCD can easily be installed using Helm in a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to add the Flux Helm chart repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to add a custom resource definition that FluxCD requires in order
    to be able to work with Helm releases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can install the FluxCD Operator (which is the core of FluxCD functionality
    on Kubernetes) and the FluxCD Helm Operator, we need to create a namespace for
    FluxCD to live in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now we can install the main pieces of FluxCD, but we'll need to give FluxCD
    some additional information about our Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because FluxCD uses a GitOps pattern for updates and deployments. This
    means that FluxCD will actively reach out to our Git repository every few minutes,
    instead of responding to Git hooks such as CodeBuild, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: FluxCD will also respond to new ECR images via a pull-based strategy, but we'll
    get to that in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: To install the main pieces of FluxCD, run the following two commands and replace
    `GITHUB_USERNAME` and `REPOSITORY_NAME` with the GitHub user and repository that
    you will be storing your workload specs (Kubernetes YAML or Helm charts) in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This instruction set assumes that the Git repository is public, which it likely
    isn''t. Since most organizations use private repositories, FluxCD has specific
    configurations to handle this case – just check the docs at [https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/](https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/).
    In fact, to see the real power of FluxCD, you''ll need to give it advanced access
    to your Git repository in any case, since FluxCD can write to your Git repository
    and automatically update manifests as new container images are created. However,
    we won''t be getting into that functionality in this book. The FluxCD docs are
    definitely worth a close read as this is a complex piece of technology with many
    features. To tell FluxCD which GitHub repository to look at, you can set variables
    when installing using Helm, as in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we need to pass our GitHub username, the name of our repository,
    and a name that will be used for our GitHub secret in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, FluxCD is fully installed in our cluster and pointed at our infrastructure
    repository on Git! As mentioned before, this GitHub repository will contain Kubernetes
    YAML or Helm charts on the basis of which FluxCD will update workloads running
    in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To actually give Flux something to do, we need to create the actual manifest
    for Flux. We do so using a `HelmRelease` YAML file, which looks like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'helmrelease-1.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Let's pick this file apart. We are specifying the Git repository where Flux
    will find the Helm chart for our application. We are also marking the `HelmRelease`
    with an `automated` annotation, which tells Flux to go and poll the container
    image repository every few minutes and see whether there is a new version to deploy.
    To aid this, we include a `chart-image` filter pattern, which the tagged container
    image must match in order to trigger a redeploy. Finally, in the values section,
    we have Helm values that will be used for the initial installation of the Helm
    chart.
  prefs: []
  type: TYPE_NORMAL
- en: To give FluxCD this information, we simply need to add this file to the root
    of our GitHub repository and push up a change.
  prefs: []
  type: TYPE_NORMAL
- en: Once we add this release file, `helmrelease-1.yaml`, to our Git repository,
    Flux will pick it up within a few minutes, and then look for the specified Helm
    chart in the `chart` value. There's just one problem – we haven't made it yet!
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, our infrastructure repository on GitHub only contains our single
    Helm release file. The folder contents look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To close the loop and allow Flux to actually deploy our Helm chart, we need
    to add it to this infrastructure repository. Let''s do so, making the final folder
    contents in our GitHub repository look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now, when FluxCD next checks the infrastructure repository on GitHub, it will
    first find the Helm release YAML file, which will then point it to our new Helm
    chart.
  prefs: []
  type: TYPE_NORMAL
- en: FluxCD, with a new release and a Helm chart, will then deploy our Helm chart
    to Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: Then, any time a change is made to either the Helm release YAML or any file
    in our Helm chart, FluxCD will pick it up and, within a few minutes (on its next
    loop), will deploy the change.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, any time a new container image with a matching tag to the filter
    pattern is pushed to the image repository, a new version of the app will automatically
    be deployed – it's that easy. This means that FluxCD is listening to two locations
    – the infrastructure GitHub repository and the container repository, and will
    deploy any changes to either location.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how this maps to our out-of-cluster CI/CD implementation where we
    had one CodePipeline to deploy new versions of our App container, and another
    CodePipeline to deploy any changes to our infrastructure repository. FluxCD does
    the same thing in a pull-based way.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about template code generation on Kubernetes. We
    reviewed how to create flexible resource templates using both Helm and Kustomize.
    With this knowledge, you will be able to template your complex applications using
    either solution, create, or deploy releases. Then, we reviewed two types of CI/CD
    on Kubernetes; first, external CI/CD deployment to Kubernetes via kubectl, and
    then in-cluster CI paradigms using FluxCD. With these tools and techniques, you
    will be able to set up CI/CD to Kubernetes for production applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will review security and compliance on Kubernetes, an
    important topic in today's software environment.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are two differences between Helm and Kustomize templating?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How should Kubernetes API credentials be handled when using an external CI/CD
    setup?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the reasons as to why an in-cluster CI setup may be preferable
    to an out-of-cluster setup? And vice versa?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kustomize docs: https:[https://kubernetes-sigs.github.io/kustomize/](https://kubernetes-sigs.github.io/kustomize/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helm docs [https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/](https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

- en: '*Chapter 13*: Extending Kubernetes with CRDs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explains the many possibilities for extending the functionality
    of Kubernetes. It begins with a discussion of the **Custom Resource Definition**
    (**CRD**), a Kubernetes-native way to specify custom resources that can be acted
    on by the Kubernetes API using familiar `kubectl` commands such as `get`, `create`,
    `describe`, and `apply`. It is followed by a discussion of the Operator pattern,
    an extension of the CRD. It then details some of the hooks that cloud providers
    attach to their Kubernetes implementations, and ends with a brief introduction
    to the greater cloud-native ecosystem. Using the concepts learned in this chapter,
    you will be able to architect and develop extensions to your Kubernetes cluster,
    unlocking advanced usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The case study in this chapter will include creating two simple CRDs to support
    an example application. We'll begin with CRDs, which will give you a good base
    understanding of how extensions can build on the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to extend Kubernetes with **Custom Resource Definitions** (**CRDs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-managing functionality with Kubernetes operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cloud-specific Kubernetes extensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating with the ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13).
  prefs: []
  type: TYPE_NORMAL
- en: How to extend Kubernetes with custom resource definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with the basics. What is a CRD? We know that Kubernetes has an API
    model where we can perform operations against resources. Some examples of Kubernetes
    resources (which you should be well acquainted with by now) are Pods, PersistentVolumes,
    Secrets, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what if we want to implement some custom functionality in our cluster,
    write our own controllers, and store the state of our controllers somewhere? We
    could, of course, store the state of our custom functionality in a SQL or NoSQL
    database running on Kubernetes or elsewhere (which is actually one of the strategies
    for extending Kubernetes) – but what if our custom functionality acts more as
    an extension of Kubernetes functionality, instead of a completely separate application?
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases like this, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom resource definitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API aggregation allows advanced users to build their own resource APIs outside
    of the Kubernetes API server and use their own storage – and then aggregate those
    resources at the API layer so they can be queried using the Kubernetes API. This
    is obviously highly extensible and is essentially just using the Kubernetes API
    as a proxy to your own custom functionality, which may or may not actually integrate
    with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The other option is CRDs, where we can use the Kubernetes API and underlying
    data store (`etcd`) instead of building our own. We can use the `kubectl` and
    `kube api` methods that we know to interact with our own custom functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will not discuss API aggregation. While definitely more flexible
    than CRDs, this is an advanced topic that deserves a thorough understanding of
    the Kubernetes API and a thorough perusal of the Kubernetes documentation to do
    it right. You can learn more about API aggregation in the Kubernetes documentation
    at [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we know that we are using the Kubernetes control plane as our own
    stateful store for our new custom functionality, we need a schema. Similar to
    how the Pod resource spec in Kubernetes expects certain fields and configurations,
    we can tell Kubernetes what we expect for our new custom resources. Let's go through
    the spec for a CRD now.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a custom resource definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For CRDs, Kubernetes uses the OpenAPI V3 specification. For more information
    on OpenAPI V3, you can check the official documentation at [https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md),
    but we'll soon see how exactly this translates into Kubernetes CRD definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at an example CRD spec. Now let's be clear, this is not how
    YAMLs of any specific record of this CRD would look. Instead, this is simply where
    we define the requirements for the CRD inside of Kubernetes. Once created, Kubernetes
    will accept resources matching the spec and we can start making our own records
    of this type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example YAML for a CRD spec, which we are calling `delayedjob`.
    This highly simplistic CRD is intended to start a container image job on a delay,
    which prevents users from having to script in a delayed start for their container.
    This CRD is quite brittle, and we don''t recommend anyone actually use it, but
    it does well to highlight the process of building a CRD. Let''s start with a full
    CRD spec YAML, then break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom-resource-definition-1.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's review the parts of this file. At first glance, it looks like your typical
    Kubernetes YAML spec – and that's because it is! In the `apiVersion` field, we
    have `apiextensions.k8s.io/v1`, which is the standard since Kubernetes `1.16`
    (before then it was `apiextensions.k8s.io/v1beta1`). Our `kind` will always be
    `CustomResourceDefinition`.
  prefs: []
  type: TYPE_NORMAL
- en: The `metadata` field is when things start to get specific to our resource. We
    need to structure the `name` metadata field as the `plural` form of our resource,
    then a period, then its group. Let's take a quick diversion from our YAML file
    to discuss how groups work in the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Kubernetes API groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Groups are a way that Kubernetes segments resources in its API. Each group corresponds
    to a different subpath of the Kubernetes API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, there is a legacy group called the core group – which corresponds
    to resources accessed on the `/api/v1` endpoint in the Kubernetes REST API. By
    extension, these legacy group resources have `apiVersion: v1` in their YAML specs.
    An example of one of the resources in the core group is the Pod.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, there is the set of named groups – which correspond to resources that
    can be accessed on `REST` URLs formed as `/apis/<GROUP NAME>/<VERSION>`. These
    named groups form the bulk of Kubernetes resources. However, the oldest and most
    basic resources, such as the Pod, Service, Secret, and Volume, are in the core
    group. An example of a resource that is in a named group is the `StorageClass`
    resource, which is in the `storage.k8s.io` group.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To see which resource is in which group, you can check the official Kubernetes
    API docs for whatever version of Kubernetes you are using. For example, the version
    `1.18` docs would be at [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18).
  prefs: []
  type: TYPE_NORMAL
- en: CRDs can specify their own named group, which means that the specific CRD will
    be available on a `REST` endpoint that the Kubernetes API server can listen on.
    With that in mind, let's get back to our YAML file, so we can talk about the main
    portion of the CRD – the versions spec.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding custom resource definition versions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you can see, we have chosen the group `delayedresources.mydomain.com`. This
    group would theoretically hold any other CRDs of the delayed kind – for instance,
    `DelayedDaemonSet` or `DelayedDeployment`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the main portion of our CRD. Under `versions`, we can define one
    or more CRD versions (in the `name` field), along with the API specification for
    that version of the CRD. Then, when you create an instance of your CRD, you can
    define which version you will be using for the version parameter in the `apiVersion`
    key of your YAML – for instance, `apps/v1`, or in this case, `delayedresources.mydomain.com/v1`.
  prefs: []
  type: TYPE_NORMAL
- en: Each version item also has a `served` attribute, which is essentially a way
    to define whether the given version is enabled or disabled. If `served` is `false`,
    the version will not be created by the Kubernetes API, and the API requests (or
    `kubectl` commands) for that version will fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, it is possible to define a `deprecated` key on a specific version,
    which will cause Kubernetes to return a warning message when requests are made
    to the API using the deprecated version. This is how a CRD. `yaml` file with a
    deprecated version looks – we have removed some of the spec to keep the YAML short:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom-resource-definition-2.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have marked `v1` as deprecated, and also include a deprecation
    warning for Kubernetes to send as a response. If we do not include a deprecation
    warning, a default message will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Moving further down, we have the `storage` key, which interacts with the `served`
    key. The reason this is necessary is that while Kubernetes supports multiple active
    (aka `served`) versions of a resource at the same time, only one of those versions
    can be stored in the control plane. However, the `served` attribute means that
    multiple versions of a resource can be served by the API. So how does that even
    work?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that Kubernetes will convert the CRD object from whatever the
    stored version is to the version you ask for (or vice versa, when creating a resource).
  prefs: []
  type: TYPE_NORMAL
- en: How is this conversion handled? Let's skip past the rest of the version attributes
    to the `conversion` key to see how.
  prefs: []
  type: TYPE_NORMAL
- en: The `conversion` key lets you specify a strategy for how Kubernetes will convert
    CRD objects between whatever your served version is and whatever the stored version
    is. If the two versions are the same – for instance, if you ask for a `v1` resource
    and the stored version is `v1`, then no conversion will happen.
  prefs: []
  type: TYPE_NORMAL
- en: The default value here as of Kubernetes 1.13 is `none`. With the `none` setting,
    Kubernetes will not do any conversion between fields. It will simply include the
    fields that are supposed to be present on the `served` (or stored, if creating
    a resource) version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other possible conversion strategy is `Webhook`, which allows you to define
    a custom webhook that will take in one version and do the proper conversion to
    your intended version. Here is an example of our CRD with a `Webhook` conversion
    strategy – we''ve cut out some of the version schema for conciseness:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom-resource-definition-3.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `Webhook` strategy lets us define a URL that requests will
    be made to with information about the incoming resource object, its current version,
    and the version it needs to be converted to.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that our `Webhook` server will then handle the conversion and pass
    back the corrected Kubernetes resource object. The `Webhook` strategy is complex
    and can have many possible configurations, which we will not get into in depth
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To see how conversion Webhooks can be configured, check the official Kubernetes
    documentation at [https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, back to our `version` entry in the YAML! Under the `served` and `storage`
    keys, we see the `schema` object, which contains the actual specification of our
    resource. As previously mentioned, this follows the OpenAPI Spec v3 schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `schema` object, which was removed from the preceding code block for space
    reasons, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom-resource-definition-3.yaml (continued)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we support a field for `delaySeconds`, which will be an integer,
    and `image`, which is a string that corresponds to our container image. If we
    really wanted to make the `DelayedJob` production-ready, we would want to include
    all sorts of other options to make it closer to the original Kubernetes Job resource
    – but that isn't our intent here.
  prefs: []
  type: TYPE_NORMAL
- en: Moving further back in the original code block, outside the versions list, we
    see some other attributes. First is the `scope` attribute, which can be either
    `Cluster` or `Namespaced`. This tells Kubernetes whether to treat instances of
    the CRD object as namespace-specific resources (such as Pods, Deployments, and
    so on) or instead as cluster-wide resources – like namespaces themselves, since
    getting namespace objects within a namespace doesn't make any sense!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `names` block, which lets you define both a plural and
    singular form of your resource name, to be used in various situations (for instance,
    `kubectl get pods` and `kubectl get pod` both work).
  prefs: []
  type: TYPE_NORMAL
- en: The `names` block also lets you define the camel-cased `kind` value, which will
    be used in the resource YAML, as well as one or more `shortNames`, which can be
    used to refer to the resource in the API or `kubectl` – for instance, `kubectl
    get po`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our CRD specification YAML explained, let''s take a look at an instance
    of our CRD – as defined by the spec we just reviewed, the YAML will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Delayed-job.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is just like our CRD defined this object. Now, with all
    our pieces in place, let's test out our CRD!
  prefs: []
  type: TYPE_NORMAL
- en: Testing a custom resource definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s go ahead and test out our CRD concept on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create the CRD spec in Kubernetes – the same way we would create
    any other object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, Kubernetes will accept requests for our `DelayedJob` resource. We can
    test this out by finally creating one using the preceding resource YAML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we''ve defined our CRD properly, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the Kubernetes API server has successfully created our instance
    of `DelayedJob`!
  prefs: []
  type: TYPE_NORMAL
- en: Now, you may be asking a very relevant question – now what? This is an excellent
    question, because the truth is that we have accomplished nothing more so far than
    essentially adding a new `table` to the Kubernetes API database.
  prefs: []
  type: TYPE_NORMAL
- en: Just because we gave our `DelayedJob` resource an application image and a `delaySeconds`
    field does not mean that any functionality like what we intend will actually occur.
    By creating our instance of `DelayedJob`, we have just added an entry to that
    `table`. We can fetch it, edit it, or delete it using the Kubernetes API or `kubectl`
    commands, but no application functionality has been implemented.
  prefs: []
  type: TYPE_NORMAL
- en: In order to actually get our `DelayedJob` resource to do something, we need
    a custom controller that will take our instance of `DelayedJob` and do something
    with it. In the end, we still need to implement actual container functionality
    using the official Kubernetes resources – Pods et al.
  prefs: []
  type: TYPE_NORMAL
- en: This is what we're going to discuss now. There are many ways to build custom
    controllers for Kubernetes, but a popular way is the **Operator pattern**. Let's
    move onto the next section to see how we can give our `DelayedJob` resource a
    life of its own.
  prefs: []
  type: TYPE_NORMAL
- en: Self-managing functionality with Kubernetes operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No discussion of Kubernetes operators would be possible without first discussing
    the **Operator Framework**. A common misconception is that operators are specifically
    built via the Operator Framework. The Operator Framework is an open source framework
    originally created by Red Hat to make it easy to write Kubernetes operators.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, an operator is simply a custom controller that interfaces with Kubernetes
    and acts on resources. The Operator Framework is one opinionated way to make Kubernetes
    operators, but there are many other open source frameworks you can use – or, you
    can make one from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: When building an operator using frameworks, two of the most popular options
    are the aforementioned **Operator Framework** and **Kubebuilder**.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these projects have a lot in common. They both make use of `controller-tools`
    and `controller-runtime`, which are two libraries for building Kubernetes controllers
    that are officially supported by the Kubernetes project. If you are building an
    operator from scratch, using these officially supported controller libraries will
    make things much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the Operator Framework, Kubebuilder is an official part of the Kubernetes
    project, much like the `controller-tools` and `controller-runtime` libraries –
    but both projects have their pros and cons. Importantly, both these options, and
    the Operator pattern in general, have the controller running on the cluster. It
    may seem obvious that this is the best option, but you could run your controller
    outside of the cluster and have it work the same. To get started with the Operator
    Framework, check the official GitHub at [https://github.com/operator-framework](https://github.com/operator-framework).
    For Kubebuilder, you can check [https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder).
  prefs: []
  type: TYPE_NORMAL
- en: Most operators, regardless of the framework, follow a control-loop paradigm
    – let's see how this idea works.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the operator control loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A control loop is a control scheme in system design and programming that consists
    of a never-ending loop of logical processes. Typically, a control loop implements
    a measure-analyze-adjust approach, where it measures the current state of the
    system, analyzes what changes are required to bring it in line with the intended
    state, and then adjusts the system components to bring it in line with (or at
    least closer to) the intended state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes operators or controllers specifically, this operation usually
    works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: First, a `watch` step – that is, watching the Kubernetes API for changes in
    the intended state, which is stored in `etcd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, an `analyze` step – which is the controller deciding what to do to bring
    the cluster state in line with the intended state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And lastly, an `update` step – which is updating the cluster state to fulfill
    the intent of the cluster changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To help understand the control loop, here is a diagram showing how the pieces
    fit together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Measure Analyze Update Loop](image/B14790_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Measure Analyze Update Loop
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the Kubernetes scheduler – which is itself a control loop process
    – to illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a hypothetical cluster in a steady state: all Pods are scheduled,
    Nodes are healthy, and everything is operating normally.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, a user creates a new Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ve discussed before that the kubelet works on a `pull` basis. This means
    that when a kubelet creates a Pod on its Node, that Pod was already assigned to
    that Node via the scheduler. However, when Pods are first created via a `kubectl
    create` or `kubectl apply` command, the Pod isn''t scheduled or assigned anywhere.
    This is where our scheduler control loop starts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is **Measure**, where the scheduler reads the state of the Kubernetes
    API. When listing Pods from the API, it discovers that one of the Pods is not
    assigned to a Node. It now moves to the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the scheduler performs an analysis of the cluster state and Pod requirements
    in order to decide which Node the Pod should be assigned to. As we discussed in
    previous chapters, this takes into account Pod resource limits and requests, Node
    statuses, placement controls, and so on, which makes it a fairly complex process.
    Once this processing is complete, the update step can start.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, **Update** – the scheduler updates the cluster state by assigning the
    Pod to the Node obtained from the *step 2* analysis. At this point, the kubelet
    takes over on its own control loop and creates the relevant container(s) for the
    Pod on its Node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let's take what we learned from the scheduler control loop and apply it
    to our very own `DelayedJob` resource.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an operator for a custom resource definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually, coding an operator for our `DelayedJob` CRD is outside the scope of
    our book since it requires knowledge of a programming language. If you're choosing
    a programming language to build an operator with, Go offers the most interoperability
    with the Kubernetes SDK, **controller-tools**, and **controller-runtime**, but
    any programming language where you can write HTTP requests will work, since that
    is the basis for all of the SDKs.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will still walk through the steps of implementing an operator for
    our `DelayedJob` CRD with some pseudocode. Let's take it step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Measure'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First comes the **Measure** step, which we will implement in our pseudocode
    as a `while` loop that runs forever. In a production implementation, there would
    be debouncing, error handling, and a bunch of other concerns, but we'll keep it
    simple for this illustrative example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the pseudo code for this loop, which is essentially the main
    function of our application:'
  prefs: []
  type: TYPE_NORMAL
- en: Main-function.pseudo
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the loop in our `main` function calls the Kubernetes API to
    find a list of the `delayedjobs` CRDs stored in `etcd`. This is the `measure`
    step. It then calls the analysis step, and with the results of that, calls the
    update step to schedule any `DelayedJobs` that need to be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the Kubernetes scheduler is still going to do the actual container
    scheduling in this example – but we need to boil down our `DelayedJob` into an
    official Kubernetes resource first.
  prefs: []
  type: TYPE_NORMAL
- en: After the update step, our loop waits for a full 5 seconds before performing
    the loop again. This sets the cadence of the control loop. Next, let's move on
    to the analysis step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Analyze'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, let''s review the **Analysis** step of our operator, which is the `analyzeDelayedJobs`
    function in our controller pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis-function.pseudo
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding function loops through the list of `DelayedJob`
    objects from the cluster as passed from the **Measure** loop. It then checks to
    see if the `DelayedJob` has been scheduled yet by checking the value of one of
    the object's annotations. If it hasn't been scheduled yet, it adds an object to
    an array called `listOfJobsToSchedule`, which contains the image specified in
    the `DelayedJob` object, a command to sleep for the number of seconds that was
    specified in the `DelayedJob` object, and the original name of the `DelayedJob`,
    which we will use to mark as scheduled in the **Update** step.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the **Analyze** step the `analyzeDelayedJobs` function returns our
    newly created `listOfJobsToSchedule` array back to the main function. Let's wrap
    up our Operator design with the final update step, which is the `scheduleDelayedJobs`
    function in our main loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Update'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, the **Update** part of our control loop will take the outputs from
    our analysis and update the cluster as necessary to create the intended state.
    Here''s the pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: Update-function.pseudo
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we are taking our regular Kubernetes object, which was derived
    from our `DelayedJob` object, and creating it in Kubernetes so the `Kube` scheduler
    can pick up on it, create the relevant Pod, and manage it. Once we create the
    regular Job object with the delay, we also update our `DelayedJob` CRD instance
    with an annotation that sets the `is-scheduled` annotation to `true`, preventing
    it from getting rescheduled.
  prefs: []
  type: TYPE_NORMAL
- en: This completes our control loop – from this point, the `Kube` scheduler takes
    over and our CRD is given life as a Kubernetes Job object, which controls a Pod,
    which is finally assigned to a Node and a container is scheduled to run our code!
  prefs: []
  type: TYPE_NORMAL
- en: This example is of course highly simplified, but you would be surprised how
    many Kubernetes operators perform a simple control loop to coordinate CRDs and
    boil them down to basic Kubernetes resources. Operators can get very complicated
    and perform application-specific functions such as backing up databases, emptying
    Persistent Volumes, and others – but this functionality is usually tightly coupled
    to whatever is being controlled.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discussed the Operator pattern in a Kubernetes controller, we
    can talk about some of the open source options for cloud-specific Kubernetes controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Using cloud-specific Kubernetes extensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually available by default in managed Kubernetes services such as Amazon EKS,
    Azure AKS, and Google Cloud's GKE, cloud-specific Kubernetes extensions and controllers
    can integrate tightly with the cloud platform in question and make it easy to
    control other cloud resources from Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Even without adding any additional third-party components, a lot of this cloud-specific
    functionality is available in upstream Kubernetes via the **cloud-controller-manager**
    (**CCM**) component, which contains many options for integrating with the major
    cloud providers. This is the functionality that is usually enabled by default
    in the managed Kubernetes services on each public cloud – but they can be integrated
    with any cluster running on that specific cloud platform, managed or not.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will review a few of the more common cloud extensions to
    Kubernetes, both in **cloud-controller-manager (CCM)** and functionality that
    requires the installation of other controllers such as **external-dns** and **cluster-autoscaler**.
    Let's start with some of the heavily used CCM functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the cloud-controller-manager component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As reviewed in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, CCM is an officially supported Kubernetes controller
    that provides hooks into the functionality of several public cloud services. To
    function, the CCM component needs to be started with access permissions to the
    cloud service in question – for instance, an IAM role in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: For officially supported clouds such as AWS, Azure, and Google Cloud, CCM can
    simply be run as a DaemonSet within the cluster. We use a DaemonSet since CCM
    can perform tasks such as creating persistent storage in the cloud provider, and
    it needs to be able to attach storage to specific Nodes. If you're using a cloud
    that isn't officially supported, you can run CCM for that specific cloud, and
    you should follow the specific instructions in that project. These alternate types
    of CCM are usually open source and can be found on GitHub. For the specifics of
    installing CCM, let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing cloud-controller-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, CCM is configured when the cluster is created. As mentioned in the
    previous section, managed services such as EKS, AKS, and GKE will already have
    this component enabled, but even Kops and Kubeadm expose the CCM component as
    a flag in the installation process.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you have not installed CCM any other way and plan to use one of the
    officially supported public clouds from the upstream version, you can install
    CCM as a DaemonSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you will need a `ServiceAccount`:'
  prefs: []
  type: TYPE_NORMAL
- en: Service-account.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This `ServiceAccount` will be used to give the necessary access to the CCM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll need a `ClusterRoleBinding`:'
  prefs: []
  type: TYPE_NORMAL
- en: Clusterrolebinding.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we need to give the `cluster-admin` role access to our CCM service
    account. The CCM will need to be able to edit Nodes, among other things.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can deploy the CCM `DaemonSet` itself. You will need to fill in
    this YAML file with the proper settings for your specific cloud provider – check
    your cloud provider's documentation on Kubernetes for this information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DaemonSet` spec is quite long, so we''ll review it in two parts. First,
    we have the template for the `DaemonSet` with the required labels and names:'
  prefs: []
  type: TYPE_NORMAL
- en: Daemonset.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, to match our `ServiceAccount`, we are running the CCM in the
    `kube-system` namespace. We are also labeling the `DaemonSet` with the `k8s-app`
    label to distinguish it as a Kubernetes control plane component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have the spec of the `DaemonSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: Daemonset.yaml (continued)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are a couple of places in this spec that you will need
    to review your chosen cloud provider's documentation or cluster networking setup
    to find the proper values. Particularly in the networking flags such as `--cluster-cidr`
    and `--configure-cloud-routes`, where values could change based on how you have
    set up your cluster, even on a single cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have CCM running on our cluster one way or another, let's dive into
    some of the capabilities it provides.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the cloud-controller-manager capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default CCM provides capabilities in a few key areas. For starters, the
    CCM contains subsidiary controllers for Nodes, routes, and Services. Let's review
    each in turn to see what it affords us, starting with the Node/Node lifecycle
    controller.
  prefs: []
  type: TYPE_NORMAL
- en: The CCM Node/Node lifecycle controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CCM Node controller makes sure that the cluster state, as far as which Nodes
    are in the cluster, is equivalent to what is in the cloud provider's systems.
    A simple example of this is autoscaling groups in AWS. When using AWS EKS (or
    just Kubernetes on AWS EC2, though that requires additional configuration), it
    is possible to configure worker node groups in an AWS autoscaling group that will
    scale up or down depending on the CPU or memory usage of the nodes. When these
    nodes are added and initialized by the cloud provider, the CCM nodes controller
    will ensure that the cluster has a node resource for each Node presented by the
    cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's move on to the routes controller.
  prefs: []
  type: TYPE_NORMAL
- en: The CCM routes controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CCM routes controller takes care of configuring your cloud provider's networking
    settings in a way that supports a Kubernetes cluster. This can include the allocation
    of IPs and setting routes between Nodes. The services controller also handles
    networking – but the external aspect.
  prefs: []
  type: TYPE_NORMAL
- en: The CCM services controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CCM services controller provides a lot of the "magic" of running Kubernetes
    on a public cloud provider. One such aspect that we reviewed in [*Chapter 5*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127),
    *Services and Ingress – Communicating with the Outside World*, is the `LoadBalancer`
    service. For instance, on a cluster configured with AWS CCM, a Service of type
    `LoadBalancer` will automatically configure a matching AWS Load Balancer resource,
    providing an easy way to expose services in your cluster without dealing with
    `NodePort` settings or even Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what the CCM provides, we can venture further and talk
    about a couple of the other cloud provider extensions that are often used when
    running Kubernetes on the public cloud. First, let's look at `external-dns`.
  prefs: []
  type: TYPE_NORMAL
- en: Using external-dns with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `external-dns` library is an officially supported Kubernetes add-on that
    allows the cluster to configure external DNS providers to provide DNS resolution
    for services and ingress in an automated fashion. The `external-dns` add-on supports
    a broad range of cloud providers such as AWS and Azure, and also other DNS services
    such as Cloudflare.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In order to install `external-dns`, you can check the official GitHub repository
    at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  prefs: []
  type: TYPE_NORMAL
- en: Once `external-dns` is implemented on your cluster, it's simple to create new
    DNS records in an automated fashion. To test `external-dns` with a service, we
    simply need to create a service in Kubernetes with the proper annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: service.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we only need to add an annotation for the `external-dns` controller
    to check, with the domain record to be created in DNS. The domain and hosted zone
    must of course be accessible by your `external-dns` controller – for instance,
    on AWS Route 53 or Azure DNS. Check the specific documentation on the `external-dns`
    GitHub repository for specifics.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Service is up and running, `external-dns` will pick up the annotation
    and create a new DNS record. This pattern is excellent for multi-tenancy or per-version
    deploys since with something like a Helm chart, variables can be used to change
    the domain depending on which version or branch of the application is deployed
    – for instance, `v1.myapp.mydomain.com`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Ingress, this is even easier – you just need to specify a host on your
    Ingress record, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: ingress.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This host value will automatically create a DNS record pointing to whatever
    method your Ingress is using – for instance, a Load Balancer on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's talk about how the **cluster-autoscaler** library works.
  prefs: []
  type: TYPE_NORMAL
- en: Using the cluster-autoscaler add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to `external-dns`, `cluster-autoscaler` is an officially supported add-on
    for Kubernetes that supports some major cloud providers with specific functionality.
    The purpose of `cluster-autoscaler` is to trigger the scaling of the number of
    Nodes in a cluster. It performs this process by controlling the cloud provider's
    own scaling resources, such as AWS autoscaling groups.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster autoscaler will perform an upward scaling action the moment any
    single Pod fails to schedule due to resource constraints on a Node, but only if
    a Node of the existing Node size (for instance, a `t3.medium` sized Node in AWS)
    would allow the Pod to be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the cluster autoscaler will perform a downward scaling action the
    moment any Node could be emptied of Pods without causing memory or CPU pressure
    on any of the other Nodes.
  prefs: []
  type: TYPE_NORMAL
- en: To install `cluster-autoscaler`, simply follow the correct instructions from
    your cloud provider, for the cluster type and intended version of the `cluster-autoscaler`.
    For instance, the AWS installation instructions for `cluster-autoscaler` on EKS
    are found at [https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at how you can find open and closed source extensions for Kubernetes
    by examining the Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with the ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes (and more generally, cloud-native) ecosystem is massive, consisting
    of hundreds of popular open source software libraries, and thousands more fledgling
    ones. This can be tough to navigate since every month brings new technologies
    to vet, and acquisitions, rollups, and companies going out of business can turn
    your favorite open source library into an unmaintained mess.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, there is some structure in this ecosystem, and it's worth knowing
    about it in order to help navigate the dearth of options in cloud-native open
    source. The first big structural component of this is the **Cloud Native Computing
    Foundation** or **CNCF**.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Cloud Native Computing Foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CNCF is a sub-foundation of the Linux Foundation, which is a non-profit
    entity that hosts open source projects and coordinates an ever-changing list of
    companies that contribute to and use open source software.
  prefs: []
  type: TYPE_NORMAL
- en: The CNCF was founded almost entirely to shepherd the future of the Kubernetes
    project. It was announced alongside the 1.0 release of Kubernetes and has since
    grown to encompass hundreds of projects in the cloud-native space – from Prometheus
    to Envoy to Helm, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to see an overview of the CNCF's constituent projects is to check
    out the CNCF Cloud Native Landscape, which can be found at [https://landscape.cncf.io/](https://landscape.cncf.io/).
  prefs: []
  type: TYPE_NORMAL
- en: The CNCF Landscape is a good place to start if you are interested in possible
    solutions to a problem you are experiencing with Kubernetes or cloud-native. For
    every category (monitoring, logging, serverless, service mesh, and others), there
    are several open source options to vet and choose from.
  prefs: []
  type: TYPE_NORMAL
- en: This is both a strength and weakness of the current ecosystem of cloud-native
    technologies. There are a significant number of options available, which makes
    the correct path often unclear, but also means that you will likely be able to
    find a solution that is close to your exact needs.
  prefs: []
  type: TYPE_NORMAL
- en: The CNCF also operates an official Kubernetes forum, which can be joined from
    the Kubernetes official website at [kubernetes.io](http://kubernetes.io). The
    URL of the Kubernetes forums is [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is relevant to mention *KubeCon*/*CloudNativeCon*, a large conference
    that is run by the CNCF and encompasses topics including Kubernetes itself and
    many ecosystem projects. *KubeCon* gets larger every year, with almost 12,000
    attendees for *KubeCon* *North America* in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about extending Kubernetes. First, we talked about
    CRDs – what they are, some relevant use cases, and how to implement them in your
    cluster. Next, we reviewed the concept of an operator in Kubernetes and discussed
    how to use an operator, or custom controller, to give life to your CRD.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discussed cloud-provider-specific extensions to Kubernetes including
    `cloud-controller-manager`, `external-dns`, and `cluster-autoscaler`. Finally,
    we wrapped up with an introduction to the cloud-native open source ecosystem at
    large and some great ways to discover projects for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: The skills you used in this chapter will help you extend your Kubernetes cluster
    to interface with your cloud provider as well as your own custom functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll talk about two nascent architectural patterns as
    applied to Kubernetes – serverless and service meshes.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between a served version and a stored version of a CRD?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are three typical parts of a custom controller or operator control loop?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does `cluster-autoscaler` interact with existing cloud provider scaling
    solutions such as AWS autoscaling groups?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNCF Landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Official Kubernetes Forums: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

- en: '*Chapter 15*: Stateful Workloads on Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter details the current state of the industry when it comes to running
    stateful workloads in databases. We will discuss the use of Kubernetes (and popular
    open source projects) for running databases, storage, and queues on Kubernetes.
    Case study tutorials will include running object storage, a database, and a queue
    system on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first understand how stateful applications run on Kubernetes
    and then learn how to use Kubernetes storage for stateful applications. We will
    then learn how to run databases on Kubernetes, as well as covering messaging and
    queues. Let's start with a discussion of why stateful applications are much more
    complex than stateless applications on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stateful applications on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kubernetes storage for stateful applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running databases on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing messaging and queues on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the kubectl tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used in this chapter can be found in the book''s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter15](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter15)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stateful applications on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides excellent primitives for running both stateless and stateful
    applications, but stateful workloads have taken longer to mature on Kubernetes.
    However, in recent years, some high-profile Kubernetes-based stateful application
    frameworks and projects have proven the increasing maturity of stateful applications
    on Kubernetes. Let's review some of these first in order to set the stage for
    the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Popular Kubernetes-native stateful applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many types of stateful applications. Though most applications are
    stateful, only certain components in those applications store *state* data. We
    can remove these specific stateful components from applications and focus on those
    components in our review. In this book, we'll talk about databases, queues, and
    object storage, leaving out persistent storage components such as those we reviewed
    in [*Chapter 7*](B14790_07_Final_PG_ePub.xhtml#_idTextAnchor166), *Storage on
    Kubernetes*. We'll also go over a few, less generic components as honorable mentions.
    Let's start with databases!
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes-compatible databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to typical **databases** (**DBs**) and key-value stores such as
    **Postgres**, **MySQL**, and **Redis** that can be deployed on Kubernetes with
    StatefulSets or community operators, there are some major made-for-Kubernetes
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CockroachDB**: A distributed SQL database that can be deployed seamlessly
    on Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vitess**: A MySQL sharding orchestrator that allows global scalability for
    MySQL, also installable on Kubernetes via an operator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YugabyteDB**: A distributed SQL database similar to **CockroachDB** that
    also supports Cassandra-like querying'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's look at queuing and messaging on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Queues, streaming, and messaging on Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Again, there are industry-standard options such as **Kafka** and **RabbitMQ**
    that can be deployed on Kubernetes using community Helm charts and operators,
    in addition to some purpose-made open- and closed-source options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NATS**: Open source messaging and streaming system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KubeMQ**: Kubernetes-native message broker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's look at object storage on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Object storage on Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Object storage takes volume-based persistent storage from Kubernetes and adds
    on an object storage layer, similar to (and in many cases compatible with the
    API of) Amazon S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minio**: S3-compatible object storage built for high performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open IO**: Similar to *Minio*, this has high performance and supports S3
    and Swift storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's look at a few honorable mentions.
  prefs: []
  type: TYPE_NORMAL
- en: Honorable mentions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the preceding generic components, there are some more specialized
    (but still categorical) stateful applications that can be run on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Key and auth management**: **Vault**, **Keycloak**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container registries**: **Harbor**, **Dragonfly**, **Quay**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow management**: **Apache Airflow** with a Kubernetes Operator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've reviewed a few categories of stateful applications, let's talk
    about how these state-heavy applications are typically implemented on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding strategies for running stateful applications on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Though there is nothing inherently wrong with deploying a stateful application
    on Kubernetes with a ReplicaSet or Deployment, you will find that the majority
    of stateful applications on Kubernetes use StatefulSets. We talked about StatefulSets
    in [*Chapter 4*](B14790_04_Final_PG_ePub.xhtml#_idTextAnchor106), *Scaling and
    Deploying Your Application*, but why are they so useful for applications? We will
    review and answer this question in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason is Pod identity. Many distributed stateful applications have
    their own clustering mechanism or consensus algorithm. In order to smooth over
    the process for these types of applications, StatefulSets provide static Pod naming
    based on an ordinal system, starting from `0` to `n`. This, in combination with
    a rolling update and creation method, makes it much easier for applications to
    cluster themselves, which is extremely important for cloud-native databases such
    as CockroachDB.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how and why StatefulSets can help run stateful applications on
    Kubernetes, let's look at how we might run MySQL on Kubernetes with StatefulSets.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to be clear, running a single Pod of MySQL on Kubernetes is extremely simple.
    All we need to do is find a MySQL container image and ensure that it has the proper
    configuration and `startup` command.
  prefs: []
  type: TYPE_NORMAL
- en: However, when we look to scale our database, we start to run into issues. Unlike
    a simple stateless application, where we can scale our deployment without creating
    new state, MySQL (like many other DBs) has its own method of clustering and consensus.
    Each member of a MySQL cluster knows about the other members, and most importantly,
    it knows which member of the cluster is the leader. This is how databases like
    MySQL can offer consistency guarantees and **Atomicity, Consistency, Isolation,
    Durability** (**ACID**) compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, since each member in a MySQL cluster needs to know about the other
    members (and most importantly, the master), we need to run our DB Pods in a way
    that means they have a common way to find and communicate with the other members
    of the DB cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The way that StatefulSets offer this is, as we mentioned at the beginning of
    the section, via ordinal Pod numbering. This way, applications that need to self-cluster
    while running on Kubernetes know that a common naming scheme starting from `0`
    to `n` will be used. In addition, when a Pod at a specific ordinal restarts –
    for instance, `mysql-pod-2` – the same PersistentVolume will be mounted to the
    new Pod that starts in that ordinal spot. This allows for stateful consistency
    between restarts for a single Pod in a StatefulSet, which makes it much easier
    for applications to form a stable cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To see how this works in practice, let's look at a StatefulSet specification
    for MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: Running MySQL on StatefulSets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following YAML spec is adapted from the Kubernetes documentation version.
    It shows how we can run MySQL clusters on StatefulSets. We will review each part
    of the YAML spec separately, so we can understand exactly how the mechanisms interact
    with StatefulSet guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the first part of the spec:'
  prefs: []
  type: TYPE_NORMAL
- en: statefulset-mysql.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are going to be creating a MySQL cluster with three `replicas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There isn''t much else exciting about this piece, so let''s move onto the start
    of `initContainers`. There will be quite a few containers running in this Pod
    between `initContainers` and regular containers, so we will explain each separately.
    What follows is the first `initContainer` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This first `initContainer`, as you can see, is the MySQL container image. Now,
    this doesn't mean that we won't have the MySQL container running constantly in
    the Pod. This is a pattern you will tend to see fairly often with complex applications.
    Sometimes the same container image is used as both an `initContainer` instance
    and a normally running container in a Pod. This is because that container has
    the correct embedded scripts and tools to do common setup tasks programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the MySQL `initContainer` creates a file, `/mnt/conf.d/server-id.cnf`,
    and adds a `server` ID, corresponding to the Pod's `ordinal` ID in the StatefulSet,
    to the file. When writing the `ordinal` ID, it adds `100` as an offset, to get
    around the reserved value in MySQL of a `server-id` ID of `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, depending on whether the Pod `ordinal` D is `0` or not, it copies configuration
    for either a master or slave MySQL server to the volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the second `initContainer` in the following section (we''ve
    left out some code with volume mount information for brevity, but the full code
    is available in the GitHub repository of the book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this `initContainer` isn't MySQL at all! Instead, the container
    image is a tool called Xtra Backup. Why do we need this container?
  prefs: []
  type: TYPE_NORMAL
- en: Consider a situation where a brand-new Pod, with a brand-new, empty PersistentVolume
    joins the cluster. In this scenario, the data replication processes will need
    to copy all of the data via replication from the other members in the MySQL cluster.
    With large databases, this process could be exceedingly slow.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we have an `initContainer` instance that loads in data from
    another MySQL Pod in the StatefulSet, so that the data replication capabilities
    of MySQL have something to start with. In a case where there is already data in
    the MySQL Pod, this loading of data does not occur. The `[[ -d /var/lib/mysql/mysql
    ]] && exit 0` line is the one that checks to see whether there is existing data.
  prefs: []
  type: TYPE_NORMAL
- en: Once these two `initContainer` instances have successfully completed their tasks,
    we have all our MySQL configuration courtesy of the first `initContainer`, and
    we have a somewhat recent set of data from another member in the MySQL StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s move on to the actual containers in the StatefulSet definition,
    starting with MySQL itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this MySQL container setup is fairly basic. In addition to an
    environment variable, we mount the previously created configuration. This pod
    also has some liveness and readiness probe configuration – check the GitHub repository
    of this book for those.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s move on and check out our final container, which will look familiar
    – it''s actually another instance of Xtra Backup! Let''s see how it is configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This container setup is a bit complex, so let's review it section by section.
  prefs: []
  type: TYPE_NORMAL
- en: We know from our `initContainers` that Xtra Backup loads in data from another
    Pod in the StatefulSet in order to get the Pod somewhat ready for replicating,
    to and from other members in the StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: The Xtra Backup container in this case is the one that actually starts that
    replication! This container will first check to see whether the Pod it is running
    on is supposed to be a slave Pod in the MySQL cluster. If so, it will start a
    data replication process from the master.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the Xtra Backup container will also open a listener on port `3307`,
    which will send a clone of the data in the Pod, if requested. This is the setup
    that sends clone data to the other Pods in the StatefulSet when they request a
    clone. Remember that the first `initContainer` looks at other Pods in the StatefulSet,
    in order to get a clone. In the end, each Pod in the StatefulSet is able to request
    clones in addition to running a process that can send data clones to other Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to wrap up our spec, let''s look at `volumeClaimTemplate`. This section
    of the spec also lists volume mounts for the previous container and the volume
    setup for the Pod (but we''ve left that out for brevity. Check the GitHub repository
    of this book for the rest):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there's nothing especially interesting about the volume setup
    for the last container or the volume list. However, it's worthwhile to note the
    `volumeClaimTemplates` section, because the data will remain the same as long
    as a Pod restarts at the same ordinal spot. A new Pod added to the cluster will
    instead start with a blank PersistentVolume, which will trigger the initial data
    clone.
  prefs: []
  type: TYPE_NORMAL
- en: All together, these features of StatefulSets, in combination with the correct
    configuration of Pods and tooling, allow for the easy scaling of a stateful DB
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've talked about why stateful Kubernetes applications may use StatefulSets,
    let's go ahead and implement some to prove it! We'll start with an object storage
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying object storage on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object storage is different from filesystem or block storage. It presents a
    higher-level abstraction that encapsulates a file, gives it an identifier, and
    often includes versioning. The file can then be accessed via its specific identifier.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular object storage service is probably AWS S3, but Azure Blob Storage
    and Google Cloud Storage are similar alternatives. In addition, there are several
    self-hosted object storage technologies that can be run on Kubernetes, which we
    reviewed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: For this book, we will review the configuration and usage of **Minio** on Kubernetes.
    Minio is an object storage engine that emphasizes high performance and can be
    deployed on Kubernetes, in addition to other orchestration technologies such as
    **Docker Swarm** and **Docker Compose**.
  prefs: []
  type: TYPE_NORMAL
- en: Minio supports Kubernetes deployments using both an operator and a Helm chart.
    In this book, we will focus on the operator, but for more information on the Helm
    chart, check out the Minio docs at [https://docs.min.io/docs](https://docs.min.io/docs).
    Let's get started with the Minio Operator, which will let us review some cool
    community extensions to kubectl.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Minio Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Installing the Minio Operator will be quite different from anything we have
    done so far. Minio actually provides a `kubectl` plugin in order to manage the
    installation and configuration of the operator and Minio as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: We haven't spoken much about `kubectl` plugins in this book, but they are a
    growing part of the Kubernetes ecosystem. `kubectl` plugins can provide additional
    functionality in the form of new `kubectl` commands.
  prefs: []
  type: TYPE_NORMAL
- en: In order to install the `minio` kubectl plugin, we use Krew, which is a plugin
    manager for `kubectl` that makes it easy to search and add `kubectl` plugins with
    a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Krew and the Minio kubectl plugin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So first, let''s install Krew. The installation process varies depending on
    your OS and environment, but for macOS, it looks like the following (check out
    the Krew docs at [https://krew.sigs.k8s.io/docs](https://krew.sigs.k8s.io/docs)
    for more information):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s install the Krew CLI tool with the following Terminal commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can add Krew to our `PATH` variable with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In a new shell, we can now start using Krew! Krew is accessed using `kubectl
    krew` commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the Minio kubectl plugin, you can run the following `krew` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, with the Minio kubectl plugin installed, let's look at getting Minio set
    up on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the Minio Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First off, we need to actually install the Minio Operator on our cluster. This
    deployment will control all the Minio tasks that we need to do later:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can install the Minio Operator using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether the Minio Operator is ready to go, let''s check on our Pods
    with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the Minio Operator Pod running in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We now have the Minio Operator running properly on Kubernetes. Next up, we can
    create a Minio tenant.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Minio tenant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to create a **tenant**. Since Minio is a multi-tenant system,
    each tenant has its own namespace separation for buckets and objects, in addition
    to separate PersistentVolumes. Additionally, the Minio Operator starts Minio in
    Distributed Mode with a highly available setup and data replication.
  prefs: []
  type: TYPE_NORMAL
- en: Before creating our Minio tenant, we need to install a **Container Storage Interface**
    (**CSI**) driver for Minio. CSI is a standardized way to interface between storage
    providers and containers – and Kubernetes implements CSI in order to allow third-party
    storage providers to write their own drivers for seamless integration to Kubernetes.
    Minio recommends the Direct CSI driver in order to manage PersistentVolumes for
    Minio.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the Direct CSI driver, we need to run a `kubectl apply` command
    with Kustomize. However, the Direct CSI driver installation requires some environment
    variables to be set in order to create the Direct CSI configuration with the proper
    configuration, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s go ahead and create this environment file based on the Minio
    recommendations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: default.env
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this environment file determines where the Direct CSI driver
    will mount volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve created `default.env`, let''s load these variables into memory
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s install the Direct CSI driver with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we go ahead and create our Minio tenant, let''s check to see whether
    our CSI Pods started up properly. Run the following command to check:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to the following if the CSI Pods have started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now with our CSI driver installed, let''s create our Minio tenant – but first,
    let''s take a look at the YAML that the `kubectl minio tenant create` command
    generates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to directly create the Minio tenant without examining the YAML,
    use the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This command will just create the tenant without showing you the YAML first.
    However, since we are using the Direct CSI implementation, we will need to update
    the YAML. So, using just the command will not work. Let's take a look at the generated
    YAML file now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t look at the file in its entirety for space reasons, but let''s look
    at some parts of the `Tenant` **Custom Resource Definition** (**CRD**), which
    the Minio Operator will use to create the necessary resources to host our Minio
    tenant. First, let''s look at the upper portion of the spec, which should look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: my-minio-tenant.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this file specifies an instance of the `Tenant` CRD. This first
    part of our spec has two containers specified, a container for the Minio console
    and one for the Minio `server` itself. In addition, the `replicas` value mirrors
    what we specified in our `kubectl minio tenant create` command. Finally, it specifies
    the name of a secret for the Minio `console`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the bottom portion of the Tenant CRD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `Tenant` resource specifies a number of servers (also specified
    by the `creation` command) that matches the number of replicas. It also specifies
    the name of the internal Minio Service, as well as a `volumeClaimTemplate` instance
    to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'This spec, however, does not work for our purposes, since we are using the
    Direct CSI. Let''s update the `zones` key with a new `volumeClaimTemplate` that
    uses the Direct CSI, as follows (save this file as `my-updated-minio-tenant.yaml`).
    Here''s just the `zones` portion of that file, which we updated:'
  prefs: []
  type: TYPE_NORMAL
- en: my-updated-minio-tenant.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now go ahead and create our Minio tenant! We can do this using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the Minio Operator will start creating the necessary resources
    for our new Minio tenant, and after a couple of minutes, you should see some Pods
    start up in addition to the operator, which will look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Minio Pods output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14790_15_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.1 – Minio Pods output
  prefs: []
  type: TYPE_NORMAL
- en: We now have our Minio tenant completely up and running! Next, let's take a look
    at the Minio console to see how our tenant looks.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Minio console
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, in order to get the login information for the console, we will need to
    fetch the content of two keys, which are kept in the autogenerated `<TENANT NAME>-console-secret`
    secret.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fetch the `access` key and the `secret` key (which in our case will be autogenerated)
    for the console, let''s use the two following commands. In our case, we use our
    `my-tenant` tenant to get the `access` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And to get the `secret` key, we use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, our Minio console will be available on a service, `<TENANT NAME>-console`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s access this console using a `port-forward` command. In our case, this
    will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Our Minio console will then be available at `https://localhost:8081` on your
    browser. You will need to accept the browser security warning since we haven't
    set up TLS certificates for the console for localhost in this example. Put in
    the `access` key and `secret` key you got from the previous steps to log in!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we're logged into the console, we can start adding to our Minio tenant.
    First, let's create a bucket. To do this, click **Buckets** on the left sidebar,
    then click the **Create Bucket** button.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the popup, enter the name of the bucket (in our case, we will use `my-bucket`)
    and submit the form. You should see a new bucket in the list – see the following
    screenshot for an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – Bucket](image/B14790_15_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – Bucket
  prefs: []
  type: TYPE_NORMAL
- en: We now have our distributed Minio setup ready, together with a bucket to upload
    to. Let's wrap this example up by uploading a file to our brand-new object storage
    system!
  prefs: []
  type: TYPE_NORMAL
- en: We're going to do this upload using the Minio CLI, which makes the process of
    interacting with S3-compatible storage such as Minio much easier. Instead of using
    the Minio CLI from our local machine, we will run a container image preloaded
    with the Minio CLI from within Kubernetes, since the TLS setup will only work
    when accessing Minio from within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need to fetch the Minio `access` key and `secret`, which are
    different from the console `access` key and `secret` we fetched earlier. To get
    these keys, run the following console commands (in our case, our tenant is `my-tenant`).
    First, get the `access` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, get the `secret` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s start up that pod with the Minio CLI. To do this, let''s use this
    Pod spec:'
  prefs: []
  type: TYPE_NORMAL
- en: minio-mc-pod.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Create this Pod using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to `exec` into this `minio-mc` Pod, we run the usual `exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s configure access for our newly created Minio distributed cluster
    in the Minio CLI. We can do this with the following command (the `--insecure`
    flag is required in this config):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The Pod IP for this command can be the IP for either of our tenant Minio Pods
    – in our case, these are `my-tenant-zone-0-0` and `my-tenant-zone-0-1`. Once you
    run this command, you will be prompted for the access key and secret key. Enter
    them, and you will see a confirmation message if successful, which will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to test that the CLI configuration is working, we can create another test
    bucket using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As a final test of our setup, let's upload a file to our Minio bucket!
  prefs: []
  type: TYPE_NORMAL
- en: First, still on the `minio-mc` Pod, create a text file named `test.txt`. Fill
    the file with whatever text you'd like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s upload it to our recently created bucket using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You should see a loading bar with the upload, which should end with the entire
    file size as uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'As one last check, go to the **Dashboard** page on the Minio console and see
    whether the object shows up, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – Dashboard](image/B14790_15_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our file was successfully uploaded!
  prefs: []
  type: TYPE_NORMAL
- en: That's it for Minio – there is a lot more you can do in terms of configuration,
    but that is outside the scope of this book. Check the docs at [https://docs.min.io/](https://docs.min.io/)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, let's look at running DBs on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Running DBs on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've taken a look at object storage workloads on Kubernetes, we can
    move on to databases. As we've discussed previously in this chapter and elsewhere
    in the book, many databases support running on Kubernetes, with varying levels
    of maturity.
  prefs: []
  type: TYPE_NORMAL
- en: First off, there are several legacy and existing DB engines that support deploying
    to Kubernetes. Often, these engines will have supported Helm charts or operators.
    For instance, SQL databases such as PostgreSQL and MySQL have Helm charts and
    operators supported by various different organizations. NoSQL databases such as
    MongoDB also have supported ways to deploy to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these previously existing database engines, container orchestrators
    such as Kubernetes have lead to the creation of a new category – the **NewSQL**
    database.
  prefs: []
  type: TYPE_NORMAL
- en: These databases offer the incredible scalability of NoSQL databases in addition
    to SQL-compliant APIs. They can be thought of as a way to easily scale SQL on
    Kubernetes (and other orchestrators). CockroachDB is a popular choice here, as
    is **Vitess**, which isn't so much a replacement NewSQL database as it is a way
    to easily scale the MySQL engine.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on deploying CockroachDB, which is a modern NewSQL
    database built for distributed environments and perfect for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Running CockroachDB on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run CockroachDB on our cluster, we will use the official CockroachDB Helm
    chart:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is to add the CockroachDB Helm chart repository,
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we install the chart, let''s create a custom `values.yaml` file in order
    to tweak some of the default settings for CockroachDB. Our file for this demo
    looks like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cockroach-db-values.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are specifying a PersistentVolume size of `2` GB, Pod memory
    limits and requests of `1` GB, and the contents of a configuration file for CockroachDB.
    This configuration file includes settings for `cache` and max `memory`, which
    are set to 25% of the size of the memory limits at `256` MB. This ratio is a CockroachDB
    best practice. Keep in mind that these are not all production-ready settings,
    but they will work for our demo.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, let''s go ahead and create our CockroachDB cluster using the
    following Helm command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If successful, you will see a lengthy deploy message from Helm, which we will
    not reproduce here. Let''s check to see exactly what was deployed on our cluster
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have three Pods in a StatefulSet in addition to a setup Pod
    that was used for some initialization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to check to see whether our cluster is functional, we can use a command
    that is handily given to us in the CockroachDB Helm chart output (it will vary
    depending on your Helm release name):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If successful, a console will be opened with a prompt similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will test CockroachDB with SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Testing CockroachDB with SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we can run SQL commands to our new CockroachDB database!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a database with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a simple table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s add some data with this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s confirm the data using this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'That should give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Success!
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have a fully functional distributed SQL database. Let''s
    move on to the final stateful workload type that we will review: messaging.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing messaging and queues on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For messaging, we will be implementing RabbitMQ, an open source message queue
    system that supports Kubernetes. Messaging systems are typically used in applications
    to decouple various components of the application in order to support the scale
    and throughput, as well as asynchronous patterns such as retries and service worker
    fleets. For instance, instead of one service calling another service directly,
    a service could place a message onto a persistent message queue, at which point
    it would be picked up by a worker container that is listening to the queue. This
    allows for easy horizontal scaling and greater tolerance of entire component downtime
    as compared to a load balancing approach.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ is one of many options for message queues. As we mentioned in the first
    section of the chapter, RabbitMQ is an industry-standard option for message queues,
    not necessarily a queue system built for Kubernetes specifically. However, it's
    still a great choice and very easy to deploy, as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with implementing RabbitMQ on Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying RabbitMQ on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing RabbitMQ on Kubernetes can be easily done via an operator or via
    a Helm chart. For the purposes of this tutorial, we will use the Helm chart:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s add the proper `helm` repository (provided by **Bitnami**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a custom values file to tweak some parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Values-rabbitmq.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, in this case, we are disabling persistence, which is great for
    a quick demo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, RabbitMQ can easily be installed on the cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Once successful, you will see a confirmation message from Helm. The RabbitMQ
    Helm chart also includes a management UI, so let's use that to validate that our
    installation worked.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s start a port forward to the `rabbitmq` service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we should be able to access the RabbitMQ management UI on `http://localhost:15672`.
    It will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – RabbitMQ management console login](image/B14790_15_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – RabbitMQ management console login
  prefs: []
  type: TYPE_NORMAL
- en: Now, we should be able to log in to the dashboard using the username and password
    specified in the values file. Upon login, you will see the RabbitMQ dashboard
    main view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Importantly, you will see a list of the nodes in your RabbitMQ cluster. In
    our case, we only have a single node, which will display as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5 – RabbitMQ management console node item](image/B14790_15_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.5 – RabbitMQ management console node item
  prefs: []
  type: TYPE_NORMAL
- en: For each node, you can see the name and some metadata, including memory, uptime,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: In order to add a new queue navigate to **Queues** on the top bar, click **Add
    a new queue** toward the bottom of the screen. Fill out the form as follows, then
    click **Add queue**:![Figure 15.6 – RabbitMQ management console queue creation](image/B14790_15_006.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 15.6 – RabbitMQ management console queue creation
  prefs: []
  type: TYPE_NORMAL
- en: If successful, the screen should refresh with your new queue added to the list.
    This means our RabbitMQ setup is working properly!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, now that we have a queue, we can publish a message to it. To do this,
    click on your newly created queue on the **Queues** page, then click **Publish
    Message**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write any text in the **Payload** text box and click **Publish Message**. You
    should see a confirmation popup telling you that your message has been published
    successfully, and the screen should refresh, showing your message on the queue,
    as shown in the following figure:![Figure 15.7 – RabbitMQ management console queue
    status](image/B14790_15_007.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 15.7 – RabbitMQ management console queue status
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to emulate fetching messages from the queue, click on **Get messages**
    near the bottom of the page, which should expand to show a new section, and then
    click the **Get Message(s)** button. You should see an output of the message you
    sent, proving that the queue system works!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about running stateful workloads on Kubernetes.
    First, we reviewed a high-level overview of some of the types of stateful workloads
    and some examples of each. Then, we moved on to actually deploying one of these
    workloads – an object storage system – on Kubernetes. Next, we did the same with
    a NewSQL database, CockroachDB, showing you how to easily deploy a CockroachDB
    cluster on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we showed you how to deploy the RabbitMQ message queue on Kubernetes
    using a Helm chart. The skills you used in this chapter will help you deploy and
    use popular stateful application patterns on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: If you've made it this far, thanks for sticking with us through all 15 chapters
    of this book! I hope that you have learned how to use a broad spectrum of Kubernetes
    functionality and that you now have all the tools you need in order to build and
    deploy complex applications on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What cloud storage offering is Minio's API compatible with?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the benefits of a StatefulSet for a distributed database?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In your words, what makes stateful applications difficult to run on Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Minio Quickstart Documentation: [https://docs.min.io/docs/minio-quickstart-guide.html](https://docs.min.io/docs/minio-quickstart-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CockroachDB Kubernetes Guide: [https://www.cockroachlabs.com/docs/v20.2/orchestrate-a-local-cluster-with-kubernetes](https://www.cockroachlabs.com/docs/v20.2/orchestrate-a-local-cluster-with-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

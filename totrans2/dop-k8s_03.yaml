- en: Getting Started with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve learned the benefits that containers can bring us, but what if we need
    to scale out our services for business needs? Is there a way to build services
    across multiple machines without dealing with cumbersome network and storage settings?
    Also, is there any other easy way to manage and roll out our microservices by
    different service cycle? That''s how Kubernetes comes into play. In this chapter,
    we''ll learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes concept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes resources and their configuration file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to launch the kiosk application by Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a platform for managing application containers across multiple
    hosts. It provides lots of management features for container-oriented applications,
    such as auto scaling, rolling deployment, compute resource, and volume management.
    Same as the nature of containers, it's designed to run anywhere, so we're able
    to run it on a bare metal, in our data center, on the public cloud, or even hybrid
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes considers most of the operational needs for application containers.
    The highlights are:'
  prefs: []
  type: TYPE_NORMAL
- en: Container deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container health monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto-scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability by cluster federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes is a perfect match for microservices. With Kubernetes, we can create
    a `Deployment` to rollout, rollover, or roll back selected containers ([Chapter
    7](part0163.html#4REBM0-6c8359cae3d4492eb9973d94ec3e4f1e), *Continous Delivery*).
    Containers are considered as ephemeral. We can mount the volume into a container
    to preserve the data in a single host world. In the cluster world, a container
    might be scheduled to run on any host. How do we make the volume mounting work
    as permanent storage seamlessly? Kubernetes **Volumes** and **Persistent Volumes**
    are introduced to solve that problem ([Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources*). The lifetime of containers might be short.
    They may be killed or stopped anytime when they exceed the limit of resource,
    how do we ensure our services always serve a certain number of containers? **ReplicationController**
    or **ReplicaSet** in Kubernetes will ensure a certain number of group of containers
    are up. Kubernetes even supports **liveness probe** to help you define your application
    health. For better resource management, we can also define the maximum capacity
    on Kubernetes nodes and the resource limit for each group of containers (a.k.a
    **pod**). Kubernetes scheduler will then select a node that fulfills the resource
    criteria to run the containers. We'll learn this in [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources*. Kubernetes provides an optional horizontal
    pod auto-scaling feature. With this feature, we could scale a pod horizontally
    by resource or custom metrics. For those advanced readers, Kubernetes is designed
    with high availability (**HA**). We are able to create multiple master nodes from
    preventing single point of failure.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes includes two major players:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masters**: The Master is the heart of Kubernetes, which controls and schedules
    all the activities in the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**: Nodes are the workers that run our containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The master includes the API server, Controller Manager, scheduler, and etcd.
    All the components can run on different hosts with clustering. However, from a
    learning perspective, we'll make all the components run on the same node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00032.jpeg)Master components'
  prefs: []
  type: TYPE_NORMAL
- en: API server (kube-apiserver)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The API server provides an HTTP/HTTPS server, which provides a RESTful API for
    all the components in the Kubernetes master. For example, we could GET resource
    status, such as pod, POST to create a new resource and also watch a resource.
    API server reads and updates etcd, which is Kubernetes' backend data store.
  prefs: []
  type: TYPE_NORMAL
- en: Controller Manager (kube-controller-manager)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Controller Manager controls lots of different things in the cluster. Replication
    Controller Manager ensures all the ReplicationControllers run on the desired container
    amount. Node Controller Manager responds when the nodes go down, it will then
    evict the pods. Endpoint Controller is used to associate the relationship between
    services and pods. Service Account and Token Controller are used to control default
    account and API access tokens.
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: etcd is an open source distributed key-value store ([https://coreos.com/etcd](https://coreos.com/etcd)).
    Kubernetes stores all the RESTful API objects here. etcd is responsible for storing
    and replicating data.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler (kube-scheduler)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scheduler decides which node is suitable for pods to run on, according to the
    resource capacity or the balance of the resource utilization on the node. It also
    considers spreading the pods in the same set to different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Node components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Node components need to be provisioned and run on every node, which report the
    runtime status of the pod to the master.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00033.jpeg)Node components'
  prefs: []
  type: TYPE_NORMAL
- en: Kubelet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubelet is a major process in the nodes, which reports node activities back
    to kube-apiserver periodically, such as pod health, node health, and liveness
    probe. As the preceding graph shows, it runs containers via container runtimes,
    such as Docker or rkt.
  prefs: []
  type: TYPE_NORMAL
- en: Proxy (kube-proxy)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proxy handles the routing between pod load balancer (a.k.a. **service**) and
    pods, it also provides the routing from outside to service. There are two proxy
    modes, userspace and iptables. Userspace mode creates large overhead by switching
    kernel space and user space. Iptables mode, on the other hand, is the latest default
    proxy mode. It changes iptables **NAT** in Linux to achieve routing TCP and UDP
    packets across all containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described in [Chapter 2](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *DevOps with Container*, Docker is a container implementation. Kubernetes uses
    Docker as a default container engine.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction between Kubernetes master and nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following graph, the client uses **kubectl** to send requests to the
    API server; API server responds to the request, pushes and pulls the object information
    from etcd. Scheduler determines which node should be assigned to do the tasks
    (for example, run pods). **Controller Manager** monitors the running tasks and
    responds if any undesired state occurs. On the other hand, the **API server**
    fetches the logs from pods by kubelet, and is also a hub between other master
    components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00034.jpeg)Interaction between master and nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to set up a small single-node cluster at
    the start. Then we'll get to learn how to interact with Kubernetes via its command-line
    tool--kubectl. We will go through all the important Kubernetes API objects and
    their expression in YAML format, which is the input to kubectl, then kubectl will
    send the request to the API server accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to start is running minikube ([https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube)),
    which is a tool to run Kubernetes on a single node locally. It supports to run
    on Windows, Linux, and macOS. In the following example, we'll run on macOS. Minikube
    will launch a VM with Kubernetes installed. Then we'll be able to interact with
    it via kubectl.
  prefs: []
  type: TYPE_NORMAL
- en: Note that minikube is not suitable for production or any heavy load environment.
    There are some limitations by its single node nature. We'll learn how to run a
    real cluster in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* and [Chapter 10](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on GCP* instead.
  prefs: []
  type: TYPE_NORMAL
- en: Before installing minikube, we'll have to install Homebrew ([https://brew.sh/](https://brew.sh/))
    and VirtualBox ([https://www.virtualbox.org/](https://www.virtualbox.org/)) first.
    Homebrew is a useful package manager in macOS. We can easily install Homebrew
    via the `/usr/bin/ruby -e "$(curl -fsSL [https://raw.githubusercontent.com/Homebrew/install/master/install)](https://raw.githubusercontent.com/Homebrew/install/master/install))"`
    command, and download VirtualBox from the Oracle website and click to install
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then it''s time to start! We can install minikube via `brew cask install minikube`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After minikube is installed, we now can start the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will launch a Kubernetes cluster locally. At the time of writing, the latest
    version is `v.1.6.4` minikube. Proceed to start a VM named minikube in VirtualBox.
    Then it will be setting up `kubeconfig`, which is a configuration file to define
    the context and authentication settings of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `kubeconfig`, we''re able to switch to different clusters via the `kubectl`
    command. We could use the `kubectl config view` command to see current settings
    in `kubeconfig`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here we know we're currently using minikube context with the same name of cluster
    and user. Context is a combination of authentication information and cluster connection
    information. You could use `kubectl config use-context $context` to force switch
    the context if you have more than one context.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we''ll need to enable `kube-dns` addon in minikube. `kube-dns`
    is a DNS service in Kuberentes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kubectl` is the command to control Kubernetes cluster manager. The most general
    usage is to check the version of cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We then know our server version is up to date, which is the latest at the time
    of writingâ€”version 1.6.4\. The general syntax of `kubectl` is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `command` indicates the operation you want to perform. If you just type
    `kubectl help` in the Terminal, it will show the supported commands. `type` means
    the resource type. We'll learn major resource types in the next section. `name`
    is how we name our resources. It's always good practice to have clear and informational
    naming along the way. For the `flags`, if you type `kubectl options`, it will
    show all the flags you could pass on.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` comes in handy and we could always add `--help` to get more detailed
    information for the specific command. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We then get the full supported option in the `kubectl logs` command.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes objects are the entries in the cluster, which are stored in etcd.
    They represent the desired state of your cluster. When we create an object, we
    send the request to API Server by kubectl or RESTful API. API Server will store
    the state into etcd and interact with other master components to ensure the object
    exists. Kubernetes uses namespace to isolate the objects virtually, according
    to different teams, usages, projects, or environments. Every object has its own
    name and unique ID. Kubernetes also supports labels and annotation to let us tag
    our objects. Labels especially could be used to group the objects together.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object spec describes the desired state of Kubernetes objects. Most of the time,
    we write an object spec, and send the spec to the API Server via kubectl. Kubernetes
    will try to fulfill that desired state and update object status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Object spec could be written in YAML ([http://www.yaml.org/](http://www.yaml.org/))
    or JSON ([http://www.json.org/](http://www.json.org/)[)](http://www.json.org/)).
    YAML is more common in the Kubernetes world. We''ll use YAML format to write object
    specs in the rest of this book. The following code block shows a YAML-formatted
    spec fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Namespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes namespace is considered to be an isolation as multiple virtual clusters.
    Objects in different namespaces are invisible to each other. This is useful when
    different teams or projects are sharing the same cluster. Most of the resources
    are under a namespace (a.k.a. namespaced resources); however, some generic resources,
    such as nodes or namespace itself, don''t belong to any namespace. Kubernetes
    has three namespaces by default:'
  prefs: []
  type: TYPE_NORMAL
- en: default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kube-system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kube-public
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without explicitly assigning namespace to the namespaced resource, it will be
    located in the namespace under current context. If we never add a new namespace,
    a default namespace will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Kube-system namespaces are used by the objects created by the Kubernetes system,
    such as addon, which are the pods or services that implement cluster features,
    such as dashboard. Kube-public namespaces are newly introduced in Kubernetes 1.6,
    which is used by a beta controller manager (BootstrapSigner [https://kubernetes.io/docs/admin/bootstrap-tokens](https://kubernetes.io/docs/admin/bootstrap-tokens)),
    putting the signed cluster location information into the `kube-public` namespace,
    so this information could be visible to authenticated/unauthenticated users.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, all the namespaced resources will be located in a
    default namespace. Namespace is also very important for resource management and
    role. We'll introduce more in [Chapter 8](part0188.html#5J99O0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Cluster Administration*.
  prefs: []
  type: TYPE_NORMAL
- en: Name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every object in Kubernetes owns its own name. Object name in one resource is
    uniquely identified within the same namespace. Kubernetes uses object name as
    part of a resource URL to API Server, so it must be the combination of lower case
    of alphanumeric characters, dash and dot, less than 254 characters. Besides object
    name, Kubernetes also assigns a unique ID (UID) to every object to distinguish
    historical occurrences of similar entities.
  prefs: []
  type: TYPE_NORMAL
- en: Label and selector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Labels are a set of key/pair values, used to attach to objects. Labels are
    designed to specify meaningful, identifying information for the object. Common
    usage is micro-service name, tier, environment, and software version. Users could
    define meaningful labels that could be used with selector later. Labels syntax
    in object spec is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Along with label, label selector is used to filter the set of objects. Separated
    by commas, multiple requirements will be joined by the `AND` logical operator.
    There are two ways to filter:'
  prefs: []
  type: TYPE_NORMAL
- en: Equality-based requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set-based requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Equality-based requirement supports the operator of `=`, `==`, and `!=`. For
    example, if selector is `chapter=2,version!=0.1`, the result will be **object
    C**. If requirement is `version=0.1`, the result will be **object A** and **object
    B**. If we write the requirement in supported object spec, it''ll be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00035.jpeg)Selector example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set-based requirement supports `in`, `notin`, and `exists` (for key only).
    For example, if requirement is `chapter in (3, 4),version`, then object A will
    be returned. If requirement is `version notin (0.2), !author_info`, the result
    will be **object A** and **object B**. The following is an example if we write
    to the object spec that supports set-based requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The requirements of `matchLabels` and `matchExpressions` are combined together.
    It means the filtered objects need to be true on both requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn along the way in this chapter with ReplicationController, Service,
    ReplicaSet, and Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Annotation is a set of user-specified key/value pairs, used for specifying
    non-identifying metadata. With annotation acts such as normal tagging, for example,
    a user could add timestamp, commit hash, or build number to annotation. Some of
    the kubectl commands support the `--record` option to record the commands that
    make the changes to the objects to the annotation. Another use case of annotation
    is storing the configuration, such as Kubernetes Deployments ([https://kubernetes.io/docs/concepts/workloads/controllers/deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment))
    or Critical Add-On pods ([https://coreos.com/kubernetes/docs/latest/deploy-addons.html](https://coreos.com/kubernetes/docs/latest/deploy-addons.html)).
    Annotation syntax is as follows in the metadata section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Namespace, name, label, and annotation are located in the metadata section of
    object spec. Selector is located in the spec section of selector-supported resources,
    such as ReplicationController, service, ReplicaSet, and Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pod is the smallest deployable unit in Kubernetes. It can contain one or more
    containers. Most of the time, we just need one container per pod. In some special
    cases, more than one container is included in the same pod, such as Sidecar containers
    ([http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html](http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html)).
    The containers in the same pod run in a shared context, on the same node, sharing
    the network namespace and shared volumes. Pod is also designed as mortal. When
    a pod dies for some reasons, such as getting killed by Kubernetes controller when
    lacking resources, it won't recover by itself. Instead, Kubernetes uses controllers
    to create and manage the desired state of pods for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could use `kubectl explain <resource>` to get the detailed description for
    the resource by command line. It will show up the fields that the resource supports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we''ll show how to create two containers in a pod,
    and demonstrate how they access each other. Please note that it''s neither a meaningful
    nor classic Sidecar pattern example. Those are used in very specific scenarios.
    The following is just an example of how we access other containers within a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00036.jpeg)Containers inside a Pod are visible via localhost'
  prefs: []
  type: TYPE_NORMAL
- en: This spec will create two containers, `web` and `centos`. Web is a nginx container
    ([https://hub.docker.com/_/nginx/](https://hub.docker.com/_/nginx/)). Expose container
    port `80` by default, since centos shares the same context with nginx, when doing
    curl in [http://localhost:80/](http://localhost:80/), it should be able to access
    nginx.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, using the `kubectl create` command to launch the pod `-f` option lets
    kubectl know using the data in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Adding `--record=true` at the end of the `kubectl` command when we create the
    resources. Kubernetes will add the latest command while creating or updating this
    resource. Therefore, we won't forget which resources are created by which spec.
  prefs: []
  type: TYPE_NORMAL
- en: We could use the `kubectl get <resource>` command to get the current status
    of the object. In this case, we use the `kubectl get pods` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `--namespace=$namespace_name` could access the object in different namespaces.
    The following is an example to check the pods in the `kube-system` namespace,
    which is used by system-type pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`# kubectl get pods --namespace=kube-system`'
  prefs: []
  type: TYPE_NORMAL
- en: '`NAME READY STATUS RESTARTS AGE`'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-addon-manager-minikube 1/1 Running 2 3d`'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-dns-196007617-jkk4k 3/3 Running 3 3d`'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubernetes-dashboard-3szrf 1/1 Running 1 3d`'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the objects have their short names, which come in handy when we use
    `kubectl get <object>` to list their status. For example, pods could be called
    po, services could be called svc, and deployment could be called deploy. Type
    `kubectl get` to know more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The status of our example pod is `ContainerCreating`. In this phase, Kubernetes
    has accepted the request, trying to schedule the pod and pulling down the image.
    Zero containers are currently running. After waiting a moment, we could get the
    status again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see two containers are currently running. Uptime is three seconds. Using
    `kubectl logs <pod_name> -c <container_name>` could get `stdout` for the container,
    similar to `docker logs <container_name>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Centos in the pod shares the same networking with nginx via localhost! Kubernetes
    creates a network container along with the pod. One of the functions in the network
    container is to forward the traffic between containers within a pod. We'll learn
    more in [Chapter 5](part0126.html#3O56S0-6c8359cae3d4492eb9973d94ec3e4f1e), *Network
    and Security*.
  prefs: []
  type: TYPE_NORMAL
- en: If we specify labels in pod spec, we could use the `kubectl get pods -l <requirement>`
    command to get the pods that are satisfying the requirements. For example, `kubectl
    get pods -l 'tier in (frontend, backend)'`. Additionally, if we use `kubectl pods
    -owide`, it will list down which pod is running on which nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could use `kubectl describe <resource> <resource_name>` to get the detailed
    information of a resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we know which node this pod is running on, in minikube we only
    get a single node so it won''t make any difference. In the real cluster environment,
    knowing which node is useful for troubleshooting. We didn''t associate any labels,
    annotations, and controllers for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the containers section, we''ll see there are two containers included in
    this pod. Their states, images, and restart count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'A pod has a `PodStatus`, which including a map of array represents as `PodConditions`.
    The possible key for `PodConditions` are `PodScheduled`, `Ready`, `Initialized`,
    and `Unschedulable`. Value will be true, false, or unknown. If the pod is not
    created accordingly, `PodStatus` will give us a brief view of which part failed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Pod is associated with a service account that provides an identity for processes
    that are running a pod. It's controlled by service account and token controller
    in API Server.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will mount a read only volume to each container under `/var/run/secrets/kubernetes.io/serviceaccount`
    in a pod that contains a token for API access. Kubernetes creates a default service
    account. We could use the `kubectl get serviceaccounts` command to list them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We don't assign any selectors to this pod yet. QoS means Resource Quality of
    Service. Toleration is used to restrict how many pods that can use a node. We
    will learn more in [Chapter 8](part0188.html#5J99O0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Cluster Administration:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By seeing events, we could know what the steps are for Kubernetes to run a node.
    First, scheduler assigns the task to a node, here it is named minikube. Then kubelet
    on minikube starts pulling the first image and creates a container accordingly.
    Then kubelet pulls down the second container and runs.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSet (RS) and ReplicationController (RC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A pod is not self-healing. When a pod encounters failure, it won't recover on
    its own. **ReplicaSet** (**RS**) and **ReplicationController** (**RC**) therefore
    come into play. Both ReplicaSet and ReplicationController will ensure a specified
    number of replica pods are always up and running in the cluster. If a pod crashes
    for any reason, ReplicaSet and ReplicationController will request to spin up a
    new Pod.
  prefs: []
  type: TYPE_NORMAL
- en: After the latest Kubernetes, ReplicationController is replaced by ReplicaSet
    gradually. They share the same concept, just using different requirements for
    the pod selector. ReplicationController uses equality-based selector requirements
    while ReplicaSet uses set-based selector requirements. ReplicaSet usually is not
    created by users, but by Kubernetes Deployments objects, while ReplicationController
    is created by users ourselves. In this section, we'll explain the concept for
    RC first by walking through examples, which is much easier to understand. Then
    we'll bring in ReplicaSet at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00037.jpeg)ReplicationController with desired count 2'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we'd like to create a `ReplicationController` object, with desired
    count two. It means we will always have two pods in service. Before we write the
    spec for ReplicationController, we'll have to decide pod template first. Pod template
    is similar to the spec of pod. In ReplicationController, labels in the metadata
    section are required. ReplicationController uses pod selector to select which
    pods it manages. Labels allow ReplicationController to distinguish whether all
    the pods matching the selectors are all on track.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we''ll create two pods with the labels `project`, `service`,
    and `version`, as shown in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can use `kubectl` to get current RC status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It shows we have two desired pods, we currently have two pods and two pods are
    ready. How many pods do we have now?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It shows we have two pods up and running. As described previously, ReplicationController
    manages all the pods matching the selector. If we create a pod with the same label
    manually, in theory, it should match the pod selector of the RC we just created.
    Let''s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see if it''s up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s scheduled, and ReplicationController catches it. The amount of pods becomes
    three, which exceeds our desired count. The pod is eventually killed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00038.jpeg)ReplicationController makes sure pods are in desired
    state'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to scale on demand, we could simply use `kubectl edit <resource>
    <resource_name>` to update the spec. Here we''ll change replica count from `2`
    to `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check RC information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We have five pods now. Let''s check how RC works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: By describing the command; we can learn the spec of RC, also the events. At
    the time we created `nginx` RC, it launched two containers by spec. Then we created
    another pod manually by another spec, named `our-nginx`. RC detected that pod
    matches its pod selector. Then the amount exceeded our desired count, so it evicted
    it. Then we scaled out the replicas to five. RC detected that it didn't fulfill
    our desired state, launching three pods to fill the gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to delete an RC, simply use the `kubectl` command by `kubectl delete
    <resource> <resource_name>`. Since we have a configuration file on hand, we could
    also use `kubectl delete -f <configuration_file>` to delete the resources listing
    in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The same concept is brought to ReplicaSet. The following is RS version of `3-2-2.rc.yaml`.
    Two major differences are:'
  prefs: []
  type: TYPE_NORMAL
- en: The `apiVersion` is `extensions/v1beta1` at the time of writing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selector requirement is changed set-based requirement, with `matchLabels` and
    `matchExpressions` syntax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following the same steps with the preceding example should work exactly the
    same between RC and RS. This is just an example; however, we shouldn''t create
    RS on our own, while it should be always managed by Kubernetes `deployment` object.
    We''ll learn more in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deployment is the best primitive to manage and deploy our software in Kubernetes
    after version 1.2\. It supports gracefully deploying, rolling updating, and rolling
    back pods and ReplicaSets. We define our desired update of the software by deployment
    declaratively, and then deployment will do it for us progressively.
  prefs: []
  type: TYPE_NORMAL
- en: Before deployment, ReplicationController and kubectl rolling-update were the
    major way to implement rolling-update for the software, which is more imperative
    and slower. Deployment now becomes the major high-level object to manage our application.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a glimpse of how it works. In this section, we'll get a taste of
    how deployment is created, how to perform rolling-update and rollback. [Chapter
    7](part0163.html#4REBM0-6c8359cae3d4492eb9973d94ec3e4f1e), *Continuous Delivery*
    has more information with practical examples about how we integrate with deployments
    into our continuous delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we could use the `kubectl run` command to create a `deployment` for
    us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Before Kubernetes 1.2, the `kubectl run` command would create pods instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two pods that are deployed by deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00039.jpeg)The relationship in deployments, ReplicaSets, and
    pods'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we delete one of the pods, the replaced pod will be scheduled and launched
    immediately. That''s because deployments creates a ReplicaSet behind the scenes,
    which will ensure the number of replicas is matched with our desired count. In
    general, deployments manage ReplicaSets, ReplicaSets manage pods. Note that we
    shouldn''t manually manipulate ReplicaSets that deployments managed, just like
    there is no sense to change pods directly if they''re managed by ReplicaSets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also expose the port for deployment by the `kubectl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Deployments can be created by spec as well. The previous deployments and service
    launched by kubectl can be converted to the following spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'For performing rolling update, we''ll have to add rolling update strategy.
    There are three parameters used to control the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameters** | **Description** | **Default value** |'
  prefs: []
  type: TYPE_TB
- en: '| `minReadySeconds` | Warm-up time. How long a newly created pod is considered
    to be available. By default, Kubernetes assumes the application will be available
    once it is successfully launched. | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| `maxSurge` | How many pods can be surged when doing rolling update process.
    | 25% |'
  prefs: []
  type: TYPE_TB
- en: '| `maxUnavailable` | How many pods can be unavailable when doing rolling update
    process. | 25% |'
  prefs: []
  type: TYPE_TB
- en: 'The `minReadySecond` is an important setting. If our application is not available
    immediately when the pod is up, pods are rolling too fast without proper waiting.
    Although all the new pods are up, the application might be still warming up; there
    are chances a service outage might occur. In the following example, we''ll add
    the configuration into the `Deployment.spec` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: It indicates that we allow one of the pods to be unavailable at a time and one
    more pod could be launched when rolling the pods. The warm-up time before proceeding
    to the next operation will be three seconds. We can use either `kubectl edit deployments
    nginx` (edit directly) or `kubectl replace -f 3-2-3_deployments_rollingupdate.yaml`
    to update the strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we want to simulate new software rollout, from nginx 1.12.0 to 1.13.1\.
    We still could use the preceding two commands to change image version, or use
    `kubectl set image deployment nginx nginx=nginx``:1.13.1` to trigger the update.
    If we use `kubectl describe` to check what''s going on, we will see deployments
    have triggered rolling updates on ReplicaSets by deleting/creating pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00040.jpeg)Illustration of deployments'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows the illustration of the deployment. At a certain
    point of time, we have two (desired count) and one (`maxSurge`) pods. After launching
    each new pod, Kubernetes will wait three (`minReadySeconds`) seconds and then
    performs the next action.
  prefs: []
  type: TYPE_NORMAL
- en: If we use the command `kubectl set image deployment nginx nginx=nginx:1.12.0
    to previous version 1.12.0`, deployments will do the rollback for us.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Service in Kubernetes is an abstraction layer for routing traffic to a logical
    set of pods. With service, we don't need to trace the IP address of each pod.
    Service usually uses label selector to select the pods that it needs to route
    to (in some cases service is created without selector in purpose). The service
    abstraction is powerful. It enables the decoupling and makes communication between
    micro-services possible. Currently Kubernetes service supports TCP and UDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Service doesn''t care how we create the pod. Just like ReplicationController,
    it only cares that the pods match its label selectors, so the pods could belong
    to different ReplicationControllers. The following is an illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00041.jpeg)Service maps pods via label selector'
  prefs: []
  type: TYPE_NORMAL
- en: In the graph, all the pods match the service selector, so service will be responsible
    to distribute the traffic into all the pods without explicit assignment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Service types**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four types of services: ClusterIP, NodePort, LoadBalancer, and ExternalName.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00042.jpeg)LoadBalancer includes the features of NodePort and
    ClusterIP'
  prefs: []
  type: TYPE_NORMAL
- en: '**ClusterIP**'
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP is the default service type. It exposes the service on a cluster-internal
    IP. Pods in the cluster could reach the service via the IP address, environment
    variables, or DNS. In the following example, we'll learn how to use both native
    service environment variables and DNS to access the pods behind services in the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting a service, we''d like to create two sets of RC shown in the
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we could make our pod selector, targeting project and service labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Since `service` object might create a DNS label, service name must follow the
    combination of characters a-z, 0-9, or - (hyphen). A hyphen at the beginning or
    end of a label is not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we could use `kubectl describe service <service_name>` to check the service
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: One service could expose multiple ports. Just extend `.spec.ports` list in the
    service spec.
  prefs: []
  type: TYPE_NORMAL
- en: We can see it's a ClusterIP type service, assigned internal IP is 10.0.0.188\.
    Endpoints show we have four IPs behind the service. Pod IP could be found by the
    `kubectl describe pods <pod_name>` command. Kubernetes creates an `endpoints`
    object along with a `service` object for routing the traffic to matching pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the service is created with selectors, Kubernetes will create corresponding
    endpoints entries and keep updating, which will tell the destination that service
    routes into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: ClusterIP could be defined within your cluster, though most of the time we don't
    explicitly use IP address to access clusters. Using `.spec.clusterIP` could do
    the work.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Kubernetes will expose seven environment variables for each service.
    In most cases, the first two will be used for using `kube-dns` addon to do service
    discovery for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '`${SVCNAME}_SERVICE_HOST`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`${SVCNAME}_SERVICE_PORT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`${SVCNAME}_PORT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`${SVCNAME}_PORT_${PORT}_${PROTOCAL}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`${SVCNAME}_PORT_${PORT}_${PROTOCAL}_PROTO`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`${SVCNAME}_PORT_${PORT}_${PROTOCAL}_PORT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`${SVCNAME}_PORT_${PORT}_${PROTOCAL}_ADDR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we''ll use `${SVCNAME}_SERVICE_HOST` in another pod
    to check if we could access our nginx pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00043.jpeg)The illustration of accessing ClusterIP via environment
    variables and DNS names'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll then create a pod called `clusterip-chk` to access nginx containers
    via `nginx-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We could check the `stdout` of `cluserip-chk` pod via the `kubectl logs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This abstraction level decouples the communication between pods. Pods are mortal.
    With RC and service, we can build robust services without caring whether one pod
    might influence all micro-services.
  prefs: []
  type: TYPE_NORMAL
- en: With `kube-dns` addon enabled, the pods in the same cluster and same namespace
    with services could access services via services DNS records. Kube-dns creates
    DNS records for newly created services by watching the Kubernetes API. The DNS
    format for the cluster IP is `$servicename.$namespace`, and the port is `_$portname_$protocal.$servicename.$namespace`.
    The spec of the `clusterip_chk` pod will be similar with environment variables
    one. Just changing the URL to [`http://nginx-service.default:_http_tcp.nginx-service.default/`](http://nginx-service.default:_http_tcp.nginx-service.default/)
    in our previous example, and they should work exactly the same!
  prefs: []
  type: TYPE_NORMAL
- en: '**NodePort**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the service is set as NodePort, Kubernetes will allocate a port within a
    certain range on each node. Any traffic going to nodes on that port will be routed
    to the service port. Port number could be user-specified. If not specified, Kubernetes
    will randomly choose a port from range 30000 to 32767 without collision. On the
    other hand, if specified, the user should be responsible to manage the collision
    by themselves. NodePort includes the feature of ClusterIP. Kubernetes assigns
    an internal IP to the service. In the following example, we''ll see how we create
    a NodePort service and leverage it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Then you should be able to access the service via `http://${NODE_IP}:80`. Node
    could be any node. The `kube-proxy` watches any update of service and endpoints,
    and updates iptables rules accordingly (if using default iptables proxy-mode).
  prefs: []
  type: TYPE_NORMAL
- en: If you're using minikube, you could access the service via the `minikube service
    [-n NAMESPACE] [--url] NAME` command. In this example, it's `minikube service
    nginx-nodeport`.
  prefs: []
  type: TYPE_NORMAL
- en: '**LoadBalancer**'
  prefs: []
  type: TYPE_NORMAL
- en: This type is only usable with cloud provider support, such as Google Cloud Platform
    ([Chapter 10](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e), *Kubernetes
    on GCP*) and Amazon Web Service ([Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS*). By creating LoadBalancer service, Kubernetes will provision
    a load balancer by the Cloud provider to the service.
  prefs: []
  type: TYPE_NORMAL
- en: '**ExternalName (kube-dns version >= 1.7)**'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we leverage different services in the cloud. Kubernetes is flexible
    enough to be hybrid. ExternalName is one of the bridges to create a **CNAME**
    for external endpoints into the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Service without selectors**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Service uses selectors to match the pods to direct the traffic. However, sometimes
    you need to implement a proxy to be the bridge between Kubernetes cluster and
    another namespace, another cluster, or external resources. In the following example,
    we''ll demonstrate how to implement a proxy for [http://www.google.com](http://www.google.com)
    in your cluster. It''s just an example while the source of the proxy might be
    the endpoint of your databases or other resources in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00044.jpeg)Illustration of how service without selector works'
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration file is similar to the previous one, just without the selector
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: No Kubernetes endpoint will be created since there is no selector. Kubernetes
    doesn't know where to route the traffic since no selector could match the pods.
    We'll have to create that on our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Endpoints` object, the source addresses can''t be DNS name, so we''ll
    use `nslookup` to find the current Google IP from the domain, and add them into
    `Endpoints.subsets.addresses.ip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create another pod in the cluster to access our Google proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the `stdout` from the pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Hurray! We can now confirm the proxy works. The traffic to the service will
    be routed to the endpoints we specified. If it doesn't work, make sure you add
    the proper inbound rules to the network of your external resources.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints don't support DNS as source. Alternatively, we could use ExternalName,
    which doesn't have selectors either. It requires kube-dns version >= 1.7.
  prefs: []
  type: TYPE_NORMAL
- en: In some use cases, users need neither load balancing nor proxy functionalities
    for the service. In that case, we can set `CluterIP = "None"` as so-called headless
    services. For more information, please refer to [https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services).
  prefs: []
  type: TYPE_NORMAL
- en: Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A container is ephemeral, so is its disk. We either use the `docker commit [CONTAINER]`
    command or mount data volumes into a container ([Chapter 2](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *DevOps with Container*). In Kubernetes' world, volume management becomes critical,
    since pods might run on any node. Also, ensuring that containers in the same pod
    could share the same files becomes extremely hard. This is a large topic in Kubernetes.
    [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e), *Working with
    Storage and Resources* introduces volume management.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Secret, just like its name, is an object that stores the secrets in key-value
    format for providing sensitive information to pods, which could be a password,
    access key, or token. Secret is not landed to the disk; instead, it's stored in
    a per-node `tmpfs` filesystem. Kubelet on the mode will create a `tmpfs` filesystem
    to store secret. Secret is not designed to store large amounts of data due to
    storage management consideration. The current size limit of one secret is 1MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a secret based on a file, directory, or specified literal value
    by launching kubectl to create a secret command or by spec. There are three types
    of secret format: generic (or opaque, if encoded), docker registry, and TLS.'
  prefs: []
  type: TYPE_NORMAL
- en: Generic/opaque is the text that we'll use in our application. Docker registry
    is used to store the credential of a private docker registry. TLS secret is used
    to store the CA certificate bundle for cluster administration.
  prefs: []
  type: TYPE_NORMAL
- en: The docker-registry type of secret is also called **imagePullSecrets**, which
    is used to pass the password of a private docker registry via kubelet when pulling
    the image. This comes in handy so that we don't need to do `docker login` for
    each provisioned node. The command is `kubectl create secret docker-registry`
    `<registry_name>` `--docker-server``=<docker_server> --docker-username=<docker_username>`
    `-``-docker-password=<docker_password> --docker-email=<docker_email>`
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with a generic-type of example to show how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The options for creating secrets based on directory and literal value are pretty
    similar with the file ones. If specifying a directory after `--from-file`, the
    files in the directory will be iterated, the file name will be the secret key
    if its a legal secret name, and other non-regular files will be ignored subdirectories,
    symlinks, devices, pipes. On the other hand, `--from-literal=<key>=<value>` is
    the option if you want to specify plain text directly from the command, for example,
    `--from-literal=username=root`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we create a secret name `mypassword` from the file `mypassword.txt`.
    By default, the key of the secret is the file name, which is equivalent to the
    `--from-file=mypassword=./mypassword.txt` option. We could append multiple `--from-file`
    as well. Using the `kubectl get secret` `<secret_name>` `-o yaml` command could
    check out the detailed information of the secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the type of the secret becomes `Opaque` since the text has been
    encrypted by kubectl. It''s base64 encoded. We could use a simple bash command
    to decode it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: There are two ways for a pod to retrieve the secret. The first one is by file,
    and the second one is by environment variable. The first method is implemented
    by volume. The syntax is adding `containers.volumeMounts` in container specs,
    and adding a volumes section with secret configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieving secret via files**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to read secrets from files inside a pod first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The secret file will be mounted in `/<mount_point>/<secret_name>` without specifying
    `items``key` and `path`, or `/<mount_point>/<path>` in the pod. In this case,
    it''s under `/secret/password-example`. If we describe the pod, we can find there
    are two mount points in this pod. First is the read-only volume storing our secret,
    the second one stores the credentials to communicate with API servers, which is
    created and managed by Kubernetes. We''ll learn more in [Chapter 5](part0126.html#3O56S0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Network and Security*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We can delete a secret by using the `kubectl delete secret` `<secret_name>`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'After describing the pod, we can find a `FailedMount` event, since the volume
    no longer exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Same idea, if the pod is generated before a secret is created, the pod will
    encounter failure as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now learn how to create a secret by command line. Next we''ll briefly
    introduce its spec format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Since the spec is plain text, we need to encode the secret by our own `echo
    -n <password>` `| base64`. Please note that the type here becomes `Opaque`. Following
    along it should work the same as the one we create via command line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieving secret via environment variables**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we could use environment variables to retrieve secret, which
    is more flexible to use for short credentials, such as a password. This way, applications
    are able to use environment variables to retrieve database passwords without tackling
    files and volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: Secret should always be created before the pods that need it. Otherwise the
    pods won't get launched successfully.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The declaration is under `spec.containers[].env[]`. We'll need the secret name
    and the key name. Both are `mypassword` in this case. The example should work
    the same with the one retrieving via files.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ConfigMap is a mean that is able to leave your configuration outside of a Docker
    image. It injects the configuration data as key-values pairs into pods. Its properties
    are similar to secret, more specifically, secret is used to store sensitive data,
    such as password, and ConfigMap is used to store insensitive configuration data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Same as secret, ConfigMap could be based on a file, directory, or specified
    literal value. With similar syntax/command with secrets, ConfigMap uses `kubectl
    create configmap` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Since two `config` files are located in the same folder name `config`, we could
    pass a `config` folder instead of specifying the files one by one. The equivalent
    command to create is `kubectl create configmap example --from-file=config` in
    this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we describe the ConfigMap, it will show current information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We could use `kubectl edit configmap` `<configmap_name>` to update the configuration
    after creation.
  prefs: []
  type: TYPE_NORMAL
- en: We also could use `literal` as the input. The equivalent commands for the preceding
    example will be `kubectl create configmap example --from-literal=app.properties.name=name=DevOps-with-Kubernetes`
    which is not always so practical when we have many configurations in an app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to leverage it inside a pod. There are two ways to use ConfigMap
    inside a pod too: by volume or environment variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Using ConfigMap via volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to previous examples in the secret section, we mount a volume with
    syntax `configmap`, and add `volumeMounts` inside a container template. The command
    in `centos` will loop to `cat ${MOUNTPOINT}/$CONFIG_FILENAME`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We then could use this method to inject our non-sensitive configuration into
    the pod.
  prefs: []
  type: TYPE_NORMAL
- en: Using ConfigMap via environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For using ConfigMap inside a pod, you''ll have to use `configMapKeyRef` as
    the value source in the `env` section. It will populate whole ConfigMap pairs
    to environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The Kubernetes system itself also leverages ConfigMap for doing some authentication.
    For example, kube-dns uses it to put client CA files. You could check the system
    ConfigMap by adding `--namespace=kube-system` when describing ConfigMaps.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-containers orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll revisit our ticketing service: a kiosk web service
    as frontend, providing interface for get/put tickets. There is a Redis acting
    as cache, to manage how many tickets we have. Redis also acts as a publisher/subscriber
    channel. Once a ticket is sold, kiosk will publish an event into it. Subscriber
    is called recorder, which will write a timestamp and record it to the MySQL database.
    Please refer to the last section in [Chapter 2](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *DevOps with Container* for the detailed Dockerfile and Docker compose implementation.
    We''ll use `Deployment`, `Service`, `Secret`, `Volume`, and `ConfigMap` objects
    to implement this example in Kubernetes. Source code can be found at [https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter3/3-3_kiosk](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter3/3-3_kiosk).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00045.jpeg)An example of kiosk in Kubernetes world'
  prefs: []
  type: TYPE_NORMAL
- en: We'll need four kinds of pods. Deployment is the best choice to manage/deploy
    the pods. It will reduce the pain when we do the deployment in the future by its
    deployment strategy feature. Since kiosk, Redis, and MySQL will be accessed by
    other components, we'll associate services to their pods. MySQL acts as a datastore,
    for the simplicity, we'll mount a local volume to it. Please note that Kubernetes
    offers a bunch of choices. Please check out the details and examples in [Chapter
    4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e), *Working with Storage
    and Resources*. Sensitive information such as our MySQL root and user password,
    we'll want them to be stored in secrets. The other insensitive configuration,
    such as DB name or DB username, we'll leave to ConfigMap.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll launch MySQL first, as recorder depends on it. Before creating MySQL,
    we''ll have to create corresponding `secret` and `ConfigMap` first. To create
    `secret`, we need to generate base64 encrypted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we''re able to create the secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we come to our ConfigMap. Here, we put database user and database name
    as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it''s time to launch MySQL and its service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We can put more than one spec into a file by adding three dashes as separation.
    Here we mount `hostPath /mysql/data` into pods with the path `/var/lib/mysql`.
    In the environment section, we leverage the syntax of secret and ConfigMap by
    `secretKeyRef` and `configMapKeyRef`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating MySQL, Redis would be the next good candidate, since it is others''
    dependency, but it needs no prerequisite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it would be a good time to start kiosk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Here, we expose `lcredis-service.default` to environment variables to kiosk
    pods, which is the DNS name that kube-dns creates for `Service` object (referred
    to as service in this chapter). Thus, kiosk could access Redis host via environment
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we''ll create recorder. Recorder doesn''t expose any interface
    to others, so it doesn''t need a `Service` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Recorder needs to access both Redis and MySQL. It uses root credential that
    is injected via secret. Both endpoints for Redis and MySQL are accessed via service
    DNS name `<service_name>.<namespace>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then could check `deployment` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we have four `deployment` objects with different desired count
    for pods.
  prefs: []
  type: TYPE_NORMAL
- en: As we expose kiosk as NodePort, we should be able to access its service endpoint
    and see if it works properly. Assume we have a node, IP is `192.168.99.100`, and
    the NodePort that Kubernetes allocates is 30520.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re using minikube, `minikube service [-n NAMESPACE] [--url] NAME` could
    help you access service NodePort via your default browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '`// open kiosk console`'
  prefs: []
  type: TYPE_NORMAL
- en: '`# minikube service kiosk-service`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Opening kubernetes service default/kiosk-service in default browser...`'
  prefs: []
  type: TYPE_NORMAL
- en: Then we could know the IP and the port.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could then create and a get ticket by `POST` and `GET /tickets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the basic concept of Kubernetes. We learned Kubernetes
    master has kube-apiserver to handle the requests, and controller managers are
    the control center of Kubernetes, for example, it ensures our desired container
    amount is fulfilled, controls the endpoint to associate pods and services, and
    controls API access token. We also have Kubernetes nodes, which are the workers
    to host the containers, receive the information from master, and route the traffic
    based on the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: We then used minikube to demonstrate basic Kubernetes objects, including pod,
    ReplicaSets, ReplicationControllers, deployments, services, secrets, and ConfigMap.
    In the end, we demonstrated how to combine all the concepts we've learned into
    kiosk application deployment.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, the data inside containers will be gone when a container
    is gone. Therefore, volume is extremely important to persist the data in container
    world. In the next chapter, we'll be learning how volume works and its options,
    how to use persistent volume, and so on.
  prefs: []
  type: TYPE_NORMAL
